Title,URL/DOI,Abstract,screening_result,screening_result_reason
Supporting systematic literature reviews using deep-learning-based language models,https://doi.org/10.1145/3528588.3528658,"Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.",No,"이 논문은 체계적 문헌 고찰(Systematic Literature Review)을 반자동화하기 위한 딥러닝 기반 클러스터링 기법을 제안하고 실험을 통해 그 성능을 평가하고 있습니다. 그러나 논문의 내용은 방법론의 개발과 실험적 검증에 초점을 맞추고 있으며, 사용자가 직접 상호작용하여 이용할 수 있는 실제 툴이나 시스템의 개발에 대한 언급은 없습니다. 연구는 기법의 유효성을 입증하는 단계에 머물러 있습니다."
A Reproducibility and Generalizability Study of Large Language Models for Query Generation,https://doi.org/10.1145/3673791.3698432,"Systematic literature reviews (SLRs) are a cornerstone of academic research, yet they are often labour-intensive and time-consuming due to the detailed literature curation process. The advent of generative AI and large language models (LLMs) promises to revolutionize this process by assisting researchers in several tedious tasks, one of them being the generation of effective Boolean queries that will select the publications to consider including in a review. This paper presents an extensive study of Boolean query generation using LLMs for systematic reviews, reproducing and extending the work of Wang et al. and Alaniz et al. Our study investigates the replicability and reliability of results achieved using ChatGPT and compares its performance with open-source alternatives like Mistral and Zephyr to provide a more comprehensive analysis of LLMs for query generation. Therefore, we implemented a pipeline, which automatically creates a Boolean query for a given review topic by using a previously defined LLM, retrieves all documents for this query from the PubMed database and then evaluates the results. With this pipeline we first assess whether the results obtained using ChatGPT for query generation are reproducible and consistent. We then generalize our results by analyzing and evaluating open-source models and evaluating their efficacy in generating Boolean queries. Finally, we conduct a failure analysis to identify and discuss the limitations and shortcomings of using LLMs for Boolean query generation. This examination helps to understand the gaps and potential areas for improvement in the application of LLMs to information retrieval tasks. Our findings highlight the strengths, limitations, and potential of LLMs in the domain of information retrieval and literature review automation. Our code is available online.",No,"이 논문은 LLM을 사용한 Boolean 쿼리 생성의 재현성과 일반화 가능성을 연구하는 것이 목적입니다. 연구를 위한 파이프라인을 구현했다고 언급하고 있으나, 이는 실험 및 평가를 위한 것이며, 사용자가 직접 상호작용하며 이용할 수 있는 툴 개발이 주요 목적이 아닙니다. 코드가 온라인에 공개되어 있다고 하지만, 이는 연구 재현성을 위한 것으로 보입니다."
Comparing Generative AI Literature Reviews Versus Human-Led Systematic Literature Reviews: A Case Study on Big Data Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938577,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are transforming research methodologies, including Systematic Literature Reviews (SLRs). While traditional, human-led SLRs are labor-intensive, AI-driven approaches promise efficiency and scalability. However, the reliability and accuracy of AI-generated literature reviews remain uncertain. This study investigates the performance of GPT-4-powered Consensus in conducting an SLR on Big Data research, comparing its results with a manually conducted SLR. To evaluate Consensus, we analyzed its ability to detect relevant studies, extract key insights, and synthesize findings. Our human-led SLR identified 32 primary studies (PSs) and 207 related works, whereas Consensus detected 22 PSs, with 16 overlapping with the manual selection and 5 false positives. The AI-selected studies had an average citation count of 202 per study, significantly higher than the 64.4 citations per study in the manual SLR, indicating a possible bias toward highly cited papers. However, none of the 32 PSs selected manually were included in the AI-generated results, highlighting recall and selection accuracy limitations. Key findings reveal that Consensus accelerates literature retrieval but suffers from hallucinations, reference inaccuracies, and limited critical analysis. Specifically, it failed to capture nuanced research challenges and missed important application domains. Precision, recall, and F1 scores of the AI-selected studies were 76.2%, 38.1%, and 50.6%, respectively, demonstrating that while AI retrieves relevant papers with high precision, it lacks comprehensiveness. To mitigate these limitations, we propose a hybrid AI-human SLR framework, where AI enhances search efficiency while human reviewers ensure rigor and validity. While AI can support literature reviews, human oversight remains essential for ensuring accuracy and depth. Future research should assess AI-assisted SLRs across multiple disciplines to validate generalizability and explore domain-specific LLMs for improved performance.",No,"이 논문은 기존 상용 도구인 GPT-4 기반 Consensus의 성능을 평가하고 인간 주도 체계적 문헌 리뷰와 비교 분석하는 연구입니다. 논문에서는 하이브리드 AI-인간 SLR 프레임워크를 제안하고 있으나, 사용자가 직접 상호작용할 수 있는 새로운 툴을 개발하는 것이 아니라 기존 도구의 한계를 분석하고 개선 방향을 제시하는 데 초점을 두고 있습니다."
Work-in-Progress: Course-based Undergraduate Research Experience (CURE) with Generative AI in a Computer Science Course,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893139,"This work-in-progress innovative practice paper describes a novel integration of Generative AI with Course-based Undergraduate Research Experiences (CUREs). CUREs integrate research activities into the curriculum, allowing all students in a course to participate in inquiry-based research projects. Generative Artificial Intelligence (AI) applications are advanced AI designed to generate human-like responses by processing natural language inputs. These applications leverage machine learning models to produce outputs that can assist users in a variety of tasks from writing to coding. The integration of Generative AI with CURE had been adopted in a text-based machine learning course during the Fall 2023 semester. A comparative analysis had been conducted on student survey responses from Fall 2022 and Fall 2023 to evaluate the effectiveness of Generative AI in a CURE integrated course. Descriptive statistics and statistical tests were conducted to assess differences in student perceptions between the two semesters. Although the differences were not statistically significant, the results indicate a promising trend towards improved student perceptions of both the overall course effectiveness and the benefits of Generative AI in enhancing various aspects of the research process, especially the literature review.",No,"이 논문은 교육 과정에 Generative AI를 통합한 CURE(Course-based Undergraduate Research Experience) 프로그램의 효과성을 평가하는 연구입니다. 기존에 존재하는 Generative AI 도구들을 교육에 활용한 사례를 분석하고 있으며, 연구자들이 직접 새로운 툴을 개발하거나 사용자가 상호작용할 수 있는 툴을 만드는 내용은 포함되어 있지 않습니다."
Effectiveness of Generative Artificial Intelligence for Scientific Content Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313167,"Generative artificial intelligence (GenAI) in general, and large language models (LLMs) in particular, are highly fashionable. As they have the ability to generate coherent output based on prompts in natural language, they are promoted as tools to free knowledge workers from tedious tasks such as content writing, customer support and routine computer code generation. Unsurprisingly, their application is also attractive to professionals in the research domain, where mundane and laborious tasks, such as literature screening, are commonplace. We evaluate Vertex AI ‘text-bison’, a foundational LLM model, in a real-world academic scenario by replicating parts of a popular systematic review in the information management domain. By comparing the results of a zero-shot LLM-based approach with those of the original study, we gather evidence on the suitability of state-of-the-art general-purpose LLMs for the analysis of scientific content. We show that the LLM-based approach delivers good scientific content analysis performance for a general classification problem (ACC =0.9), acceptable performance for a domain-specific classification problem (ACC =0.8) and borderline performance for a text comprehension problem (ACC ≈0.69). We conclude that some content analysis tasks with moderate accuracy requirements may be supported by current LLMs. As the technology will evolve rapidly in the foreseeable future, studies on large corpora, where some inaccuracies are tolerable, or workflows that prepare large data sets for human processing, may increasingly benefit from the capabilities of GenAI.",No,"이 논문은 Vertex AI의 'text-bison' LLM 모델을 사용하여 과학 문헌 분석의 효과성을 평가하는 연구입니다. 기존에 존재하는 LLM 모델의 성능을 테스트하고 평가하는 것이 목적이며, 사용자가 직접 상호작용할 수 있는 새로운 툴이나 시스템을 개발하는 내용은 포함되어 있지 않습니다."
Machine learning based system for the automation of systematic literature reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385372,"The paper gives an overview of a machine learning-based system developed to support systematic literature reviews (SLR). The objective of the system is to provide scientists and anyone else who gives scientific advice supporting policy development with a tool for literature search and appraisal that reduces the human effort. The structure of the system is presented along with the description of the communication between modules and data storage methods. The Kafka technology is used for inter-module communication and the system consists of several independent modules which can be easily expanded with new modules without the need to introduce significant changes. We propose to semi–automate the SLR processes by applying an active learning approach which is based on machine learning classification models and on manual screening by experts of a subset of articles. Using classification algorithms requires a numerical representations of articles. This work investigates the utility of bag of concepts approach for text representations in order to create classification models used as components of automated systematic literature review systems. The presented study uses the bag of concepts approach in which a set of concepts identified by an annotator is extended by the concepts which lie, within a given distance, on paths from the originally identified concepts to the root of the ontology tree. Experiments are performed on datasets from systematic literature reviews in the medical domain. We summarize the performance of the proposed system by evaluating the WSS@95% metrics of active learning processes for several SLR case studies.",Yes,"이 논문은 체계적 문헌 고찰(SLR)을 지원하기 위한 머신러닝 기반 시스템을 개발하고 있습니다. 논문에서 ""provide scientists and anyone else who gives scientific advice supporting policy development with a tool""이라고 명시하여, 과학자들과 정책 개발자들이 문헌 검색 및 평가에 직접 사용할 수 있는 도구를 제공하는 것을 목표로 하고 있습니다. 능동 학습 접근법을 통해 전문가의 수동 스크리닝과 자동화를 결합한 실용적인 시스템을 구축하고 있습니다."
Zero-BertXGB: An Empirical Technique for Abstract Classification in Systematic Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845770,"Abstract classification in systematic reviews (SRs) is a crucial step in evidence synthesis but is often time-consuming and labour-intensive. This study evaluates the effectiveness of various Machine Learning (ML) models and embedding techniques in automating this process. Five diverse datasets are utilized: Aceves-Martins (2021), comprising 1,258 excluded and 230 included abstracts on the utilization of animal models in depressive behaviour studies; Bannach-Brown (2016), with 896 excluded and 73 included abstracts focusing on the methodological rigour of environmental health systematic reviews; Meijboom (2021), containing 599 excluded and 32 included abstracts on the retransitioning of Etanercept in rheumatic disease patients; Menon (2022), with 896 excluded and 73 included abstracts on environmental health reviews; and a custom Clinical Review Paper Abstract (CRPA) dataset, featuring 500 excluded and 50 included abstracts. A significant research gap in abstract classification has been identified in previous literature, particularly in comparing Large Language Models (LLMs) with traditional ML and Natural Language Processing (NLP) techniques regarding scalability, adaptability, computational efficiency, and real-time application. Addressing this gap, this study employs GloVe for word embedding via matrix factorization, FastText for character n-gram representation, and Doc2Vec for capturing paragraph-level semantics. A novel Zero-BertXGB technique is introduced, integrating a transformer-based language model, zero-shot learning, and an ML classifier to enhance abstract screening and classification into “Include” or “Exclude” categories. This approach leverages contextual understanding and precision for efficient abstract processing. The Zero-BertXGB technique is compared against other prominent LLMs, including BERT, PaLM, LLaMA, GPT-3.5, and GPT-4, to validate its effectiveness. The Zero-BertXGB model achieved accuracy values of 99.3% for Aceves-Martins2021, 92.6% for Bannach-Brown2016, 85.7% for Meijboom2021, 94.1% for Menon2022, and 98.8% for CRPA. The findings indicate that the Zero-BertXGB model, alongside other LLMs, can deliver reliable results with minimal human intervention, enhancing abstract screening efficiency and potentially revolutionizing systematic review workflows.",No,"이 논문은 체계적 문헌고찰에서 초록 분류를 자동화하기 위한 Zero-BertXGB 기법을 제안하고 다양한 데이터셋에서 그 효과를 평가하는 연구입니다. 논문은 기법의 개발과 성능 검증에 초점을 맞추고 있으며, 사용자가 직접 상호작용할 수 있는 툴이나 시스템의 개발에 대한 언급은 없습니다."
Optimizing Article Screening and Information Extraction: A Hybrid Approach with GeminiAI and Vector Database,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740208,"The exponential growth of scientific literature poses a significant challenge to researchers, resulting in redundancy in R&D due to inefficient review mechanisms. Manual literature reviews are time-consuming and resource-intensive, particularly when screening abstracts and titles, highlighting the need for innovative solutions to optimize the review process. This study introduces a three-step methodology using the GeminiAI model to streamline literature reviews: (1) Initial Screening, (2) Abstract Detail Extraction, and (3) Final Integration, with a Vector Database enabling efficient semantic searches in PDF files. In the first phase, GeminiAI achieved an accuracy of 88.66% in evaluating titles and abstracts based on specific inclusion and exclusion criteria, demonstrating its capability to filter relevant literature efficiently. The second phase enhanced this process by extracting key details, such as research-related modalities, thereby significantly reducing the pool of relevant papers. In the final step, the integration of the Vector Database with GeminiAI excelled, achieving an 80% similarity score in extracting detailed information, which greatly facilitated review writing and minimized manual effort. The user-friendly website designed for this purpose enables seamless paper uploads, with the Vector Database automatically extracting relevant details to streamline the workflow and accelerate innovation. This approach underscores the power of AI in optimizing literature reviews, reducing manual screening time and resources, and mitigating the risk of overlooking critical information. The entire system, including source code and supplementary materials, is available on our publicly accessible GitHub repository. https://github.com/mammona/ai-powered-litreview.git",Yes,"이 논문은 사용자가 논문을 업로드하고 자동으로 관련 정보를 추출할 수 있는 사용자 친화적인 웹사이트(user-friendly website)를 설계했다고 명시하고 있습니다. 또한 전체 시스템의 소스 코드와 보조 자료를 GitHub 저장소를 통해 공개적으로 제공하고 있어, 사용자가 직접 상호작용하며 이용할 수 있는 툴을 개발했음이 분명합니다."
A Trend of AI Conference Convergence in Similarity: An Empirical Study Through Trans-Temporal Heterogeneous Graph,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049682,"Publishing the research works on academic publications is an important part of the scientific process. Since the development of computer science research is very fast, researchers tend to publish the research works in a fast way, such as conferences whose review processes are faster than the journals. In the past decades, one conference usually focuses on a specific research field and the topic or method overlap between conferences is low. We have noticed that, in recent years, some topics or methods which were once studied in a small number of specific research fields have become popular in many other fields. Naturally, we come up with two research questions: (1) Do the conferences indeed become similar? and (2) How do conferences become similar? In this paper, we first use a trans-temporal heterogeneous graph network to model academic conferences in recent 20 years. Due to the large number of conferences, we categorize these conferences into 6 research fields for brevity. Then, we first quantitatively and qualitatively assess “Do the research fields become similar?” and then focus on exploring “How do research fields become similar?”. From the result, we find the reason for the research fields in computer science become similar is that AI becomes pervasive and researchers tend to apply the machine learning methods to different application fields. Since the methods become universal between different research fields, researchers should pay more attention to advanced information in other fields to motivate more interdisciplinary works. To assist the researchers to explore related interdisciplinary advanced information, it is crucial to measure the cross-field impact of papers using the citation information and recommend the paper which has a high cross-field impact on the related researchers. As for the newly published papers which do not have any citations, we also propose a cross-field impact prediction model to recommend the cutting-edge research works to related researchers accurately. Experiments conducted on real-world datasets verify the effectiveness of the proposed method.",No,"이 논문은 AI 학회들 간의 유사성 수렴 현상을 분석하고, 교차 분야 영향력 예측 모델을 제안하는 연구입니다. 논문 추천 시스템에 대한 방법론을 제시하고 있으나, 사용자가 직접 상호작용하며 이용할 수 있는 실제 툴이나 시스템의 개발보다는 현상 분석과 예측 모델의 효과성 검증에 초점을 맞추고 있습니다."
Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707620,"Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and to support the selection of relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine with approximately 74% recall and 15% precision. The use of such algorithms with conservative thresholds to minimize the risk of missing papers can already significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE.",Yes,"논문에서 체계적 문헌 고찰(SLR) 업데이트를 위한 자동화된 도구 프로토타입(automated tool prototype)을 개발했다고 명시하고 있습니다. 이 도구는 스노우볼링 검색 기법을 수행하고 머신러닝 알고리즘을 사용하여 관련 연구를 선택하는 기능을 제공하며, 실제 평가를 통해 그 효과성을 검증했습니다. 연구자들이 문헌 검색 및 선택 작업에 직접 활용할 수 있는 실용적인 툴을 개발한 것으로 판단됩니다."
Towards the Use of Language Models in Scientific Paper Recommender Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578590,"Within the educational and research community, Research Paper Recommender Systems debuted in the late 1990s and today, they constitute a specific research area. In this work, it is explored how the use of neural networks together with the incorporation of Natural Language Processing techniques, such as word embeddings and language models, affect the recom-mendation process of scientific papers. Three Deep Learning-based recommenders are explored: a neural collaborative filtering recommender, a recommender that uses word embeddings, and a recommender that incorporates language models. In addition, the results obtained are evaluated on two different datasets to see the effect of each of them on the recommendation process. While the first dataset only includes papers that have interested the user, the second one also includes papers that have not interested the user. The collaborative Deep Learning-based recommender constitutes the baseline against which to compare the rest of the developed recommenders. To evaluate the recommenders, each model is used to recommend 10 research papers for each user. The recommendations are evaluated and considered appropriate if they are related to the research field the user is interested in. The results confirm that the use of NLP techniques improves the performance of pure collaborative recommenders.",No,"이 논문은 과학 논문 추천 시스템에서 언어 모델과 자연어 처리 기법을 활용하는 방법을 연구하고 있습니다. 세 가지 딥러닝 기반 추천 모델을 개발하고 평가했지만, 사용자가 직접 상호작용하며 이용할 수 있는 실제 툴이나 시스템을 개발했다는 내용은 없습니다. 연구는 추천 알고리즘의 성능 비교와 평가에 초점을 맞추고 있습니다."
