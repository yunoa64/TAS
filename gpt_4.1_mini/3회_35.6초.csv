Title,URL/DOI,Abstract,screening_result,screening_result_reason
Supporting systematic literature reviews using deep-learning-based language models,https://doi.org/10.1145/3528588.3528658,"Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.",Yes,논문 초록에서 BERT와 S-BERT와 같은 딥러닝 기반 언어 모델을 사용하여 클러스터링 기법을 실험하고 평가한 내용을 명확히 언급하고 있습니다. 또한 두 개의 수동으로 수행된 SLR을 대상으로 실험을 수행하여 모델의 성능을 평가했다고 기술하고 있습니다.
A Reproducibility and Generalizability Study of Large Language Models for Query Generation,https://doi.org/10.1145/3673791.3698432,"Systematic literature reviews (SLRs) are a cornerstone of academic research, yet they are often labour-intensive and time-consuming due to the detailed literature curation process. The advent of generative AI and large language models (LLMs) promises to revolutionize this process by assisting researchers in several tedious tasks, one of them being the generation of effective Boolean queries that will select the publications to consider including in a review. This paper presents an extensive study of Boolean query generation using LLMs for systematic reviews, reproducing and extending the work of Wang et al. and Alaniz et al. Our study investigates the replicability and reliability of results achieved using ChatGPT and compares its performance with open-source alternatives like Mistral and Zephyr to provide a more comprehensive analysis of LLMs for query generation. Therefore, we implemented a pipeline, which automatically creates a Boolean query for a given review topic by using a previously defined LLM, retrieves all documents for this query from the PubMed database and then evaluates the results. With this pipeline we first assess whether the results obtained using ChatGPT for query generation are reproducible and consistent. We then generalize our results by analyzing and evaluating open-source models and evaluating their efficacy in generating Boolean queries. Finally, we conduct a failure analysis to identify and discuss the limitations and shortcomings of using LLMs for Boolean query generation. This examination helps to understand the gaps and potential areas for improvement in the application of LLMs to information retrieval tasks. Our findings highlight the strengths, limitations, and potential of LLMs in the domain of information retrieval and literature review automation. Our code is available online.",Yes,"논문 초록에서 ChatGPT, Mistral, Zephyr와 같은 대형 언어 모델(LLM)을 이용해 Boolean 쿼리를 생성하고, 이를 기반으로 PubMed 데이터베이스에서 문서를 검색한 후 결과를 평가하는 실험 및 평가 과정을 명확히 기술하고 있다. 따라서 인공지능 모델을 이용한 실험 및 평가가 이루어졌다고 판단된다."
Comparing Generative AI Literature Reviews Versus Human-Led Systematic Literature Reviews: A Case Study on Big Data Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938577,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are transforming research methodologies, including Systematic Literature Reviews (SLRs). While traditional, human-led SLRs are labor-intensive, AI-driven approaches promise efficiency and scalability. However, the reliability and accuracy of AI-generated literature reviews remain uncertain. This study investigates the performance of GPT-4-powered Consensus in conducting an SLR on Big Data research, comparing its results with a manually conducted SLR. To evaluate Consensus, we analyzed its ability to detect relevant studies, extract key insights, and synthesize findings. Our human-led SLR identified 32 primary studies (PSs) and 207 related works, whereas Consensus detected 22 PSs, with 16 overlapping with the manual selection and 5 false positives. The AI-selected studies had an average citation count of 202 per study, significantly higher than the 64.4 citations per study in the manual SLR, indicating a possible bias toward highly cited papers. However, none of the 32 PSs selected manually were included in the AI-generated results, highlighting recall and selection accuracy limitations. Key findings reveal that Consensus accelerates literature retrieval but suffers from hallucinations, reference inaccuracies, and limited critical analysis. Specifically, it failed to capture nuanced research challenges and missed important application domains. Precision, recall, and F1 scores of the AI-selected studies were 76.2%, 38.1%, and 50.6%, respectively, demonstrating that while AI retrieves relevant papers with high precision, it lacks comprehensiveness. To mitigate these limitations, we propose a hybrid AI-human SLR framework, where AI enhances search efficiency while human reviewers ensure rigor and validity. While AI can support literature reviews, human oversight remains essential for ensuring accuracy and depth. Future research should assess AI-assisted SLRs across multiple disciplines to validate generalizability and explore domain-specific LLMs for improved performance.",Yes,"논문 초록에서 GPT-4 기반의 AI 모델인 Consensus를 사용하여 체계적 문헌 검토(SLR)를 수행하고, AI가 선택한 연구들의 정밀도, 재현율, F1 점수 등 평가 지표를 제시하며 AI 모델의 성능을 실험 및 평가한 내용을 명확히 포함하고 있다."
Work-in-Progress: Course-based Undergraduate Research Experience (CURE) with Generative AI in a Computer Science Course,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893139,"This work-in-progress innovative practice paper describes a novel integration of Generative AI with Course-based Undergraduate Research Experiences (CUREs). CUREs integrate research activities into the curriculum, allowing all students in a course to participate in inquiry-based research projects. Generative Artificial Intelligence (AI) applications are advanced AI designed to generate human-like responses by processing natural language inputs. These applications leverage machine learning models to produce outputs that can assist users in a variety of tasks from writing to coding. The integration of Generative AI with CURE had been adopted in a text-based machine learning course during the Fall 2023 semester. A comparative analysis had been conducted on student survey responses from Fall 2022 and Fall 2023 to evaluate the effectiveness of Generative AI in a CURE integrated course. Descriptive statistics and statistical tests were conducted to assess differences in student perceptions between the two semesters. Although the differences were not statistically significant, the results indicate a promising trend towards improved student perceptions of both the overall course effectiveness and the benefits of Generative AI in enhancing various aspects of the research process, especially the literature review.",Yes,논문 초록에서 Generative AI를 활용한 CURE 통합 수업에서 학생 설문조사를 통한 비교 분석과 통계적 평가가 이루어졌음을 명시하고 있습니다. 이는 인공지능 모델을 이용한 실험 및 평가가 실제로 수행되었음을 의미합니다.
STAR-ML: A Rapid Screening Tool for Assessing Reporting of Machine Learning in Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918312,"Literature review provides researchers with an overview of the field and when presented as a systematic assessment, it summarizes state-of-the-art information and identifies knowledge gaps. While there are many tools for assessing quality and risk-of-bias within studies, there is currently no generalized tool for evaluating the transparency, reproducibility, and correctness of machine learning (ML) reporting in the literature. This study proposes a new tool (Screening Tool for Assessing Reporting of Machine Learning; STAR-ML) that can be used to screen articles for a systematic or scoping review focusing on the reporting of the ML algorithm. This paper describes the development of the tool to assess the quality of ML research reporting and how it can be applied to improve the literature review methodology. The tool was tested and updated using three independent raters on 15 studies. The inter-rater reliability and the time used to review an article were evaluated. The current version of STAR-ML has a very high inter-rater reliability of 0.923, and the average time to screen an article was 4.73 minutes. This new tool will allow for filtering ML-related papers that can be included in a systematic or scoping review by ensuring transparent, reproducible, and correct screening of research for inclusion in the review article.",No,"논문 초록에서는 인공지능 모델을 이용한 실험이나 평가를 수행했다는 내용이 없으며, 대신 머신러닝 연구 보고의 투명성 및 재현성을 평가하기 위한 도구(STAR-ML)를 개발하고 이를 문헌 리뷰에 적용하는 방법에 대해 설명하고 있습니다. 따라서 인공지능 모델 자체를 이용한 실험 및 평가는 이루어지지 않았습니다."
Check It Before You Wreck It: A Guide to STAR-ML for Screening Machine Learning Reporting in Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251957,"Machine learning (ML) is a technique that learns to detect patterns and trends in data. However, the quality of reporting ML in research is often suboptimal, leading to inaccurate conclusions and hindering progress in the field, especially if disseminated in literature reviews that provide researchers with an overview of a field, current knowledge gaps, and future directions. While various tools are available to assess the quality and risk-of-bias of studies, there is currently no generalized tool for assessing the reporting quality of ML in the literature. To address this, this study presents a new screening tool called STAR-ML (Screening Tool for Assessing Reporting of Machine Learning), accompanied by a guide to using it. A pilot scoping review looking at ML in chronic pain was used to investigate the tool. The time it took to screen papers and how the selection of the threshold affected the papers included were explored. The tool provides researchers with a reliable and systematic way to evaluate the quality of reporting of ML studies and to make informed decisions about the inclusion of studies in scoping or systematic reviews. In addition, this study provides recommendations for authors on how to choose the threshold for inclusion and use the tool proficiently. Lastly, the STAR-ML tool can serve as a checklist for researchers seeking to develop or implement ML techniques effectively.",No,"논문 초록에서는 인공지능 모델을 이용한 실험이나 평가를 직접 수행했다는 내용이 없으며, 대신 머신러닝 연구 보고의 질을 평가하기 위한 도구인 STAR-ML을 개발하고 이를 활용한 문헌 스크리닝 방법을 제시하는 데 초점이 맞춰져 있다. 따라서 인공지능 모델을 이용한 실험 및 평가를 실시했다고 보기 어렵다."
Effectiveness of Generative Artificial Intelligence for Scientific Content Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313167,"Generative artificial intelligence (GenAI) in general, and large language models (LLMs) in particular, are highly fashionable. As they have the ability to generate coherent output based on prompts in natural language, they are promoted as tools to free knowledge workers from tedious tasks such as content writing, customer support and routine computer code generation. Unsurprisingly, their application is also attractive to professionals in the research domain, where mundane and laborious tasks, such as literature screening, are commonplace. We evaluate Vertex AI ‘text-bison’, a foundational LLM model, in a real-world academic scenario by replicating parts of a popular systematic review in the information management domain. By comparing the results of a zero-shot LLM-based approach with those of the original study, we gather evidence on the suitability of state-of-the-art general-purpose LLMs for the analysis of scientific content. We show that the LLM-based approach delivers good scientific content analysis performance for a general classification problem (ACC =0.9), acceptable performance for a domain-specific classification problem (ACC =0.8) and borderline performance for a text comprehension problem (ACC ≈0.69). We conclude that some content analysis tasks with moderate accuracy requirements may be supported by current LLMs. As the technology will evolve rapidly in the foreseeable future, studies on large corpora, where some inaccuracies are tolerable, or workflows that prepare large data sets for human processing, may increasingly benefit from the capabilities of GenAI.",Yes,"논문 초록에서 Vertex AI ‘text-bison’이라는 대형 언어 모델(LLM)을 사용하여 실제 학술 시나리오에서 실험을 수행하고, 분류 문제에 대한 성능 평가(정확도 수치 제공)를 통해 인공지능 모델의 효과를 평가한 점이 명확히 나타나 있습니다. 따라서 인공지능 모델을 이용한 실험 및 평가가 이루어졌다고 판단됩니다."
Machine learning based system for the automation of systematic literature reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385372,"The paper gives an overview of a machine learning-based system developed to support systematic literature reviews (SLR). The objective of the system is to provide scientists and anyone else who gives scientific advice supporting policy development with a tool for literature search and appraisal that reduces the human effort. The structure of the system is presented along with the description of the communication between modules and data storage methods. The Kafka technology is used for inter-module communication and the system consists of several independent modules which can be easily expanded with new modules without the need to introduce significant changes. We propose to semi–automate the SLR processes by applying an active learning approach which is based on machine learning classification models and on manual screening by experts of a subset of articles. Using classification algorithms requires a numerical representations of articles. This work investigates the utility of bag of concepts approach for text representations in order to create classification models used as components of automated systematic literature review systems. The presented study uses the bag of concepts approach in which a set of concepts identified by an annotator is extended by the concepts which lie, within a given distance, on paths from the originally identified concepts to the root of the ontology tree. Experiments are performed on datasets from systematic literature reviews in the medical domain. We summarize the performance of the proposed system by evaluating the WSS@95% metrics of active learning processes for several SLR case studies.",Yes,"논문 초록에서 머신러닝 분류 모델을 사용한 능동 학습(active learning) 접근법을 적용하여 반자동화된 체계적 문헌 검토 시스템을 개발하고, 의료 분야 데이터셋을 대상으로 실험을 수행하여 성능을 평가했다고 명시하고 있습니다. 따라서 인공지능 모델을 이용한 실험 및 평가가 이루어졌다고 판단됩니다."
Zero-BertXGB: An Empirical Technique for Abstract Classification in Systematic Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845770,"Abstract classification in systematic reviews (SRs) is a crucial step in evidence synthesis but is often time-consuming and labour-intensive. This study evaluates the effectiveness of various Machine Learning (ML) models and embedding techniques in automating this process. Five diverse datasets are utilized: Aceves-Martins (2021), comprising 1,258 excluded and 230 included abstracts on the utilization of animal models in depressive behaviour studies; Bannach-Brown (2016), with 896 excluded and 73 included abstracts focusing on the methodological rigour of environmental health systematic reviews; Meijboom (2021), containing 599 excluded and 32 included abstracts on the retransitioning of Etanercept in rheumatic disease patients; Menon (2022), with 896 excluded and 73 included abstracts on environmental health reviews; and a custom Clinical Review Paper Abstract (CRPA) dataset, featuring 500 excluded and 50 included abstracts. A significant research gap in abstract classification has been identified in previous literature, particularly in comparing Large Language Models (LLMs) with traditional ML and Natural Language Processing (NLP) techniques regarding scalability, adaptability, computational efficiency, and real-time application. Addressing this gap, this study employs GloVe for word embedding via matrix factorization, FastText for character n-gram representation, and Doc2Vec for capturing paragraph-level semantics. A novel Zero-BertXGB technique is introduced, integrating a transformer-based language model, zero-shot learning, and an ML classifier to enhance abstract screening and classification into “Include” or “Exclude” categories. This approach leverages contextual understanding and precision for efficient abstract processing. The Zero-BertXGB technique is compared against other prominent LLMs, including BERT, PaLM, LLaMA, GPT-3.5, and GPT-4, to validate its effectiveness. The Zero-BertXGB model achieved accuracy values of 99.3% for Aceves-Martins2021, 92.6% for Bannach-Brown2016, 85.7% for Meijboom2021, 94.1% for Menon2022, and 98.8% for CRPA. The findings indicate that the Zero-BertXGB model, alongside other LLMs, can deliver reliable results with minimal human intervention, enhancing abstract screening efficiency and potentially revolutionizing systematic review workflows.",Yes,논문 초록에서 Zero-BertXGB라는 인공지능 모델과 여러 대형 언어 모델(LLMs)을 이용한 실험 및 평가를 수행했다고 명시하고 있습니다. 또한 다양한 데이터셋에 대해 모델의 정확도를 측정하여 성능을 검증한 점에서 인공지능 모델을 활용한 평가가 이루어졌음을 알 수 있습니다.
Optimizing Article Screening and Information Extraction: A Hybrid Approach with GeminiAI and Vector Database,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740208,"The exponential growth of scientific literature poses a significant challenge to researchers, resulting in redundancy in R&D due to inefficient review mechanisms. Manual literature reviews are time-consuming and resource-intensive, particularly when screening abstracts and titles, highlighting the need for innovative solutions to optimize the review process. This study introduces a three-step methodology using the GeminiAI model to streamline literature reviews: (1) Initial Screening, (2) Abstract Detail Extraction, and (3) Final Integration, with a Vector Database enabling efficient semantic searches in PDF files. In the first phase, GeminiAI achieved an accuracy of 88.66% in evaluating titles and abstracts based on specific inclusion and exclusion criteria, demonstrating its capability to filter relevant literature efficiently. The second phase enhanced this process by extracting key details, such as research-related modalities, thereby significantly reducing the pool of relevant papers. In the final step, the integration of the Vector Database with GeminiAI excelled, achieving an 80% similarity score in extracting detailed information, which greatly facilitated review writing and minimized manual effort. The user-friendly website designed for this purpose enables seamless paper uploads, with the Vector Database automatically extracting relevant details to streamline the workflow and accelerate innovation. This approach underscores the power of AI in optimizing literature reviews, reducing manual screening time and resources, and mitigating the risk of overlooking critical information. The entire system, including source code and supplementary materials, is available on our publicly accessible GitHub repository. https://github.com/mammona/ai-powered-litreview.git",Yes,"논문 초록에서 GeminiAI라는 인공지능 모델을 사용하여 문헌 스크리닝과 정보 추출 실험을 수행하고, 정확도 및 유사도 평가 결과를 제시하고 있다. 따라서 인공지능 모델을 이용한 실험 및 평가가 명확히 이루어졌음을 알 수 있다."
A Trend of AI Conference Convergence in Similarity: An Empirical Study Through Trans-Temporal Heterogeneous Graph,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049682,"Publishing the research works on academic publications is an important part of the scientific process. Since the development of computer science research is very fast, researchers tend to publish the research works in a fast way, such as conferences whose review processes are faster than the journals. In the past decades, one conference usually focuses on a specific research field and the topic or method overlap between conferences is low. We have noticed that, in recent years, some topics or methods which were once studied in a small number of specific research fields have become popular in many other fields. Naturally, we come up with two research questions: (1) Do the conferences indeed become similar? and (2) How do conferences become similar? In this paper, we first use a trans-temporal heterogeneous graph network to model academic conferences in recent 20 years. Due to the large number of conferences, we categorize these conferences into 6 research fields for brevity. Then, we first quantitatively and qualitatively assess “Do the research fields become similar?” and then focus on exploring “How do research fields become similar?”. From the result, we find the reason for the research fields in computer science become similar is that AI becomes pervasive and researchers tend to apply the machine learning methods to different application fields. Since the methods become universal between different research fields, researchers should pay more attention to advanced information in other fields to motivate more interdisciplinary works. To assist the researchers to explore related interdisciplinary advanced information, it is crucial to measure the cross-field impact of papers using the citation information and recommend the paper which has a high cross-field impact on the related researchers. As for the newly published papers which do not have any citations, we also propose a cross-field impact prediction model to recommend the cutting-edge research works to related researchers accurately. Experiments conducted on real-world datasets verify the effectiveness of the proposed method.",Yes,"논문 초록에서 인공지능 모델인 ""trans-temporal heterogeneous graph network""을 사용하여 학술대회 간 유사성을 모델링하고, 교차 분야 영향력 예측 모델을 제안하여 실험을 수행했다고 명시하고 있습니다. 또한, 실제 데이터셋을 이용한 실험을 통해 제안 방법의 효과를 검증했다고 하여 인공지능 모델을 이용한 평가가 이루어졌음을 알 수 있습니다."
Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707620,"Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and to support the selection of relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine with approximately 74% recall and 15% precision. The use of such algorithms with conservative thresholds to minimize the risk of missing papers can already significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE.",Yes,"논문 초록에서 머신러닝(ML) 알고리즘을 사용하여 자동화된 도구를 개발하고, Linear Support Vector Machine을 이용한 실험 및 평가를 수행했다고 명시하고 있습니다. 따라서 인공지능 모델을 이용한 실험 및 평가가 이루어졌음을 알 수 있습니다."
Towards the Use of Language Models in Scientific Paper Recommender Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578590,"Within the educational and research community, Research Paper Recommender Systems debuted in the late 1990s and today, they constitute a specific research area. In this work, it is explored how the use of neural networks together with the incorporation of Natural Language Processing techniques, such as word embeddings and language models, affect the recom-mendation process of scientific papers. Three Deep Learning-based recommenders are explored: a neural collaborative filtering recommender, a recommender that uses word embeddings, and a recommender that incorporates language models. In addition, the results obtained are evaluated on two different datasets to see the effect of each of them on the recommendation process. While the first dataset only includes papers that have interested the user, the second one also includes papers that have not interested the user. The collaborative Deep Learning-based recommender constitutes the baseline against which to compare the rest of the developed recommenders. To evaluate the recommenders, each model is used to recommend 10 research papers for each user. The recommendations are evaluated and considered appropriate if they are related to the research field the user is interested in. The results confirm that the use of NLP techniques improves the performance of pure collaborative recommenders.",Yes,"논문 초록에서 세 가지 딥러닝 기반 추천 시스템(신경망 협업 필터링, 단어 임베딩, 언어 모델)을 사용하여 실험을 수행하고, 두 개의 데이터셋에서 평가한 결과를 제시하고 있다. 이는 인공지능 모델을 이용한 실험 및 평가가 이루어졌음을 명확히 보여준다."
