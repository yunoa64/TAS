Title,URL/DOI,Abstract,screening_result,screening_result_reason
Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey,https://dl.acm.org/doi/10.1145/3712000,"Differential privacy has become a widely popular method for data protection in machine learning, especially since it allows formulating strict mathematical privacy guarantees. This survey provides an overview of the state of the art of differentially private centralized deep learning, thorough analyses of recent advances and open problems, as well as a discussion of potential future developments in the field. Based on a systematic literature review, the following topics are addressed: emerging application domains, differentially private generative models, auditing and evaluation methods for private models, protection against a broad range of threats and attacks, and improvements of privacy-utility tradeoffs.",No,논문 초록에서 해당 논문은 체계적인 문헌 조사를 기반으로 한 서베이 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들을 종합하고 분석하는 내용에 초점이 맞춰져 있습니다.
Opportunities and Challenges of Using ChatGPT as a Teaching Assistant in English Language Teaching: A Systematic Literature Review,https://doi.org/10.1145/3700297.3700362,"With the development of artificial intelligence (AI) technology, particularly the widespread application of ChatGPT in the field of natural language processing, the education sector has gradually recognized its potential in enhancing the effectiveness of English language teaching (ELT). Although there are numerous relevant studies, there has yet to be a comprehensive analysis of this topic through a systematic literature review (SLR). Therefore, this study adopts the method of systematic literature review, selecting relevant literature from the Scopus and Web of Science (WoS) databases, and rigorously evaluating and screening studies that meet the criteria, to systematically analyze the current state of ChatGPT's application in ELT. Through analyzing the publication trends, geographical distribution, opportunities, and challenges, as well as the most frequent keywords, this study provides valuable insights for scholars, educators, and policymakers, particularly regarding how to more effectively utilize ChatGPT to enhance the quality of ELT. The study also proposes future research directions, including: (1) increasing attention to the potential opportunities of ChatGPT in ELT, (2) deepening the analysis of the challenges encountered in its application, and (3) offering specific teaching suggestions for the integration of ChatGPT into teaching practices. Although research in this field is still in its initial development stages, with ongoing technological advancements and changing educational needs, this field holds significant potential for future research and application. ChatGPT is poised to bring groundbreaking progress and innovation, offering new pathways and tools for the reform and optimization of ELT.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 분석하고 종합하는 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않는다. 따라서 새로운 연구 내용을 포함한 연구 논문으로 보기 어렵다."
"Cold Start Latency in Serverless Computing: A Systematic Review, Taxonomy, and Future Directions",https://doi.org/10.1145/3700875,"Recently, academics and the corporate sector have paid attention to serverless computing, which enables dynamic scalability and an economic model. In serverless computing, users only pay for the time they actually use resources, enabling zero scaling to optimise cost and resource utilisation. However, this approach also introduces the serverless cold start problem. Researchers have developed various solutions to address the cold start problem, yet it remains an unresolved research area. In this article, we propose a systematic literature review on cold start latency in serverless computing. Furthermore, we create a detailed taxonomy of approaches to cold start latency, which we use to investigate existing techniques for reducing the cold start time and frequency. We have classified the current studies on cold start latency into several categories such as caching and application-level optimisation-based solutions, as well as Artificial Intelligence/Machine Learning-based solutions. Moreover, we have analyzed the impact of cold start latency on quality of service, explored current cold start latency mitigation methods, datasets, and implementation platforms, and classified them into categories based on their common characteristics and features. Finally, we outline the open challenges and highlight the possible future directions.",No,"본 논문은 서버리스 컴퓨팅의 콜드 스타트 지연 문제에 대한 체계적인 문헌 리뷰와 분류 체계를 제시하는 리뷰 논문이다. 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구들을 분석하고 정리하는 데 중점을 두고 있어, 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Deep Learning for Variable Renewable Energy: A Systematic Review,https://doi.org/10.1145/3586006,"In recent years, both fields, AI and VRE, have received increasing attention in scientific research. Thus, this article’s purpose is to investigate the potential of DL-based applications on VRE and as such provide an introduction to and structured overview of the field. First, we conduct a systematic literature review of the application of Artificial Intelligence (AI), especially Deep Learning (DL), on the integration of Variable Renewable Energy (VRE). Subsequently, we provide a comprehensive overview of specific DL-based solution approaches and evaluate their applicability, including a survey of the most applied and best suited DL architectures. We identify ten DL-based approaches to support the integration of VRE in modern power systems. We find (I) solar PV and wind power generation forecasting, (II) system scheduling and grid management, and (III) intelligent condition monitoring as three high potential application areas.",No,"본 논문은 딥러닝과 가변 재생에너지 분야의 기존 연구들을 체계적으로 검토하고 정리한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 평가에 중점을 두고 있습니다."
An In-Depth Analysis of Artificial Intelligence on Service Capabilities of Humanoid Robots,https://doi.org/10.1145/3700706.3700730,"Artificial intelligence (AI) emulates human intelligence in digital devices and machines. A humanoid robot is designed to imitate the look and form of a human body and is primarily used for humanitarian assistance and interaction. AI is applied to humanoid robots to allow intelligent behavior and decision-making, resulting in the ability to execute service activities and achieve goals intelligently in the human-robot interaction environment. The paper is a systematic literature review of studies published in the last four years that presents and assesses how AI techniques and methods have been applied to humanoid robots to implement and enhance service capabilities. The research papers used for the systematic literature review were carefully selected under the directives of the PRISMA method. Eighty studies were screened, and eighteen were used for the review. The results show that the primary AI technique applied in humanoid robots is the Convolutional Neural Network (CNN). The results show that the effective AI techniques used in humanoid robots based on a specific area are Hybrid Convolutional Neural Network (CNN) – Long-Short Term Memory (LSTM), Artificial Neural Network (ANN), Convolutional Neural Network (CNN), Deep Convolutional Neural Network DCNN), Recurrent Neural Network (RNN), and Grow-when-required Network (GWR). The study provides AI techniques and methods utilized in modern studies that can be used as a reference for future studies, research, and innovation.",No,"본 논문은 최근 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석한 내용입니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 평가에 초점이 맞춰져 있습니다."
Systematic Review of Virtual Reality Solutions Employing Artificial Intelligence Methods,https://doi.org/10.1145/3488162.3488209,"This paper first presents a systematic literature review of artificial intelligence (AI) methods used in virtual reality (VR) solutions. Based on the systematic literature review, a methodology for locating existing studies, selecting and evaluating contributions, performing analyses, and synthesizing data was proposed. We used search engines, such as Google Scholar and databases such as Elsevier's Scopus, ACM Digital Library, and IEEE Xplore Digital Library. A set of inclusion and exclusion criteria was used to select documents. The results showed that the AI scientific technique most applied in VR applications is machine learning. The findings revealed several fields adopting real-world applications that employ AI in VR: human–robot interaction, emotion interaction and behavior recognition, education, agriculture, transport, manufacturing, and health.",No,"본 논문은 인공지능 기법이 가상현실에 적용된 기존 연구들을 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 중점을 둔 논문입니다."
The Ethics of AI Assisted Learning: A Systematic Literature Review on the Impacts of ChatGPT Usage in Education,https://doi.org/10.1145/3606094.3606101,"This systematic literature review explores how gamification in legal education might be In recent years, ChatGPT has become a noteworthy subject in the educational field due to the popularity it gained amongst students across different levels of education all over the world, who use this technology to assess their academic homework, transforming ChatGPT in some sort of auxiliary tool that aids them with the completion of certain tasks that would take more time to complete, such as research and data comparison, to name a few examples; but this form of AI assisted learning, as it were, has also become a problematic subject. This artificial intelligence chatbot is, undeniably, a remarkable advancement in AI regarding the improvements it presents compared to other similar technologies, and it clearly paves the way for future applications not only in education, but also at a social level, in a world more driven towards the development and optimization of digital tools with the help of machine learning. Nevertheless, this sort of technology should be question ed when its application permeates deeply in the performance and development of students and their learning process, especially when taking in consideration the level of accessibility that ChatGPT has worldwide. Students should have an ethical standpoint on whether they use ChatGPT to complement their learning process and how much input is this technology having in their academic work, so they learn to use it more effectively and avoid the abuse of ChatGPT usage, in order to seize the benefits that this AI may have on education. This study's objective is to analyze the current literature around the use of ChatGPT in education, for which we conducted a Systematic Literature Review (SLR) across multiple journal databases such as Scopus, ScienceDirect, ProQuest, IEEE Xplore and ACM Digital Library.",No,"이 논문은 ChatGPT 사용에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 초점이 맞춰져 있습니다."
Advancements and Challenges of Generative AI in Higher Educational Content Creation A Technical Perspective,https://doi.org/10.1145/3641032.3641055,"Generative Artificial Intelligence (AI) has witnessed remarkable advancements, igniting interest in various domains, including Higher Education. This research paper explores the impacts and challenges of integrating Generative AI in content creation within Higher Education. We utilise a literature review and case study approach to gain insights into the potential benefits and complexities of implementing Generative AI in educational settings. Specific research questions are formulated to investigate the influence of Generative AI on content creation efficiency, productivity, quality, and adaptability. The paper also highlights ethical considerations and the evolving role of educators in the AI-driven educational landscape. Furthermore, the research paper examines the practical applications of Generative AI tools such as OpenAI GPT, GPT-Neo, Hugging Face's Transformers Library, Cognii, MosaChat-AI, TeacherMatic, and OpenAI Codex in Higher Education content creation. This comprehensive analysis aims to provide educators, instructional designers, and policymakers with valuable insights and concrete examples of how Generative AI can be leveraged to create personalised learning materials, improve assessment strategies, and enhance the overall educational experience for students pursuing advanced technical subjects. The culmination of this research presents a vision for a future where Generative AI, thoughtfully implemented and ethically managed, empowers educational institutions to meet the diverse and evolving needs of learners in the digital era.",No,"본 논문은 문헌 리뷰와 사례 연구를 통해 생성형 AI의 고등교육 콘텐츠 제작에 미치는 영향과 도전 과제를 탐구하는 종합적 분석에 초점을 맞추고 있으며, 직접적인 실험이나 새로운 연구 결과를 제시하지 않는다. 따라서 독창적인 연구 내용이나 직접적인 기여가 포함된 연구 논문으로 보기 어렵다."
Exploring the Impact of Unsupervised Clustering Methods in Systematic Literature Reviews,https://doi.org/10.1145/3641555.3705184,"About 15 years ago, education researchers conducted a systematic literature review (SLR) on change strategies for improving undergraduate STEM education instruction. Analyzing 191 articles from 1995 to 2008, the researchers identified four broad categories of change strategies through a comprehensive interdisciplinary literature review: (1) disseminating curriculum and pedagogy, (2) developing reflective teachers, (3) enacting policy, and (4) developing a shared vision. With recent developments in STEM education practices, it is imperative to conduct a follow-up SLR comparing the effects of change strategies and associated student success. Similarly, with the influx of scientific articles published in recent decades, it is time consuming to conduct comprehensive SLRs without the assistance of machine learning (ML) analysis techniques. Working with qualitative researchers, we investigate the impact and ability of using ML to assist in these analyses. While most ML approaches can be easily written with a few lines of code, using them to extract meaningful information for literature reviews is challenging. In this work, we describe our experience with integrating machine learning techniques into the analysis pipeline of SLRs. Specifically, this poster will: (1) share results from clustering analysis to identify themes of the chosen abstracts, (2) explore the effects of data bias on found clusters, (3) present the challenges of adding machine learning into SLRs, and (4) assess if the addition of ML in SLRs aligns with the expected goals of qualitative researchers.",No,"본 논문은 기존 문헌을 체계적으로 검토하는 과정에서 머신러닝 기법을 활용한 경험과 방법론을 공유하는 내용으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 방법론적 논의 및 사례 보고에 해당합니다."
The Application Landscape and Research Status of Artificial Intelligence in Teacher Education: A Systematic Literature Review,https://doi.org/10.1145/3691720.3691766,"The integration of Artificial Intelligence (AI) has become a key driver in the evolution and transformation of teacher education. However, there has been no comprehensive review of AI applications in this field. In light of this, our study employs a systematic literature review, selecting 45 relevant articles from databases such as Web of Science, EBSCO, and Scopus, aiming to present a comprehensive overview of the field's development, key research themes, and propose a valuable future agenda. Findings are as follows: 1) In the realm of AI applications in teacher education, the United States emerges as a core contributor. Regarding the types of research, theoretical and empirical studies are equally prevalent; there has been a surge in publications in this field since 2021. 2) The research in this field primarily revolves around three themes: Performance Evaluation of AI Technology in Teacher Education, AI Technology's Role in Enhancing Teachers' Core Skills, and AI Assistance in Teacher Education. 3) Based on the literature review, this study offers a meaningful future agenda from both theoretical and practical perspectives. Theoretically, it involves strengthening research in teacher training and professional development, and enhancing educational decision support systems. Practically, it entails developing specialized AI applications for teacher education and promoting AI-assisted educational development.",No,"본 논문은 체계적 문헌고찰(Systematic Literature Review)을 수행하여 기존 연구들을 종합하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 향후 연구 방향을 제안하는 데 중점을 두고 있다."
A Systematic Literature Mapping of Early Generative AI Research is CS Education,https://doi.org/10.1145/3641555.3705121,"The widespread release of generative AI tools has led to a rapid rise in publications evaluating their impact on CS education. While there is no doubt that the area is new and rapidly evolving, it is important to begin to catalogue and map the literature at this early stage. In this work, we systematically search and map 82 papers evaluating the impact of generative AI tools on CS education. We then build a literature map of these papers using the axes of population, use of generative AI, and method of evaluation. This work will serve as both a snapshot of the first generation of generative AI papers in the field, and a road-map for further classification and literature review as the field develops.",No,"본 논문은 기존 연구들을 체계적으로 수집하고 분류하는 문헌 맵핑 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 연구 논문이라기보다는 문헌 리뷰 및 분류에 해당합니다."
"Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance",https://dl.acm.org/doi/10.5555/3716662.3716785,"Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.",No,"본 논문은 AI 관련 설문조사의 현황과 문제점을 비판적으로 검토하고, 설문조사 방법론에 대한 성찰과 권고를 제시하는 리뷰 및 분석 연구에 해당합니다. 따라서 직접적인 독창적 실험이나 새로운 연구 결과를 제시하는 연구 논문으로 보기 어렵습니다."
The Social Impact of Generative AI: An Analysis on ChatGPT,https://doi.org/10.1145/3582515.3609555,"In recent months, the impact of Artificial Intelligence (AI) on citizens’ lives has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a citizen-centric AI.",No,"논문 초록은 기존 문헌을 종합하여 ChatGPT와 생성형 AI의 사회적 영향을 분석하는 문헌 리뷰 성격을 띠고 있으며, 독창적인 실험이나 새로운 연구 결과를 제시하지 않는다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Deep Learning for Android Malware Defenses: A Systematic Literature Review,https://doi.org/10.1145/3544968,"Malicious applications (particularly those targeting the Android platform) pose a serious threat to developers and end-users. Numerous research efforts have been devoted to developing effective approaches to defend against Android malware. However, given the explosive growth of Android malware and the continuous advancement of malicious evasion technologies like obfuscation and reflection, Android malware defense approaches based on manual rules or traditional machine learning may not be effective. In recent years, a dominant research field called deep learning (DL), which provides a powerful feature abstraction ability, has demonstrated a compelling and promising performance in a variety of areas, like natural language processing and computer vision. To this end, employing DL techniques to thwart Android malware attacks has recently garnered considerable research attention. Yet, no systematic literature review focusing on DL approaches for Android malware defenses exists. In this article, we conducted a systematic literature review to search and analyze how DL approaches have been applied in the context of malware defenses in the Android environment. As a result, a total of 132 studies covering the period 2014–2021 were identified. Our investigation reveals that, while the majority of these sources mainly consider DL-based Android malware detection, 53 primary studies (40.1%) design defense approaches based on other scenarios. This review also discusses research trends, research focuses, challenges, and future research directions in DL-based Android malware defenses.",No,이 논문은 딥러닝을 활용한 안드로이드 악성코드 방어에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들을 분석하고 요약하는 데 중점을 두고 있다.
"Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools",https://doi.org/10.1145/3689187.3709614,"Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.",No,"초록에서 본 논문은 Generative AI 관련 연구 동향, 교육 실천, 도구에 대한 종합적인 리뷰와 설문, 인터뷰를 통한 현황 파악에 초점을 맞추고 있습니다. 이는 기존 연구들을 요약하고 분석하는 문헌 리뷰로, 독창적인 실험이나 새로운 연구 결과를 직접 제시하는 연구 논문은 아닙니다."
Metrics of Success: Evaluating User Satisfaction in AI Chatbots,https://doi.org/10.1145/3704137.3704182,"The rapid advancement of Artificial Intelligence (AI), particularly through Large Language Models (LLMs), has catalysed a technological revolution, leading to the widespread adoption of AI-driven chatbots across industries. OpenAI’s customisable generative pre-trained transformer (GPT) offerings have popularised generative AI, enabling organisations of all sizes to implement chatbots for customer support. This development presents an opportunity for businesses to offer 24/7, cost-efficient customer service that can overcome the historical limitations of chatbots that lack a ""human element."" However, despite the proliferation of AI chatbots, there remains a crucial need to evaluate their effectiveness in meeting user needs and preferences for human-like interaction. Current service quality assessment tools, such as SERVQUAL and E-SERVQUAL, are unable to evaluate AI-specific capabilities like language intelligence and recognition. Existing research also lacks information on factors that affect user satisfaction and the continued use of AI chatbots. Based on a mixed-methods study, this paper proposes a new instrument for measuring user satisfaction with AI chatbots, specifically for customer support roles. Using the Stanford five-step Design Thinking Process, this study devised a customer support AI chatbot evaluation instrument through a literature review, Cheatstorming, and SCAMPER techniques, followed by testing in a Danish company. The research employs Prentice and Nguyen’s three-stage scale development process to ensure content, reliability, and construct validity, addressing gaps in current scholarship and advancing understanding of AI chatbot user satisfaction.",Yes,"본 논문은 AI 챗봇 사용자 만족도를 측정하기 위한 새로운 평가 도구를 제안하고, 이를 실제 기업에서 테스트하며 신뢰도와 타당성을 검증하는 독창적인 연구를 수행하였다. 기존 연구의 한계를 보완하고 새로운 측정 방법론을 개발한 점에서 직접 기여하는 연구 논문으로 판단된다."
"A Systematic Literature Review of the Opportunities and Advantages for AIGC (OpenAI ChatGPT, Copilot, Codex) in Programming Course",https://doi.org/10.1145/3704289.3704301,"This systematic literature review explored the opportunities and advantages of integrating Artificial Intelligence Generated Content (AIGC) tools like OpenAI's ChatGPT, Copilot, and Codex in programming education. From an initial pool of 1,173 papers, 24 were rigorously selected for detailed analysis. The findings highlighted the dominant use of ChatGPT, particularly versions 3/3.5 and 4, underscoring its effectiveness and accessibility. Python emerged as the most frequently studied language, followed by Java, C, R, and Scala. A notable research gap was identified in block-based programming languages and online/blended learning environments. Key opportunities and advantages identified included enhanced code review, where AIGC tools offer efficient and comprehensive assessments; personalized learning, with ChatGPT providing individualized feedback and improving student comprehension; and increased student engagement and motivation through interactive features. Additionally, AIGC tools significantly improved problem-solving and debugging support, effectively identifying and correcting coding errors. They also supported diverse learning styles by offering varied examples and solutions, facilitated innovative teaching strategies that improved educational outcomes, and reduced teacher workload by automating routine tasks. These insights demonstrated the transformative potential of AIGC tools in revolutionizing programming education.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)로, 기존 연구들을 종합하고 분석하는 리뷰 논문에 해당합니다. 따라서 직접적인 독창적 연구 결과나 실험 데이터를 제시하는 연구 논문은 아닙니다."
Design Principles for Generative AI Applications,https://doi.org/10.1145/3613904.3642466,"Generative AI applications present unique design challenges. As generative AI technologies are increasingly being incorporated into mainstream applications, there is an urgent need for guidance on how to design user experiences that foster effective and safe use. We present six principles for the design of generative AI applications that address unique characteristics of generative AI UX and offer new interpretations and extensions of known issues in the design of AI applications. Each principle is coupled with a set of design strategies for implementing that principle via UX capabilities or through the design process. The principles and strategies were developed through an iterative process involving literature review, feedback from design practitioners, validation against real-world generative AI applications, and incorporation into the design process of two generative AI applications. We anticipate the principles to usefully inform the design of generative AI applications by driving actionable design recommendations.",Yes,"논문은 문헌 검토, 실무자 피드백, 실제 사례 검증 및 두 개의 생성형 AI 애플리케이션 설계 과정에 원칙을 적용하는 반복적 과정을 통해 독창적인 설계 원칙과 전략을 제시하고 있다. 이는 기존 연구를 종합하는 데 그치지 않고 새로운 설계 원칙을 개발하고 실제 적용한 연구 기여로 판단된다."
Virtual Reality Solutions Employing Artificial Intelligence Methods: A Systematic Literature Review,https://doi.org/10.1145/3565020,"Although there are methods of artificial intelligence (AI) applied to virtual reality (VR) solutions, there are few studies in the literature. Thus, to fill this gap, we performed a systematic literature review of these methods. In this review, we apply a methodology proposed in the literature that locates existing studies, selects and evaluates contributions, analyses, and synthesizes data. We used Google Scholar and databases such as Elsevier's Scopus, ACM Digital Library, and IEEE Xplore Digital Library. A set of inclusion and exclusion criteria were used to select documents. The results showed that when AI methods are used in VR applications, the main advantages are high efficiency and precision of algorithms. Moreover, we observe that machine learning is the most applied AI scientific technique in VR applications. In conclusion, this paper showed that the combination of AI and VR contributes to new trends, opportunities, and applications for human-machine interactive devices, education, agriculture, transport, 3D image reconstruction, and health. We also concluded that the usage of AI in VR provides potential benefits in other fields of the real world such as teleconferencing, emotion interaction, tourist services, and image data extraction.",No,"이 논문은 인공지능과 가상현실 관련 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 초점이 맞춰져 있습니다."
How to Playfully Teach AI to Young Learners: a Systematic Literature Review,https://doi.org/10.1145/3605390.3605393,"Children are experiencing Artificial Intelligence (AI) devices in their daily lives. It is crucial to provide them with knowledge concerning how AI works, for enabling them to use AI responsibly and participate actively in their AI-driven future. To support motivation and engagement, playful tools are often used in technology education for K-12 children. This paper offers a systematic literature review of tools for teaching AI to K-12 learners in a playful manner. The most relevant articles are classified and analysed in terms of the nature of the tools they use, that is, whether tools are digital, partly physical and partly digital, or unplugged. Their analysis also considers the target age, the educational focus, and whether their impact is evaluated. According to the results of the review, there are tools for learners of all school grades, and digital tools are the most investigated. Moreover, several studies with tools tend to evaluate engagement and learning but in different manners. The paper concludes by discussing the evaluation aspect, general future work directions and limitations in relation to HCI and education for children.",No,"본 논문은 AI 교육 도구에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하고 분석하는 데 초점이 맞춰져 있습니다."
"A Comprehensive Survey on Big Data Analytics: Characteristics, Tools and Techniques",https://doi.org/10.1145/3718364,"Modern computing devices generate vast amounts of diverse data. It means that a fast transition through various computing devices leads to big data production. Big data with high velocity, volume, and variety presents challenges like data inconsistency, scalability, real-time analysis, and tool selection. Although numerous solutions have been proposed for big data processing, they are often limited in scope and effectiveness. This survey aims to address the lack of comprehensive analysis of big data challenges in relation to machine learning (ML) and the Internet of Things (IoT) environments, particularly concerning the 7Vs of big data. It emphasizes the significance of selecting suitable tools to address each unique big data characteristic, providing a structured approach to manage these challenges effectively. The article systematically reviews big data characteristics and associated techniques, with a detailed discussion of various tools and their applications. Additionally, it analyzes existing ML methods and techniques for IoT data analytics in big data contexts. Through a systematic literature review (SLR), we examine key aspects, including core concepts, benefits, limitations, and the impact of big data on ML algorithms and IoT data analytics. We highlight groundbreaking studies addressing big data challenges to impact future research and enhance big data-driven applications.",No,"초록에서 해당 논문은 빅데이터 분석에 관한 기존 연구들을 종합적으로 정리한 서베이 논문임을 명확히 밝히고 있습니다. 독창적인 연구 결과나 새로운 실험, 방법론 제시보다는 기존 연구들을 체계적으로 리뷰하고 분석하는 데 중점을 두고 있어 연구 논문으로 보기 어렵습니다."
The Human Side of XAI: Bridging the Gap between AI and Non-expert Audiences,https://doi.org/10.1145/3615335.3623062,"Machine Learning is widely used by practitioners to solve complex challenges. However, despite being trusted by 76% of the public, scientists struggle to explain the rationale behind machine learning-based decisions. This is concerning because research has shown that people often rely on inaccurate machine learning recommendations, even when the system is not confident or they have prior knowledge. To address these issues, there is a crucial need for greater transparency and education around machine learning decision making. In this work, we propose a set of guidelines and design implications to communicate eXplainable Artificial Intelligence models to the general audience. We do this through a literature review of the latest and eXplainable Artificial Intelligence methods and validate these insights through a user study encompassing the participants’ interpretations of the eXplainable Artificial Intelligence solutions. Combining the insights from this mixed-method study, we identify seven main communication guidelines for improving machine learning models understanding. This study contributes to the broader discussion of ethical implications surrounding opaque machine learning models in decision-making. Through the development of guidelines, we hope to bridge the gap between machine learning experts and the public, enabling a better common understanding of its increasing importance in our lives.",Yes,"본 논문은 최신 XAI 방법에 대한 문헌 검토와 사용자 연구를 통해 새로운 커뮤니케이션 가이드라인을 제안하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 실험적 사용자 연구를 통해 제안된 가이드라인을 검증하여 직접적인 연구 기여를 하고 있음을 보여줍니다."
Understanding Design Collaboration Between Designers and Artificial Intelligence: A Systematic Literature Review,https://doi.org/10.1145/3610217,"Recent interest in design through the artificial intelligence (AI) lens is rapidly increasing. Designers, as a special user group interacting with AI, have received more attention in the Human-Computer Interaction community. Prior work has discussed emerging challenges that persist in designing for AI. However, few systematic reviews focus on AI for design to understand how designers and AI can augment each other's complementary strengths in design collaboration. In this work, we conducted a landscape analysis of AI for design, via a systematic literature review of 93 papers. The analysis first provides a bird's eye view of overall patterns in this area. The analysis also reveals three themes interpreted from the paper corpus associated with AI for design, including AI assisting designers, designers assisting AI, and characterizing designer-AI collaboration. We discuss the implications of our findings and suggested methodological proposals to guide HCI toward research and practices that center on collaborative creativity.",No,"본 논문은 93편의 기존 연구를 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하는 데 중점을 두고 있다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 정리하는 리뷰 논문에 해당한다."
What do we know about Hugging Face? A systematic literature review and quantitative validation of qualitative claims,https://doi.org/10.1145/3674805.3686665,"Background: Software Package Registries (SPRs) are an integral part of the software supply chain. These collaborative platforms unite contributors, users, and code for streamlined package management. Prior work has characterized the SPRs associated with traditional software, such as NPM (JavaScript) and PyPI (Python). Pre-Trained Model (PTM) Registries are an emerging class of SPR of increasing importance, because they support the deep learning supply chain. A growing body of empirical research has examined PTM registries from various angles, such as vulnerabilities, reuse processes, and evolution. However, no synthesis provides a systematic understanding of current knowledge. Furthermore, much of the existing research includes non-quantified qualitative observations. Aims: First, we aim to provide a systematic knowledge synthesis. Second, we quantify qualitative claims. Methods: We conducted a systematic literature review (SLR). We then observed that some of the claims are qualitative, lacking quantitative evidence. We identify quantifiable metrics associated with those claims, and measure in order to substantiate these claims. Results: We identify 12 claims about PTM reuse on the HuggingFace platform, 4 of which lack quantitative support. We tested 3 of these claims through a quantitative analysis, and directly compare the fourth with traditional software. Our most notable findings are: (1) PTMs have a significantly higher turnover rate than traditional software, indicating more rapid evolution; and (2) There is a strong correlation between documentation quality and PTM popularity. Conclusions: Our findings validate several qualitative research claims with concrete metrics, confirming prior research. Our measures motivate further research on the dynamics of PTM reuse.",Yes,"이 논문은 체계적인 문헌 리뷰를 수행하고, 기존의 정성적 주장들을 정량적으로 검증하는 독창적인 연구를 포함하고 있습니다. 또한, PTM 재사용에 관한 새로운 정량적 분석 결과를 제시하여 직접적인 연구 기여를 하고 있습니다."
AI Integration in the IT Professional Workplace: A Scoping Review and Interview Study with Implications for Education and Professional Competencies,https://doi.org/10.1145/3689187.3709607,"As Artificial Intelligence (AI) continues transforming workplaces globally, particularly within the Information Technology (IT) industry, understanding its impact on IT professionals and computing curricula is crucial. This research builds on joint work from two countries, addressing concerns about AI's increasing influence in IT sector workplaces and its implications for tertiary education. The study focuses on AI technologies such as generative AI (GenAI) and large language models (LLMs). It examines how they are perceived and adopted and their effects on workplace dynamics, task allocation, and human-system interaction. IT professionals, noted as early adopters of AI, offer valuable insights into the interplay between AI and work engagement, highlighting the significant competencies required for digital workplaces. This study employs a dual-method approach, combining a systematic and multi-vocal literature review and qualitative research methods. These included a thematic analysis of a set of 47 interviews conducted between March and May of 2024 with IT professionals in two countries (New Zealand and Sweden). The research aimed to understand the implications for computing students, education curricula, and the assessment of emerging professional competencies. The literature review found insufficient evidence addressing comprehensive AI practice methodologies, highlighting the need to both develop and regulate professional competencies for effective AI integration. Key interview findings revealed diverse levels of GenAI adoption, ranging from individual experimentation to institutional integration. Participants generally expressed positive attitudes toward the technology and were actively pursuing self-learning despite some concerns. The themes emerging from the interviews included AI's role in augmenting human tasks, privacy and security concerns, productivity enhancements, legal and ethical challenges, and the evolving need for new competencies in the workplace. The study underscores the critical role of competency frameworks in guiding professional development and ensuring preparedness for an AI-driven environment. Additionally, it highlights the need for educational institutions to adapt curricula to address these emerging demands effectively.",Yes,"본 논문은 AI 기술의 IT 직장 내 통합에 관한 체계적 문헌 검토와 47명의 IT 전문가 인터뷰를 통해 직접 수집한 질적 데이터를 분석하여 새로운 통찰과 교육적 시사점을 도출하는 독창적인 연구를 수행하였다. 따라서 기존 연구를 단순 정리한 리뷰가 아니라, 독자적 연구 결과를 포함한 연구 논문으로 판단된다."
The impact of Artificial Intelligence on the Socioeconomic factors in the UAE,https://doi.org/10.1145/3639592.3639609,"Artificial Intelligence (AI) is rapidly transforming industries worldwide, including the United Arab Emirates (UAE). As the UAE aims to become a leader in AI adoption and deployment, it is crucial to understand its impact on the country's socioeconomic factors. This research proposal aims to investigate the impact of AI on employment, wage inequality, productivity, innovation, and ethical implications in the UAE. The research will adopt a mixed-methods approach combining a systematic literature review and a quantitative survey to gather and assess data. The literature review will identify relevant literature to establish the research framework, while the survey will target leaders, employees, and end-users of organizations operating in the UAE across various sectors. A sample size of no less than 250 respondents who are predominantly leadership executives will be used, and the survey will consist of 30 questions, including demographic questions, and a 5-point Likert scale to measure respondents’ level of agreement or disagreement with the statements presented. Ethical considerations, including informed consent and anonymity, will be addressed. The findings from this research will contribute to a better understanding of the impact of AI on the socioeconomic factors in the UAE and help inform the development of appropriate policies and strategies for the responsible adoption and deployment of AI technologies in the country. The research proposal presents an exciting opportunity to explore a relevant and timely topic that will shed light on the potential impact of AI on the UAE's socioeconomic development.",No,"주어진 초록은 연구 계획서(proposal)로서, 아직 수행된 연구 결과나 독창적인 연구 내용이 포함되어 있지 않습니다. 따라서 직접 기여하는 연구 논문으로 보기 어렵습니다."
Using Generative AI Chatbot for enhancing people's digital participation,https://doi.org/10.1145/3680127.3680194,"Aiming to improve communication and increase the access to available channels and tools, chatbots, or conversational agents, are being used to support people´s digital participation and foster civic engagement. However, conversational agents attend only expected cases of participation, that is, those based on e-consulting, e-decision making and e-informing. Hence, chatbots don´t concern non mapped cases of self-mobilization, besides there are problems in human-computer communication and in the transmission of information caused by digital divide that hinder peoples ‘participation. Literature review indicates that generative AI chatbot shall be used in non-expected interactions between human and conversational agents. Therefore, this paper discusses how chatbots could interact within self-mobilizations’ types of digital participation, concerning obstacles like lack of infrastructure and digital illiteracy. Using design science research, we propose the GenPart, a generative AI-chatbot model for enhancing people´s digital participation. We claim that this artifact can be used by researchers and practitioners who want to promote the integration between people, organizations, and information in digital platforms, promoting citizen participation and civic engagement.",Yes,"논문은 디자인 과학 연구 방법을 사용하여 새로운 생성형 AI 챗봇 모델(GenPart)을 제안하고 있으며, 이는 디지털 참여를 향상시키기 위한 독창적인 연구 기여를 포함하고 있다. 따라서 단순 문헌 리뷰가 아닌 직접적인 연구 개발과 제안이 이루어진 연구 논문으로 판단된다."
Guidelines for the Evolving Role of Generative AI in Introductory Programming Based on Emerging Practice,https://doi.org/10.1145/3649217.3653602,"In the rapidly evolving Generative AI (GenAI) landscape, source code and natural language are being mixed and used in new ways. This presents opportunities for rethinking teaching practice in Introductory Programming (CS1) courses that includes, but goes beyond, assessment. In this paper we examine the reasons why and how instructors who are early adopters of GenAI are using it in their teaching, and why others are not. We also explore the changes and adaptations that are currently being made to practice. This is achieved by synthesizing insights from several recent studies that have collected primary data from introductory programming instructors who are teaching with, considering teaching with, or actively not teaching with GenAI. Due to the fast pace of GenAI development and adoption, the fixed-pace and cyclical nature of education, and the relatively slow pace of research (including ethical approvals) and publication cycles, research with primary data from instructors is only being published relatively recently. In computing education, there is not yet enough published research with primary data from CS1 instructors to warrant a systematic literature review, although in the next year this will likely be possible. Based on an analysis of the nascent research that has been published, we propose emerging and flexible guidelines on how CS1 instructors could adapt their practice based on what others have done so far. These guidelines highlight important factors to consider when integrating GenAI in CS1 courses, which for many is only beginning.",No,본 논문은 여러 최근 연구에서 수집된 2차 데이터를 종합하여 초급 프로그래밍 교육에서 생성형 AI 활용에 대한 가이드라인을 제안하는 리뷰 및 제안 논문입니다. 직접적인 독창적 연구 결과나 실험 데이터를 새로 제시하지 않고 기존 연구를 분석하는 데 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks,https://doi.org/10.1145/3638242,"Artificial Intelligence (AI) and, in particular, Machine Learning (ML), have emerged to be utilized in various applications due to their capability to learn how to solve complex problems. Over the past decade, rapid advances in ML have presented Deep Neural Networks (DNNs) consisting of a large number of neurons and layers. DNN Hardware Accelerators (DHAs) are leveraged to deploy DNNs in the target applications. Safety-critical applications, where hardware faults/errors would result in catastrophic consequences, also benefit from DHAs. Therefore, the reliability of DNNs is an essential subject of research. In recent years, several studies have been published accordingly to assess the reliability of DNNs. In this regard, various reliability assessment methods have been proposed on a variety of platforms and applications. Hence, there is a need to summarize the state-of-the-art to identify the gaps in the study of the reliability of DNNs. In this work, we conduct a Systematic Literature Review (SLR) on the reliability assessment methods of DNNs to collect relevant research works as much as possible, present a categorization of them, and address the open challenges. Through this SLR, three kinds of methods for reliability assessment of DNNs are identified, including Fault Injection (FI), Analytical, and Hybrid methods. Since the majority of works assess the DNN reliability by FI, we characterize different approaches and platforms of the FI method comprehensively. Moreover, Analytical and Hybrid methods are propounded. Thus, different reliability assessment methods for DNNs have been elaborated on their conducted DNN platforms and reliability evaluation metrics. Finally, we highlight the advantages and disadvantages of the identified methods and address the open challenges in the research area. We have concluded that Analytical and Hybrid methods are light-weight yet sufficiently accurate and have the potential to be extended in future research and to be utilized in establishing novel DNN reliability assessment frameworks.",No,"본 논문은 딥 뉴럴 네트워크의 하드웨어 신뢰성 평가 방법에 대한 체계적인 문헌 리뷰(Systematic Literature Review)를 수행한 것으로, 기존 연구들을 종합하고 분류하며 연구 동향과 한계를 제시하는 데 중점을 두고 있다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵다."
Systemization of Knowledge (SoK): Creating a Research Agenda for Human-Centered Real-Time Risk Detection on Social Media Platforms,https://doi.org/10.1145/3613904.3642315,"Accurate real-time risk identification is vital to protecting social media users from online harm, which has driven research towards advancements in machine learning (ML). While strides have been made regarding the computational facets of algorithms for “real-time” risk detection, such research has not yet evaluated these advancements through a human-centered lens. To this end, we conducted a systematic literature review of 53 peer-reviewed articles on real-time risk detection on social media. Real-time detection was mainly operationalized as “early” detection after-the-fact based on pre-defined chunks of data and evaluated based on standard performance metrics, such as timeliness. We identified several human-centered opportunities for advancing current algorithms, such as integrating human insight in feature selection, algorithms’ improvement considering human behavior, and utilizing human evaluations. This work serves as a critical call-to-action for the HCI and ML communities to work together to protect social media users before, during, and after exposure to risks.",No,"본 논문은 53편의 기존 연구를 체계적으로 검토한 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 향후 연구 방향을 제안하는 내용이다. 따라서 새로운 연구 기여보다는 연구 현황 정리 및 연구 의제 설정에 초점이 맞춰져 있다."
Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding the Development and Assessment of AI Systems,https://doi.org/10.1145/3657054.3657141,"As artificial intelligence (AI) continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges. The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse. While the European Commission has taken a decisive step by reaching a political agreement on the EU AI Act, the world’s first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems. In response to this challenge, this study conducts a systematic literature review (SLR) with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security. Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems. This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in the development and critical assessment of AI systems. Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles. In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics.",No,"본 논문은 체계적 문헌 고찰(SLR)을 통해 기존 연구를 종합하고 개념적 프레임워크를 제안하는 데 중점을 두고 있으며, 직접적인 실험이나 새로운 데이터 분석 등 독창적인 연구 결과를 제시하지는 않는다. 따라서 기존 연구를 정리하고 방향성을 제시하는 리뷰 논문에 가깝다."
Fake News Detection over the Social Media by using Machine Learning Techniques: A&nbsp;Systematic Literature Review,https://doi.org/10.1145/3607947.3608048,"In an internet era, the main source for spreading the fake news is social media. Detection of fake news is a necessary need in order to avoid the misconception caused due to the fake news. Spread of fake news not only has individual effects but also leaves its effects in the ares of health, finance, politics and many more. Therefore, machine learning approaches can be a useful way to detect fake news. In this paper, a systematic literature review has been performed, so that the various machine learning techniques used for the detection of fake news over social media during the period of 2017-2022 can find out. Initially we downloaded a total of 2,699 papers from five different digital libraries. We performed the scrutinization of the studies on the different levels. After performing the quality assessment of the primary selected papers, 35 papers were selected for the final review. Therefore, we analyzed that support vector machine is the most used machine learning algorithm and twitter is the most used social media platform used for spreading the fake news.",No,"본 논문은 머신러닝 기법을 이용한 가짜 뉴스 탐지에 관한 기존 연구들을 체계적으로 문헌 검토한 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하지 않고, 기존 연구들을 분석하고 요약하는 데 중점을 두고 있다."
Computational Techniques in PET/CT Image Processing for Breast Cancer: A Systematic Mapping Review,https://doi.org/10.1145/3648359,"The problem arises from the lack of sufficient and comprehensive information about the necessary computer techniques. These techniques are crucial for developing information systems that assist doctors in diagnosing breast cancer, especially those related to positron emission tomography and computed tomography (PET/CT). Despite global efforts in breast cancer prevention and control, the scarcity of literature poses an obstacle to a complete understanding in this area of interest. The methodologies studied were systematic mapping and systematic literature review. For each article, the journal, conference, year of publication, dataset, breast cancer characteristics, PET/CT processing techniques, metrics and diagnostic yield results were identified. Sixty-four articles were analyzed, 44 (68.75%) belong to journals and 20 (31.25%) belong to the conference category. A total of 102 techniques were identified, which were distributed in preprocessing with 7 (6.86%), segmentation with 15 (14.71%), feature extraction with 15 (14.71%), and classification with 65 (63.73%). The techniques with the highest incidence identified in each stage are: Gaussian Filter, SLIC, Local Binary Pattern, and Support Vector Machine with 4, 2, 7, and 35 occurrences, respectively. Support Vector Machine is the predominant technique in the classification stage, due to the fact that Artificial Intelligence is emerging in medical image processing and health care to make expert systems increasingly intelligent and obtain favorable results.",No,"본 논문은 PET/CT 영상 처리 기법에 대한 체계적 문헌 고찰(Systematic Mapping Review)로, 기존 연구들을 분석하고 분류하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 새로운 기법 제안이 포함된 연구 논문으로 보기 어렵습니다."
Physics-Informed Computer Vision: A Review and Perspectives,https://doi.org/10.1145/3689037,"The incorporation of physical information in machine learning frameworks is opening and transforming many application domains. Here the learning process is augmented through the induction of fundamental knowledge and governing physical laws. In this work, we explore their utility for computer vision tasks in interpreting and understanding visual data. We present a systematic literature review of more than 250 papers on formulation and approaches to computer vision tasks guided by physical laws. We begin by decomposing the popular computer vision pipeline into a taxonomy of stages and investigate approaches to incorporate governing physical equations in each stage. Existing approaches are analyzed in terms of modeling and formulation of governing physical processes, including modifying input data (observation bias), network architectures (inductive bias), and training losses (learning bias). The taxonomy offers a unified view of the application of the physics-informed capability, highlighting where physics-informed learning has been conducted and where the gaps and opportunities are. Finally, we highlight open problems and challenges to inform future research. While still in its early days, the study of physics-informed computer vision has the promise to develop better computer vision models that can improve physical plausibility, accuracy, data efficiency, and generalization in increasingly realistic applications.",No,"본 논문은 250편 이상의 기존 연구를 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분류하고 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 향후 과제를 제시하는 개관서에 해당합니다."
"Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit",https://doi.org/10.1145/3664597,"Code intelligence leverages machine learning techniques to extract knowledge from extensive code corpora, with the aim of developing intelligent tools to improve the quality and productivity of computer programming. Currently, there is already a thriving research community focusing on code intelligence, with efforts ranging from software engineering, machine learning, data mining, natural language processing, and programming languages. In this paper, we conduct a comprehensive literature review on deep learning for code intelligence, from the aspects of code representation learning, deep learning techniques, and application tasks. We also benchmark several state-of-the-art neural models for code intelligence, and provide an open-source toolkit tailored for the rapid prototyping of deep-learning-based code intelligence models. In particular, we inspect the existing code intelligence models under the basis of code representation learning, and provide a comprehensive overview to enhance comprehension of the present state of code intelligence. Furthermore, we publicly release the source code and data resources to provide the community with a ready-to-use benchmark, which can facilitate the evaluation and comparison of existing and future code intelligence models (https://xcodemind.github.io). At last, we also point out several challenging and promising directions for future research.",No,"본 논문은 딥러닝 기반 코드 인텔리전스 분야에 대한 종합적인 문헌 리뷰, 벤치마크, 그리고 툴킷 제공에 중점을 두고 있으며, 직접적인 독창적 연구 결과나 새로운 방법론 제안보다는 기존 연구를 정리하고 평가하는 서베이 논문에 해당합니다. 따라서 연구 논문으로 보기 어렵습니다."
“Who is the right homeless client?”: Values in Algorithmic Homelessness Service Provision and Machine Learning Research,https://doi.org/10.1145/3544548.3581010,"Homelessness presents a long-standing problem worldwide. Like other welfare services, homeless services have gained increased traction in Machine Learning (ML) research. Unhoused persons are vulnerable and using their data in the ML pipeline raises serious concerns about the unintended harms and consequences of prioritizing different ML values. To address this, we conducted a critical analysis of 40 research papers identified through a systematic literature review in ML homelessness service provision research. We found that the values of novelty, performance, and identifying limitations were uplifted in these papers, whereas (in)efficiency, (low/high) cost, fast, (violated) privacy, and (homeless condition) reproducibility valuescollapse. Consequently, unhoused persons were lost (i.e., humans were deprioritized) at multi-level ML abstraction of predictors, categories, and algorithms. Our findings illuminate potential pathways forward at the intersection of data science, HCI and STS by situating humans at the center to support this vulnerable community.",No,"본 논문은 40개의 기존 연구 논문을 체계적으로 문헌 검토하고 비판적으로 분석한 연구로, 직접적인 독창적 실험이나 새로운 알고리즘 개발보다는 기존 연구의 가치와 한계를 평가하는 데 중점을 두고 있습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
"A Roadmap of Explainable Artificial Intelligence: Explain to Whom, When, What and How?",https://doi.org/10.1145/3702004,"Explainable artificial intelligence (XAI) has gained significant attention, especially in AI-powered autonomous and adaptive systems (AASs). However, a discernible disconnect exists among research efforts across different communities. The machine learning community often overlooks “explaining to whom,” while the human-computer interaction community has examined various stakeholders with diverse explanation needs without addressing which XAI methods meet these requirements. Currently, no clear guidance exists on which XAI methods suit which specific stakeholders and their distinct needs. This hinders the achievement of the goal of XAI: providing human users with understandable interpretations. To bridge this gap, this article presents a comprehensive XAI roadmap. Based on an extensive literature review, the roadmap summarizes different stakeholders, their explanation needs at different stages of the AI system lifecycle, the questions they may pose, and existing XAI methods. Then, by utilizing stakeholders’ inquiries as a conduit, the roadmap connects their needs to prevailing XAI methods, providing a guideline to assist researchers and practitioners to determine more easily which XAI methodologies can meet the specific needs of stakeholders in AASs. Finally, the roadmap discusses the limitations of existing XAI methods and outlines directions for future research.",No,본 논문은 광범위한 문헌 조사를 바탕으로 XAI 분야의 현황과 연구 방향을 정리한 로드맵을 제시하는 리뷰 논문에 해당합니다. 독창적인 실험이나 새로운 연구 결과를 직접 제시하지 않고 기존 연구들을 종합하여 가이드라인을 제공하는 내용이므로 연구 논문으로 보기 어렵습니다.
A Review of Deep Learning IDS for DDoS Attacks in WLANs,https://doi.org/10.1145/3484824.3484897,"An Intrusion detection system (IDS) is a critical security organ which serves as a gatekeeper against the ever-evolving cyber space attacks. Among the recent technological advancement, high speed communication networks and Artificial Intelligence are notably the key driving factors for the evolution of such IDS from the rule-based to classic machine learning and recently, the deep learning using either manual feature engineering or representation learning techniques. This paper is a methodological literature review of various generations of machine learning-based IDS to assess their effectiveness. The review result is a pretext and blueprint for a new method that attempts to combine representation and manual feature learning techniques to effectively detect Distributed Denial of Service attacks (DDoS) in Wireless Local Area Networks (WLANs).",No,"본 논문은 다양한 머신러닝 기반 IDS에 대한 문헌 리뷰를 수행한 것으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 평가하는 내용에 초점이 맞춰져 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 분석에 해당합니다."
Methods for Automatic Image-Based Classification of Winged Insects Using Computational Techniques: A Systematic Literature Review,https://doi.org/10.1145/3411564.3411641,"Artificial intelligence has been used in conjunction with computational and statistical techniques for automatic identification of insect species because of the current growing demand for this type of solution. This study presents the results of a Systematic Literature Review conducted to evaluate the state of the art of automatic image-based classification methods of winged insects. One thousand and sixty studies were researched and thirty-six were fully analyzed, and, from the results obtained, it was concluded that the area is consolidating with an uptrend and that there are techniques with satisfactory results for the proposed objectives, but there are still major problems to be solved.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)로, 기존 연구들을 종합하고 평가하는 데 중점을 두고 있어 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않는다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Using Artificial intelligence for Recycling- A Case Study in Taiwan's Resource Recycling Industry,https://doi.org/10.1145/3624875.3624897,"Resource recycling industries are restricted by the mixed composition of waste, the ratio of recycled material dependent on experience, and the quality inspection relies on physical assessment, resulting in problems such as the inability to optimize manufacturing process parameters and unstable product quality. Artificial Intelligence (AI) has developed rapidly in recent years and adopted in various industries. From literature review, AI is being increasingly used in the resource recycling industry to optimize processes, improve efficiency, and enhance sustainability. This study used machine learning, an AI technology, to conduct model training and identify optimal model validation in Taiwan's resource recycling industry. A case example was implemented to reduce the cost by AI's recommendation of more accurate borax blending ratios. By accepting AI's recommendation of accurate borax blending ratios, it can reduce the non-compliance rate by 45% and at the same time reduce the raw material usage by approximately 2∼3%, saving approximately N.T.D. 1-2 million per year. This study pioneered artificial intelligence technology to improve manufacturing process efficiency and optimization of product quality in the resource recycling industry. The empirical results demonstrate that the application of AI technology contributes to the improvement of traditional manufacturing process problems and enhances the industry's overall production efficiency and quality.",Yes,본 논문은 인공지능 기술을 활용하여 대만 자원 재활용 산업의 제조 공정 최적화와 제품 품질 향상에 직접 기여하는 모델 훈련과 사례 연구를 수행하였다. 이는 독창적인 연구 내용과 실증적 결과를 포함하고 있어 연구 논문에 해당한다.
Ethical Tensions in Applications of AI for Addressing Human Trafficking: A Human Rights Perspective,https://doi.org/10.1145/3555186,"In the last two decades, human trafficking (where individuals are forcibly exploited for the profits of another) has seen increased attention from the artificial intelligence (AI) community. Clear focus on the ethical risks of this research is critical given that those risks are disproportionately born by already vulnerable populations. To understand and subsequently address these risks, we conducted a systematic literature review of computing research leveraging AI to combat human trafficking and apply a framework using principles from international human rights law to categorize ethical risks. This paper uncovers a number of ethical tensions including bias endemic in datasets, privacy risks stemming from data collection and reporting, and issues concerning potential misuse. We conclude by highlighting four suggestions for future research: broader use of participatory design; engaging with other forms of trafficking; developing best practices for harm prevention; and including transparent ethics disclosures in research. We find that there are significant gaps in what aspects of human trafficking researchers have focused on. Most research to date focuses on aiding criminal investigations in cases of sex trafficking, but more work is needed to support other anti-trafficking activities like supporting survivors, adequately address labor trafficking, and support more diverse survivor populations including transgender and nonbinary individuals.",No,"본 논문은 AI를 활용한 인신매매 연구의 윤리적 위험을 체계적으로 문헌 검토하고 인권 원칙에 따라 분류하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 윤리적 쟁점 분석에 중점을 두고 있습니다."
A maturity assessment framework for conversational AI development platforms,https://dl.acm.org/doi/10.1145/3412841.3442046,"Conversational Artificial Intelligence (AI) systems have recently sky-rocketed in popularity and are now used in many applications, from car assistants to customer support. The development of conversational AI systems is supported by a large variety of software platforms, all with similar goals, but different focus points and functionalities. A systematic foundation for classifying conversational AI platforms is currently lacking. We propose a framework for assessing the maturity level of conversational AI development platforms. Our framework is based on a systematic literature review, in which we extracted common and distinguishing features of various open-source and commercial (or in-house) platforms. Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs. Our framework can guide organizations in selecting a conversational AI development platform according to their needs, as well as helping researchers and platform developers improving the maturity of their platforms.",Yes,"논문은 대화형 AI 개발 플랫폼의 성숙도 평가를 위한 새로운 프레임워크를 제안하고 있으며, 이는 체계적인 문헌 검토를 바탕으로 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Tooling for Developing Data-Driven Applications: Overview and Outlook,https://doi.org/10.1145/3543758.3543779,"Machine Learning systems are, by now, an essential part of the software landscape. From the development perspective this means a paradigmatic shift, which should be reflected in the way we write software. For now, the majority of developers relies on traditional tools for data-driven development, though. To determine how research into tools is catching up, we conducted a systematic literature review, searching for tools dedicated to data-driven development. Of the 1511 search results, we analyzed 76 relevant publications in detail. The diverse sample indicated a strong interest in this topic from different domains, with different approaches and methods. While there are a number of common trends, e.g. the use of visualization, in these tools, only a limited, although increasing, number of these tools has so far been evaluated comprehensively. We therefore summarize trends, strengths and weaknesses in the status quo for data-driven development tools and conclude with a number of potential future directions this field.",No,"이 논문은 데이터 기반 개발 도구에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 현황 분석과 향후 전망을 다루고 있습니다."
A Systematic Literature Review on Client Selection in Federated Learning,https://doi.org/10.1145/3593434.3593438,"With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rounds, as testing accuracy measures the successfulness of the learning and preferably in as few communication rounds as possible, as they are very expensive. Although several possible improvements can be made with the current state of client selection, the most beneficial ones are evaluating the impact of unsuccessful clients and gaining a more theoretical understanding of the impact of fairness in FL.",No,"본 논문은 연합학습에서 클라이언트 선택에 관한 기존 연구들을 체계적으로 문헌 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하고 분석하는 데 중점을 두고 있습니다."
A Systematic Literature Review on Federated Machine Learning: From a Software Engineering Perspective,https://doi.org/10.1145/3450288,"Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.",No,"본 논문은 231개의 기존 연구를 체계적으로 검토하는 문헌 리뷰 연구로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 미래 연구 방향을 정리하는 데 중점을 두고 있습니다."
A Systematic Literature Review on Automated Software Vulnerability Detection Using Machine Learning,https://doi.org/10.1145/3699711,"In recent years, numerous Machine Learning (ML) models, including Deep Learning (DL) and classic ML models, have been developed to detect software vulnerabilities. However, there is a notable lack of comprehensive and systematic surveys that summarize, classify, and analyze the applications of these ML models in software vulnerability detection. This absence may lead to critical research areas being overlooked or under-represented, resulting in a skewed understanding of the current state of the art in software vulnerability detection. To close this gap, we propose a comprehensive and systematic literature review that characterizes the different properties of ML-based software vulnerability detection systems using six major Research Questions (RQs). Using a custom web scraper, our systematic approach involves extracting a set of studies from four widely used online digital libraries: ACM Digital Library, IEEE Xplore, ScienceDirect, and Google Scholar. We manually analyzed the extracted studies to filter out irrelevant work unrelated to software vulnerability detection, followed by creating taxonomies and addressing RQs. Our analysis indicates a significant upward trend in applying ML techniques for software vulnerability detection over the past few years, with many studies published in recent years. Prominent conference venues include the International Conference on Software Engineering (ICSE), the International Symposium on Software Reliability Engineering (ISSRE), the Mining Software Repositories (MSR) conference, and the ACM International Conference on the Foundations of Software Engineering (FSE), whereas Information and Software Technology (IST), Computers & Security (C&S), and Journal of Systems and Software (JSS) are the leading journal venues. Our results reveal that 39.1% of the subject studies use hybrid sources, whereas 37.6% of the subject studies utilize benchmark data for software vulnerability detection. Code-based data are the most commonly used data type among subject studies, with source code being the predominant subtype. Graph-based and token-based input representations are the most popular techniques, accounting for 57.2% and 24.6% of the subject studies, respectively. Among the input embedding techniques, graph embedding and token vector embedding are the most frequently used techniques, accounting for 32.6% and 29.7% of the subject studies. Additionally, 88.4% of the subject studies use DL models, with recurrent neural networks and graph neural networks being the most popular subcategories, whereas only 7.2% use classic ML models. Among the vulnerability types covered by the subject studies, CWE-119, CWE-20, and CWE-190 are the most frequent ones. In terms of tools used for software vulnerability detection, Keras with TensorFlow backend and PyTorch libraries are the most frequently used model-building tools, accounting for 42 studies for each. In addition, Joern is the most popular tool used for code representation, accounting for 24 studies. Finally, we summarize the challenges and future directions in the context of software vulnerability detection, providing valuable insights for researchers and practitioners in the field.",No,"이 논문은 머신러닝을 이용한 소프트웨어 취약점 탐지에 관한 기존 연구들을 체계적으로 정리하고 분석하는 문헌 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이 아니라, 기존 연구들을 종합하여 요약하는 리뷰 논문에 해당한다."
Application of CPU in AI and Machine Learning,https://doi.org/10.1145/3686081.3686118,"In recent years, the burgeoning fields of Artificial Intelligence (AI) and Machine Learning (ML) have increasingly permeated daily life, catalyzing significant advancements in technology. This rapid evolution has, in turn, necessitated the continual adaptation and enhancement of Central Processing Units (CPUs), which remain at the forefront of computational hardware. Despite these advancements, there has been a noticeable dearth in scholarly literature focusing on training neural network models exclusively on CPUs. This gap presents a substantial impediment to the exploration and advancement of CPU applications within AI and ML domains. Upon extensive literature review and analysis, it becomes evident that the CPU's role in AI and ML has not only been pivotal but has also retained distinct advantages despite the relative deceleration in its development trajectory. Current trends suggest that modern CPUs are increasingly moving towards an integrated architecture, incorporating specialized modules and fostering synergies with heterogeneous computing units. This paradigm shift underscores a critical question: How can we stimulate CPU development further and leverage its inherent strengths effectively? Addressing this query is essential for pushing the boundaries of CPU technology in the ever-evolving landscape of AI and ML.",No,"초록 내용은 CPU의 AI 및 ML 분야에서의 역할과 현황에 대한 문헌 리뷰와 분석에 초점을 맞추고 있으며, 독창적인 연구 결과나 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 직접 기여하는 연구 논문으로 보기 어렵습니다."
Meteorological Time Series Clustering in Agricultural Applications: A Systematic Literature Review,https://doi.org/10.1145/3658271.3658293,"Context: Clustering of meteorological time series in the agricultural context is extremely useful for improving agricultural decision support systems, mainly through climate zoning. Problem: Given the particularities of meteorological time series, the clustering task is complex, usually involving data preprocessing and feature extraction steps, in addition to the need to keep up with the advancement of technology and machine learning techniques. Solution: This study brings together the main solutions for clustering meteorological time series in agricultural applications, in a context-aware way, mapping the main challenges and seeking to understand the characteristics of the meteorological data, in order to better understand the applicability of different techniques in the agricultural context. IS Theory: This work was developed within the scope of Argumentation Theory, gathering and compiling data from primary studies on the topic, as well as evidence that proves the legitimacy of these data and conclusions in the form of statements. Method: This study presents a descriptive and systematic literature review, according to a well-defined and widely used methodology, regarding published works on clustering of meteorological time series in agricultural applications. Summary of Results: After an initial search, the papers were screened and filtered based on the review protocol, and 26 papers were selected for review. Data were then extracted about the solutions presented in each paper, such as objective, operation, experiments, and evaluation metrics. IS Contributions and Impact: The main contribution of this study is the organization of published knowledge on the research topic, in order to identify the state-of-the-art and assist researchers, as well as the discussion and highlighting of future research directions.",No,"본 논문은 메타분석 및 체계적 문헌고찰을 수행하여 기존 연구들을 정리하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 평가에 중점을 두고 있습니다."
Machine Learning-Based Automated Grading and Feedback Tools for Programming: A Meta-Analysis,https://dl.acm.org/doi/10.1145/3587102.3588822,"Research into automated grading has increased as Computer Science courses grow. Dynamic and static approaches are typically used to implement these graders, the most common implementation being unit testing to grade correctness. This paper expands upon an ongoing systematic literature review to provide an in-depth analysis of how machine learning (ML) has been used to grade and give feedback on programming assignments. We conducted a backward snowball search using the ML papers from an ongoing systematic review and selected 27 papers that met our inclusion criteria. After selecting our papers, we analysed the skills graded, the preprocessing steps, the ML implementation, and the models' evaluations. We find that most the models are implemented using neural network-based approaches, with most implementing some form of recurrent neural network (RNN), including Long Short-Term Memory, and encoder/decoder with attention mechanisms. Some graders implement traditional ML approaches, typically focused on clustering. Most ML-based automated grading, not many use ML to evaluate maintainability, readability, and documentation, but focus on grading correctness, a problem that dynamic and static analysis techniques, such as unit testing, rule-based program repair, and comparison to models or approved solutions, have mostly resolved. However, some ML-based tools, including those for assessing graphical output, have evaluated the correctness of assignments that conventional implementations cannot.",No,"본 논문은 머신러닝 기반 자동 채점 도구에 대한 메타분석으로, 기존 연구들을 체계적으로 검토하고 분석한 문헌 리뷰에 해당합니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들의 종합과 평가에 초점이 맞춰져 있습니다."
"Software-Based Dialogue Systems: Survey, Taxonomy, and Challenges",https://doi.org/10.1145/3527450,"The use of natural language interfaces in the field of human-computer interaction (HCI) is undergoing intense study through dedicated scientific and industrial research. The latest contributions in the field, including deep learning approaches like recurrent neural networks (RNNs), the potential of context-aware strategies and user-centred design approaches, have brought back the attention of the community to software-based dialogue systems, generally known as conversational agents or chatbots. Nonetheless, and given the novelty of the field, a generic, context-independent overview of the current state of research on conversational agents covering all research perspectives involved is missing. Motivated by this context, this article reports a survey of the current state of research of conversational agents through a systematic literature review of secondary studies. The conducted research is designed to develop an exhaustive perspective through a clear presentation of the aggregated knowledge published by recent literature within a variety of domains, research focuses and contexts. As a result, this research proposes a holistic taxonomy of the different dimensions involved in the conversational agents’ field, which is expected to help researchers and to lay the groundwork for future research in the field of natural language interfaces.",No,이 논문은 기존 연구들을 체계적으로 검토하고 종합하여 대화형 에이전트 분야의 현황과 분류체계를 제시하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 분석에 초점이 맞춰져 있습니다.
Software engineering approaches for TinyML based IoT embedded vision: a systematic literature review,https://doi.org/10.1145/3528227.3528569,"Internet of Things (IoT) has catapulted human ability to control our environments through ubiquitous sensing, communication, computation, and actuation. Over the past few years, IoT has joined forces with Machine Learning (ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning) has enabled the deployment of ML models for embedded vision on extremely lean edge hardware, bringing the power of IoT and ML together. However, TinyML powered embedded vision applications are still in a nascent stage, and they are just starting to scale to widespread real-world IoT deployment. To harness the true potential of IoT and ML, it is necessary to provide product developers with robust, easy-to-use software engineering (SE) frameworks and best practices that are customized for the unique challenges faced in TinyML engineering. Through this systematic literature review, we aggregated the key challenges reported by TinyML developers and identified state-of-art SE approaches in large-scale Computer Vision, Machine Learning, and Embedded Systems that can help address key challenges in TinyML based IoT embedded vision. In summary, our study draws synergies between SE expertise that embedded systems developers and ML developers have independently developed to help address the unique challenges in the engineering of TinyML based IoT embedded vision.",No,"본 논문은 TinyML 기반 IoT 임베디드 비전에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 초점이 맞춰져 있습니다."
Web Based Composition using Machine Learning Approaches: A Literature Review,https://doi.org/10.1145/3454127.3457623,"Web Service Composition technology involves the integration of various atomic web services to perform huge scale tasks. As the number of web services is continuously growing, the need for composite web services also increases simultaneously. Since 2000, academic and systematic research has significantly focused on Web-Based Composition (WBC) in many well-reputed scientific journals and presented insight on several issues related to semantic Web and machine learning. In this paper, we present a review on the main contributions related to WBC using Machine Learning approaches. Our analysis and evaluation of those papers give a remarkable insight into the different approaches and planning models with strategies that were developed by researchers and IT professionals. This literature review paper intends to give researchers, interested in WBC, a valuable source of information and also fast-track their researches towards future technologies related to artificial intelligence and machine learning in WBC. The related work onto WBC, its application, research gap, the current status of WBC, and future challenges are also discussed.",No,"본 논문은 Web Based Composition에 관한 기존 연구들을 정리하고 분석하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합하여 평가하는 리뷰 논문에 해당합니다."
Scalable Machine Learning Architectures for IPA-Driven Maintenance Task Allocation in Large-Scale Building Portfolios,https://doi.org/10.1145/3700706.3700734,"This research presents a groundbreaking approach to Building Maintenance Management (BMM) by introducing an Intelligent Process Automation (IPA)-Driven Building Maintenance Management (IBMM) model. This innovative model harnesses the synergies between Artificial Intelligence (AI), Machine Learning (ML), and Internet of Things (IoT) technologies to transition from reactive to proactive and predictive building maintenance strategies. The study highlights the critical gap in current BMM practices—the absence of intelligent systems for anticipating and addressing maintenance issues before they escalate. Through an extensive literature review, the transformative potential of AI and IoT for enhancing building maintenance management within smart cities is explored, establishing a foundation for the IBMM model's application. The core of this research lies in its novel application of scalable machine learning architectures to automate and optimize maintenance task allocation in large-scale building portfolios. The practicality of the IBMM model is demonstrated via a proof of concept (POC) in an industrial setting, evidencing its capacity to improve efficiency, reduce costs, and bolster sustainability in building maintenance operations. The model epitomizes a paradigm shift in BMM by integrating IPA, which combines AI and ML, facilitating automated, intelligent decision-making and task allocation. Among its advancements, the IBMM model introduces enhanced predictive maintenance through real-time data analysis, adaptive learning and optimization, automated decision-making, and human-machine collaboration, contributing to energy efficiency and alignment with smart city objectives. The paper delineates the methodology, design, and implementation of a machine learning model for engineer task assignments, culminating in a case study that validates the model's efficacy. This research not only signifies a significant advancement in BMM by leveraging IPA technologies for autonomous process refinement but also proposes a unique IPA-driven procedure that incorporates IoT technology and a novel smart device fixer to guide BMM processes. Anticipated outcomes include more accurate maintenance scheduling, cost efficiency, enhanced performance, and the fostering of a collaborative community through an open online documentation platform for BMM. Looking forward, the research aims to refine the IBMM model further by exploring advanced AI algorithms for more precise predictive maintenance and integrating real-time data analytics and IoT networks for improved maintenance strategy responsiveness. This work pioneers a smarter, more efficient, and sustainable approach to building maintenance, marking a new era in the management of urban infrastructure.",Yes,"본 논문은 IPA 기반의 건물 유지보수 관리 모델을 제안하고, 확장 가능한 머신러닝 아키텍처를 설계 및 구현하여 유지보수 작업 할당을 자동화하는 독창적인 연구 내용을 포함하고 있다. 또한, 산업 현장에서의 개념 증명과 사례 연구를 통해 모델의 실효성을 검증하여 직접적인 연구 기여를 명확히 하고 있다."
Complex Sequential Data Analysis: A Systematic Literature Review of Existing Algorithms,https://doi.org/10.1145/3410886.3410899,"This paper provides a review of past approaches to the use of deep-learning frameworks for the analysis of discrete irregular-patterned complex sequential datasets. A typical example of such a dataset is financial data where specific events trigger sudden irregular changes in the sequence of the data. Traditional deep-learning methods perform poorly or even fail when trying to analyse these datasets. The results of a systematic literature review reveal the dominance of frameworks based on recurrent neural networks. The performance of deep-learning frameworks was found to be evaluated mainly using mean absolute error and root mean square error accuracy metrics. Underlying challenges that were identified are: lack of performance robustness, non-transparency of the methodology, internal and external architectural design and configuration issues. These challenges provide an opportunity to improve the framework for complex irregular-patterned sequential datasets.",No,"본 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 알고리즘 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Exploring the Explainability of Time Series Clustering: A Review of Methods and Practices,https://doi.org/10.1145/3701551.3703479,"With the increasing use of time series data, particularly in critical applications and high-risk decision-making contexts, understanding and improving the explainability of time-series clustering(TSC) techniques is essential. While machine learning models excel in processing time series data, their explainability often needs enhancement, challenging human comprehension and trust. Time series data clustering, as an unsupervised learning method, extracts valuable patterns from complex datasets without prior knowledge, spanning various domains like biology and finance. However, the complexity of clustering models and their opaque decision-making processes raise concerns about understanding and trusting the results. Research in this area aims to enhance the explainability of TSC by developing new interpretation methods that not only ensure the accuracy of clustering results but also make them user-friendly and comprehensible to human users. This is crucial for overcoming challenges related to understanding and trusting the decision-making processes and outcomes of the model. In this study, we embarked on two significant endeavors: (a) We explored the use of explainable artificial intelligence (XAI) for TSC for the first time, conducting a comprehensive literature review. (b) We subdivided the research field through innovative classification methods, categorizing the explainability methods of TSC into three main categories: data preprocessing techniques based on time series data, single or hybrid methods based model training, and instance-based visualization algorithm applications. This analytical framework aims to elucidate how the explainability of TSC can be enhanced across these three dimensions, thereby improving its credibility. Our work not only opens new research avenues but also provides robust strategies for enhancing the explainability and credibility of TSC methods.",No,"본 논문은 시간 시계열 클러스터링의 설명 가능성에 대한 기존 연구들을 종합적으로 검토하고 분류하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여보다는 기존 연구의 체계적 정리와 분류에 중점을 두고 있다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Image-based detection and classification of Android malware through CNN models,https://doi.org/10.1145/3664476.3670441,"Convolutional Neural Networks (CNNs) are artificial deep learning networks widely used in computer vision and image recognition for their highly efficient capability of extracting input image features. In the literature, such a successful tool has been leveraged for detection/classification purposes in several application domains where input data are converted into images. In this work, we consider the application of CNN models, developed by employing standard Python libraries, to detect and then classify Android-based malware applications. Different models are tested, even in combination with machine learning-based classifiers, with respect to two datasets of 5000 applications each. To emphasize the adequacy of the various CNN implementations, several performance metrics are considered, as also stressed by a comprehensive comparison with related work.",Yes,"논문 초록에서 CNN 모델을 활용하여 안드로이드 악성코드를 탐지하고 분류하는 다양한 모델을 개발하고 테스트한 내용을 다루고 있어, 직접적인 연구 기여와 실험 결과를 포함한 독창적인 연구임을 알 수 있습니다. 또한, 성능 지표와 관련 연구와의 비교를 통해 연구의 유효성을 검증하고 있습니다."
"Societal, Economic, Political and Environmental: A Review of Benchmarks and AI-assisted Systematic Literature Review of Impact of Open Government Data",https://doi.org/10.1145/3657054.3657121,"The purpose of this research is to review the impact on open government data in evaluation at world level and research in academia. First, six most renowned open government data evaluations are reviewed with their impact section. Then, a systematic literature review on the impact of open government data is conducted with test of AI-assisted software using Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) Protocol, to find the current status of realized open government data impact research. Only four out of six evaluations of open government data have covered impact explicitly, and their criteria counting toward true impact is blur at best. AI-assisted software saves around 30∼40% of reviewing time in this research. Political, social, and economic impacts are all included research topics, however, no environmental impact research. Political impact outperformed other impact, both in quality and quantity. Political capital, not social capital shows more social impact on open government data. Economic impact research of open government data is scarce, and more about the open government data helps urban innovation in China. This research contributes to the open government data research in three ways. First, it reviews impact section major open government data evaluations. Second, it tests the AI-assisted software on the impact of open government data. Third, it sheds light on new research direction for impact of open government data on case study, qualitative research and mixed method.",Yes,"본 논문은 오픈 거버먼트 데이터의 영향에 대한 체계적인 문헌 리뷰와 AI 보조 소프트웨어 테스트를 포함하여 직접적인 연구 방법론과 결과를 제시하고 있습니다. 또한, 기존 평가들을 검토하고 새로운 연구 방향을 제시하는 등 독창적인 연구 기여가 포함되어 있습니다."
Operationalizing machine learning models: a systematic literature review,https://doi.org/10.1145/3526073.3527584,"Deploying machine learning (ML) models to production with the same level of rigor and automation as traditional software systems has shown itself to be a non-trivial task, requiring extra care and infrastructure to deal with the additional challenges. Although many studies focus on adapting ML software engineering (SE) approaches and techniques, few studies have summarized the status and challenges of operationalizing ML models. Model operationalization encompasses all steps after model training and evaluation, including packaging the model in a format appropriate for deployment, publishing to a model registry or storage, integrating the model into a broader software system, serving, and monitoring. This study is the first systematic literature review investigating the techniques, tools, and infrastructures to operationalize ML models. After reviewing 24 primary studies, the results show that there are a number of tools for most use cases to operationalize ML models and cloud deployment in particular. The review also revealed several research opportunities, such as dynamic model-switching, continuous model-monitoring, and efficient edge ML deployments.",No,"본 논문은 머신러닝 모델 운영화에 관한 기존 연구들을 체계적으로 정리한 문헌 리뷰 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합 분석한 리뷰 논문에 해당합니다."
ML-Based Teaching Systems: A Conceptual Framework,https://dl.acm.org/doi/10.1145/3610197,"As the shortage of skilled workers continues to be a pressing issue, exacerbated by demographic change, it is becoming a critical challenge for organizations to preserve the knowledge of retiring experts and pass it on to novices. While this knowledge transfer has traditionally occurred through personal interaction, it lacks scalability and requires significant resources and time. IT-based teaching systems have addressed this scalability issue, but their development is still tedious and time-consuming. In this work, we investigate the potential of machine learning (ML) models to facilitate knowledge transfer in an organizational context, leading to more cost-effective IT-based teaching systems. Through a systematic literature review, we examine key concepts, themes, and dimensions to understand better and design ML-based teaching systems. To do so, we capture and consolidate the capabilities of ML models in IT-based teaching systems, inductively analyze relevant concepts in this context, and determine their interrelationships. We present our findings in the form of a review of the key concepts, themes, and dimensions to understand and inform on ML-based teaching systems. Building on these results, our work contributes to research on computer-supported cooperative work by conceptualizing how ML-based teaching systems can preserve expert knowledge and facilitate its transfer from SMEs to human novices. In this way, we shed light on this emerging subfield of human-computer interaction and serve to build an interdisciplinary research agenda.",No,"본 논문은 체계적 문헌 검토를 통해 ML 기반 교육 시스템의 개념적 틀을 제시하는 연구로, 직접적인 실험이나 새로운 데이터 분석 등 독창적인 연구 결과를 포함하지 않습니다. 따라서 기존 연구를 종합하고 개념을 정리하는 개념적 연구에 해당합니다."
A Literature Review of Automatic Traceability Links Recovery for Software Change Impact Analysis,https://doi.org/10.1145/3387904.3389251,"In large-scale software development projects, change impact analysis (CIA) plays an important role in controlling software design evolution. Identifying and accessing the effects of software changes using traceability links between various software artifacts is a common practice during the software development cycle. Recently, research in automated traceability-link recovery has received broad attention in the software maintenance community to reduce the manual maintenance cost of trace links by developers. In this study, we conducted a systematic literature review related to automatic traceability link recovery approaches with a focus on CIA. We identified 33 relevant studies and investigated the following aspects of CIA: traceability approaches, CIA sets, degrees of evaluation, trace direction and methods for recovering traceability link between artifacts of different types. Our review indicated that few traceability studies focused on designing and testing impact analysis sets, presumably due to the scarcity of datasets. Based on the findings, we urge further industrial case studies. Finally, we suggest developing traceability tools to support fully automatic traceability approaches, such as machine learning and deep learning.",No,"이 논문은 자동 추적성 링크 복구에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 중점을 두고 있습니다."
Supporting systematic literature reviews using deep-learning-based language models,https://doi.org/10.1145/3528588.3528658,"Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.",Yes,"본 논문은 딥러닝 기반 언어 모델을 활용하여 체계적 문헌 검토(SLR) 과정의 일부를 반자동화하는 새로운 방법론을 제안하고, 이를 실험적으로 평가한 연구 결과를 포함하고 있다. 따라서 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단된다."
Android Source Code Vulnerability Detection: A Systematic Literature Review,https://doi.org/10.1145/3556974,"The use of mobile devices is rising daily in this technological era. A continuous and increasing number of mobile applications are constantly offered on mobile marketplaces to fulfil the needs of smartphone users. Many Android applications do not address the security aspects appropriately. This is often due to a lack of automated mechanisms to identify, test, and fix source code vulnerabilities at the early stages of design and development. Therefore, the need to fix such issues at the initial stages rather than providing updates and patches to the published applications is widely recognized. Researchers have proposed several methods to improve the security of applications by detecting source code vulnerabilities and malicious codes. This Systematic Literature Review (SLR) focuses on Android application analysis and source code vulnerability detection methods and tools by critically evaluating 118 carefully selected technical studies published between 2016 and 2022. It highlights the advantages, disadvantages, applicability of the proposed techniques, and potential improvements of those studies. Both Machine Learning (ML)-based methods and conventional methods related to vulnerability detection are discussed while focusing more on ML-based methods, since many recent studies conducted experiments with ML. Therefore, this article aims to enable researchers to acquire in-depth knowledge in secure mobile application development while minimizing the vulnerabilities by applying ML methods. Furthermore, researchers can use the discussions and findings of this SLR to identify potential future research and development directions.",No,"이 논문은 118개의 기존 연구를 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하고 평가하는 데 중점을 둔 논문입니다."
"An Investigation of Smart Parking Tools, Technologies, &amp; Challenges",https://doi.org/10.1145/3436829.3436851,"Urbanization, exceptional increase in population and advancement in technology caused the automotive industry to grow rapidly & automobiles become essential part of daily life. Consequently, finding a parking space particularly in populous zones, is a challenging task. Researchers have proposed different solutions to assist the developments in smart parking systems. In this paper, we have investigated the key tools, techniques & challenges proposed in the recent research studies. Primarily, a Systematic Literature Review is carried out, total 35 studies are explored during time interval of (2015-2019). Subsequently, five major areas are recognized where smart parking is often functional i.e. Internet of Things (IoT) (13 studies), Cloud Computing (2 studies), Model-Driven Engineering (4 studies), Fog Computing (6 studies) and Artificial Intelligence (11 studies). Furthermore, (15) primary tools and (25) algorithms are presented. This article also portray the challenges cited by different studies. The findings of this study will definitely assist the practitioners while deciding the appropriate selections.",No,"본 논문은 스마트 주차 시스템 관련 기존 연구들을 체계적으로 문헌 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험, 모델 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합 분석한 조사 논문에 해당합니다."
Deep Learning in Smart Video Surveillance for Crowd Management: A Systematic Literature Review,https://doi.org/10.1145/3473141.3473240,"A crowd is defined as a gathering of people in the same premises. When the number of people exceeds normal conditions, overcrowding becomes a concern in safety and health-related matters due to the risks that a large crowd can impose on the individuals present in the area. Crowd analysis is a growing trend in computer vision related to the concerns in crowd monitoring. To alleviate risks related to crowds, intelligent techniques applied to surveillance are used to analyze a crowd and to monitor its density and the behavior of people captured in footage. Through a systematic literature review of various papers published in the last five years related to crowd analysis, the numerous deep learning algorithms applied in past researchers are presented and are assessed to come up with a solution that will further aid in crowd management",No,"본 논문은 최근 5년간 발표된 연구들을 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 평가하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 분석 및 정리에 해당합니다."
Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering,https://doi.org/10.1145/3626234,"Responsible Artificial Intelligence (RAI) is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of Artificial Intelligence (AI). Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. In addition, significant efforts have been placed at algorithm level rather than system level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize RAI from a system perspective, in this article, we present an RAI Pattern Catalogue based on the results of a multivocal literature review. Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The RAI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and RAI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement RAI.",Yes,"본 논문은 다중 음성 문헌 리뷰(multivocal literature review)를 통해 책임 있는 AI 구현을 위한 구체적인 패턴 카탈로그를 제시하며, 이는 기존 원칙이나 알고리즘 수준을 넘어 실무에서 적용 가능한 체계적이고 실행 가능한 지침을 제공하는 독창적인 연구 내용이다. 따라서 단순한 리뷰나 개념적 논의가 아닌 직접적인 연구 기여로 판단된다."
REVEALING CHALLENGES WITHIN THE APPLICATION OF MACHINE LEARNING SERVICES – A DELPHI STUDY,https://www.rintonpress.com/xjdi2/xjdi2-1/001-029.pdf,"Over the past years, Machine Learning has been applied to an increasing number of problems across numerous industries. However, the steady rise in the application of Machine Learning has not come without challenges since companies often lack the expertise or infrastructure to build their own Machine Learning systems. These challenges led to the emergence of a new paradigm, called Machine Learning as a Service. Scientific literature has mainly analyzed this topic in the context of platform solutions that provide ready-to-use environments for companies. We recently have developed a platform-independent approach and labeled it Machine Learning Services. The aim of the present study is to identify and evaluate challenges and opportunities in the application of Machine Learning Services. To do so, we conducted a Delphi Study with a panel of machine learning experts. The study consisted of three rounds and was structured according to the five steps of the Data Science Lifecycle. A variety of challenges from the areas “Communication”, “Environment”, “Approach”, “Data”, “Retraining, Testing, Monitoring and Updating”, “Model Training and Evaluation” were identified. Subsequently, the challenges revealed by the Delphi Study were compared with previous work on Machine Learning as a Service, which resulted from a structured literature review. The identified areas serve as possible future research fields and give further implications for practice. Alleviating communication issues and assessing the business IT infrastructure prior to the machine learning project are among the key findings of our study.",Yes,"본 논문은 델파이 연구 방법을 사용하여 머신러닝 서비스 적용 시의 도전 과제와 기회를 식별하고 평가하는 독창적인 연구를 수행하였으며, 이를 통해 새로운 연구 분야와 실무적 시사점을 제시하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Human Factors in the Design of Chatbot Interactions: Conversational Design Practices,https://doi.org/10.1145/3702038.3702083,"Context: Chatbots are intelligent agents that mimic human behavior to carry on meaningful conversations. The conversational nature of chatbots poses challenges to designers since their development is different from other software and requires investigating new practices in the context of human-AI interaction and their impact on user experience. Since human dialogue involves several variables beyond verbalizing words, it is vital to design well-thought dialogues for chatbots to provide a humanized and optimal interaction. Objective: The main objective of this work is to unveil textual, visual, or interactive design practices from text-based chatbot interactions and how they can potentiate or weaken some perceptions and feelings of users, such as satisfaction, engagement, and trust, for the creation of the Guidelines for Chatbot Conversational Design (GCCD) guide. Method: We used multiple research methods to generate and validate the guide. First, we conducted a Systematic Literature Review (SRL) to identify conversational design practices and their impacts. These practices were inserted into the GCCD guide through qualitative analysis and coding of SLR results. Then, the guide was validated quantitatively through a survey and qualitatively through a case study. The survey aimed to assess the guide’s clarity and usefulness based on the reading of the guide by the participants and their responses to a questionnaire adapted from the Technology Acceptance Model. The case study aimed to assess the guide’s usefulness based on its practical application by participants in a situation that simulates a real scenario and follow-up interviews. Results: The survey showed that software developers with different levels of experience strongly agreed that the guide could induce greater user satisfaction and engagement. Furthermore, they also strongly agreed that the guide is clear, understandable, flexible, and easy to use. Although participants suggested some improvements, they reported that the guide’s main strengths are objectivity and clarity. The case study confirmed the survey findings, as participants reported positive feelings toward the guide and an intention to use it. Their extensive perceptions given through the conducted interviews unveiled that their previous experiences with chatbots and in specific software development positions influenced their design and adoption of practices. Conclusion: The guide proved to be useful for developers with different levels of knowledge, with the potential to become a strong ally for developers in the conversational design process.",Yes,"본 논문은 체계적 문헌고찰, 정성적 분석, 설문조사, 사례 연구 등 다양한 연구 방법을 사용하여 챗봇 대화 설계 가이드라인을 개발하고 검증하는 독창적인 연구를 수행하였다. 이는 기존 연구를 종합하는 데 그치지 않고 새로운 가이드라인을 제안하고 실증적으로 평가한 연구 논문임을 보여준다."
Using machine learning to generate test oracles: a systematic literature review,https://doi.org/10.1145/3472675.3473974,"Machine learning may enable the automated generation of test oracles. We have characterized emerging research in this area through a systematic literature review examining oracle types, researcher goals, the ML techniques applied, how the generation process was assessed, and the open research challenges in this emerging field. Based on a sample of 22 relevant studies, we observed that ML algorithms generated test verdict, metamorphic relation, and---most commonly---expected output oracles. Almost all studies employ a supervised or semi-supervised approach, trained on labeled system executions or code metadata---including neural networks, support vector machines, adaptive boosting, and decision trees. Oracles are evaluated using the mutation score, correct classifications, accuracy, and ROC. Work-to-date show great promise, but there are significant open challenges regarding the requirements imposed on training data, the complexity of modeled functions, the ML algorithms employed---and how they are applied---the benchmarks used by researchers, and replicability of the studies. We hope that our findings will serve as a roadmap and inspiration for researchers in this field.",No,이 논문은 머신러닝을 이용한 테스트 오라클 생성에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들의 분석과 정리를 주된 내용으로 하고 있어 연구 논문에 해당하지 않는다.
Informing Age-Appropriate AI: Examining Principles and Practices of AI for Children,https://doi.org/10.1145/3491102.3502057,"AI systems are becoming increasingly pervasive within children’s devices, apps, and services. However, it is not yet well-understood how risks and ethical considerations of AI relate to children. This paper makes three contributions to this area: first, it identifies ten areas of alignment between general AI frameworks and codes for age-appropriate design for children. Then, to understand how such principles relate to real application contexts, we conducted a landscape analysis of children’s AI systems, via a systematic literature review including 188 papers. This analysis revealed a wide assortment of applications, and that most systems’ designs addressed only a small subset of principles among those we identified. Finally, we synthesised our findings in a framework to inform a new “Code for Age-Appropriate AI”, which aims to provide timely input to emerging policies and standards, and inspire increased interactions between the AI and child-computer interaction communities.",Yes,"본 논문은 체계적인 문헌 리뷰를 통해 188편의 논문을 분석하고, AI와 아동 관련 원칙들을 도출하여 새로운 프레임워크를 제안하는 등 독창적인 연구 방법과 결과를 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Explainable Agents and Robots: Results from a Systematic Literature Review,https://dl.acm.org/doi/10.5555/3306127.3331806,"Humans are increasingly relying on complex systems that heavily adopts Artificial Intelligence (AI) techniques. Such systems are employed in a growing number of domains, and making them explainable is an impelling priority. Recently, the domain of eXplainable Artificial Intelligence (XAI) emerged with the aims of fostering transparency and trustworthiness. Several reviews have been conducted. Nevertheless, most of them deal with data-driven XAI to overcome the opaqueness of black-box algorithms. Contributions addressing goal-driven XAI (e.g., explainable agency for robots and agents) are still missing. This paper aims at filling this gap, proposing a Systematic Literature Review. The main findings are (i) a considerable portion of the papers propose conceptual studies, or lack evaluations or tackle relatively simple scenarios; (ii) almost all of the studied papers deal with robots/agents explaining their behaviors to the human users, and very few works addressed inter-robot (inter-agent) explainability. Finally, (iii) while providing explanations to non-expert users has been outlined as a necessity, only a few works addressed the issues of personalization and context-awareness.",No,"본 논문은 체계적 문헌 리뷰(Systematic Literature Review)를 수행하여 기존 연구들을 종합하고 분석하는 데 중점을 두고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당합니다."
Six usable privacy heuristics,https://doi.org/10.1145/3638067.3638111,"Enhancing privacy policy interfaces is crucial for improving users’ trust in technology and ensuring compliance with legislation. This thesis focused on developing usable interfaces that enable laypeople to protect their online privacy. Through a comprehensive analysis, including literature review, thematic and cluster analysis, and empirical evaluation, six usable privacy heuristics (push#) are established. These heuristics effectively identify catastrophic problems in privacy policy interfaces for laypeople. Additionally, preliminary usable privacy guidelines (pug#) are created, and a new process for developing usability criteria is proposed. Future research directions are suggested, including the application of these heuristics and guidelines to domains like human-robot interaction and human-artificial intelligence interaction.",Yes,"논문은 문헌 검토, 주제 및 군집 분석, 실증 평가를 통해 여섯 가지 사용성 프라이버시 휴리스틱을 개발하고, 새로운 사용성 기준 개발 프로세스를 제안하는 등 독창적인 연구 내용을 포함하고 있다. 이는 직접적인 연구 기여로 판단된다."
A Review of Intrusion Detection Research Based on Deep Learning,https://doi.org/10.1145/3654446.3654452,"In recent years, the network security situation has become critical. A key component of network security is intrusion detection. With the continuous development of deep learning technology, more and more deep learning methods are being used in intrusion detection systems. The article describes the latest research on deep learning-based network intrusion detection based on an extensive literature review. First, a brief overview of the current security landscape of networks and various intrusion detection systems is given. Next, we introduce datasets and evaluation metrics commonly used in deep learning-based intrusion detection systems. Next, deep learning models commonly used in network intrusion detection systems are summarized. Finally, seven intrusion detection models are discussed and summarized and future research directions are suggested.",No,"본 논문은 딥러닝 기반 침입 탐지 연구에 대한 광범위한 문헌 리뷰를 제공하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 정리하는 데 중점을 두고 있습니다."
A Survey of Trustworthy Representation Learning Across Domains,https://doi.org/10.1145/3657301,"As AI systems have obtained significant performance to be deployed widely in our daily lives and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts of AI systems, and representation learning is the fundamental technology in machine learning. How to make representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.",No,이 논문은 신뢰할 수 있는 표현 학습에 관한 기존 연구들을 종합적으로 정리하고 분석하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들을 리뷰하는 데 중점을 두고 있습니다.
The Principles of Digital Transformation on China's Street Vendor Economy,https://doi.org/10.1145/3675417.3675437,"One of the major components contributing to the city's economy is the Chinese street vendor economy. It also provides a large proportion of employment in China. In the era of digital technology and especially artificial intelligence, their integration into society has completely transformed the street vending sector in today's market. This study aims to explore different dimensions of how digital transformation has affected China's street vendor industry. This paper examines an emerging trend in street vending transformation by exploring the integration of artificial intelligence with other technological advancements, including mobile payments, big data analysis, and digital marketing platforms. The study aims to reveal the impact of these developments on the Chinese market. The research methodology combines a comprehensive literature review, case studies of successful digital application that support or hinder the digitalization of street vending. The paper explores the socio-economic influence of these technological applications, including their potential to foster sustainable urban development. This study also discusses the challenges posed by digital transformation. Being aware of those issues, such as the digital divide, data privacy concerns, and the need for regulatory adaptation to accommodate new forms of economic activity, help us better improve it. The findings of this research are anticipated to offer valuable insights for street vendors, urban policymakers, and technology providers. By understanding those transformative potential of AI and related technologies, the author proposes some principles for the digitalization of current vendor economy in China.",Yes,논문은 디지털 전환이 중국 노점상 경제에 미치는 영향을 탐구하기 위해 문헌 검토와 사례 연구를 포함한 연구 방법론을 사용하여 독창적인 분석과 통찰을 제공하고 있다. 이는 직접 기여하는 연구 내용이 포함된 연구 논문임을 나타낸다.
Literature Review Profiles of Specialization in Education and Profession as the basis for the development of Artificial Intelligence Website,https://doi.org/10.1145/3508259.3508296,"Profiles of specialization in education and profession are currently heavily influenced by the development of industry 4.0. The era of disruption is marked by massive changes due to innovations that change business systems and arrangements to newer levels. In various studies, there will be many professions that are extinct but there will also be many new professions that will be born. This study aims to conduct a literature review of profiles of specialization in education and profession that are relevant to industry 4.0 trends which will then be developed in a measurement through an artificial intelligence-based website. The research method used is a bibliometric approach through scientific studies of various library sources using VOSViewer with cartographic overlay techniques and density visualization maps, to represent sequences and their relationships. While the method used to develop website profiles of specialization in education and profession based on artificial intelligence is to use the waterfall method. The results of the literature review found 10 words with a high level of bibliometric correlation as the basis for developing artificial intelligence-based websites.",No,"본 논문은 기존 문헌을 검토하는 문헌 리뷰 연구에 초점을 맞추고 있으며, 직접적인 실험이나 새로운 데이터 수집을 통한 독창적인 연구 결과를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰 논문에 해당합니다."
A Reproducibility and Generalizability Study of Large Language Models for Query Generation,https://doi.org/10.1145/3673791.3698432,"Systematic literature reviews (SLRs) are a cornerstone of academic research, yet they are often labour-intensive and time-consuming due to the detailed literature curation process. The advent of generative AI and large language models (LLMs) promises to revolutionize this process by assisting researchers in several tedious tasks, one of them being the generation of effective Boolean queries that will select the publications to consider including in a review. This paper presents an extensive study of Boolean query generation using LLMs for systematic reviews, reproducing and extending the work of Wang et al. and Alaniz et al. Our study investigates the replicability and reliability of results achieved using ChatGPT and compares its performance with open-source alternatives like Mistral and Zephyr to provide a more comprehensive analysis of LLMs for query generation. Therefore, we implemented a pipeline, which automatically creates a Boolean query for a given review topic by using a previously defined LLM, retrieves all documents for this query from the PubMed database and then evaluates the results. With this pipeline we first assess whether the results obtained using ChatGPT for query generation are reproducible and consistent. We then generalize our results by analyzing and evaluating open-source models and evaluating their efficacy in generating Boolean queries. Finally, we conduct a failure analysis to identify and discuss the limitations and shortcomings of using LLMs for Boolean query generation. This examination helps to understand the gaps and potential areas for improvement in the application of LLMs to information retrieval tasks. Our findings highlight the strengths, limitations, and potential of LLMs in the domain of information retrieval and literature review automation. Our code is available online.",Yes,"본 논문은 기존 연구를 재현하고 확장하며, ChatGPT와 오픈소스 LLM들의 Boolean 쿼리 생성 성능을 비교 평가하는 독창적인 실험과 분석을 수행하고 있다. 또한, 실패 분석을 통해 LLM의 한계와 개선점을 도출하는 등 직접적인 연구 기여가 포함되어 있다."
The application of “5G+AI” information technology in college English teaching,https://doi.org/10.1145/3660043.3660200,"With the rapid development of information technology, the widespread implementation of 5G and artificial intelligence (AI) technology has become evident across various fields. This study aims to explore the utilization of ""5G+AI"" information technology in college English teaching, analyze its advantages and obstacles, and provide appropriate recommendations. It emphasizes three key benefits of this integration: providing access to a vast array of learning resources, delivering personalized learning experiences, and facilitating efficient teaching evaluation. Through extensive literature review and comprehensive case studies analysis, it has been observed that the integration of ""5G+AI"" technology presents augmented learning resources, customized educational experiences, and streamlined teaching evaluations, all of which significantly contribute to improving college English instruction. However, the application of this technology also faces challenges such as privacy protection, network security, and technological costs. Therefore, educational departments and institutions should strengthen teacher training and support by formulating relevant policies and standards to foster the extensive integration of ""5G+AI"" information technology in college English instruction.",No,"본 논문은 5G와 AI 기술의 대학 영어 교육 적용에 대한 문헌 고찰과 사례 분석을 통해 장점과 문제점을 논의하고 있으며, 직접적인 실험이나 새로운 연구 결과를 제시하지 않는다. 따라서 독창적인 연구 내용보다는 기존 연구의 종합 및 제언에 초점이 맞춰져 있다."
A Systematic Review of Ethical Concerns with Voice Assistants,https://doi.org/10.1145/3600211.3604679,"Since Siri’s release in 2011 there have been a growing number of AI-driven domestic voice assistants that are increasingly being integrated into devices such as smartphones and TVs. But as their presence has expanded, a range of ethical concerns have been identified around the use of voice assistants, such as the privacy implications of having devices that are always listening and the ways that these devices are integrated into the existing social order of the home. This has created a burgeoning area of research across a range of fields including computer science, social science, and psychology. This paper takes stock of the foundations and frontiers of this work through a systematic literature review of 117 papers on ethical concerns with voice assistants. In addition to analysis of nine specific areas of concern, the review measures the distribution of methods and participant demographics across the literature. We show how some concerns, such as privacy, are operationalized to a much greater extent than others like accessibility, and how study participants are overwhelmingly drawn from a small handful of Western nations. In so doing we hope to provide an outline of the rich tapestry of work around these concerns and highlight areas where current research efforts are lacking.",No,"이 논문은 117편의 기존 연구를 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과를 제시하기보다는 기존 연구들을 종합하고 분석하는 데 초점을 맞추고 있습니다. 따라서 새로운 실험이나 데이터 수집을 통한 직접 기여 연구 논문으로 보기 어렵습니다."
Ethics of AI: A Systematic Literature Review of Principles and Challenges,https://doi.org/10.1145/3530019.3531329,"Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers, and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assesses the ethical capabilities of AI systems and provides best practices for further improvements.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 종합하고 분석한 결과를 제시하고 있으며, 직접적인 독창적 연구나 실험 결과를 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 해석에 초점을 맞추고 있습니다."
A Human-Centered Systematic Literature Review of Cyberbullying Detection Algorithms,https://doi.org/10.1145/3476066,"Cyberbullying is a growing problem across social media platforms, inflicting short and long-lasting effects on victims. To mitigate this problem, research has looked into building automated systems, powered by machine learning, to detect cyberbullying incidents, or the involved actors like victims and perpetrators. In the past, systematic reviews have examined the approaches within this growing body of work, but with a focus on the computational aspects of the technical innovation, feature engineering, or performance optimization, without centering around the roles, beliefs, desires, or expectations of humans. In this paper, we present a human-centered systematic literature review of the past 10 years of research on automated cyberbullying detection. We analyzed 56 papers based on a three-prong human-centeredness algorithm design framework - spanning theoretical, participatory, and speculative design. We found that the past literature fell short of incorporating human-centeredness across multiple aspects, ranging from defining cyberbullying, establishing the ground truth in data annotation, evaluating the performance of the detection models, to speculating the usage and users of the models, including potential harms and negative consequences. Given the sensitivities of the cyberbullying experience and the deep ramifications cyberbullying incidents bear on the involved actors, we discuss takeaways on how incorporating human-centeredness in future research can aid with developing detection systems that are more practical, useful, and tuned to the diverse needs and contexts of the stakeholders.",No,"이 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 알고리즘 개발을 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합하고 분석하는 리뷰 논문에 해당합니다."
The Design and Observed Effects of Robot-performed Manual Gestures: A Systematic Review,https://doi.org/10.1145/3549530,"Communication using manual (hand) gestures is considered a defining property of social robots, and their physical embodiment and presence, therefore, we see a need for a comprehensive overview of the state-of-the-art in social robots that use gestures. This systematic literature review aims to address this need by (1) describing the gesture production process of a social robot, including the design and planning steps, and (2) providing a survey of the effects of robot-performed gestures on human-robot interactions in a multitude of domains. We identify patterns and themes from the existing body of literature, resulting in nine outstanding questions for research on robot-performed gestures regarding: developments in sensor technology and AI, structuring the gesture design and evaluation process, the relationship between physical appearance and gestures, the effects of planning on the overall interaction, standardizing measurements of gesture “quality,” individual differences, gesture mirroring, whether human-likeness is desirable, and universal accessibility of robots. We also reflect on current methodological practices in studies of robot-performed gestures and suggest improvements regarding replicability, external validity, measurement instruments used, and connections with other disciplines. These outstanding questions and methodological suggestions can guide future work in this field of research.",No,"이 논문은 기존 연구들을 체계적으로 검토하고 요약하는 문헌 리뷰(Systematic Review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 향후 연구 방향 제시에 초점이 맞춰져 있습니다."
AI-Powered Digital Media Platform and Its Applications,https://doi.org/10.1145/3433996.3434018,"Previously, the main method to create digital media such as animation was by humans. With the development of artificial intelligence technology, the work of making media content from raw materials is gradually replaced by computers. In this paper, we conceive a digital media production and interaction platform supported by artificial intelligence, based on some existing artificial intelligence and virtual reality display technologies. We also discuss the feasibility of some possible application scenarios for this platform.Based on our literature review and feasibility analysis, we conclude that current technology development is capable of supporting the AI-powered digital media platform that we propose in this paper.",No,"논문 초록은 기존 기술을 바탕으로 AI 기반 디지털 미디어 플랫폼을 구상하고 가능성을 논의하는 내용에 집중되어 있으며, 직접적인 실험 결과나 새로운 연구 방법론, 독창적인 연구 기여가 명확히 제시되어 있지 않습니다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Intelligent Food Recommendation Framework Based on Social Media Behavioral Data,https://doi.org/10.1145/3660853.3660883,"This research introduces an Intelligent Food Recommendation Framework leveraging social media Behavioral Data, explicitly focusing on Instagram. It posits that users' engagements with food-related content on Instagram offer insights into their food preferences, thereby enhancing the precision of food type recommendations. The investigation aims to substantiate this assertion through a thorough literature review, data source selection, comparison of food assessment surveys, exploration of image recognition methodologies, and deployment and validation of the proposed solution framework utilizing deep learning models. Through interdisciplinary efforts, the study seeks to advance personalized recommendation systems in the context of food consumption.",Yes,"논문 초록에서 제안된 프레임워크의 개발과 검증, 딥러닝 모델을 활용한 솔루션 배포 등 직접적인 연구 수행과 실험적 기여가 명확히 드러나 있어 독창적인 연구 내용이 포함된 연구 논문으로 판단된다."
Research on Artistic Design Methods for Chinese Traditional Font Elements Based on AIGC Technology,https://doi.org/10.1145/3677892.3677966,"This study explores the integration of traditional Chinese font elements with Artificial Intelligence Generated Content technology, aiming to develop innovative design workflow. The research investigates three key aspects: analyzing the visual features and cultural significance of traditional Chinese fonts, exploring the application of AIGC technology in font art design, and proposing an artistic design workflow based on the Stable Diffusion algorithm. The methodology involves: (1) conduct a literature review to study the background of AIGC technology, (2) establishing a project direction, (3) exploring traditional font elements through interdisciplinary collaboration, (4) creating font prototypes, (5) utilizing a locally deployed Stable Diffusion model to generate stylized font designs. (6) Evaluates the practical effects of these designs, highlighting both the advantages and limitations of the design workflow. The results showed that the design methodology workflow largely met the expected outcomes in cultural relevance, diversity, practicality, and creativity. However, the stability rating was lower, primarily due to Stable Diffusion's current limitations in recognizing traditional Chinese elements, leading to instability in generating details of Chinese traditional font elements. This issue was particularly evident in Chinese ink painting styles, brushstroke fonts, and color rendering, highlighting the necessity for further training in these areas.",Yes,"본 논문은 AIGC 기술을 활용한 전통 중국 서체 요소의 예술적 디자인 방법을 제안하고, Stable Diffusion 알고리즘 기반의 디자인 워크플로우를 개발 및 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 실험 결과와 한계점 분석을 통해 실질적인 기여를 하고 있어 연구 논문에 해당한다."
"Corrigimus, verificamus, vincimus: Ensuring algorithmic accuracy in an age of uncertainty",https://doi.org/10.1145/3666000.3672621,"For nearly as long as it has been developing fast heuristic, approximate, and randomized algorithms, the computer algebra community has also been keen to produce methods by which we can verify accuracy and even correct a small number of errors. These routines are much more efficient than trivially recomputing the result, but are often themselves randomized and can be wrong with controllably small probability. Nonetheless, provable probabilistic correctness is useful when running code on unreliable or cloud-based hardware, or when the original computation relies on unproven heuristics. The latter case may be increasingly relevant in the coming years as the code produced by generative AI models continues to improve in quality and inevitably makes its way into production. We will examine a few recent methods for interactive verification and error correction for some basic problems in linear algebra, pointing out connections and differences to related work from the coding theory and applied cryptography communities.",Yes,"논문 초록에서 최근 방법들을 검토하고, 알고리즘 정확성 보장 및 오류 수정에 관한 새로운 기법들을 다루고 있어 독창적인 연구 내용을 포함하고 있음을 알 수 있습니다. 또한, 관련 분야와의 연결점 및 차이점을 분석하는 점에서 직접적인 연구 기여가 있다고 판단됩니다."
A Survey of Threats to Research Literature-dependent Medical AI Solutions,https://doi.org/10.1145/3592597,"Medical Artificial Intelligence (MedAI) harnesses the power of medical research through AI algorithms and vast data to address healthcare challenges. The security, integrity, and credibility of MedAI tools are paramount, because human lives are at stake. Predatory research, in a culture of “publish or perish,” is exploiting the “pay for publish” model to infiltrate he research literature repositories. Although, it is challenging to measure the actual predatory research induced data pollution and patient harm, our work shows that the breached integrity of MedAI inputs is a serious threat to trust the MedAI output. We review a wide range of research literature discussing the threats of data pollution in the research literature, feasible attacks impacting MedAI solutions, research literature-based tools, and influence on healthcare. Our contribution lies in presenting a comprehensive literature review, addressing the gap of predatory research vulnerabilities affecting MedAI solutions, and helping to develop robust MedAI solutions in the future.",No,본 논문은 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구 문헌을 종합적으로 검토하는 서베이 논문입니다. 따라서 독창적인 연구 내용보다는 문헌 리뷰와 분석에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Unmasking Data Secrets: An Empirical Investigation into Data Smells and Their Impact on Data Quality,https://doi.org/10.1145/3644815.3644960,"Artificial Intelligence (AI) is rapidly advancing with a data-centered approach suitable for various domains. Nevertheless, AI faces significant challenges, particularly in data quality. Data collection from diverse sources can introduce quality issues that may threaten the development of AI-enabled systems. A growing concern in this context is the emergence of data smells - issues specific to the data used in building AI models, which can have long-term consequences. In this paper, we aim at enlarging the current body of knowledge on data smells, by proposing a two-step investigation into the matter. First, we updated an existing literature review in an effort of cataloguing the currently existing data smells and the tools to detect them. Afterward, we assess the prevalence of data smells and their correlation with data quality metrics. We identify a novel set composed of 12 data smells distributed across three additional categories. Secondly, we observe that the correlation between data smells and data quality is notably impactful, exhibiting a pronounced and substantial effect, especially in highly diffused data smell instances. This research sheds light on the complex relationship between data smells and data quality, providing valuable insights into the challenges of maintaining AI-enabled systems.",Yes,"논문은 기존 문헌 리뷰를 업데이트하고 새로운 데이터 냄새(data smells)를 식별하는 독창적인 연구를 수행하며, 데이터 냄새와 데이터 품질 간의 상관관계를 실증적으로 분석하는 직접적인 연구 기여를 포함하고 있다. 따라서 연구 논문에 해당한다."
Bridging Multimedia Modalities: Enhanced Multimodal AI Understanding and Intelligent Agents,https://doi.org/10.1145/3577190.3614225,"With the increasing availability of multimodal data, especially in the sports and medical domains, there is growing interest in developing Artificial Intelligence (AI) models capable of comprehending the world in a more holistic manner. Nevertheless, various challenges exist in multimodal understanding, including the integration of multiple modalities and the resolution of semantic gaps between them. The proposed research aims to leverage multiple input modalities for the multimodal understanding of AI models, enhancing their reasoning, generation, and intelligent behavior. The research objectives focus on developing novel methods for multimodal AI, integrating them into conversational agents with optimizations for domain-specific requirements. The research methodology encompasses literature review, data curation, model development and implementation, evaluation and performance analysis, domain-specific applications, and documentation and reporting. Ethical considerations will be thoroughly addressed, and a comprehensive research plan is outlined to provide guidance. The research contributes to the field of multimodal AI understanding and the advancement of sophisticated AI systems by experimenting with multimodal data to enhance the performance of state-of-the-art neural networks.",Yes,"논문 초록에서 새로운 멀티모달 AI 모델 개발, 구현, 평가 및 성능 분석을 포함한 구체적인 연구 방법론과 실험적 기여가 명시되어 있어 독창적인 연구 내용을 포함한 연구 논문임을 알 수 있습니다. 또한, 도메인 특화 최적화와 윤리적 고려사항도 다루고 있어 실제 연구 수행을 기반으로 한 논문임이 분명합니다."
GANs in the Panorama of Synthetic Data Generation Methods,https://doi.org/10.1145/3657294,"This article focuses on the creation and evaluation of synthetic data to address the challenges of imbalanced datasets in machine learning (ML) applications, using fake news detection as a case study. We conducted a thorough literature review on generative adversarial networks (GANs) for tabular data, synthetic data generation methods, and synthetic data quality assessment. By augmenting a public news dataset with synthetic data generated by different GAN architectures, we demonstrate the potential of synthetic data to improve ML models’ performance in fake news detection. Our results show a significant improvement in classification performance, especially in the underrepresented class. We also modify and extend a data usage approach to evaluate the quality of synthetic data and investigate the relationship between synthetic data quality and data augmentation performance in classification tasks. We found a positive correlation between synthetic data quality and performance in the underrepresented class, highlighting the importance of high-quality synthetic data for effective data augmentation.",Yes,본 논문은 GAN을 활용한 합성 데이터 생성 및 평가를 통해 가짜 뉴스 탐지 성능 향상에 직접 기여하는 실험적 연구를 수행하고 있습니다. 또한 합성 데이터 품질과 분류 성능 간의 관계를 분석하는 등 독창적인 연구 내용을 포함하고 있어 연구 논문에 해당합니다.
Formalizing Multimedia Recommendation through Multimodal Deep Learning,https://doi.org/10.1145/3662738,"Recommender systems (RSs) provide customers with a personalized navigation experience within the vast catalogs of products and services offered on popular online platforms. Despite the substantial success of traditional RSs, recommendation remains a highly challenging task, especially in specific scenarios and domains. For example, human affinity for items described through multimedia content (e.g., images, audio, and text), such as fashion products, movies, and music, is multi-faceted and primarily driven by their diverse characteristics. Therefore, by leveraging all available signals in such scenarios, multimodality enables us to tap into richer information sources and construct more refined user/item profiles for recommendations. Despite the growing number of multimodal techniques proposed for multimedia recommendation, the existing literature lacks a shared and universal schema for modeling and solving the recommendation problem through the lens of multimodality. Given the recent advances in multimodal deep learning for other tasks and scenarios where precise theoretical and applicative procedures exist, we also consider it imperative to formalize a general multimodal schema for multimedia recommendation. In this work, we first provide a comprehensive literature review of multimodal approaches for multimedia recommendation from the last eight years. Second, we outline the theoretical foundations of a multimodal pipeline for multimedia recommendation by identifying and formally organizing recurring solutions/patterns; at the same time, we demonstrate its rationale by conceptually applying it to selected state-of-the-art approaches in multimedia recommendation. Third, we conduct a benchmarking analysis of recent algorithms for multimedia recommendation within Elliot, a rigorous framework for evaluating recommender systems, where we re-implement such multimedia recommendation approaches. Finally, we highlight the significant unresolved challenges in multimodal deep learning for multimedia recommendation and suggest possible avenues for addressing them. The primary aim of this work is to provide guidelines for designing and implementing the next generation of multimodal approaches in multimedia recommendation.",Yes,"논문은 멀티모달 딥러닝을 활용한 멀티미디어 추천 시스템에 대한 이론적 정립, 문헌 리뷰, 알고리즘 재구현 및 벤치마킹 분석을 포함하고 있어 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단된다. 또한, 향후 연구 방향과 해결 과제도 제시하여 직접적인 연구 기여를 하고 있다."
A Survey on Collaborative Learning for Intelligent Autonomous Systems,https://doi.org/10.1145/3625544,"This survey examines approaches to promote Collaborative Learning in distributed systems for emergent Intelligent Autonomous Systems (IAS). The study involves a literature review of Intelligent Autonomous Systems based on Collaborative Learning, analyzing aspects in four dimensions: computing environment, performance concerns, system management, and privacy concerns, mapping the significant requirements of systems to the emerging Artificial intelligence models. Furthermore, the article explores Collaborative Learning Taxonomy for IAS to demonstrate the correlation between IoT, Big Data, and Human-in-the-Loop. Several technological open issues exist in the aforementioned domains (such as in applications of autonomous driving, robotics in healthcare, cyber security, and others) to effectively achieve the future deployment of Intelligent Autonomous Systems. This Survey aims to organize concepts around IAS, indicating the approaches used to extract knowledge from data in Collaborative Learning for IAS, and identifying open issues. Moreover, it presents a guide to overcoming the existing challenges in decision-making mechanisms with IAS, providing a holistic vision of Big Data and Human-in-the-Loop.",No,"본 논문은 기존 연구들을 종합하여 Collaborative Learning과 Intelligent Autonomous Systems에 관한 개념과 기술적 이슈를 정리한 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않고, 문헌 리뷰와 분류 체계 제시에 중점을 두고 있습니다."
"Educational Technology in the Post-Pandemic Era: Current Progress, Potential, and Challenges",https://doi.org/10.1145/3629296.3629303,"Information technology has been significantly developed during the past decades. Teachers leverage various technologies to enhance teaching experience and knowledge delivery. Students explore diverse knowledge from numerous learning resources on the Web. Furthermore, the COVID-19 pandemic accelerates processing technology integration in education. During the pandemic, Zoom and Google Classroom were widely used for course content delivery, and they are still presenting their values in the post-pandemic era. Some other applications are also continuously adopted when in-person teaching and learning have been resumed. In this paper, a literature review-based approach is applied to summarize recent achievements in educational technology and present several promising directions for the future of education, including artificial intelligence-powered content generation and metaverse. A roadmap is proposed to illustrate the current and future progress of education technology. Meanwhile, safeguards are also provided to guide schools and parents to be aware of the potential hazards during technology use at the K-12 education level.",No,"본 논문은 문헌 리뷰를 기반으로 최근 교육 기술의 발전 현황과 미래 방향을 요약하고 제안하는 내용으로, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 전망에 초점을 맞추고 있습니다."
Study of sustainable supply chain management in the healthcare industry,https://doi.org/10.1145/3625469.3625480,"Objective -The Objective of this research is to identify the key attributes of a sustainable health care supply chain. This paper presents a new, four-dimensional model, consisting of social, economic, environmental and technological dimensions. Design/Methodology/Approach-The proposed research methodology involved four independent variables leading to an efficient, sustainable SCM framework for the healthcare industry, which were identified based on the research gaps, solidly supported by latest quality literature review. The data are gathered using a Likert scale questionnaire together with a scheduled interview. The study adopted a quantitative survey approach to randomly collect and analyzed the primary data from a large sample of 400 supply chain and health care executives and was analyzed statistically using Adanco 2.3.2. The findings were further analyzed to search for areas that necessitate research to understand a sustainable SCM framework to gain a competitive edge and operational efficiency, reduce costs and improve profitability with societal impact on socio-economic, environmental factors and technological factors. Quantitative analysis tools include reliability indicators, convergent validity, discriminant validity, multicollinearity etc. All are supported by structural equation model (SEM) justifying the hypothesis formulated. Findings & Research Outcome- The goals of green supply chain management are to eliminate or minimize waste, reduce environmental impacts (air and water pollution) and avoid wasting resources. It has a significant positive outcome on costs, waste disposal, resource consumption and greenhouse emissions. Technology was found to have a positive outcome on supply chain management through customized information technology, use of block chain, Artificial intelligence (AI) and machine learning, smart practices implementation, sophisticated IT applications and user-friendly smartphone interface. The use of technology improves automation, reduces counterfeit medicines and ensures secure data storage. Social dimension was found to have positive outcome on sustainable supply management through perceived value preposition, customer satisfaction index, informational quality, fair operating practices and Involvement and relationship level, which are measured by outcome of achievement of social sustainability.",Yes,"논문은 설문조사와 통계적 분석(SEM)을 통해 지속 가능한 의료 공급망 관리에 관한 새로운 4차원 모델을 제안하고 검증하는 독창적인 연구를 수행하고 있다. 이는 기존 문헌 검토를 바탕으로 한 실증적 연구로, 직접적인 연구 기여가 포함된 연구 논문에 해당한다."
A Review of Machine Learning and Meta-heuristic Methods for Scheduling Parallel Computing Systems,https://doi.org/10.1145/3230905.3230906,"Optimized software execution on parallel computing systems demands consideration of many parameters at run-time. Determining the optimal set of parameters in a given execution context is a complex task, and therefore to address this issue researchers have proposed different approaches that use heuristic search or machine learning. In this paper, we undertake a systematic literature review to aggregate, analyze and classify the existing software optimization methods for parallel computing systems. We review approaches that use machine learning or meta-heuristics for scheduling parallel computing systems. Additionally, we discuss challenges and future research directions. The results of this study may help to better understand the state-of-the-art techniques that use machine learning and meta-heuristics to deal with the complexity of scheduling parallel computing systems. Furthermore, it may aid in understanding the limitations of existing approaches and identification of areas for improvement.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 분류하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 방법론을 제시하지 않습니다. 따라서 연구 논문보다는 문헌 종합 및 분석에 초점을 맞춘 리뷰 논문에 해당합니다."
Machine Learning for Detecting Data Exfiltration: A Review,https://doi.org/10.1145/3442181,"Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.",No,"이 논문은 머신러닝 기반 데이터 유출 탐지 기법에 대한 체계적인 문헌 리뷰를 수행한 것으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구들을 분류하고 분석하는 데 초점을 맞추고 있습니다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당합니다."
A Systematic Review of Trust Assessments in Human–Robot Interaction,https://doi.org/10.1145/3706123,"The integration of robots into daily life has increased significantly, spanning applications from social-care to industrial settings with collaborative robots. Ensuring a safe, secure environment and equitable workload distribution in human-robot collaborations is crucial. Trust is a key factor in these environments, essential for enhancing collaboration and achieving tasks while maintaining safety. Under-trusting robots can hinder productivity, while over-trusting them can lead to accidents. A comprehensive literature review of 100 publications from 2003 to 2023 analyzes trust and its influencing factors in industrial and social-care contexts. Findings reveal that in industrial settings, robot-related factors are more influential, whereas in social-care, human and environmental factors play a significant role. Furthermore, the review delves into gauging trust through observable behavior, while also exploring various trust evaluation methodologies. Results show that trust can be gauged through human behaviors, physical cues, and physiological measurements. Concerning trust evaluation methodologies, traditional questionnaires have limitations, opening new opportunities for machine learning and sensor-based approaches to real-time trust evaluation, as trust is a dynamic cognitive value that evolves over time. Notably, 97% of the reviewed articles were published in the last decade, underscoring a growing interest in human–robot interaction and trust within the scientific community.",No,"본 논문은 100편의 기존 연구를 체계적으로 검토한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 기존 연구들을 종합하여 분석한 내용에 초점을 맞추고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 동향과 결과를 정리하는 데 목적이 있습니다."
A Systematic Review on Anomaly Detection for Cloud Computing Environments,https://doi.org/10.1145/3442536.3442550,"The detection of anomalies in data is a far-reaching field of research which also applies to the field of cloud computing in several different ways: from the detection of various types of intrusions to the detection of hardware failures, many publications address how far anomaly detection methods are able to meet the specific requirements of a cloud-based network. Since there is still no comprehensive overview of this constantly growing field of research, this literature review provides a systematic evaluation of 215 publications that can be considered as representative for the last ten years of this scientific development. Our analysis identifies three main methodological areas (machine learning, deep learning, statistical approaches) and summarizes how exactly the corresponding models are applied for the detection of anomalies. Furthermore, we clarify which concrete application areas are typically addressed by anomaly detection in the context of cloud computing environments and which related public datasets are often used for evaluations. Finally, we discuss the implications of the literature review and provide directions for future research.",No,"이 논문은 215개의 기존 연구를 체계적으로 검토하는 문헌 리뷰로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하고 분석하는 데 중점을 둔 논문입니다."
Manipulating Recommender Systems: A Survey of Poisoning Attacks and Countermeasures,https://doi.org/10.1145/3677328,"Recommender systems have become an integral part of online services due to their ability to help users locate specific information in a sea of data. However, existing studies show that some recommender systems are vulnerable to poisoning attacks, particularly those that involve learning schemes. A poisoning attack is where an adversary injects carefully crafted data into the process of training a model with the goal of manipulating the system’s final recommendations. Based on recent advancements in artificial intelligence (AI), such attacks have gained importance recently. At present, we do not have a full and clear picture of why adversaries mount such attacks, nor do we have comprehensive knowledge of the full capacity to which such attacks can undermine a model or the impacts that might have. While numerous countermeasures to poisoning attacks have been developed, they have not yet been systematically linked to the properties of the attacks. Consequently, assessing the respective risks and potential success of mitigation strategies is difficult, if not impossible. This survey aims to fill this gap by primarily focusing on poisoning attacks and their countermeasures. This is in contrast to prior surveys that mainly focus on attacks and their detection methods. Through an exhaustive literature review, we provide a novel taxonomy for poisoning attacks, formalise its dimensions, and accordingly organise 31 attacks described in the literature. Further, we review 43 countermeasures to detect and/or prevent poisoning attacks, evaluating their effectiveness against specific types of attacks. This comprehensive survey should serve as a point of reference for protecting recommender systems against poisoning attacks. The article concludes with a discussion on open issues in the field and impactful directions for future research. A rich repository of resources associated with poisoning attacks is available at https://github.com/tamlhp/awesome-recsys-poisoning.",No,본 논문은 기존 연구들을 종합하여 추천 시스템의 poisoning 공격과 대응책에 대해 체계적으로 정리한 서베이 논문입니다. 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 문헌 리뷰와 분류 체계 제안에 중점을 두고 있어 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다.
Using Natural Language Processing to Explore Instructional Change Strategies in Undergraduate Science Education Literature,https://doi.org/10.1145/3626253.3635341,"Over ten years ago, our collaborators conducted a NSF-funded project identifying four broad categories of change strategies used to improve undergraduate STEM education through a comprehensive interdisciplinary literature review of articles from 1995 to 2008. Since this first iteration, there have been many major developments in undergraduate STEM education; particularly, the rapid development in sophisticated technology tools. These developments affect the nature of classroom instruction as well as expand the bounds on analyzing a corpus of articles. Thus, it is crucial to repeat this review to better understand the changes in STEM education from the more recent past. Our goal is to use machine learning to identify, potentially new, themes in the recent literature. We plan to compare and contrast both AI-assisted modeling and traditional, human-qualitative coding approaches in an effort to: (1) identify the benefits and faults of using AI verses human coding, and (2) portray a comprehensive story of change instruction literature from 2010. This lightning talk will describe the data extraction process and preliminary results from machine learning models. In addition to sharing the beginnings of our work, we hope to gain new perspectives and ideas from computing education scientists regarding our data representation and modeling choices.",Yes,"본 논문은 머신러닝을 활용하여 최근 학부 STEM 교육 문헌에서 새로운 주제를 식별하고 AI와 인간 코딩 방식을 비교하는 등 직접적인 연구 방법론과 결과를 제시하고 있다. 이는 기존 연구를 반복하는 데 그치지 않고, 새로운 데이터 추출과 모델링을 통해 독창적인 연구 기여를 포함하고 있음을 보여준다."
A Survey on Trust Evaluation Based on Machine Learning,https://doi.org/10.1145/3408292,"Trust evaluation is the process of quantifying trust with attributes that influence trust. It faces a number of severe issues such as lack of essential evaluation data, demand of big data process, request of simple trust relationship expression, and expectation of automation. In order to overcome these problems and intelligently and automatically evaluate trust, machine learning has been applied into trust evaluation. Researchers have proposed many methods to use machine learning for trust evaluation. However, the literature still lacks a comprehensive literature review on this topic. In this article, we perform a thorough survey on trust evaluation based on machine learning. First, we cover essential prerequisites of trust evaluation and machine learning. Then, we justify a number of requirements that a sound trust evaluation method should satisfy, and propose them as evaluation criteria to assess the performance of trust evaluation methods. Furthermore, we systematically organize existing methods according to application scenarios and provide a comprehensive literature review on trust evaluation from the perspective of machine learning’s function in trust evaluation and evaluation granularity. Finally, according to the completed review and evaluation, we explore some open research problems and suggest the directions that are worth our research effort in the future.",No,본 논문은 머신러닝 기반 신뢰 평가에 관한 기존 연구들을 종합적으로 정리하고 평가하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들을 리뷰하는 데 중점을 두고 있습니다.
Fake News Detection in Social Media: A Systematic Review,https://doi.org/10.1145/3411564.3411648,"The growth of social networks platforms leverages the consumption of news due to its easy access, spreading behavior, and low cost. However, this revolution in the way that information is released has provided the growth of something that always walked side by side with the real news: we are talking about fake news. After the 2016 U.S. presidential election this term became more popular and dangerous because of its negative effect on society. In this context, recent contributions has appeared addressing several related topics, such as spreading behavior, methods for spreading contention, and fake news detection algorithms. Despite of the growth of this type of research, it is difficult for a researcher to identify the current state-of-the-art literature about fake news detection. To overcome this obstacle, this paper presents a systematic review of the literature that brings an overview of this research area and analyzes the the high-quality studies about fake news detection. Through this systematic literature review, more than 6,000 articles were found according to our search protocol. Then, we put these studies through stages of screening to ensure that they were quality assessed. Were elected 32 high-quality studies according to our PRISMA flow diagram defined in this paper. These studies were then categorized by their contribution type and algorithm. This work shown that Twitter and Weibo1 are the social media platform most applied by selected studies, and deep learning algorithms given the best detection results, specially LSTM. Besides, this SR exposes the lack of research for fake news detection in other language than english. Finally, we expect this study can help researchers identify the greatest contributions as well as research opportunities.",No,이 논문은 직접적인 독창적 연구 결과를 제시하기보다는 기존 연구들을 체계적으로 검토하고 분류하는 체계적 문헌 리뷰(Systematic Review) 논문입니다. 따라서 새로운 실험이나 알고리즘 개발 등의 직접 기여가 포함된 연구 논문으로 보기 어렵습니다.
A Review on Software Defect Prediction Using Machine Learning,https://doi.org/10.1145/3590837.3590918,"Software plays an important role in many of the systems and devices that make up our modern societies. In order to provide their customers with software of a higher quality in a shorter amount of time, numerous software companies are developing software systems of varying sizes for various purposes. It is too challenging to produce high-quality software in a shorter amount of time due to the constraints of software development and the growing size of software data. Therefore, prior to delivering the software product, defect prediction can significantly contribute to a project's success in terms of; cost and quality to evaluate the quality of their software. The goal of the literature review is to investigate about the current trends of software defect prediction approaches. Conclusion of the literature review introduce that many machine learning algorithms are implemented named with Random forest, Logistic regression, Naïve Bayes and Artificial neutral Network etc. with different software metrics like CK metrics, Source code metric etc. The performance measurement of the model done by various methods like accuracy, precision etc.",No,"본 논문은 소프트웨어 결함 예측에 관한 기존 연구들을 종합적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 정리하는 데 초점이 맞춰져 있습니다."
Measuring the Desirability of an Intelligent Advisor for Predicting the Perceived Quality of News Articles,https://doi.org/10.1145/3609987.3610009,"Abstract. This paper presents the initial findings of the iQJournalism system’s prototype, an effort to create an intelligent advisor for predicting the perceived quality of news articles. Accordingly, artificial intelligence methodologies are utilized, with the purpose of providing real-time recommendations to journalists looking to improve the overall quality and engagement of their articles. The iQJournalism is designed using a user-centered design approach, in order to facilitate the specific needs of the journalists and editors using it. In this paper we discuss preliminary results of a study that was conducted with 20 users. Following an extensive literature review and the insights of a focus group with 10 professional journalists and MSc students, we created an interactive prototype of the iQJournalism system. The main aim is to facilitate the interaction of the users with the computational layer of the system for estimating the perceived quality of an article before its release. We organized a moderated desirability (light usability) study, in order to capture the users’ feedback. We used user experience and usability measurement tools like UEQ, SUS, Product reaction cards, perceived satisfaction and system adoption items, and open-ended questions to collect more comprehensive insights around the acceptance and usefulness of our prototype. Initial results show an overall favorable user experience, effective and efficient interaction when users engage with a series of situation-specific tasks.",Yes,"논문은 iQJournalism 시스템의 프로토타입 개발과 이를 통한 인공지능 기반 뉴스 기사 품질 예측에 관한 초기 연구 결과를 제시하고 있어, 독창적인 연구 내용과 실험적 사용자 평가를 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Improving Efficiency in IoT Data Streaming Through the Integration of Machine Learning: A Review,https://doi.org/10.1145/3659677.3659679,"Cloud Internet of Things as a term has emerged recently, it’s a combination of Cloud Computing and IoT (Internet of things) services with their combined features to solve the problem of collection and storage of massive data streaming. there are a lot of basic ordinary solutions to solve an IoT’s bottlenecks and defiances but it is still not enough to make the devices perfect and the smartest. To arrive at this level of perfection, should use the data collected from each environment to build the ideal smarter decision system through the integration of Machine Learning methods with big data technologies and use this system to improve the efficiency of IoT devices. and of course, this will take time to become our reality. But the Internet of Things now is one big resource of massive data collection and also a most common field in the 21st century in research, which helps to develop and improve it quickly and gives us a small vision of the future. The research objective of this work is to explain and discuss the bottlenecks and challenges faced by Internet of Things devices during real-time data streaming to the cloud platforms and study how can Machine Learning algorithms be utilized to optimize the performance of IoT devices and face the challenges and weaknesses of the survey of literature review and research of other researchers over recent 7 years.",No,초록에서 본 논문은 기존 연구들을 종합하여 IoT 데이터 스트리밍과 머신러닝 통합에 관한 문제점과 해결책을 논의하는 문헌 리뷰임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않은 리뷰 논문으로 판단됩니다.
Taxonomy and Recent Advance of Game Theoretical Approaches in Adversarial Machine Learning: A Survey,https://doi.org/10.1145/3600094,"Carefully perturbing adversarial inputs degrades the performance of traditional machine learning (ML) models. Adversarial machine learning (AML) that takes adversaries into account during training and learning emerges as a valid technique to defend against attacks. Due to the complexity and uncertainty of adversaries’ attack strategies, researchers utilize game theory to study the interactions between an adversary and an ML system designer. By configuring different game rules and analyzing game outcomes in an adversarial game, it is possible to effectively predict attack strategies and to produce optimal defense strategies for the system designer. However, the literature still lacks a holistic review of adversarial games in AML. In this paper, we extend the scope of previous surveys and provide a thorough overview of existing game theoretical approaches in AML for adaptively defending against adversarial attacks. For evaluating these approaches, we propose a set of metrics to discuss their merits and drawbacks. Finally, based on our literature review and analysis, we raise several open problems and suggest interesting research directions worthy of special investigation.",No,"본 논문은 기존 연구들을 종합하여 게임 이론적 접근법을 체계적으로 정리하고 평가하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 리뷰 및 분석에 중점을 두고 있습니다."
Towards the Selection of the Best Machine Learning Techniques and Methods for Urinalysis,https://doi.org/10.1145/3383972.3384031,"Urinalysis is a significant technique used for determining and inspecting the urinary system. Urine has numerous chemical materials secreted; these materials can be used to diagnose diseases and conditions such as urinary tract infection, diabetes, kidney diseases, and pregnancy, at the earliest. Accessing medical care and health screenings are quite essential, but it has become extremely difficult since screening techniques are either very expensive or convoluted for people of low-income communities. Despite the fact, that numerous urinalysis methods and techniques have been brought forth by researchers over the years, no research has been conducted to scrutinize and review state-of-the-art developments in the stated area. Hence, this research aims to conduct a Systematic Literature Review of urinalysis methods proposed and assessed from 2007 to 2019 and recognizes 33 studies. This leads to the identification of 10 methods, 8 technologies, 12 challenges, and 10 diseases. An insight analysis of the identified methods and models reveals that Genetic Based Fuzzy Classifying Method and Automatic Urinary Particle Recognition Method are the most optimum options due to the fact that their computational time is minimum as well as they do not require any supportive hardware. Moreover, the analysis of technologies reveals that Mobile App (Augmented Reality) is the best option among the identified technologies. It has also been discovered that machine/deep learning classification techniques have been used to classify the sample and provide reliable and accurate results within time. Nevertheless, a complete analysis of methods and technologies has been presented. The findings of this article are extremely useful for practitioners as well as academics of the area.",No,"본 논문은 2007년부터 2019년까지 발표된 연구들을 체계적으로 문헌 검토(Systematic Literature Review)한 결과를 제시하고 있으며, 직접적인 독창적 연구나 실험 결과를 포함하지 않고 기존 연구들의 분석과 정리를 주된 내용으로 하고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합적 평가에 초점이 맞춰져 있습니다."
Supporting Novice Researchers to Write Literature Review using Language Models,https://doi.org/10.1145/3613905.3650787,"A literature review requires more than summarization. While language model-based services and systems increasingly assist in analyzing accurate content in papers, their role in supporting novice researchers to develop independent perspectives on literature remains underexplored. We propose the design and evaluation of a system that supports the writing of argumentative narratives from literature. Based on the barriers faced by novice researchers before, during, and after writing, identified through semi-structured interviews, we propose a prototype of a language-model-assisted academic writing system that scaffolds the literature review writing process. A series of workshop studies revealed that novice researchers found the support valuable as they could initiate writing, co-create satisfying contents, and develop agency and confidence through a long-term dynamic partnership with the AI.",Yes,"논문은 반구조화된 인터뷰를 통해 초보 연구자들이 문헌 리뷰 작성에서 겪는 어려움을 분석하고, 이를 바탕으로 언어 모델을 활용한 학술 글쓰기 지원 시스템을 설계 및 평가하는 독창적인 연구를 수행하였다. 따라서 새로운 시스템 제안과 평가를 포함한 직접적인 연구 기여가 있다."
Explainability for Natural Language Processing,https://doi.org/10.1145/3447548.3470808,"This lecture-style tutorial, which mixes in an interactive literature browsing component, is intended for the many researchers and practitioners working with text data and on applications of natural language processing (NLP) in data science and knowledge discovery. The focus of the tutorial is on the issues of transparency and interpretability as they relate to building models for text and their applications to knowledge discovery. As black-box models have gained popularity for a broad range of tasks in recent years, both the research and industry communities have begun developing new techniques to render them more transparent and interpretable. Reporting from an interdisciplinary team of social science, human-computer interaction (HCI), and NLP/knowledge management researchers, our tutorial has two components: an introduction to explainable AI (XAI) in the NLP domain and a review of the state-of-the-art research; and findings from a qualitative interview study of individuals working on real-world NLP projects as they are applied to various knowledge extraction and discovery at a large, multinational technology and consulting corporation. The first component will introduce core concepts related to explainability in NLP. Then, we will discuss explainability for NLP tasks and report on a systematic literature review of the state-of-the-art literature in AI, NLP and HCI conferences. The second component reports on our qualitative interview study, which identifies practical challenges and concerns that arise in real-world development projects that require the modeling and understanding of text data.",No,"본 논문은 튜토리얼 형식으로 기존 연구들을 소개하고, 문헌 리뷰와 질적 인터뷰 연구 결과를 보고하는 내용으로 구성되어 있습니다. 직접적인 독창적 연구 결과나 새로운 방법론 제시는 포함되어 있지 않아 연구 논문으로 보기 어렵습니다."
Integrating Ethics within Machine Learning Courses,https://doi.org/10.1145/3341164,"This article establishes and addresses opportunities for ethics integration into Machine-learning (ML) courses. Following a survey of the history of computing ethics and the current need for ethical consideration within ML, we consider the current state of ML ethics education via an exploratory analysis of course syllabi in computing programs. The results reveal that though ethics is part of the overall educational landscape in these programs, it is not frequently a part of core technical ML courses. To help address this gap, we offer a preliminary framework, developed via a systematic literature review, of relevant ethics questions that should be addressed within an ML project. A pilot study with 85 students confirms that this framework helped them identify and articulate key ethical considerations within their ML projects. Building from this work, we also provide three example ML course modules that bring ethical thinking directly into learning core ML content. Collectively, this research demonstrates: (1) the need for ethics to be taught as integrated within ML coursework, (2) a structured set of questions useful for identifying and addressing potential issues within an ML project, and (3) novel course models that provide examples for how to practically teach ML ethics without sacrificing core course content. An additional by-product of this research is the collection and integration of recent publications in the emerging field of ML ethics education.",Yes,"본 논문은 머신러닝 교육 과정에 윤리 통합을 위한 새로운 프레임워크와 교육 모듈을 제안하고, 이를 학생 대상 파일럿 연구를 통해 검증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구를 단순히 정리한 것이 아니라 직접적인 기여를 하는 연구 논문으로 판단된다."
Machine Learning as a Service: Challenges in Research and Applications,https://dl.acm.org/doi/10.1145/3428757.3429152,"This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.",No,"논문 초록은 MLaaS에 관한 기존 연구들을 문헌 리뷰를 통해 분석하고 분류하는 데 초점을 맞추고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 제시하지 않고 있다. 따라서 본 논문은 연구 동향 분석 및 문제점 도출에 중점을 둔 리뷰 논문으로 판단된다."
A Comparative Study of De-identification Tools to Apply to Free-Text Clinical Notes,https://dl.acm.org/doi/10.5555/3566055.3566077,"Application of machine learning on clinical patient data can help generate valuable insights to improve the quality of patient care. However, ensuring privacy and security when processing medical data is of utmost importance and requires ethics clearance before the data can be used in research. De-identification of medical data is a mandatory step that removes identifiable information from the data, but offers difficult challenges for unstructured medical chart note data due to the need to retain key information for an-alytics. We performed a literature review of the state-of-the-art de-identification tools that have commonly been applied to unstruc-tured medical text data. In this paper, we present a comparative analysis of the tools based on the published work. We also present experimental results that we obtained from evaluating some of these tools and discuss their strengths and weaknesses.",Yes,본 논문은 기존 문헌 리뷰뿐만 아니라 여러 비식별화 도구를 직접 평가한 실험 결과를 포함하고 있어 독창적인 연구 기여가 포함되어 있다. 따라서 단순 리뷰가 아닌 직접적인 연구 수행 논문으로 판단된다.
Explaining Artificial Intelligence with Tailored Interactive Visualisations,https://doi.org/10.1145/3490100.3516481,"Artificial intelligence (AI) is becoming ubiquitous in the lives of both researchers and non-researchers, but AI models often lack transparency. To make well-informed and trustworthy decisions based on these models, people require explanations that indicate how to interpret the model outcomes. This paper presents our ongoing research in explainable AI, which investigates how visual analytics interfaces and visual explanations, tailored to the target audience and application domain, can make AI models more transparent and allow interactive steering based on domain expertise. First, we present our research questions and methods, contextualised by related work at the intersection of AI, human-computer interaction, and information visualisation. Then, we discuss our work so far in healthcare, agriculture, and education. Finally, we share our research ideas for additional studies in these domains.",No,"초록에서 제시된 내용은 진행 중인 연구와 연구 아이디어를 소개하는 수준이며, 구체적인 실험 결과나 독창적인 연구 기여가 명확히 드러나지 않습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Fighting organized crime by automatically detecting money laundering-related financial transactions,https://dl.acm.org/doi/abs/10.1145/3465481.3469196,"Money laundering is the set of operations aimed at giving a legitimate appearance to capital whose origin is illegal, thus making it more difficult to identify and subsequently recover it. It is one of the phenomena on which the so-called underground economy relies and therefore constitutes a crime for which the charge for money laundering applies. For supporting the fight against this phenomenon, the interest towards analysis models for Anti-Money Laundering (AML) based on a combined use of automatic tools and artificial intelligence (AI) techniques increases, as it is also shown by the European Central Bank (ECB) during recent press conferences. Following this direction, this paper proposes a model for enhancing the detection of suspicious transactions related to money laundering. It is based on a set of features that are defined by considering different aspects such as the time, the amount of money, number of transactions, type of operations and level of internationalization. An AI-based computational approach centered on Machine Learning (ML) techniques has been adopted to evaluate the goodness of such feature-based model, in supporting the automatic detection of suspicious transactions, by experimenting 5 different classifiers. From the experiments emerged that the Random Forest provided the best performance not only among the classifiers tested within the paper, but also in comparison to those presented in the related work with an accuracy, a recall and f1-score greater than 94% by decreasing the False Positive Rate (FPR). Furthermore, an analysis on the feature importance has been provided, to understand which feature, among the proposed ones, plays the major role in such application domain.",Yes,"본 논문은 자금세탁 관련 의심 거래 탐지를 위한 새로운 특징 집합을 정의하고, 이를 기반으로 머신러닝 기법을 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한 여러 분류기를 실험하고 최적의 모델을 제시하는 등 직접적인 연구 기여가 명확하다."
Evaluation of an Ensemble Technique for Prediction of Crop Yield,https://doi.org/10.1145/3647444.3647833,"Crop yield prediction plays a crucial role in agricultural management and decision-making processes. Traditional approaches to crop yield prediction often face limitations in terms of accuracy and robustness due to the complex and dynamic nature of agricultural systems. Machine learning is an emerging technology to understand practical and real world use cases for agricultural production. Machine learning is a supporting tool for the agricultural production which helps to make decisions on what crops to be cultivated, crop yield prediction and crop management practices. In this research paper, we propose ensemble techniques to improve crop yield prediction accuracy. Ensemble methods such as bagging, boosting, and stacking that combine multiple models to improve their predictive power which have shown promising results in various domains. We collected a comprehensive dataset consisting of historical crop yield data, weather information, and soil characteristics. The data underwent pre-processing steps, including cleaning, normalization, and feature engineering. We developed an ensemble model architecture, selecting appropriate base models and training them using a validation process. To evaluate the effectiveness of an ensemble technique for predicting crop yield, several factors should be considered such as data, ensemble composition, evaluation metrics, generalizability and interpretability. Important parameters related to climatic conditions such as rainfall, humidity, soil type and temperature were taken into consideration for crop yield prediction. From literature review, it is understanding that Decision Tree, Random Forest and Neural Networks are the algorithms mostly used. The proposed work compared Random Forest and Boosting algorithms based on the score like Mean Squared Error (MSE), Mean Absolute Error (MAE) and R2 score to improve the weak learner for most expected outcome. Finally concluded that ensemble of Random Forest with Gradient Boosting Regressor achieved more accuracy and most expected outcome. At the same time, Mean Squared Error(MSE), Mean Absolute Error(MAE) were smaller in the proposed work. The results demonstrated that the ensemble technique consistently outperformed individual models, achieving higher prediction accuracy and reducing prediction errors. Our findings suggest that ensemble techniques are promising for improving crop yield prediction, offering more robust and accurate insights for agricultural planning and decision-making.",Yes,"본 논문은 작물 수확량 예측을 위해 앙상블 기법을 제안하고, 데이터 수집, 전처리, 모델 개발 및 평가를 직접 수행한 연구 내용을 포함하고 있다. 또한 기존 알고리즘과의 성능 비교 및 실험 결과를 제시하여 독창적인 연구 기여가 명확하다."
Reverse Engineering Variability from Natural Language Documents: A Systematic Literature Review,https://doi.org/10.1145/3106195.3106207,"Identifying features and their relations (i.e., variation points) is crucial in the process of migrating single software systems to software product lines (SPL). Various approaches have been proposed to perform feature extraction automatically from different artifacts, for instance, feature location in legacy code. Usually such approaches a) omit variability information and b) rely on artifacts that reside in advanced phases of the development process, thus, being only of limited usefulness in the context of SPLs. In contrast, feature and variability extraction from natural language (NL) documents is more favorable, because a mapping to several other artifacts is usually established from the very beginning. In this paper, we provide a multi-dimensional overview of approaches for feature and variability extraction from NL documents by means of a systematic literature review (SLR). We selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used, tool support, or accuracy of the results. In a nutshell, our key insights are that i) standard NLP techniques are commonly used, ii) post-processing often includes clustering & machine learning algorithms, iii) only in rare cases, the approaches support variability extraction, iv) tool support, apart from text pre-processing is often not available, and v) many approaches lack a comprehensive evaluation. Based on these observations, we derive future challenges, arguing that more effort need to be invested for making such approaches applicable in practice.",No,"본 논문은 자연어 문서에서 변이성을 추출하는 기존 연구들을 체계적으로 검토한 문헌 리뷰(SLR) 논문으로, 직접적인 독창적 연구 결과나 새로운 방법론을 제안하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합하고 분석하는 리뷰 논문에 해당합니다."
A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations,https://dl.acm.org/doi/10.1145/3527848,"Machine learning is increasingly used to inform decision making in sensitive situations where decisions have consequential effects on individuals’ lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions toward which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.",No,본 논문은 알고리즘적 조치(algorithmic recourse)에 관한 기존 연구들을 종합적으로 검토하고 정리하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과보다는 문헌 리뷰와 연구 방향 제시에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
SOTERIA: Preserving Privacy in Distributed Machine Learning,https://doi.org/10.1145/3555776.3578591,"We propose Soteria, a system for distributed privacy-preserving Machine Learning (ML) that leverages Trusted Execution Environments (e.g. Intel SGX) to run code in isolated containers (enclaves). Unlike previous work, where all ML-related computation is performed at trusted enclaves, we introduce a hybrid scheme, combining computation done inside and outside these enclaves. The conducted experimental evaluation validates that our approach reduces the runtime of ML algorithms by up to 41%, when compared to previous related work. Our protocol is accompanied by a security proof, as well as a discussion regarding resilience against a wide spectrum of ML attacks.",Yes,"본 논문은 기존 연구와 차별화된 하이브리드 계산 방식을 제안하고, 이를 통해 머신러닝 알고리즘의 실행 시간을 최대 41% 단축하는 실험적 평가 결과를 제시하고 있다. 또한 보안 증명과 공격 저항성 논의를 포함하여 독창적인 연구 기여를 포함하고 있으므로 연구 논문에 해당한다."
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey,https://doi.org/10.1145/3424660,"In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users’ local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users’ sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.",No,"본 논문은 기존 연구들을 종합적으로 정리하고 리뷰하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 기여를 포함하고 있지 않습니다. 따라서 연구 논문보다는 문헌 조사 및 정리 목적의 리뷰 논문에 해당합니다."
Mixed-Type Wafer Failure Pattern Recognition,https://doi.org/10.1145/3566097.3568363,"The ongoing evolution in process fabrication enables us to step below the 5nm technology node. Although foundries can pattern and etch smaller but more complex circuits on silicon wafers, a multitude of challenges persist. For example, defects on the surface of wafers are inevitable during manufacturing. To increase the yield rate and reduce time-to-market, it is vital to recognize these failures and identify the failure mechanisms of these defects. Recently, applying machine learning-powered methods to combat single defect pattern classification has made significant progress. However, as the processes become increasingly complicated, various single-type defect patterns may emerge and be coupled on a wafer and thus shape a mixed-type pattern. In this paper, we will survey the recent pace of progress on advanced methodologies for wafer failure pattern recognition, especially for mixed-type one. We sincerely hope this literature review can highlight the future directions and promote the advancement of the wafer failure pattern recognition.",No,"본 논문은 최신 연구 동향과 방법론을 조사하는 문헌 리뷰(survey) 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 미래 방향을 제시하는 데 중점을 두고 있습니다."
Witnessing Erosion of Membership Inference Defenses: Understanding Effects of Data Drift in Membership Privacy,https://doi.org/10.1145/3607199.3607224,"Data drift is the phenomenon when the input data distribution in testing time is different from the training time. This strengthens the generalization gap in a model, which is known to severely deteriorate the model’s performance. Meanwhile, previous studies state that membership inference attacks (MIA) take advantage of the generalization gap of a machine learning model. By transitive logic, we can deduce that data drift would affect these privacy attacks. In this work, we consider data drift when applied to the privacy threat of MIA. As the first work to explore the detrimental extent of data drift on membership privacy, we conduct a literature review on current MIA defense works under selected dimensions associated with data drift. Our study reveals that not only has data drift never been tested in MIA defense, but there is also no infrastructure to juxtapose data drift with MIA defense. We overcome this by proposing a design for simulating authentic and synthetic data drift and evaluate the benchmark MIA defense methods on various settings. The evaluation shows that data drift strongly enhances the attack success rate of MIA, regardless of defense. In this, we propose MIAdapt, a proof of concept of a MIA defense that allows update in data drift. From this evaluation, we provide security insight into possible solutions in negating the effects of data drift. We hope our work brings attention to the threat of data drift and instigates the development of MIA defense that are adaptable to data drift.",Yes,"본 논문은 데이터 드리프트가 멤버십 추론 공격(MIA) 방어에 미치는 영향을 실험적으로 평가하고, 이를 극복하기 위한 새로운 방어 기법(MIAdapt)을 제안하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
A Taxonomy and Archetypes of Business Analytics in Smart Manufacturing,https://doi.org/10.1145/3583581.3583584,"Fueled by increasing data availability and the rise of technological advances for data processing and communication, business analytics is a key driver for smart manufacturing. However, due to the multitude of different local advances as well as its multidisciplinary complexity, both researchers and practitioners struggle to keep track of the progress and acquire new knowledge within the field, as there is a lack of a holistic conceptualization. To address this issue, we performed an extensive structured literature review, yielding 904 relevant hits, to develop a quadripartite taxonomy as well as to derive archetypes of business analytics in smart manufacturing. The taxonomy comprises the following meta-characteristics: application domain, orientation as the objective of the analysis, data origins, and analysis techniques. Collectively, they comprise eight dimensions with a total of 52 distinct characteristics. Using a cluster analysis, we found six archetypes that represent a synthesis of existing knowledge on planning, maintenance (reactive, offline, and online predictive), monitoring, and quality management. A temporal analysis highlights the push beyond predictive approaches and confirms that deep learning already dominates novel applications. Our results constitute an entry point to the field but can also serve as a reference work and a guide with which to assess the adequacy of one's own instruments.",No,본 논문은 광범위한 문헌 조사를 바탕으로 스마트 제조에서 비즈니스 분석의 분류체계와 전형 유형을 제시하는 개념적 연구에 해당합니다. 직접적인 실험이나 새로운 데이터 분석 결과를 제시하는 독창적인 연구 내용보다는 기존 연구를 종합하고 체계화하는 데 중점을 두고 있습니다.
Semi-automated Literature Review for Scientific Assessment of Socioeconomic Climate Change Scenarios,https://doi.org/10.1145/3487553.3524659,"Climate change is now recognized as a global threat, and the literature surrounding it continues to increase exponentially. Expert bodies such as the Intergovernmental Panel on Climate Change (IPCC) are tasked with periodically assessing the literature to extract policy-relevant scientific conclusions that might guide policymakers. However, concerns have been raised that climate change research may be too voluminous for traditional literature review to adequately cover. It has been suggested that practices for literature review for scientific assessment be updated/augmented with semi-automated approaches from bibliometrics or scientometrics. In this study, we explored the feasibility of such recommendations for the scientific assessment of literature around socioeconomic climate change scenarios, so-called Shared Socioeconomic Pathways (SSPs). For automated literature reviews, most methods can be subsumed under two broad categories of classification tasks that use either (1) Natural Language Processing (NLP) or (2) Citation Networks. We performed two levels of classification tasks: (1) identifying SSP articles from a large corpus of climate change research and developing a database of SSP-related articles; (2) classifying SSP articles into different sectoral categories. We applied three machine learning algorithms for the text classification task: Multinomial Naïve Bayes, Logistic Regression, and Linear Support Vector Classification. However, the vocabulary of the SSP literature too closely resembles the vocabulary of broader climate change research for an NLP approach to be effective. We then attempted a citation network approach. We compared two sets of different community detection algorithms (the Louvain algorithm and the Fluid community detection algorithm), with one iteration of each algorithm containing 8 clusters and the next set containing 16. The citation network approach outperformed NLP with respect to false negatives. It also provided the ability to assess the uptake of SSPs across different sectors of climate change research. We concluded that, at the time of the study, the SSP corpus may not yet be large enough or diverse enough from broader climate change research for applying machine learning techniques for automated literature review. However, our research suggests that until there is a critical mass of SSP studies, there is the potential to divide labor between human and machine readers. Some of the data collection tasks currently done by human author teams, such as assessing scenario research, could be semi-automated to ensure and enhance the coverage of the literature. We also drew conclusions about the uptake of the SSP framework over its first 5 years in the broader climate change research literature. We observed that the uptake of SSPs in certain sub-disciplines (e.g., food systems) progressed slowly. Hence, to keep SSPs relevant, it may be fruitful to target SSP studies to particular research communities (e.g., sectors with slower uptake).",Yes,"본 논문은 기계 학습 알고리즘과 인용 네트워크 분석을 활용하여 기후변화 관련 문헌을 분류하고 데이터베이스를 구축하는 독창적인 연구 방법론을 제시하고 있습니다. 또한, 반자동 문헌 검토의 가능성을 탐구하고 SSP 문헌의 확산 양상을 분석하는 등 직접적인 연구 기여를 포함하고 있습니다."
Context sight: model understanding and debugging via interpretable context,https://doi.org/10.1145/3546930.3547502,"Model interpretation is increasingly important for successful model development and deployment. In recent years, many explanation methods are introduced to help humans understand how a machine learning model makes a decision on a specific instance. Recent studies show that contextualizing an individual model decision within a set of relevant examples can improve the model understanding. However, there is a lack of systematic study on what factors are considered when generating and using the context examples to explain model predictions, and how context examples help with model understanding and debugging in practice. In this work, we first identify a taxonomy of context generation and summarization through literature review. We then present Context Sight, a visual analytics system that integrates customized context generation and multiple-level context summarization to assist context exploration and interpretation. We evaluate the usefulness of the system through a detailed use case. This work is an initial step for a set of systematic research on how contextualization can help data scientists and practitioners understand and diagnose model behaviors, based on which we will gain a better understanding of the usage of context.",Yes,"논문은 문헌 조사를 통해 맥락 생성과 요약의 분류체계를 제시하고, 이를 바탕으로 Context Sight라는 시각적 분석 시스템을 개발하여 모델 이해와 디버깅을 지원하는 독창적인 연구 내용을 포함하고 있다. 또한 시스템의 유용성을 사례 연구를 통해 평가하는 등 직접적인 연구 기여가 명확하다."
A Human-Centered Systematic Literature Review of the Computational Approaches for Online Sexual Risk Detection,https://doi.org/10.1145/3479609,"In the era of big data and artificial intelligence, online risk detection has become a popular research topic. From detecting online harassment to the sexual predation of youth, the state-of-the-art in computational risk detection has the potential to protect particularly vulnerable populations from online victimization. Yet, this is a high-risk, high-reward endeavor that requires a systematic and human-centered approach to synthesize disparate bodies of research across different application domains, so that we can identify best practices, potential gaps, and set a strategic research agenda for leveraging these approaches in a way that betters society. Therefore, we conducted a comprehensive literature review to analyze 73 peer-reviewed articles on computational approaches utilizing text or meta-data/multimedia for online sexual risk detection. We identified sexual grooming (75%), sex trafficking (12%), and sexual harassment and/or abuse (12%) as the three types of sexual risk detection present in the extant literature. Furthermore, we found that the majority (93%) of this work has focused on identifying sexual predators after-the-fact, rather than taking more nuanced approaches to identify potential victims and problematic patterns that could be used to prevent victimization before it occurs. Many studies rely on public datasets (82%) and third-party annotators (33%) to establish ground truth and train their algorithms. Finally, the majority of this work (78%) mostly focused on algorithmic performance evaluation of their model and rarely (4%) evaluate these systems with real users. Thus, we urge computational risk detection researchers to integrate more human-centered approaches to both developing and evaluating sexual risk detection algorithms to ensure the broader societal impacts of this important work.",No,"본 논문은 73편의 기존 연구를 체계적으로 검토하는 문헌 리뷰 연구로, 직접적인 독창적 실험이나 새로운 알고리즘 개발 등의 연구 기여를 포함하지 않는다. 따라서 기존 연구들을 종합하고 분석하는 리뷰 논문에 해당한다."
Open-ended questions automated evaluation: proposal of a new generation,https://doi.org/10.1145/3632971.3632980,"Abstract. Exams grading for the knowledge validation to recognise competences is an essential element for any learning process. There are two main modes for their evaluation: subjective and automated. Subjective evaluation is accused of many flaws such as the inconsistency of the human corrector and the time it requires. Automating the assessment of open-ended questions saves a lot of time, provides quick feedback to learners and ensures the consistency expected from the human correctors. However, this is a challenging problem to implement because the computer does not have the same faculties as a human. To address this issue, we conducted a literature review on open-ended questions automated evaluation to implement an automatic exam grading system with similar or even higher accuracy than a human corrector. This study allows us to classify the different approaches in three generations: “bag of words” based approaches, classical semantic similarity-based approaches and machine learning based approaches. The third generation offers the best state-of-the-art results despite criticism of it. These approaches rely on neural networks which need to have a large dataset for effective training. To tackle this handicap, we propose a fourth generation (section 3). This contribution relies on the use of pre-trained models for which a dataset for training is not necessary knowing that they are zero-shot-learners. After implementing our architecture, we conducted our experiments with the three main French-speaking models available on Hugging Face. The best model agrees with the human corrector at 96%.",Yes,"논문은 기존 연구들을 분류하고 한계를 지적한 후, 사전학습된 모델을 활용한 새로운 4세대 자동 평가 방식을 제안하고 이를 구현하여 실험한 결과를 제시하고 있다. 이는 독창적인 연구 기여와 실험적 검증을 포함한 연구 논문에 해당한다."
Accessible Web Development: Opportunities to Improve the Education and Practice of web Development with a Screen Reader,https://doi.org/10.1145/3458024,"There are a growing number of jobs related to web development, yet there is little formal literature about the accessibility of web development with a screen reader. This article describes research to explore (1) web development accessibility issues and their impact on blind learners and programmers; (2) tools and strategies used to address issues; and (3) opportunities for creating inclusive web development curriculum and supportive tools. We conducted a Comprehensive Literature Review (CLR) to formulate accessibility issue categories, then interviewed 12 blind programmers to validate and expand on both issues in education and practice. The CLR yielded five issue categories: (1) visual information without an accessible equivalent, (2) orienting, (3) navigating, (4) lack of support, and (5) knowledge and use of supportive technologies. Our interview findings validated the use of CLR-derived categories and revealed nuances specific to learning and practicing web development. Blind web developers grapple with the inaccessibility of demonstrations and explanations of web design concepts, wireframing software, independent verification of computed Cascading Style Sheets (CSS), and navigating browser-based developer tool interfaces. Tools and strategies include seeking out alternative education materials to learn independently, use of CSS frameworks, collaboration with sighted colleagues, and avoidance of design and front-end development. This work contributes to our understanding of accessibility issues specific to web development and the strategies that blind web developers employ in both educational and applied contexts. We identify areas in which greater awareness and application of accessibility best practices are required in Web education, a need to disseminate existing screen reader strategies and accessible tools, and to develop new tools that support Web design and validation of CSS. Finally, this research signals future directions for the development of accessible web curriculum and supportive tools, including solutions that leverage artificial intelligence, tactile graphics, and supportive-online communities of practice.",Yes,"본 논문은 문헌 검토와 인터뷰를 통해 웹 개발 접근성 문제를 체계적으로 분석하고, 교육 및 실무에서의 구체적인 전략과 도구를 제안하는 독창적인 연구 내용을 포함하고 있습니다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단됩니다."
"Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers",https://doi.org/10.1145/3678182,"The emerging integration of Internet of Things (IoT) and AI has unlocked numerous opportunities for innovation across diverse industries. However, growing privacy concerns and data isolation issues have inhibited this promising advancement. Unfortunately, traditional centralized Machine Learning (ML) methods have demonstrated their limitations in addressing these hurdles. In response to this ever-evolving landscape, Federated Learning (FL) has surfaced as a cutting-edge ML paradigm, enabling collaborative training across decentralized devices. FL allows users to jointly construct AI models without sharing their local raw data, ensuring data privacy, network scalability, and minimal data transfer. One essential aspect of FL revolves around proficient knowledge aggregation within a heterogeneous environment. Yet, the inherent characteristics of FL have amplified the complexity of its practical implementation compared to centralized ML. This survey delves into three prominent clusters of FL research contributions: personalization, optimization, and robustness. The objective is to provide a well-structured and fine-grained classification scheme related to these research areas through a unique methodology for selecting related work. Unlike other survey papers, we employed a hybrid approach that amalgamates bibliometric analysis and systematic scrutinizing to find the most influential work in the literature. Therefore, we examine challenges and contemporary techniques related to heterogeneity, efficiency, security, and privacy. Another valuable asset of this study is its comprehensive coverage of FL aggregation strategies, encompassing architectural features, synchronization methods, and several federation motivations. To further enrich our investigation, we provide practical insights into evaluating novel FL proposals and conduct experiments to assess and compare aggregation methods under IID and non-IID data distributions. Finally, we present a compelling set of research avenues that call for further exploration to open up a treasure of advancement.",No,"본 논문은 연합학습(Federated Learning)에 관한 종합적인 서베이 논문으로, 기존 연구들을 분류하고 비교 분석하는 데 중점을 두고 있습니다. 직접적인 독창적 연구 결과나 새로운 방법론 제안보다는 기존 연구의 체계적 정리와 실험적 비교에 초점을 맞추고 있어 연구 논문으로 보기 어렵습니다."
A Machine Learning Based Load Value Approximator Guided by the Tightened Value Locality,https://doi.org/10.1145/3583781.3590207,"This paper addresses two essential memory bottlenecks: 1) memory wall, and 2) bandwidth wall. To accomplish this objective, we propose a machine learning (ML) based model that estimates the values to be loaded from the memory by a wide range of error-resilient applications. The proposed model exploits the feature of tightened value locality, which consists of a periodic load of few unique values. The proposed ML-based load value approximator (LVA) requires minimal overhead as it relies on a hash that encodes the history of events, e.g., history of accessed addresses, and values that can be extracted from the load instruction to be approximated. The proposed LVA completely eliminates memory accesses, i.e., 100% of accesses, in runtime and thus addresses the issue of memory wall and bandwidth wall. Compared to related work, our LVA delivers a maximum accuracy of 95.16% while offering a higher reduction in memory accesses.",Yes,"본 논문은 메모리 병목 현상을 해결하기 위해 머신러닝 기반의 새로운 모델을 제안하고, 이를 통해 메모리 접근을 완전히 제거하는 방법을 연구하였다. 이는 기존 연구와 차별화된 독창적인 연구 내용과 실험 결과를 포함하고 있어 연구 논문에 해당한다."
How Deep Features Have Improved Event Recognition in Multimedia: A Survey,https://doi.org/10.1145/3306240,"Event recognition is one of the areas in multimedia that is attracting great attention of researchers. Being applicable in a wide range of applications, from personal to collective events, a number of interesting solutions for event recognition using multimedia information sources have been proposed. On the other hand, following their immense success in classification, object recognition, and detection, deep learning has been shown to perform well in event recognition tasks also. Thus, a large portion of the literature on event analysis relies nowadays on deep learning architectures. In this article, we provide an extensive overview of the existing literature in this field, analyzing how deep features and deep learning architectures have changed the performance of event recognition frameworks. The literature on event-based analysis of multimedia contents can be categorized into four groups, namely (i) event recognition in single images; (ii) event recognition in personal photo collections; (iii) event recognition in videos; and (iv) event recognition in audio recordings. In this article, we extensively review different deep-learning-based frameworks for event recognition in these four domains. Furthermore, we also review some benchmark datasets made available to the scientific community to validate novel event recognition pipelines. In the final part of the manuscript, we also provide a detailed discussion on basic insights gathered from the literature review, and identify future trends and challenges.",No,"이 논문은 기존 연구들을 종합적으로 정리하고 분석하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
A Design Space for Intelligent Dialogue Augmentation,https://doi.org/10.1145/3708359.3712096,"The use of intelligent agents in communication is a growing trend aimed at enhancing the efficiency and quality of interactions. As such, dialogue augmentation systems—text processing systems that interactively enhance ongoing written or spoken communication—are gaining significant popularity across domains. While technical limitations had previously inhibited their real-time usage for effective communication augmentation, recent developments in language processing have improved their capabilities to contribute to dialogue as intelligent, emancipated, and proactive agents. While other works on dialogue augmentation focus on evaluating design considerations for specific applications of these systems, we lack a unified understanding of the broader design principles that apply to dialogue more generally. Through a literature review and mixed-methods analysis of 78 existing systems, we iteratively define a comprehensive design space for intelligent dialogue augmentation systems. To further ground our analysis, we interweave Clark’s [27] models of dialogue with concepts in human-AI collaboration and discuss trends in the evolving role of dialogue augmentation systems along five dimensions—dialogue context, augmentation context, task, interaction, and model. Based on the identified trends, we discuss concrete challenges for broader adoption, highlighting the need to design trusted, seamless, and timely, and accessible augmentations. The design space contributes as a mechanism for researchers to facilitate defining design choices during development, situate their systems in the current landscape of works, and understand opportunities for future research.",Yes,"본 논문은 78개의 기존 시스템에 대한 문헌 검토와 혼합 방법 분석을 통해 지능형 대화 증강 시스템의 포괄적인 설계 공간을 정의하는 독창적인 연구를 수행하고 있습니다. 또한, 대화 모델과 인간-AI 협업 개념을 결합하여 새로운 설계 원칙과 향후 연구 기회를 제시하므로 연구 논문에 해당합니다."
An Enhanced Random Forests Approach to Predict Heart Failure From Small Imbalanced Gene Expression Data,https://doi.org/10.1109/TCBB.2020.3041527,"Myocardial infarctions and heart failure are the cause of more than 17 million deaths annually worldwide. ST-segment elevation myocardial infarctions (STEMI) require timely treatment, because delays of minutes have serious clinical impacts. Machine learning can provide alternative ways to predict heart failure and identify genes involved in heart failure. For these scopes, we applied a Random Forests classifier enhanced with feature elimination to microarray gene expression of 111 patients diagnosed with STEMI, and measured the classification performance through standard metrics such as the Matthews correlation coefficient (MCC) and area under the receiver operating characteristic curve (ROC AUC). Afterwards, we used the same approach to rank all genes by importance, and to detect the genes more strongly associated with heart failure. We validated this ranking by literature review and gene set enrichment analysis. Our classifier employed to predict heart failure achieved MCC = +0.87 and ROC AUC = 0.918, and our analysis identified KLHL22, WDR11, OR4Q3, GPATCH3, and FAH as top five protein-coding genes related to heart failure. Our results confirm the effectiveness of machine learning feature elimination in predicting heart failure from gene expression, and the top genes found by our approach will be able to help biologists and cardiologists further our understanding of heart failure.",Yes,"본 논문은 소규모 불균형 유전자 발현 데이터를 이용해 심부전 예측을 위한 향상된 랜덤 포레스트 기법을 제안하고, 성능 평가 및 유의한 유전자 도출을 포함한 독창적인 연구 내용을 담고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Text Recognition in the Wild: A Survey,https://doi.org/10.1145/3440756,"The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research topic in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency. This article aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition, (2) introduce new insights and ideas, (3) provide a comprehensive review of publicly available resources, and (4) point out directions for future work. In summary, this literature review attempts to present an entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field and could be helpful in inspiring future research. Related resources are available at our GitHub repository: https://github.com/HCIILAB/Scene-Text-Recognition.",No,"이 논문은 장면 텍스트 인식 분야에 대한 종합적인 문헌 리뷰(survey)로, 기존 연구들을 요약하고 정리하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적인 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Blockchain-Based Federated Learning for Securing Internet of Things: A Comprehensive Survey,https://doi.org/10.1145/3560816,"The Internet of Things (IoT) ecosystem connects physical devices to the internet, offering significant advantages in agility, responsiveness, and potential environmental benefits. The number and variety of IoT devices are sharply increasing, and as they do, they generate significant data sources. Deep learning (DL) algorithms are increasingly integrated into IoT applications to learn and infer patterns and make intelligent decisions. However, current IoT paradigms rely on centralized storage and computing to operate the DL algorithms. This key central component can potentially cause issues in scalability, security threats, and privacy breaches. Federated learning (FL) has emerged as a new paradigm for DL algorithms to preserve data privacy. Although FL helps reduce privacy leakage by avoiding transferring client data, it still has many challenges related to models’ vulnerabilities and attacks. With the emergence of blockchain and smart contracts, the utilization of these technologies has the potential to safeguard FL across IoT ecosystems. This study aims to review blockchain-based FL methods for securing IoT systems holistically. It presents the current state of research in blockchain, how it can be applied to FL approaches, current IoT security issues, and responses to outline the need to use emerging approaches toward the security and privacy of IoT ecosystems. It also focuses on IoT data analytics from a security perspective and the open research questions. It also provides a thorough literature review of blockchain-based FL approaches for IoT applications. Finally, the challenges and risks associated with integrating blockchain and FL in IoT are discussed to be considered in future works.",No,초록에서 해당 논문은 블록체인 기반 연합학습과 IoT 보안에 관한 기존 연구들을 종합적으로 검토하는 서베이 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 분석에 중점을 두고 있습니다.
Graph neural network in traffic forecasting: a review,https://doi.org/10.1145/3475851.3475864,"Traffic Forecasting is an important and challenging problem. The recent developed deep learning models are becoming dominant in this area. Especially, graph neural networks (GNNs) are being applied in traffic forecasting in recent years. In this paper, I give a review of the related work and the applications of GNNs in different traffic forecasting problems, e.g., bike sharing, metro flow, road traffic flow prediction, etc. I find that GNNs are only applied in recent years, and there is still a great research potential for this direction.",No,"논문 초록에서 해당 논문은 기존 연구들을 정리하고 리뷰하는 내용임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 새로운 기여를 제시하는 내용은 포함되어 있지 않다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Use of Artificial Intelligence in the Personalisation of In-vehicle Interaction Interfaces,https://doi.org/10.1145/3672919.3672950,"This study proposes a theoretical architecture model of an AI-driven in-vehicle interactive system, optimizing and adjusting key aspects such as target group selection, driving stages, consumer needs, interaction modes, and interface design through detailed demonstration. It emphasizes the personalization factor and proposes a model more aligned with the user's personalized needs by subdividing the driving stages and analyzing consumer needs. In terms of interaction modes, attention is paid to the integration of clicks, voice control, gestures, sliding, and other interaction methods to improve the flexibility of the user's operating experience. In terms of interface design, through elements such as colour, icons, typography, and fonts, combined with the automatic optimization of AI, personalized design is achieved to adapt better to the user's driving situation and preferences. Overall, this model aims to improve in-vehicle interactive systems' performance and user experience and provide a valuable reference for the future intelligent driving field. The study also profoundly explores the application of AI in the personalized design of vehicle interactive interfaces through a literature review and empirical cases, providing critical insights into achieving personalization and intelligence for future vehicle interaction design.",No,"본 논문은 AI 기반 차량 내 인터페이스 개인화에 대한 이론적 아키텍처 모델을 제안하고 문헌 검토 및 사례 분석을 통해 통찰을 제공하지만, 직접적인 실험 결과나 새로운 데이터 생성 등 독창적인 연구 결과를 포함하고 있지 않아 연구 논문으로 보기 어렵다. 주로 기존 연구를 종합하고 모델을 제안하는 이론적 연구에 가깝다."
A Weight-Based Channel Pruning Algorithm for Depth-Wise Separable Convolution Unit,https://doi.org/10.1145/3508546.3508568,"Deep learning has become the hotspot of academia and industry, and has found its wide applications in many fields, especially computer vision. However, the computational and storage cost of the CNN model is now a headache to researchers. The famous model structures such as AlexNet, VGG and ResNet, are all going wider and deeper. Then it becomes hard to deploy these models on mobile and embedded devices. Accordingly, the model lightweight method is desiderated. This paper adapts the channel pruning method to features of the depth-wise separable convolution unit and its variant. By using the united prune ratio setting, the additional sparsity restraint, and the certain dataset preprocessing, our experiments show higher accuracy on the CIFAR-10 dataset than related work in some cases.",Yes,"논문 초록에서 제안된 방법은 깊이별 분리 합성곱 유닛에 특화된 채널 프루닝 알고리즘을 개발하고, 실험을 통해 CIFAR-10 데이터셋에서 기존 연구보다 높은 정확도를 달성했다고 명시하고 있습니다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문임을 나타냅니다."
Influence of MRI modality on accuracy of deep learning segmentation of abdominal organs,https://doi.org/10.1145/3441369.3441379,"Learning to segment MRI sequences is an exciting current application of deep learning networks. Most recent related work on this issue propose architecture modifications and ensembles with voting to try to improve the quality of the result. One relevant issue that also deserves further investigation is how the MRI modality influences quality of segmentation. In this paper we discuss two well-known MRI modalities (T1-DUAL and T2-SPIR) and compare the quality of segmentation with each. We build and train three segmentation network architectures, then evaluate them with the two modalities to evaluate. We conclude that segmentation of T1-DUAL modality achieves around 6% higher IoU than T2-SPIR for our experimental dataset. Based on the results we conclude that, from the perspective of segmentation performance, T1-DUAL exhibits better contrast to individualize abdominal organs.",Yes,"본 논문은 MRI 모달리티가 복부 장기 분할의 정확도에 미치는 영향을 실험적으로 분석하고, 세 가지 딥러닝 네트워크 아키텍처를 구축 및 평가하여 직접적인 연구 결과를 제시하고 있다. 이는 독창적인 실험과 분석을 포함한 연구 논문에 해당한다."
What's Your Stake in Sustainability of AI?: An Informed Insider's Guide,https://dl.acm.org/doi/10.5555/3716662.3716726,"It's no secret that AI systems come with a significant environmental cost. This raises the question: What are the roles and responsibilities of computing professionals regarding the sustainability of AI? Informed by a year-long informal literature review on the subject, we employ stakeholder identification, analysis, and mapping to highlight the complex and interconnected roles that five major stakeholder groups (industry, practitioners, regulatory, advocacy, and the general public) play in the sustainability of AI. Swapping the traditional final step of stakeholder methods (stakeholder engagement) for entanglement, we demonstrate the inherent entwinement of choices made with regard to the development and maintenance of AI systems and the people who impact (or are impacted by) these choices. This entanglement should be understood as a system of human and non-human agents, with the implications of each choice ricocheting into the use of natural resources and climate implications. We argue that computing professionals (AI-focused or not) may belong to multiple stakeholder groups, and that we all have multiple roles to play in the sustainability of AI. Further, we argue that the nature of regulation in this domain will look unlike others in environmental preservation (e.g., legislation around water contaminants). As a result, we call for ongoing, flexible bodies and policies to move towards the regulation of AI from a sustainability angle, as well as suggest ways in which individual computing professionals can contribute to fighting the environmental and climate effects of AI.",No,"본 논문은 AI 지속 가능성에 대한 이해와 관련 이해관계자 분석을 중심으로 한 문헌 리뷰 및 개념적 논의를 제공하며, 직접적인 실험적 연구나 독창적인 데이터 분석 결과를 포함하지 않습니다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Sojourn Time Minimization of Successful Jobs,https://doi.org/10.1145/3561074.3561083,"Due to a growing interest in deep learning applications [5], compute-intensive and long-running (hours to days) training jobs have become a significant component of datacenter workloads. A large fraction of these jobs is often exploratory, with the goal of determining the best model structure (e.g., the number of layers and channels in a convolutional neural network), hyperparameters (e.g., the learning rate), and data augmentation strategies for the target application. Notably, training jobs are often terminated early if their learning metrics (e.g., training and validation accuracy) are not converging, with only a few completing successfully. For this motivating application, we consider the problem of scheduling a set of jobs that can be terminated at predetermined checkpoints with known probabilities estimated from historical data. We prove that, in order to minimize the time to complete the first K successful jobs on a single server, optimal scheduling does not require preemption (even when preemption overhead is negligible) and provide an optimal policy; advantages of this policy are quantified through simulation. Related Work. While job scheduling has been investigated extensively in many scenarios (see [6] and [2] for a survey of recent result), most policies require that the cost of waiting times of each job be known at scheduling time; in contrast, in our setting the scheduler does not know which job will be the K-th successful job, and sojourn times of subsequent jobs do not contribute to the target metric. For example, [4, 3] minimize makespan (i.e., the time to complete all jobs) for known execution times and waiting time costs; similarly, Gittins index [1] and SR rank [7] minimize expected sojourn time of all jobs, i.e., both successfully completed jobs and jobs terminated early. Unfortunately, scheduling policies not distinguishing between these two types of jobs may favor jobs where the next stage is short and leads to early termination with high probability, which is an undesirable outcome in our applications of interest.",Yes,본 논문은 딥러닝 훈련 작업의 스케줄링 문제에 대해 새로운 최적 정책을 증명하고 시뮬레이션을 통해 그 우수성을 입증하는 등 독창적인 연구 내용을 포함하고 있다. 기존 연구와 차별화된 문제 설정과 해결책을 제시하여 직접적인 연구 기여를 하고 있음을 알 수 있다.
Current methods for quality control in manufacturing companies in the Czech Republic,https://doi.org/10.1145/3361785.3361788,"The fourth industrial revolution (Industry 4.0) fundamentally transforms modern production, thanks to new technological achievements including digitalization, robotization, artificial intelligence, etc. With the advent of the Industry 4.0 concept, many new technologies are coming into the field of production. Technologies such as the Internet of Things, autonomous robots, big data, additive manufacturing or QR codes have emerged. These modern technologies can be implemented to different areas of production but we are mainly focused on big data and following data mining application in quality management, especially product quality control. Academic literature is paying increased attention to this topic but the question arises: Are these modern methods also already deployed for quality control in the real manufacturing companies in the Czech Republic? The paper aims to respond to this and several more questions. For the purpose of this research, a structured questionnaire has been distributed to manufacturing companies from various specializations and different sizes in the Czech Republic. In the first part of this study, we made a comparison of current trends in manufacturing (mainly in quality control) according to the critical literature review and findings from the questionnaires describing the real situation of the companies. We investigated that only 7.73% of asked companies use some of data mining methods for quality control. On the 5% success rate we say that used methods can probably depend on the company's location, the Chi-square test proved it by result of 0.9153. Contrariwisely we found out that used methods do not depend on company's size based on results from the Chi-square test value (5.108e-05).",Yes,"본 논문은 체코 공화국 제조업체에서 품질 관리를 위한 데이터 마이닝 방법의 실제 적용 현황을 조사하기 위해 설문조사를 실시하고, 문헌 검토와 실증 분석을 통해 새로운 연구 결과를 제시하고 있다. 이는 독창적인 연구 내용과 직접적인 실증 데이터를 포함한 연구 논문에 해당한다."
SOVEREIGN - Towards a Holistic Approach to Critical Infrastructure Protection,https://doi.org/10.1145/3664476.3671410,"In the digital age, cyber-threats are a growing concern for individuals, businesses, and governments alike. These threats can range from data breaches and identity theft to large-scale attacks on critical infrastructure. The consequences of such attacks can be severe, leading to financial losses, threats to national security, and the loss of lives. This paper presents a holistic approach to increase the security of critical infrastructures. For that, we propose an open, self-configurable, and AI-based automated cyber-defense platform that runs on specifically hardened devices and own hardware, can be deeply embedded in critical infrastructures and provides full visibility on network, endpoints, and software. In this paper, starting from a thorough analysis of related work, we describe the vision of our SOVEREIGN platform in the form of an architecture, discuss individual building blocks, and evaluate it qualitatively with respect to our requirements.",Yes,"본 논문은 기존 연구 분석을 바탕으로 SOVEREIGN이라는 AI 기반 자동 사이버 방어 플랫폼의 아키텍처와 구성 요소를 제안하고, 이를 평가하는 내용을 포함하고 있어 독창적인 연구 기여를 담고 있다. 따라서 연구 논문에 해당한다고 판단된다."
Reconstruction-based anomaly detection for the cloud: A comparison on the Yahoo! Webscope S5 dataset,https://doi.org/10.1145/3416921.3416934,"The detection of anomalies in cloud metrics is an important way to identify suspicious data instances that indicate a system problem such as hardware failures, performance bottlenecks or intrusions. Yet, especially in a cloud computing infrastructure where the amount and variety of services is constantly increasing, it is getting more and more challenging to monitor and maintain the system manually. Thus, it is beneficial to use machine learning to detect anomalies at least partially in an automated way. The contribution of this paper is two-folded: firstly, we evaluate three unsupervised, reconstruction-based methods for anomaly detection (PCA, Autoencoder, LSTM-Encoder-Decoder) on the Yahoo! Webscope S5 benchmark dataset. Secondly, we compare our chosen models to a widely-used density-based approach and show that our reconstruction-based approaches outperform the related work.",Yes,"본 논문은 세 가지 비지도 학습 기반 재구성 방법을 평가하고, 이를 기존 밀도 기반 접근법과 비교하여 성능 우위를 입증하는 독창적인 연구 내용을 포함하고 있습니다. 이는 새로운 실험적 결과와 분석을 제공하는 연구 논문에 해당합니다."
A Flexible Toolflow for Mapping CNN Models to High Performance FPGA-based Accelerators,https://doi.org/10.1145/3543622.3573131,"There have been many studies on developing automatic tools for mapping CNN models onto FPGAs. However, challenges remain in designing an easy-to-use toolflow. First, the toolflow should be able to handle models exported from various deep learning frameworks and models with different topologies. Second, the hardware architecture should make better use of on-chip resources to achieve high performance. In this work, we build a toolflow upon Open Neural Network Exchange (ONNX) IR to support different DL frameworks. We also try to maximize the overall throughput via multiple hardware-level efforts. We propose to accelerate the convolution operation by applying parallelism not only at the input and output channel level, but also at the output feature map level. Several on-chip buffers and corresponding management algorithms are also designed to leverage abundant memory resources. Moreover, we employ a fully pipelined systolic array running at 400 MHz as the convolution engine, and develop a dedicated bus to implement the im2col algorithm and provide feature inputs to the systolic array. We generated 4 accelerators with different systolic array shapes and compiled 12 CNN models for each accelerator. Deployed on a Xilinx VCU118 evaluation board, the performance of convolutional layers can reach 3267.61 GOPS, which is 99.72% of the ideal throughput (3276.8 GOPS). We also achieve an overall throughput of up to 2424.73 GOPS. Compared with previous studies, our toolflow is more user-friendly. The end-to-end performance of the generated accelerators is also better than that of related work at the same DSP utilization.",Yes,"본 논문은 CNN 모델을 FPGA 기반 가속기에 매핑하는 자동화 도구를 개발하고, 하드웨어 아키텍처 설계 및 최적화 기법을 제안하는 등 독창적인 연구 내용을 포함하고 있습니다. 또한, 실제 하드웨어에서 성능 평가를 수행하여 기존 연구 대비 우수한 성능을 입증하였으므로 연구 논문에 해당합니다."
Fairify: Fairness Verification of Neural Networks,https://doi.org/10.1109/ICSE48619.2023.00134,"Fairness of machine learning (ML) software has become a major concern in the recent past. Although recent research on testing and improving fairness have demonstrated impact on real-world software, providing fairness guarantee in practice is still lacking. Certification of ML models is challenging because of the complex decision-making process of the models. In this paper, we proposed Fairify, an SMT-based approach to verify individual fairness property in neural network (NN) models. Individual fairness ensures that any two similar individuals get similar treatment irrespective of their protected attributes e.g., race, sex, age. Verifying this fairness property is hard because of the global checking and non-linear computation nodes in NN. We proposed sound approach to make individual fairness verification tractable for the developers. The key idea is that many neurons in the NN always remain inactive when a smaller part of the input domain is considered. So, Fairify leverages white-box access to the models in production and then apply formal analysis based pruning. Our approach adopts input partitioning and then prunes the NN for each partition to provide fairness certification or counterexample. We leveraged interval arithmetic and activation heuristic of the neurons to perform the pruning as necessary. We evaluated Fairify on 25 real-world neural networks collected from four different sources, and demonstrated the effectiveness, scalability and performance over baseline and closely related work. Fairify is also configurable based on the domain and size of the NN. Our novel formulation of the problem can answer targeted verification queries with relaxations and counterexamples, which have practical implications.",Yes,"본 논문은 신경망의 개별 공정성 검증을 위한 SMT 기반 접근법인 Fairify를 제안하며, 이를 통해 공정성 보증 문제를 해결하는 독창적인 방법론과 실험 결과를 제시하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문에 해당한다."
Handwriting Recognition with Artificial Neural Networks a Decade Literature Review,https://doi.org/10.1145/3386723.3387884,"Deep Learning Artificial Neural Networks has pushed forward researches in the field of pattern recognition, furthermore in human handwriting recognition. From online to offline approach, signature verification, writing or writer identification, segmentation or features extraction, a multitude of Artificial Neural Networks (ANNs) models are applied in the process. This paper focuses on the literature review of human handwriting recognition with ANN's over the last decade. We propose an exploratory analysis of 294 research papers collected from five indexed research engines: ACM Digital Library, IEEE digital library, Science Direct, Scopus and Web of Science. Our aim is to provide a research papers distribution across years and journals, a Keywords frequency analysis using cloud visualization, and a Natural Language Processing Topic Modeling using Non-Negative Matrix Factorization (NMF). The results of this study show that the number of research papers reached noticeably a peak in the 2010 with 44 published papers; also Pattern Recognition was the top publishing journal with 12 published papers. As for the topic modeling using NMF we obtained 3 topics listed as follows: 1) Feature Extraction and segmentation techniques for Handwritten Texts Recognition; 2) Signature Verification in Biometric security for Off-line Authentication; 3) Assessment Systems for Student Identification",No,"이 논문은 지난 10년간 인공신경망을 이용한 필기 인식 연구를 종합적으로 검토한 문헌 리뷰 논문입니다. 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구들을 분석하고 요약하는 데 중점을 두고 있어, 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Spatial-Net: A Self-Adaptive and Model-Agnostic Deep Learning Framework for Spatially Heterogeneous Datasets,https://doi.org/10.1145/3474717.3483970,"Knowledge discovery from spatial data is essential for many important societal applications including crop monitoring, solar energy estimation, traffic prediction and public health. This paper aims to tackle a key challenge posed by spatial data - the intrinsic spatial heterogeneity commonly embedded in their generation processes - in the context of deep learning. In related work, the early rise of convolutional neural networks showed the promising value of explicit spatial-awareness in deep architectures (i.e., preservation of spatial structure among input cells and the use of local connection). However, the issue of spatial heterogeneity has not been sufficiently explored. While recent developments have tried to incorporate awareness of spatial variability (e.g., SVANN), these methods either rely on manually-defined space partitioning or only support very limited partitions (e.g., two) due to reduction of training data. To address these limitations, we propose a Spatial-Net to simultaneously learn a space-partitioning scheme and a deep network architecture with a Significance-based Grow-and-Collapse (SIG-GAC) framework. SIG-GAC allows collaborative training between partitions and uses an exponential reduction tree to control the network size. Experiments using real-world datasets show that Spatial-Net can automatically learn the pattern underlying heterogeneous spatial process and greatly improve model performance.",Yes,"논문은 공간 이질성을 다루기 위한 새로운 딥러닝 프레임워크인 Spatial-Net을 제안하고, 이를 위한 독창적인 SIG-GAC 학습 방법을 개발하였다. 또한 실제 데이터셋을 이용한 실험을 통해 성능 향상을 입증하였으므로, 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Machine learning for accessible web navigation,https://doi.org/10.1145/3493612.3520463,"This research looks at the application of Machine Learning to Web Accessibility. It considers how Machine Learning (ML) can be used to help make the processes of Web Navigation more accessible in line with Web Content Accessibility Guideline (WCAG) 2.4 Navigable, which demands that ways be provided ""to help users navigate, find content, and determine where they are."" ML techniques such as reinforcement learning have been applied to website navigation in diverse ways. These include goal-directed search to answer questions and task-oriented problems such as booking a flight. Related work includes Web Automation and Testing. These techniques typically involve a state space exploration where actions such as clicking links, filling forms, and pressing buttons move the agent to a new state. The search space can be represented as a directed graph whose nodes represent the state. The choice of action involves a reward in reinforcement learning, which is used to learn behavior. Sequences of actions are collected into policies and the objective is to identify the best policy. For many websites, the potential search space is exceptionally large, and these techniques can provide ways of navigating them more efficiently. While the outcomes of these approaches can help to make the processes of search and task completion easier and thereby help accessibility, it is not the primary focus. The question under consideration is whether these techniques can be adapted to improve the website's navigability from an accessibility perspective. The approach seems promising. The state-space can be explored automatically, and the state representation can then be mined using ML techniques for structure, content and other information, which has the potential to improve accessibility Examples of the application of these approaches, which reflect the goals of the success criteria around WCAG Guideline 2.1 would include the generation of good anchor text for links where this is not provided, optimizing pathways to website functionality, and optimizing the processes of querying the website. There are open questions about how state representation can be enhanced to improve the prospects of reaching goal states. For example, can the process of calculating reward exploit features such as Search Engine Optimization information, which is designed to reflect the purpose of the page directly? Indexing techniques from Information science can serve a similar purpose. From an accessibility standpoint, there are exciting directions to explore. For example, what role can accessibility features, e.g., Headings, Anchor Text, and alternative text, play in calculating rewards. By way of illustration, a case study on one of these page enhancement techniques, DocTTTTTQuery, will be presented. This enhances a document by generating potential queries from its content. The system is trained by matching these generated questions against a set of historical questions asked of the site. This knowledge can then be exploited to lead the user more directly to potential answers to queries. Other studies have shown the effectiveness of this approach. Augmenting the page with these queries has shown improved search performance.",Yes,"본 논문은 머신러닝 기법을 웹 접근성 향상을 위해 적용하는 구체적인 방법과 사례(예: DocTTTTTQuery)를 제시하며, 직접적인 연구 기여와 실험적 증거를 포함하고 있다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 판단된다."
Predictive analytics in healthcare epileptic seizure recognition,https://dl.acm.org/doi/10.5555/3291291.3291327,"Introduction Clinical applications of electroencephalography (EEG) span a very broad range of diagnostic conditions. Epileptic seizure is the fourth most common neurological disorder in that. Related Work There has been considerable progress in clinical understanding of epilepsy, however many aspects of seizure prevention are still a mystery. Predictive modeling of EEG can provide significant value addition to substantiate the diagnosis of epilepsy. Methodology Machine learning algorithms are applied to predict the probability of epileptic seizure using an open source multi-class dataset. Results and Discussion Comparing the F-score from different classifiers, it is found that XGBoost gives the best performance in binary classification and Random Forest provides the best performance in multinomial classification. Conclusion Our results show that it is possible to predict epileptic seizure with significant accuracy from non-epileptic parameters using a suitable machine learning algorithm. We also observe that binary classification methods have higher prediction accuracy.",Yes,"논문 초록에서 머신러닝 알고리즘을 적용하여 간질 발작 예측 모델을 개발하고 성능을 비교하는 구체적인 연구 방법과 결과를 제시하고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다."
The challenge of detecting sophisticated attacks: Insights from SOC Analysts,https://doi.org/10.1145/3230833.3233280,"The ever-increasing rate of sophisticated cyber-attacks and its subsequent impact on networks has remained a menace to the security community. Existing network security solutions, including those applying machine learning algorithms, often centre their detection on the identification of threats in individual network events, which is proven inadequate in detecting sophisticated multi-stage attacks. Similarly, SOC analysts whose roles involve detecting advanced threats are faced with a significant amount of false-positive alerts from the existing tools. Their ability to detect novel attacks or variants of existing ones is limited by the lack of expert input from SOC analysts in their creation of the tools; and the use of features that are closely linked to the structure of specific malware which detection models aim to identify. In this work, we conduct a literature review on malware detection tools, reflect on the features used in these approaches and extend the feature-set with novel ones identified by interviewing experienced SOC analysts. We conduct thematic analysis to the qualitative data obtained from the interviews, and our results indicate not only the presence novel generic malware characteristics based on network and application events (web proxy, firewall, DNS), but identify valuable lessons for developing effective SOCs regarding their structure and processes.",Yes,본 논문은 기존 연구를 검토하고 SOC 분석가들과의 인터뷰를 통해 새로운 특징(feature)을 도출하는 등 독창적인 연구 방법과 결과를 제시하고 있다. 이는 단순한 리뷰가 아니라 새로운 데이터 수집과 분석을 포함한 직접적인 연구 기여로 판단된다.
Evaluating visual analytics for text information retrieval,https://doi.org/10.1145/3472301.3484320,"Retrieving information from document collections is necessary in many contexts, for example, researchers wish to retrieve papers on a research topic, physicians search for patient records related to a certain condition, police investigators seek for relationships in criminal reports. Common to these scenarios are users in need of identifying relevant textual information in a document collection. The task is challenging, especially when users hope for a retrieval process that misses none or very few of the relevant documents. Visual Analytics (VA) approaches are often advocated to support document retrieval tasks. VA relies on integrating interactive visualizations and machine learning algorithms so that a domain expert can gradually steer a system into identifying the relevant documents. As an example, TRIVIR is a state-of-the-art system that allows exploring a corpus while providing feedback to a classifier that suggests potentially relevant documents to a reference query document. Assessing VA-supported Information Retrieval (IR) strategies is also challenging, as using these systems typically involves many conceptual and practical aspects and text retrieval tasks can demand considerable cognitive effort. In this paper, we present results from observational studies on VA-supported text information retrieval. We conducted sessions with graduate students and researchers using TRIVIR to explore scientific papers for purposes of literature review. A first study allowed us to collect opinions and identify some usability issues and practical limitations of the available implementation. After handling some critical issues observed at the interface level, we conducted a second round of sessions in order to collect further user opinions regarding a retrieval process assisted with VA. We concluded that most users have a very positive view of the system's usability and its ability to facilitate their retrieval tasks. Nonetheless, we also learnt that a proper introduction to the role of the interface elements is important and that conveying the underlying conceptual model and its limitations can be difficult. We observed considerable variation in user assessment of the specific functionalities and some users may face practical difficulties in using the system autonomously in an optimal way.",Yes,본 논문은 TRIVIR이라는 시각적 분석 시스템을 활용한 텍스트 정보 검색에 대한 관찰 연구를 수행하여 사용자 의견과 사용성 문제를 분석하고 개선한 결과를 제시하고 있다. 이는 기존 시스템 평가와 사용자 연구를 통해 직접적인 연구 기여와 새로운 지식을 제공하는 독창적인 연구 내용으로 판단된다.
UX Research on Conversational Human-AI Interaction: A Literature Review of the ACM Digital Library,https://doi.org/10.1145/3491102.3501855,"Early conversational agents (CAs) focused on dyadic human-AI interaction between humans and the CAs, followed by the increasing popularity of polyadic human-AI interaction, in which CAs are designed to mediate human-human interactions. CAs for polyadic interactions are unique because they encompass hybrid social interactions, i.e., human-CA, human-to-human, and human-to-group behaviors. However, research on polyadic CAs is scattered across different fields, making it challenging to identify, compare, and accumulate existing knowledge. To promote the future design of CA systems, we conducted a literature review of ACM publications and identified a set of works that conducted UX (user experience) research. We qualitatively synthesized the effects of polyadic CAs into four aspects of human-human interactions, i.e., communication, engagement, connection, and relationship maintenance. Through a mixed-method analysis of the selected polyadic and dyadic CA studies, we developed a suite of evaluation measurements on the effects. Our findings show that designing with social boundaries, such as privacy, disclosure, and identification, is crucial for ethical polyadic CAs. Future research should also advance usability testing methods and trust-building guidelines for conversational AI.",No,"본 논문은 ACM 디지털 라이브러리에서 발표된 기존 연구들을 문헌 리뷰(literature review) 방식으로 종합하고 분석한 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 기존 연구를 정리하고 평가하는 리뷰 논문에 해당합니다."
Discovering important source code terms,https://doi.org/10.1145/2889160.2891037,"Terms in source code have become extremely important in Software Engineering research. These ""important"" terms are typically used as input to research tools. Therefore, the quality of the output of these tools will depend on the quality of the term extraction technique. Currently, there is no definitive best technique for predicting the importance of terms during program comprehension. In my work, I perform a literature review of several techniques. I then propose a unified importance prediction model based on a machine learning algorithm. I evaluate my model in a field study involving professional programmers, as well as a standard 10-fold synthetic study. I found that my model predicts the top quartile of most-important source code terms with approximately 50% precision and recall, outperforming tf/idf and other popular techniques. Furthermore, I found that, during actual program comprehension tasks, the predictions from my model help programmers equivalent to a real set of most-important terms.",Yes,"논문은 기존 기법들을 검토한 후, 기계 학습 기반의 중요도 예측 모델을 제안하고 이를 실제 프로그래머를 대상으로 평가하는 독창적인 연구를 수행하였다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Social Transparency in Network Monitoring and Security Systems,https://dl.acm.org/doi/abs/10.1145/3626705.3627773,"System administrators (sysadmins) are key to keeping computer networks safe. As networks grow in size and complexity, partial workflow automation with the help of AI has been introduced to assist them. However, AI-aided tools often lack transparency, which may lead to the sysadmin’s reluctance to use the new software, slower response time in case of a security breach, and increasing errors. Related work suggests that the concept of social transparency (ST), when applied to the IT-security context, enables peer support and could provide the missing knowledge to the user facilitating explainability of the system and improving human-AI trust. In this paper, we investigate the profile of sysadmins and confirm that ST can indeed yield benefits for them but only when coupled with relevant contextual information and only when it adheres to the sysadmins’ quality standards. Finally, we contribute design recommendations for incorporating ST into the existing workflows of sysadmins.",Yes,"본 논문은 시스템 관리자들의 작업 환경에서 사회적 투명성(Social Transparency)을 적용하여 AI 도구의 설명 가능성과 신뢰성을 향상시키는 방법을 조사하고, 관련 설계 권고안을 제시하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 바탕으로 새로운 실증적 분석과 설계 기여를 제공하는 연구 논문으로 판단된다."
Agents in the Computing Continuum: the MLSysOps Perspective,https://dl.acm.org/doi/10.5555/3639940.3639991,"Multi-agent systems (MASs) have gained considerable attention in the field of distributed computing due to their ability to provide technical interoperability, resource sharing and flexible coordination. Consequently, MAS are well-suited to address the challenges posed by the distributed and heterogeneous nodes within the device-edge-cloud continuum, including orchestration and standardization, optimal resource allocation,micro service placement policies, security and privacy.The objective of this study is to introduce the MLSysOps project, which aims at the autonomous management of the entire continuum tackling some of the challenges mentioned before. MLSysOps utilizes a hierarchical agent-based AI architecture to interface with the underlying resource management and application deployment/orchestration mechanisms. A comparative analysis is conducted between the existing related work and the proposed framework, highlighting the peculiarities and advantages of our approach.",Yes,"논문 초록에서 MLSysOps 프로젝트를 소개하며, 계층적 에이전트 기반 AI 아키텍처를 활용한 자율 관리 프레임워크를 제안하고 기존 연구와 비교 분석하는 등 독창적인 연구 내용과 기여를 포함하고 있음을 알 수 있습니다. 이는 직접적인 연구 기여가 포함된 연구 논문임을 시사합니다."
Towards smartphone-based sensing of social interaction for ambulatory assessment,https://doi.org/10.1145/2800835.2801642,"In ambulatory assessment, especially when handling subjects with personality disorders, it is important to monitor the subject's social interactions. Smartphones are already applied to assess information via self-reports. Related work also uses them to log context information or prompt event-specific self-reports. We see a high potential for them to be used for monitoring social interactions in in-field studies as they are constant companions in real life and a platform for virtual interactions. Our system will apply pattern recognition and machine learning algorithms to physical sensor measurements such as microphone and radio signals, but also to virtual sensor information such as call and message history and activity of messaging apps. We will evaluate to which extent and how well our system can automatically and unobtrusively find indicators for social interactions and, based on them, identify anomalies in the subject's behavior.",Yes,"논문 초록에서 스마트폰 센서 데이터를 활용한 사회적 상호작용 모니터링 시스템을 개발하고, 패턴 인식 및 머신러닝 알고리즘을 적용하여 자동으로 사회적 상호작용 지표를 탐지하는 연구를 수행한다고 명시하고 있습니다. 이는 독창적인 연구 내용과 실험적 평가를 포함하는 연구 논문임을 나타냅니다."
Machine learning in computer forensics (and the lessons learned from machine learning in computer security),https://doi.org/10.1145/2046684.2046700,"In this paper, we discuss the role that machine learning can play in computer forensics. We begin our analysis by considering the role that machine learning has gained in computer security applications, with the aim of aiding the computer forensics community in learning the lessons from the experience of the computer security community. Afterwards, we propose a brief literature review, with the purpose of illustrating the areas of computer forensics where machine learning techniques have been used until now. Then, we remark the technical requirements that should be meet by tools for computer security and computer forensics applications, with the goal of illustrating in which way machine learning algorithms can be of any practical help. We intend this paper to foster applications of machine learning in computer forensics, and we hope that the ideas in this paper may represent promising directions to pursue in the quest for more efficient and effective computer forensics tools.",No,"논문 초록에서 주로 기존 문헌 리뷰와 기계 학습이 컴퓨터 포렌식에 어떻게 적용될 수 있는지에 대한 논의에 초점을 맞추고 있으며, 독창적인 연구 결과나 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 직접 기여하는 연구 논문으로 보기 어렵습니다."
Revisiting Habitability in Conversational Systems,https://doi.org/10.1145/3334480.3383014,"Conversational systems are inherently disadvantaged when indicating either what capabilities they have or the state they are in. The notion of habitability, the appropriate balancing in design between the language people use and the language a system can accept, emerged out of these early difficulties with conversational systems. This literature review aims to summarize progress in habitability research and explore implications for the design of current AI-enabled conversational systems. We found that i) the definitions of habitability focus mostly on matching between user expectations and system capabilities by employing well-balanced restrictions on language use; ii) there are two comprehensive design perspectives on different domains of habitability; iii) there is one standardized questionnaire with a sub-scale to measure habitability in a limited way. The review has allowed us to propose a working definition of habitability and some design implications that may prove useful for guiding future research and practice in this field.",No,초록에 따르면 이 논문은 기존 연구들을 종합하여 습관성(habitability) 연구의 진행 상황을 요약하고 설계 시사점을 탐구하는 문헌 리뷰이다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 해석에 초점이 맞춰져 있다.
"Developing High-Performance, Portable OpenCL Code via Multi-Dimensional Homomorphisms",https://doi.org/10.1145/3318170.3318171,"A key challenge in programming high-performance applications is achieving portable performance, such that the same program code can reach a consistent level of performance over the variety of modern parallel processors, including multi-core CPU and manycore Graphics Processing Units (GPU), and over the variety of problem sizes. Popular approaches to parallel programming are either restricted to the hardware of a particular vendor (like CUDA for NVIDIA) or, even if they provide code portability (like OpenCL), performance portability is usually not available: for example, a parallel program achieving high performance on a GPU often yields poor performance on a CPU, or even on another GPU model. The reason is that hardware architectures differ significantly in their characteristics, e.g., GPU provide a high number of cores but small caches while CPU have a low number of cores and big caches; also GPU from different vendors (e.g., NVIDIA vs. AMD) pose different or even contradicting requirements on the code for achieving the full performance potential of the corresponding architecture. Performance differs also across input sizes. For example, a high-performance implementation of GEneral Matrix-Matrix Multiplication (GEMM) targeting big input matrices differs significantly from a GEMM implementation optimized for small matrices, e.g., as used in deep learning. This is because high performance on big matrices is achieved by computing all elements of the resulting matrix simultaneously and each of them sequentially, whereas for high performance on small matrices, the computation of each element should be parallelized as well. The lack of performance portability often requires re-designing program code for every new target architecture and/or another problem size. In this talk, we address an approach to performance portability based on patterns of parallelism and auto-tuning. We extend the functional formalism of Multi-Dimensional Homomorphisms (MDH) that allows expressing a wide range of applications (including the popular BLAS routines and stencil computations) as MDH-instances. For MDH, we develop a generic OpenCL implementation schema. This schema is performance-portable: it is parametrized with the performance-critical parameters of OpenCL's platform and memory model, such that, for each particular MDH-instance, particular problem size and particular target architecture, we can fully automatically find the well-performing parameter values using our novel Auto-Tuning Framework (ATF), and thereby adapt the OpenCL code correspondingly. Our experiments with linear algebra routines (BLAS) and stencil applications demonstrate that we reach competitive and often even significantly better performance than the related work -- e.g., speedup factors of up to 5x over the hand-implemented, vendor-provided BLAS libraries Intel MKL and NVIDIA cuBLAS -- on representative parallel architectures and for important input sizes that are used in deep learning.",Yes,"논문은 Multi-Dimensional Homomorphisms를 활용한 성능 이식성 있는 OpenCL 코드 개발과 이를 위한 자동 튜닝 프레임워크를 제안하며, 실험을 통해 기존 라이브러리 대비 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
SAWACMMM'17: The 1st Workshop on Multi Media Applications within the South African Context,https://doi.org/10.1145/3123266.3132052,"The South African research community has strong individual interests in pattern recognition and machine learning, but to date has had limited interactions with the worldwide multimedia research community. In an attempt to redress this, this workshop aims to introduce a selection of South African researchers to the multimedia community, and expose the multimedia community to a range of multimedia-related work, primarily from South Africa. The theme to be presented will be broader than strict multimedia, but as far as the methodologies applied are concerned, investigations explored are relevant to real time practical problems encountered in multimedia. Many of the problems to be discussed are also particular to Southern Africa. The applications proposed for discussion vary from biological research to communications protocol optimisation and computer vision. More generally, the workshop will focus on probabalistic graphical models, a powerful modelling technique that we believe will be of great interest to the multimedia research community.",No,"본 논문은 워크숍 소개 및 연구자 교류를 목적으로 한 내용으로, 직접적인 독창적 연구 결과나 실험 데이터가 포함되어 있지 않습니다. 따라서 연구 논문이라기보다는 워크숍 개요 및 주제 소개에 해당합니다."
A Clairvoyant Approach to Evaluating Software (In)Security,https://doi.org/10.1145/3102980.3102991,"Nearly all modern software has security flaws---either known or unknown by the users. However, metrics for evaluating software security (or lack thereof) are noisy at best. Common evaluation methods include counting the past vulnerabilities of the program, or comparing the size of the Trusted Computing Base (TCB), measured in lines of code (LoC) or binary size. Other than deleting large swaths of code from project, it is difficult to assess whether a code change decreased the likelihood of a future security vulnerability. Developers need a practical, constructive way of evaluating security. This position paper argues that we actually have all the tools needed to design a better, empirical method of security evaluation. We discuss related work that estimates the severity and vulnerability of certain attack vectors based on code properties that can be determined via static analysis. This paper proposes a grand, unified model that can predict the risk and severity of vulnerabilities in a program. Our prediction model uses machine learning to correlate these code features of open-source applications with the history of vulnerabilities reported in the CVE (Common Vulnerabilities and Exposures) database. Based on this model, one can incorporate an analysis into the standard development cycle that predicts whether the code is becoming more or less prone to vulnerabilities.",Yes,"논문은 기존 평가 방법의 한계를 지적하고, 머신러닝 기반의 예측 모델을 제안하여 소프트웨어 취약점 위험도를 평가하는 새로운 방법론을 제시하고 있다. 이는 독창적인 연구 내용과 실증적 모델 개발을 포함하므로 연구 논문에 해당한다."
Cognitive Discovery: Accelerating Technical R&amp;D with AI,https://doi.org/10.1145/3369583.3393610,"Cognitive Discovery is an overarching framework that uses AI to achieve scientific knowledge extraction and representation, to intelligently design and guide simulations, in order to drastically accelerate the pace of scientific discovery. Cognitive Discovery targets to accelerate scientific workflows in technical disciplines and provide a new generation of tools. The workflows follow the cycle: a) Massive literature review in order to understand the problem at hand. Literature refers to all aspects such as mathematical modelling, solution methods, actual computer models and HPC deployment. b) Enrichment of literature data with experimental data and formation of hypotheses. c) Running simulations to test hypotheses and generate new knowledge in order to close any knowledge gaps. All three phases suffer today major disruptions. Simply put: the volume of new literature is exploding (e.g. roughly 450K new publications in materials science are published every year, tens of thousands of papers in numerical and HPC methods need to be reviewed). IoT advances as well as advances in measuring all aspects of HPC systems create an explosion of data. High fidelity models lead to massive configuration spaces the complexity of which clearly outpaces our capability to scale and efficiently run modern HPC systems. We will showcase how AI can help dramatically improve this setting and lead to a massive acceleration for scientific discovery.",No,"논문 초록은 AI를 활용한 과학적 발견 가속화 프레임워크의 개념과 적용 가능성을 설명하고 있으나, 구체적인 실험 결과나 독창적인 연구 방법론, 데이터 분석 결과 등 직접적인 연구 기여 내용은 포함되어 있지 않습니다. 따라서 본 논문은 연구 논문보다는 개념적 제안 또는 리뷰 성격에 가깝다고 판단됩니다."
"Measurable and Deployable Security: Gaps, Successes, and Opportunities",https://doi.org/10.1145/3422337.3450328,"Security measurement helps identify deployment gaps and present extremely valuable research opportunities. However, such research is often deemed as not novelty by academia. I will first share my research journey designing and producing a high-precision tool CryptoGuard for scanning cryptographic vulnerabilities in large Java projects. That work led us to publish two benchmarks used for systematically assessing state-of-the-art academic and commercial solutions, as well as help Oracle Labs integrate our detection in their routine scanning. Other specific measurement and deployment cases to discuss include the Payment Card Industry Data Security Standard, which was involved in high-profile data breach incidents, and fine-grained Address Space Layout Randomization (ASLR). The talk will also point out the need for measurement in AI development in the context of code repair. Broadening research styles by accepting and encouraging deployment-related work will facilitate our field to progress towards maturity.",Yes,"논문 초록에서 CryptoGuard라는 고정밀 도구를 설계하고 제작한 연구 여정을 공유하며, 이를 통해 두 개의 벤치마크를 발표하고 실제 상용 솔루션과의 평가를 수행한 점이 명확히 기술되어 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문임을 나타냅니다."
AI Governor for the Quality and the Strength of Bridges,https://doi.org/10.1145/3407982.3408020,"Construction architectures draw a new world pattern. Designing innovative shapes and their building cost a fortune and investors expect the relevant return in money and population benefits. Some of the representative examples of expensive constructive creativity are wide highways, bridges, skyscrapers, grand halls, malls, tall flats, etc. Each newly started or finalized construction must operate with a smart fault-tolerant risk management mechanism because the world has been a witness of many construction failures. They lead to additional reversible and irreversible damages, some of which cost people's lives. In 2018 the road viaduct in Genoa collapsed. As a result of the tragedy, at least thirty-eight people died, thirty-five cars and three trucks fall. In the basement of that collapse are a wrong construction project, the neglected need for reconstruction and the lack of a system to inform citizens about the bridge condition. Such accidents bear the question of how to prevent similar situations in advanced. Bridge engineering is difficult for standardization, therefore there is a need for a new technique to handle the issue. The authors of the report examine the way modern information and communication technologies impact quality control. The researchers make a relevant scientific literature review. They explore qualitative, quantitative and statistical methods of evaluation and apply computer comparison and modelling to create a new form for measuring and controlling the quality and strength of bridges. The new model introduces an AI governor for bridge management and a self-learning machine to adapt existing construction standards and to create new ones according to the situation and environmental conditions. The proposed solution incorporates real-time data analyses.",Yes,논문 초록에서 연구자들이 기존 문헌 검토를 바탕으로 새로운 AI 기반 모델과 자가 학습 기계를 제안하여 교량의 품질과 강도를 측정하고 관리하는 독창적인 방법을 개발한 점이 명확히 드러납니다. 이는 직접 기여하는 연구 내용으로 판단됩니다.
Critical-Reflective Human-AI Collaboration: Exploring Computational Tools for Art Historical Image Retrieval,https://dl.acm.org/doi/10.1145/3610054,"Just as other disciplines, the humanities explore how computational research approaches and tools can meaningfully contribute to scholarly knowledge production. Building on related work from the areas of CSCW and HCI, we approach the design of computational tools through the analytical lens of 'human-AI collaboration.' Such work investigates how human competencies and computational capabilities can be effectively and meaningfully combined. However, there is no generalizable concept of what constitutes 'meaningful' human-AI collaboration. In terms of genuinely human competencies, we consider criticality and reflection as guiding principles of scholarly knowledge production and as deeply embedded in the methodologies and practices of the humanities. Although (designing for) reflection is a recurring topic in CSCW and HCI discourses, it has not been centered in work on human-AI collaboration. We posit that integrating both concepts is a viable approach to supporting 'meaningful' human-AI collaboration in the humanities and other qualitative, interpretivist, and hermeneutic research areas. Our research, thus, is guided by the question of how critical reflection can be enabled in human-AI collaboration. We address this question with a use case that centers on computer vision (CV) tools for art historical image retrieval. Specifically, we conducted a qualitative interview study with art historians to explore a) what potentials and affordances art historians ascribe to human-AI collaboration and CV in particular, and b) in what ways art historians conceptualize critical reflection in the context of human-AI collaboration. We extended the interviews with a think-aloud software exploration. We observed and recorded participants' interaction with a ready-to-use CV tool in a possible research scenario. We found that critical reflection, indeed, constitutes a core prerequisite for 'meaningful' human-AI collaboration in humanities research contexts. However, we observed that critical reflection was not fully realized during interaction with the CV tool. We interpret this divergence as supporting our hypothesis that computational tools need to be intentionally designed in such a way that they actively scaffold and support critical reflection during interaction. Based on our findings, we suggest four empirically grounded design implications for 'critical-reflective human-AI collaboration': supporting reflection on the basis of transparency, foregrounding epistemic presumptions, emphasizing the situatedness of data, and strengthening interpretability through contextualized explanations.",Yes,"본 논문은 인문학 분야에서 인간-AI 협업을 지원하는 컴퓨터 비전 도구의 설계와 활용에 대해 질적 인터뷰 연구와 소프트웨어 탐색을 통해 직접적인 연구를 수행하고 있으며, 이를 바탕으로 설계 시사점을 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 연구 논문에 해당한다."
Towards Deferred Execution of a SYCL Command Graph,https://doi.org/10.1145/3585341.3585375,"A key concept in SYCL’s execution model is the use of command groups that create a directed acyclic graph of kernel executions at runtime. A command group object defines a set of dependencies or edges that must be satisfied for kernels or nodes to be executed. However, because command group submission is tied to execution on the queue, without having a prior construction step before starting execution, optimization opportunities can be missed from the runtime not being made aware of a defined dependency graph ahead of execution. This represents de facto a built-in eager execution mode in SYCL in contrast to a lazy execution mode where definition and submission of work is decoupled. We propose an extension to the SYCL 2020 specification [6], which closes this gap by introducing the concept of a command graph. We add new mechanisms for the user to build a command graph for later execution. Commands are added to a graph, finalized to prepare for execution, and finally executed on a queue. The extension decouples overhead associated with submission by performing expensive operations and optimizations at finalize time and allowing for batching of commands at submission time. This command batching is supported by many SYCL backends but not exposed to users through the SYCL API. In addition to the benefits to the SYCL runtime, there are also advantages to the user developing SYCL applications. Repetitive workloads no longer must redundantly issue the same sequence of commands. Instead, a graph is only constructed once and submitted for execution as many times as is necessary, only changing the data in input buffers or USM (Unified Shared Memory) allocations. For applications from specific domains, such as machine learning as well as computer vision, where the same command group pattern is run repeatedly for different inputs, this is particularly useful. This talk is presented in two sections. First, we provide an overview of the specification for the extension. This includes two distinct mechanisms for graph building: An explicit API that provides a new set of functions for expressing a command graph directly in SYCL code, and the “Record & Replay” API that is designed to retrofit existing codebases and enable the use of existing libraries and frameworks with minor modifications. We discuss the mechanisms available for modifying a graph after construction and the motivation for the API design compared to other similar mechanisms in use today in other programming models. In the second section of our talk, we detail the work in progress for implementing the extension in Intel’s DPC++ runtime, in particular the early-stage prototype [3]. We will show execution traces demonstrating the potential overhead reduction that is possible, as well as current limitations, and what we’ve learned from implementing it so far. This includes an overview of how our implementation maps to the various backends available and how to address situations where there is no backend support. We also examine plans for the future of our proposal and implementation and the optimization possibilities that it enables such as inter-node memory reuse and interactions with other relevant SYCL extensions. RELATED WORK CUDA Graphs supports deferred work submission in CUDA. It enables a similar work submission model as described in this work but limited to NVIDIA GPUs [5]. The OpenCL command-buffer extension [1] provides a mechanism to record a set of commands for repetitive enqueuing. It can be seen as lower level compared to the work we are presenting in this abstract. Higher level and vendor-independent approaches such as the C++ Executors’ proposal (p2300r5) [4] or Kokkos Graph [2] have implementations (at least in experimental stages) that use CUDA Graphs as a backend to reduce latencies. The work as described in our proposal would represent a viable alternative for a vendor- independent backend, including benefits like better maintainability.",Yes,"논문은 SYCL 명령 그래프의 지연 실행을 위한 확장 제안을 포함하고 있으며, 이를 구현한 프로토타입과 성능 평가를 다루고 있어 독창적인 연구 내용과 직접적인 기여가 포함되어 있다. 또한 기존 기술과의 차별점과 향후 최적화 가능성도 논의하고 있어 연구 논문에 해당한다."
Open Knowledge Enrichment for Long-tail Entities,https://doi.org/10.1145/3366423.3380123,"Knowledge bases (KBs) have gradually become a valuable asset for many AI applications. While many current KBs are quite large, they are widely acknowledged as incomplete, especially lacking facts of long-tail entities, e.g., less famous persons. Existing approaches enrich KBs mainly on completing missing links or filling missing values. However, they only tackle a part of the enrichment problem and lack specific considerations regarding long-tail entities. In this paper, we propose a full-fledged approach to knowledge enrichment, which predicts missing properties and infers true facts of long-tail entities from the open Web. Prior knowledge from popular entities is leveraged to improve every enrichment step. Our experiments on the synthetic and real-world datasets and comparison with related work demonstrate the feasibility and superiority of the approach.",Yes,"본 논문은 기존 연구의 한계를 지적하고, 장기 꼬리 엔티티에 특화된 지식 보강 방법을 새롭게 제안하며, 이를 실험적으로 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Real-time human pose recognition in parts from single depth images,https://doi.org/10.1145/2398356.2398381,"We propose a new method to quickly and accurately predict human pose---the 3D positions of body joints---from a single depth image, without depending on information from preceding frames. Our approach is strongly rooted in current object recognition strategies. By designing an intermediate representation in terms of body parts, the difficult pose estimation problem is transformed into a simpler per-pixel classification problem, for which efficient machine learning techniques exist. By using computer graphics to synthesize a very large dataset of training image pairs, one can train a classifier that estimates body part labels from test images invariant to pose, body shape, clothing, and other irrelevances. Finally, we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs in under 5ms on the Xbox 360. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state-of-the-art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.",Yes,"논문은 단일 깊이 이미지에서 실시간으로 인간 자세를 인식하는 새로운 방법을 제안하며, 이를 위해 중간 표현 설계, 대규모 합성 데이터셋 생성, 효율적인 분류기 학습 등 독창적인 연구 내용을 포함하고 있습니다. 또한, 실험을 통해 기존 방법 대비 우수한 성능을 입증하여 직접적인 연구 기여가 있음을 보여줍니다."
Predicting clicks in a vocabulary learning system,https://dl.acm.org/doi/10.5555/2000976.2000994,"We consider the problem of predicting which words a student will click in a vocabulary learning system. Often a language learner will find value in the ability to look up the meaning of an unknown word while reading an electronic document by clicking the word. Highlighting words likely to be unknown to a reader is attractive due to drawing his or her attention to it and indicating that information is available. However, this option is usually done manually in vocabulary systems and online encyclopedias such as Wikipedia. Furthurmore, it is never on a per-user basis. This paper presents an automated way of highlighting words likely to be unknown to the specific user. We present related work in search engine ranking, a description of the study used to collect click data, the experiment we performed using the random forest machine learning algorithm and finish with a discussion of future work.",Yes,"논문 초록에서 사용자의 클릭 데이터를 수집하고, 랜덤 포레스트 기계학습 알고리즘을 사용한 실험을 수행했다고 명시하고 있어 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 알 수 있습니다. 또한, 기존 수동 방식과 달리 자동화된 개인 맞춤형 단어 하이라이팅 방법을 제안하고 있어 직접적인 연구 기여가 있습니다."
A framework for agent-based distributed machine learning and data mining,https://doi.org/10.1145/1329125.1329243,"This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents. We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes. This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions. We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms. We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.",Yes,"이 논문은 에이전트 기반 분산 기계학습 및 데이터 마이닝을 위한 새로운 프레임워크를 제안하고, 이를 실제 분산 클러스터링 응용에 적용하여 실험 결과를 보고하고 있다. 따라서 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단된다."
Legal reasoning - a jurisprudential description,https://doi.org/10.1145/74014.74034,"This paper provides a description of a legal reasoning process. The presentation originates from a research project combining Law and Artificial Intelligence (AI) and contains theoretical results from system-developing activities that have been carried out in cooperation with the Swedish Court Administration and a major Swedish employer's association. The research project, and several parallel projects at the Swedish Law and Informatics Research Institute (IRI), is being documented in the series IRI-reports. Related work, especially focusing on computerized formalization of legal norms and legal decision processes from a jurisprudential perspective is carried out at The Norwegian Research Center for Computers and Law, but contributions have also been made by many others1 and legal reasoning has been investigated from somewhat different perspectives2.",Yes,"논문 초록에서 법적 추론 과정을 설명하며, 스웨덴 법원 행정처 및 고용주 협회와의 협력 하에 시스템 개발 활동에서 도출된 이론적 결과를 포함하고 있음을 명시하고 있습니다. 이는 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 나타냅니다."
Perspectives in deductive databases (Abstract only),https://doi.org/10.1145/28659.28673,"I will discuss my experiences, some of the work that I have done and related work that influenced me, concerning deductive databases over the last 30 years. It will be convenient to divide this time period into roughly three equal parts, 1957 - 1968, 1969 - 1978, 1979 - present. For the first portion I will describe how my interest started in deductive databases in 1957, at a time when not even the field of databases existed I will describe work in the beginning years, leading to the start of deductive databases in about 1968 with the work of Cordell Green and Bertram Raphael. The second period saw a great deal of work in theorem proving as well as the introduction of logic programming. The existence and importance of deductive databases as a formal and viable discipline received its impetus at a workshop held in Toulouse, France, in 1977, which culminated in the book, Logic and Data Bases. The relationship of deductive databases and logic programming was recognized at that time. During the third and most recent period we have seen formal theories of databases come about as an outgrowth of that work, and the recognition that artificial intelligence and deductive databases are closely related, at least through the so-called expert database systems. I expect that the relationships between techniques from formal logic, databases, logic programming, and artificial intelligence will continue to be explored and the field of deductive databases will become a more prominent area of computer science in coming years.",No,"초록 내용은 저자의 경험과 관련 연구를 회고하는 개요적 성격을 띠고 있으며, 새로운 연구 결과나 독창적인 실험, 분석 내용이 포함되어 있지 않습니다. 따라서 직접 기여하는 독창적인 연구 논문으로 보기 어렵습니다."
Comparing Generative AI Literature Reviews Versus Human-Led Systematic Literature Reviews: A Case Study on Big Data Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938577,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are transforming research methodologies, including Systematic Literature Reviews (SLRs). While traditional, human-led SLRs are labor-intensive, AI-driven approaches promise efficiency and scalability. However, the reliability and accuracy of AI-generated literature reviews remain uncertain. This study investigates the performance of GPT-4-powered Consensus in conducting an SLR on Big Data research, comparing its results with a manually conducted SLR. To evaluate Consensus, we analyzed its ability to detect relevant studies, extract key insights, and synthesize findings. Our human-led SLR identified 32 primary studies (PSs) and 207 related works, whereas Consensus detected 22 PSs, with 16 overlapping with the manual selection and 5 false positives. The AI-selected studies had an average citation count of 202 per study, significantly higher than the 64.4 citations per study in the manual SLR, indicating a possible bias toward highly cited papers. However, none of the 32 PSs selected manually were included in the AI-generated results, highlighting recall and selection accuracy limitations. Key findings reveal that Consensus accelerates literature retrieval but suffers from hallucinations, reference inaccuracies, and limited critical analysis. Specifically, it failed to capture nuanced research challenges and missed important application domains. Precision, recall, and F1 scores of the AI-selected studies were 76.2%, 38.1%, and 50.6%, respectively, demonstrating that while AI retrieves relevant papers with high precision, it lacks comprehensiveness. To mitigate these limitations, we propose a hybrid AI-human SLR framework, where AI enhances search efficiency while human reviewers ensure rigor and validity. While AI can support literature reviews, human oversight remains essential for ensuring accuracy and depth. Future research should assess AI-assisted SLRs across multiple disciplines to validate generalizability and explore domain-specific LLMs for improved performance.",Yes,"본 논문은 GPT-4 기반 AI와 인간 주도 체계적 문헌고찰(SLR)을 비교 분석하는 독창적인 연구를 수행하였으며, AI의 성능 평가와 한계점 도출, 그리고 하이브리드 프레임워크 제안을 포함하고 있어 직접 기여하는 연구 내용이 포함되어 있다. 따라서 연구 논문에 해당한다."
Factors Affecting the Effectiveness of Generative Artificial Intelligence Apps on University Students' Programming Language Learning in Sri Lanka: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10499744,"In today's era, technology has become pervasive worldwide, significantly facilitating access to learning resources. Notably, the emergence of Generative Artificial Intelligence (AI) has garnered rapid attention and interest in a short period with the introduction of ChatGPT. Many individuals have extensively discussed and evaluated this AI-powered language model, from researchers to casual internet users. Importantly, Generative AI applications are increasingly recognized for their potential in educational contexts. In the realm of education, AI has the potential to significantly broaden and improve teaching and learning in higher education. However, while numerous studies have explored the effectiveness of Generative AI applications in programming language learning, an absence of research examining their impact comprehensively exists. Hence, this study aims to identify the factors that affect the successful utilization of Generative AI applications in the context of undergraduate programming language learning, with a particular emphasis on the viewpoints of university students. A systematic literature review was undertaken to obtain the research objectives, adhering to the Prisma 2020 guidelines, which involved selecting and analyzing 47 prior studies. Mainly this study utilized a systematic literature review to comprehend the factors influencing the effective utilization of Generative AI apps by undergraduate students in their programming learning experiences. Furthermore, the study discusses the advantages and challenges university students face when learning programming using generative AI applications.",No,"본 논문은 47개의 기존 연구를 체계적으로 검토하는 문헌 리뷰 연구로, 직접적인 독창적 실험이나 새로운 데이터 수집을 통한 연구 결과를 제시하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하여 분석하는 연구에 해당한다."
Navigating the Ethical Terrain of AI-Generated Text Tools: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10813359,"This review examines the ethical, social, and technical challenges posed by AI-generated text tools, focusing on their rapid advancement and widespread adoption. An exhaustive literature search across many databases, strict inclusion/exclusion criteria, and a rigorous analysis procedure are all parts of our systematic review technique. This guarantees an impartial and complete study of the current status of AI-generated text tools. The study analyzes prominent language models, including GPT-3, GPT-4, LaMDA, PaLM, Claude, Jasper, and Llama 2, evaluating their capabilities in natural language processing and generation. The analysis reveals significant advancements, with GPT-3 demonstrating a 92% accuracy rate on standard natural language understanding benchmarks, outperforming LaMDA (88%) and PaLM (85%). To illustrate real-world implications, the review presents a case study of ChatGPT’s application in healthcare, where it achieved 80% consistency with expert opinions in assessing acute ulcerative colitis. This case highlights both the potential benefits and ethical concerns of AI in critical domains. Quantitative bias analysis shows that GPT-3 generated biased content in 15% of test cases involving sensitive topics, a higher rate than LaMDA (12%) and PaLM (10%). We provide an in-depth analysis of fairness and bias issues, particularly in image generation tasks depicting professional roles. Our research synthesizes insights from technical advancements, ethical considerations, and real-world applications across healthcare, education, and creative sectors. We address critical privacy concerns and data protection challenges, noting struggles in AI-generated text detection and investigating AI’s potential in enabling cyberattacks. We underscore the need for comprehensive governance systems and multidisciplinary cooperation. To provide a cohesive analysis of the ethical considerations surrounding AI-generated text tools, we employ a multifaceted ethical framework drawing on established theories. Utilitarianism, which seeks to maximize happiness for everyone; deontology, which places an emphasis on right and wrong; and Virtue Ethics, which analyzes the moral nature of deeds and actors, are all included in this framework. In this article, we use this approach to investigate AI ethics from a variety of angles, including privacy, prejudice, and social implications, as well as concerns of justice and fairness. Moreover, the study critically examines existing and proposed legal frameworks addressing AI ethics, identifying regulatory gaps and proposing adaptive policy recommendations to address the unique challenges posed by AI-generated text tools. Our review contributes a critical analysis of AI-generated text tools, their impacts, and the need for responsible innovation. The study provides precise guidelines for the ethical development and implementation of AI, highlighting the need to strike a balance between technical progress and ethical concerns to guarantee that AI technologies have a beneficial effect on society while protecting human values. The emergence of generative artificial intelligence (AI) signifies a substantial revolution in our methods of interacting with language and information.",No,"본 논문은 AI 생성 텍스트 도구의 윤리적, 사회적, 기술적 문제를 종합적으로 검토하는 체계적 문헌고찰(review) 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 분석하고 종합한 리뷰 논문에 해당합니다."
Work-in-Progress: Course-based Undergraduate Research Experience (CURE) with Generative AI in a Computer Science Course,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893139,"This work-in-progress innovative practice paper describes a novel integration of Generative AI with Course-based Undergraduate Research Experiences (CUREs). CUREs integrate research activities into the curriculum, allowing all students in a course to participate in inquiry-based research projects. Generative Artificial Intelligence (AI) applications are advanced AI designed to generate human-like responses by processing natural language inputs. These applications leverage machine learning models to produce outputs that can assist users in a variety of tasks from writing to coding. The integration of Generative AI with CURE had been adopted in a text-based machine learning course during the Fall 2023 semester. A comparative analysis had been conducted on student survey responses from Fall 2022 and Fall 2023 to evaluate the effectiveness of Generative AI in a CURE integrated course. Descriptive statistics and statistical tests were conducted to assess differences in student perceptions between the two semesters. Although the differences were not statistically significant, the results indicate a promising trend towards improved student perceptions of both the overall course effectiveness and the benefits of Generative AI in enhancing various aspects of the research process, especially the literature review.",Yes,"논문은 Generative AI를 CURE에 통합한 새로운 교육 방식을 실제로 적용하고, 학생 설문 데이터를 통해 그 효과를 분석하는 독창적인 연구 내용을 포함하고 있다. 이는 직접적인 연구 기여를 포함한 연구 논문으로 판단된다."
Reconfigurable Transmitarray Design with Generative Adversarial Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10906961,"The advent of artificial intelligence (AI) has introduced innovative approaches for the design of metasurfaces, significantly reducing the reliance on traditional iterative analytical methods based on simulations. Numerous studies (e.g., S. An et al., “Deep Convolutional Neural Networks to Predict Mutual Coupling Effects in Metasurfaces”, Advanced Optical Materials, vol. 10, 2022) have demonstrated substantial advancements by leveraging various deep learning networks to address macroscopic-level challenges, such as beam steering, pattern optimization, and aperture efficiency enhancement. Conversely, there is a growing interest in employing generative AI algorithms to address the more intricate task of designing unit cells, which is crucial to achieving the desired electromagnetic (EM) surface properties (C. Niu et al., “A Diffusion Model for Multi-Layered Metasurface Unit Cell Synthesis”, IEEE Open Journal of Antennas and Propagation, vol. 4, 2023). This unit-cell-level design process has long been recognized as one of the most time-intensive aspects of metasurface development, primarily due to the extensive literature review and the iterative nature of the trial-and-error approach conventionally required for achieving optimal designs. At the meta-atom level, AI-based design methodologies are commonly applied to solve the inverse design problem, where the process begins with the desired surface performance and seeks to generate the appropriate unit-cell geometry. A notable example of this is the work of Sensong An et al. in “A Deep Learning Approach for Objective-Driven All-Dielectric Metasurface Design” (ACS Photonics, vol. 6, issue 12, Nov. 2019), which demonstrated the potential of AI to design passive metasurfaces in a matter of seconds by utilizing variants of generative adversarial networks (GANs) for the rapid and efficient creation of multifunctional passive designs. Reconfigurable metasurfaces with beam-steering capabilities, such as transmitarrays or reflectarrays, traditionally time-consuming to design, are also promising candidates for the application of AI-driven methods. However, unlike passive meta-atom designs, reconfigurable metasurfaces pose significant challenges for AI-based approaches. A major limitation is the difficulty in generating high-quality, effective datasets for training. Moreover, the active structures, which retain fixed geometrical configurations while representing different phase states (or electrical connections), introduce an added complexity in terms of dimensionality, further complicating the AI-based modeling and optimization processes. Nevertheless, it is of great interest to conduct a pilot study on the application of AI-based design approaches for reconfigurable transmitarrays/reflectarrays to evaluate any potential advantages they offer over conventional design techniques.",No,"초록은 AI 기반 설계 방법론의 가능성과 기존 연구들을 소개하며, 재구성 가능한 전송배열 설계에 AI를 적용하는 파일럿 연구의 필요성을 언급하고 있으나, 직접적인 독창적 연구 결과나 구체적인 실험 및 기여 내용은 포함되어 있지 않습니다. 따라서 본 논문은 연구 논문이라기보다는 연구 방향성 제시나 리뷰에 가까운 것으로 판단됩니다."
Impact of Artificial Intelligence in Nursing for Geriatric Clinical Care for Chronic Diseases: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10654298,"Nurses are essential in managing the healthcare of older adults, particularly those over 65, who often face multiple chronic conditions. This group requires comprehensive physical, mental, and functional care. Recent advancements in artificial intelligence (AI) have significantly improved nursing capabilities by enabling real-time health monitoring, thus bolstering the early detection and prevention of severe health issues. Despite these advancements, the current systematic literature predominantly focuses on machine learning (ML) applications for a limited set of chronic diseases, often overlooking the extensive capabilities of deep learning (DL) technologies. Additionally, these reviews cover a narrow spectrum of studies, potentially needing broader insights and developments in the field. To address these shortcomings, our study conducts a systematic literature review of ML and DL applications in geriatric care for chronic disease management. We meticulously analyzed peer-reviewed articles published from 2014 to 2024, concentrating on AI technologies in elderly care. This review included 76 selected articles from leading publishers such as Elsevier, Springer, IEEE, MDPI, Wiley, Taylor & Francis, Nature, Cambridge University Press, Oxford University Press, and arXiv, which we categorized into three main groups: Neurological disorders (27 articles), Mental Health disorders (22 articles), and Physical/physiological disorders (27 articles). Our findings reveal that Random Forest, logistic regression, and convolutional neural network (CNN) are the most frequently used AI techniques, typically evaluated by accuracy metrics and the area under the curve (AUC). The findings indicate that although AI applications in geriatric care are promising, they require significant enhancements in technology and methodology to improve accuracy and reliability. Future research should focus on developing advanced AI tools, integrating cutting-edge deep learning models and comprehensive datasets to refine diagnostics and treatment protocols for chronic diseases in the elderly, ultimately enhancing patient outcomes.",No,"본 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 한계를 분석하는 데 중점을 두고 있습니다."
Early Detection of Autism Spectrum Disorder Using AI and Machine Learning Models: A Systematic Review for Effective Intervention,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895404,"Autism is a brain developmental condition that may impact the everyday life of affected individuals. Toddlers suffering from ASD are labeled by difficulties in social interaction, connecting, lack of eye contact, ignoring danger, preferring to play alone, sensitivity to loud noises, and repetitive behavior compared to normally developed (ND) children. This systematic review examines the application of artificial intelligence (AI) and machine learning (ML) models for the early detection of Autism Spectrum Disorder (ASD), emphasizing their potential for enabling timely and effective interventions. The significance of this research lies in addressing the critical need for early ASD diagnosis, which can substantially improve outcomes for affected individuals. The novelty of this study is its comprehensive analysis of cutting-edge AI and ML techniques specifically applied to ASD detection, providing insights into their efficacy and limitations. The methodology involved a systematic search of peer-reviewed literature published between 2010 and 2022 across major databases, including PubMed, IEEE Xplore, and Scopus. Studies were selected based on predefined inclusion criteria, focusing on AI and ML models used for early ASD detection. Key findings reveal that AI and ML models, particularly deep learning algorithms and ensemble methods, demonstrate promising results in early ASD detection, with some models achieving accuracy rates exceeding 95%. The review identified that behavioral data, neuroimaging, and genetic information are the most commonly used input features for these models. However, challenges persist in terms of model interpretability, dataset bias, and generalizability across diverse populations. This systematic review highlights the potential of AI and ML models as valuable tools for early ASD detection, potentially revolutionizing diagnostic processes and enabling earlier interventions. Future research should focus on developing more robust, interpretable models and validating them across larger, more diverse datasets to enhance their clinical applicability and reliability in real-world settings.",No,"이 논문은 인공지능과 머신러닝 모델을 이용한 자폐 스펙트럼 장애 조기 발견에 관한 기존 연구들을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합적 분석에 초점이 맞춰져 있습니다."
STAR-ML: A Rapid Screening Tool for Assessing Reporting of Machine Learning in Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918312,"Literature review provides researchers with an overview of the field and when presented as a systematic assessment, it summarizes state-of-the-art information and identifies knowledge gaps. While there are many tools for assessing quality and risk-of-bias within studies, there is currently no generalized tool for evaluating the transparency, reproducibility, and correctness of machine learning (ML) reporting in the literature. This study proposes a new tool (Screening Tool for Assessing Reporting of Machine Learning; STAR-ML) that can be used to screen articles for a systematic or scoping review focusing on the reporting of the ML algorithm. This paper describes the development of the tool to assess the quality of ML research reporting and how it can be applied to improve the literature review methodology. The tool was tested and updated using three independent raters on 15 studies. The inter-rater reliability and the time used to review an article were evaluated. The current version of STAR-ML has a very high inter-rater reliability of 0.923, and the average time to screen an article was 4.73 minutes. This new tool will allow for filtering ML-related papers that can be included in a systematic or scoping review by ensuring transparent, reproducible, and correct screening of research for inclusion in the review article.",Yes,"본 논문은 ML 연구 보고의 투명성, 재현성, 정확성을 평가하는 새로운 도구(STAR-ML)를 개발하고 검증하는 독창적인 연구 내용을 포함하고 있습니다. 도구 개발과 평가 과정에서 실험적 데이터(상호 평가자 신뢰도, 소요 시간)를 제시하여 직접적인 연구 기여를 하고 있으므로 연구 논문에 해당합니다."
Prediction Models for Type 2 Diabetes Progression: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10606225,"Diabetes, especially type 2 diabetes (T2D), is a chronic disease affecting millions of people worldwide. The increasing prevalence of T2D, coupled with the complex interplay between genetic, environmental, and lifestyle factors, presents a major challenge for effective disease management. The traditional methods for predicting T2D progression and determining appropriate treatment strategies are often subjective and less accurate, resulting in treatment delays. Therefore, artificial intelligence (AI) based prediction models become crucial, as they offer a more objective and data-driven approach to T2D management. By leveraging advanced statistical techniques and machine learning algorithms, AI-based prediction models can better identify patients at high risk for T2D progression and predict responses to different treatment options. This can ultimately lead to improved outcomes for patients suffering from T2D. Therefore, this paper aims to review the existing research articles published from 2018 to 2022 using a systematic literature review (SLR) approach. From 40 selected articles, a taxonomy of the most common techniques for developing a prediction model in diabetes progression is drawn in three approaches: mathematical, machine learning (ML), and deep learning (DL). In addition, the best practices of dataset characteristics, pre-processors, and evaluation metrics of the existing algorithms are also provided, focusing on the context of diabetes progression prediction. The findings found that the majority of the selected papers employed ML, specifically the RF model, proven to have superiority in performance. This review also discusses current challenges faced in building prediction models for diabetes progression and proposes future research directions to overcome these challenges. The promising directions drawn include 1) incorporating feature reduction or importance tools to explore the relationship between variables, 2) developing an interpretable predictive model to provide analytical results that are understandable to clinicians, and 3) validating the model with multiple large-sample size datasets and seeking clinical advice from experts.",No,"이 논문은 2018년부터 2022년까지 발표된 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 새로운 예측 모델 개발을 제시하지 않고 기존 연구들을 종합하여 분석하고 있다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Terahertz Data Extraction and Analysis Based on Deep Learning Techniques for Emerging Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418122,"Following the recent progress in the development of Terahertz (THz) generation and detection, THz technology is being widely used to characterize test sample properties in various applications including nondestructive testing, security inspection and medical applications. In this paper, we have presented a broad review of the recent usage of artificial intelligence (AI) particularly, deep learning techniques in various THz sensing, imaging, and spectroscopic applications with emphasis on their implementation for medical imaging of cancerous cells. Initially, the fundamentals principles and techniques for THz generation and detection, imaging and spectroscopy are introduced. Subsequently, a brief overview of AI – machine learning and deep learning techniques is summarized, and their performance is compared. Further, the usage of deep learning algorithms in various THz applications is reported, with focus on metamaterials design and classification, detection, reconstruction, segmentation, parameter extraction and denoising tasks. Moreover, we also report the metrics used to evaluate the performance of deep learning models and finally, the existing research challenges in the application of deep learning in THz cancer imaging applications are identified and possible solutions are suggested through emerging trends. With the continuous increase of acquired THz data – sensing, spectral and imaging, artificial intelligence has emerged as a dominant paradigm for embedded data extraction, understanding, perception, decision making and analysis. Towards this end, the integration of state-of-the-art machine learning techniques such as deep learning with THz applications enable detailed computational and theoretical analysis for better validation and verification than modelling techniques that precede the era of machine learning. The study will facilitate the large-scale clinical applications of deep learning enabled THz imaging systems for the development of smart and connected next generation healthcare systems as well as provide a roadmap for future research direction.",No,"본 논문은 THz 기술과 딥러닝의 적용 현황을 종합적으로 리뷰하는 논문으로, 기존 연구들을 정리하고 향후 연구 방향을 제시하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵습니다."
Informetric Analysis of Researches on Application of Artificial Intelligence in Legal Practice,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10091079,"The advent of artificial intelligence technology is the telling sign of the fourth industrial revolution, which has deeply changed our social life. The advantage of deep learning and big-data analysis of artificial intelligence technology makes it widely used in legal research, e-discovery, analysis, and documentation. However, the positioning of artificial intelligence is still in dispute, and the application of artificial intelligence in legal practice might lead to potential risks. In this paper, research on the application of artificial intelligence in legal practice was analyzed by the informetric method. 2932 papers indexed in Thomson Reuters's Web of Science have been studied from the perspectives of keywords co-occurrence. The analysis of keywords co-occurrence shows the application of machine learning and deep learning in legal practice. We also analyzed the review literature on the application of artificial intelligence in legal practice in the Web of Science. We found that these papers can be divided into the following three categories: the application of artificial intelligence in legal services and judicial adjudication, the positioning of AI in legal relations, and third is the legal measures to prevent the potential harm of artificial intelligence.",No,"본 논문은 인공지능의 법률 실무 적용에 관한 기존 연구들을 계량서지학적 방법으로 분석한 문헌 연구에 해당하며, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 종합하고 분류하는 리뷰 성격이 강합니다."
A Review on Machine Learning Styles in Computer Vision—Techniques and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903420,"Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.",No,"본 논문은 다양한 머신러닝 스타일에 대한 문헌 리뷰와 미래 방향성을 제시하는 종합적인 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하고 있지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석한 리뷰 논문에 해당합니다."
Data-Driven Analytics for Reliability in the Buildings-to-Grid Integrated System Framework: A Systematic Text-Mining-Assisted Literature Review and Trend Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323487,"Data-driven machine learning-based methods have provided immense capabilities, revolutionizing sectors like the Buildings-to-grid (B2G) integrated system. Since the penetration rate of distributed energy resources increases towards a net-zero emissions power system, so does the need for advanced services that ensure B2G-integrated system reliability. The convergence of advancements in machine learning, computational resources at the entire cloud-edge continuum, and large datasets from sensing devices enable the development of these data-driven energy analytics services. This work conducts a systematic text-mining-based literature review to examine the diverse range and trends of machine learning methods used to enhance reliability in B2G-integrated systems. While traditional manual sampling and analysis approaches have limited the effectiveness of previous literature review papers in this field, this systematic literature review work aims to synthesize and summarize the existing body of research more efficiently and effectively. To achieve this, this study collected almost 10,500 papers from Scholar and Scopus databases. It employed text-mining-assisted BERTopic-based topic modelling and statistical trend analysis techniques to uncover semantic patterns and explore the temporal evolution of research themes. A two-dimensional taxonomy was derived to analyze the technical papers from a business and machine learning-related perspective. By quantifying the temporal trends within these topics, the study unveiled insights about the state-of-the-art analytics that ensure reliability in the B2G-integrated system domain while proposing future research directions.",No,"본 논문은 데이터 기반 문헌 리뷰와 텍스트 마이닝 기법을 활용하여 기존 연구들을 체계적으로 분석하고 연구 동향을 파악하는 리뷰 논문입니다. 직접적인 독창적 연구 결과나 실험, 모델 개발 등의 기여보다는 기존 연구의 종합과 분석에 중점을 두고 있어 연구 논문으로 보기 어렵습니다."
IoT based Chatbots using NLP and SVM Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9853095,"As of late, there has been a developing revenue in creating AI-empowered chatbot-based indication checker (CSC) applications in the medical services market. CSC applications give expected determinations to clients and help them with self-triaging in light of Artificial Intelligence (AI) procedures utilizing human-like discussions. The review presents an original PC application going about as an individual virtual specialist that has been ideally planned and broadly prepared to collaborate with patients like people. This application depends on a serverless engineering and it totals the administrations of a specialist by giving preventive measures, home cures, intuitive directing meetings, medical care tips, and side effects covering the most pervasive sicknesses in provincial India. Man-made reasoning (AI) is progressively being utilized in medical services. Here, AI-based chatbot frameworks can go about as robotized conversational specialists, equipped for advancing wellbeing, giving schooling, and conceivably provoking conduct change. Investigating the inspiration to utilize wellbeing chatbots is expected to foresee take-up. This conversational application has brought about diminishing the hindrances for admittance to medical services offices and secures shrewd interviews from a distance to permit opportune consideration and quality therapy, along these lines really helping the general public. This conversational application has achieved decreasing the obstructions for permission to clinical benefits workplaces and ties down canny meetings from a distance to allow perfect thought and quality treatment, thusly truly helping the overall population [1]. Therefore, in this research paper, we have discussed about the different roles of artificial intelligence, the way it uniquely involves itself in chatbots and cover it and make it to the most important working part of the model. Specifically, in this, we have discussed about the role in medical healthcare related chatbots along with the integration of IoT devices such as batteries and sensors in this case with the chatbot.",Yes,"논문 초록에서 AI 기반 챗봇 애플리케이션을 설계하고 구현한 구체적인 연구 내용을 다루고 있으며, IoT 기기와의 통합 등 독창적인 시스템 개발에 대한 설명이 포함되어 있습니다. 이는 직접 기여하는 연구 논문임을 나타냅니다."
Explainable AI and Big Data Analytics for Data Security Risk and Privacy Issues in the Financial Industry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912422,"This paper is a comprehensive review of the current literature on the importance of data security in the financial industry when working with Explainable Artificial Intelligence (XAI) and Big Data Analytics. Collaborative initiatives in the financial industry have significantly propelled the rapid evolution of e-Commerce with the help of data and have spurred the implementation of advanced technology such as data analysis and explainable AI (XAI) that can provide human-understandable explanations on the reason for providing a good level of transparency and traceability for decisions made [1]. This growth raises concerns regarding secure data processing and preserving privacy within the database used for data mining and cost-effective big data analytics including structured, unstructured, and semi-structured data crucial for decision making along with maintaining the integrity and accuracy of stored data. The reliance on explainable artificial intelligence (XAI) for data analytics has raised critical concerns, highlighting the need for comprehensive strategies to state the importance of maintaining data privacy and security. Ensuring the protection of sensitive information while harnessing the power of analytics for decision making becomes a critical challenge that demands immediate attention. Explainable artificial intelligence (XAI) is the most important technology used in the financial sector for decision-making on applications that require explaining the reasons that made the algorithm model come with the outcome, as customers have the right to know on what basis the result of their application was evaluated. Although much research has been conducted on the technological function of XAI research on how to process these data securely and safely remains very limited [2]. To address these necessities, this research aims to provide insights into the best ways to conduct data analytics in a privacy-conscious world and to ensure that data analytics can be conducted with the help of XAI, which will help maximize the efficiency of automated data processing and provides the result with an explanation for the reason for the result. This research paper will contribute to the existing literature on data analytics with AI and provides analysis for the financial industry to improve its data analytics capabilities, providing insights and guidance in optimizing big data analytics within the financial sector addressing cost-effectiveness, privacy concerns, and efficiency in data model selection for enhanced decision-making and operational performance. The unique insights or advancements the paper offers to the existing body of knowledge are a thorough analysis of current explainable AI techniques and their applications in addressing data security, risk, and privacy challenges specific to the financial sector, a new framework that integrates explainable AI with big data analytics to enhance data security and privacy in financial institutions, introduction of an innovative risk assessment model that leverages explainable AI to identify and mitigate potential security threats in financial data systems, a new innovative risk assessment model that leverages explainable AI to identify and mitigate potential security threats in financial data systems, presenting a new novel privacy-preserving techniques that utilize explainable AI to protect sensitive financial information while maintaining data utility, identification of key areas for future research, highlighting potential advancements in explainable AI that could further improve data security and privacy in the financial industry for future research directions.",Yes,"논문 초록에서 기존 문헌 리뷰뿐만 아니라 새로운 프레임워크, 혁신적인 위험 평가 모델, 그리고 프라이버시 보호 기법 등 독창적인 연구 기여를 명확히 제시하고 있어 직접 기여하는 연구 논문에 해당한다. 또한 금융 산업의 데이터 보안과 프라이버시 문제를 해결하기 위한 구체적인 방법론과 모델을 제안하고 있어 연구 논문으로 판단된다."
Methods and Applications of Artificial Intelligence In Mental Health Care,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851672,"Mental disorders affect one in eight individuals globally, but access to professional assistance and necessary information is often limited. Artificial intelligence (AI) technology has the potential to revolutionize mental health care. This paper aims to provide a theoretical overview of the recent application of AI in the field of mental health. A non-systematic review of studies from the last decade, focusing on recent findings, was conducted. The paper summarizes the commonly used machine learning techniques including Support vector machines, followed by Random forest, Naïve bayes, Logistic regression, and K-nearest neighbors for prediction and diagnosis, research, and classification. Among deep learning techniques, the most frequently applied are Convolutional neural network, Recurrent neural networks, and hybrid deep learning models for prediction, diagnosis, screening, and research. Natural language processing, wearable AI, and AI-based virtual reality are also applied for monitoring, treatment, prediction, and research. Most of the current attention is centered around depression, anxiety, and mental health problems in general. Although AI-based applications claim to improve wellbeing, limited research currently supports these claims. Furthermore, the paper explores the responsible implementation of AI in mental health care, considering technical, clinical, and ethical perspectives. The need for a regulatory framework for all AI solutions in mental health care is emphasized, and further research is called for to evaluate their reliability and risks. In conclusion AI application in mental health care is a delicate balancing act between optimism and caution.",No,"본 논문은 최근 AI 기술의 정신 건강 분야 적용에 대한 이론적 개요와 비체계적 문헌 리뷰를 제공하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않습니다. 따라서 연구 논문보다는 종합적 고찰에 해당합니다."
Biological age estimation from ECG using Artificial Intelligence approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932128,"This paper explores the application of Artificial Intelligence (AI) techniques in healthcare, specifically focusing on electrocardiogram (ECG) data analysis and biological age estimation. The study begins with an overview of ECG as a diagnostic tool and discusses the integration of Deep Learning (DL) models, particularly Convolutional Neural Networks (CNNs), in healthcare for various applications. Previous studies reviewed in this context consistently identify CNNs as the superior model for ECG analysis and prediction tasks. The primary research investigates the use of Machine Learning (ML) algorithms to estimate biological age from ECG data, employing metrics such as Mean Absolute Error (MAE) and variance to evaluate model performance. Subsequently, DL models are implemented and compared, demonstrating superior accuracy in estimating the biological age of new patients. While both ML and DL approaches were initially explored, this paper primarily focuses on deep learning methodologies due to their demonstrated superiority in healthcare applications. The findings highlight the efficacy of deep learning over traditional ML methods, underscoring its potential for enhancing diagnostic accuracy and personalized patient care. For instance, the most accurate deep learning model, the Convolutional Neural Network (CNN), achieved a Mean Absolute Error (MAE) of 1.49 and an R2 score of 0.53, whereas the best-performing traditional ML model, Random Forest, achieved an MAE of 1.54 and an R2 score of 0.55. These results clearly indicate the advantage of deep learning models in capturing complex patterns in ECG data, leading to more precise and reliable estimations of biological age.",Yes,본 논문은 ECG 데이터를 활용하여 생물학적 나이를 추정하는 딥러닝 및 머신러닝 모델을 직접 구현하고 성능을 평가하는 독창적인 연구 내용을 포함하고 있습니다. 따라서 연구 논문에 해당한다고 판단됩니다.
AI Pair Programming Acceptance: A Value-Based Approach with AHP Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10708135,"The emergence of Artificial Intelligence (AI) tools is transforming every aspect of life with new opportunities and risks. An impact of AI tools can be seen in AI pair programming which is defined as a generative and creative support tool with real-time interaction. The goal of this study is to explore the AI pair programming acceptance. To identify, describe, categorize, and rank the factors affecting the acceptance of AI pairs a literature review, a research model proposal based on an extension of the Value-based Adoption Model (VAM) framework, and an Analytic Hierarchy Process (AHP) analysis is conducted. The proposed model consists of six main factors and twenty-two sub-factors which are validated with an AHP analysis including eleven experts’ judgments. The findings presented the most essential factors as productivity, code accuracy, complexity, personal development, and innovativeness. The least significant factors were inspiration, motivation, intellectual property violation, AI interaction, and trust. This study provides insight to AI tool developers and producers in the context of programming on the key factors to consider for success.",Yes,"본 논문은 AI 페어 프로그래밍 수용에 영향을 미치는 요인들을 문헌 검토와 가치 기반 수용 모델 확장, 그리고 AHP 분석을 통해 직접 제안하고 검증하는 독창적인 연구를 수행하고 있다. 따라서 새로운 연구 모델을 제안하고 실증 분석을 포함한 연구 논문으로 판단된다."
Applications of Artificial Intelligence in Conversational Agents: A Systematic Literature Review of AI in Chatbots,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10674229,"Artificial Intelligence in Chatbots has been growing interest to a lot of people in different industries and fields. A software application that could hold a conversation with a human agent through text or text-to-speech. However there has been no clear definition of how AI is applied to these chatbots. This study aims to analyze on the applications of AI in Chatbots and understands its influence on diverse fields. The goal is to gather data on the fields in which AI chatbots are mostly utilized and understand the techniques, methods, and relevancy in applying AI chatbots in that field. The methods used in the study is the Systematic Literature Review (SLR) approach to address the proposed research question and the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) approach to help in the selection and acquisition of studies and articles. Findings show that the most common application of AI is Natural Language Processing (NLP), Machine Learning (ML), and AI Algorithms. The study also discovered a significant increase in published research about AI Chatbots every year. In the different fields, results show that Customer Support, Finance, and Education are the three fields that had the most studies. Chatbots have a better opportunity of being incorporated into other fields, as chatbots improve the workload and offer a more efficient method for handling the large workload. For future research, it is recommended to find more studies on the topic to validate and check the variety of applications of AI chatbots in various fields.",No,"이 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 분석한 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합하고 정리하는 리뷰 논문에 해당합니다."
Systematic Literature Review: Machine Learning Prediction Model for Covid-19 Spreading,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031414,"The dataset, methods, and machine learning prediction framework on the Covid-19 theme have been published widely and complex. Special publications on the spread of virus infection 19 in the form of a time series need to be mapped more comprehensively. This literature review aims to identify and analyze research trends, datasets, and methods used in predicting Covid-19 with Machine Learning Engineering research between 2019 and 2021. Identifying the need, specifying the research question evaluating review protocol, searching for papers, scanning papers, and reporting results are the eight major steps of this systematic literature review. The most critical aspect of systematic analysis is defining the research questions. The PICOC techniques are used to identify research questions. Journal candidates were filtered out using inclusion and exclusion criteria techniques to shrink the SLR scope area. based on a literature study it was found that research in 2019–2021 on the Covid-19 distribution prediction system used variables: susceptibility, infection, mortality, geography, weather, and patient clinical data to be processed into ANFIS machine learning prediction models and neural networks are several models. A classification model that is widely used for hybrid processing in calculating covid-19 infection prediction. The datasets that are often used do not fully meet the epidemiological aspects that trigger the spread of COVID-19 infections. ANFIS and NN are several classification methods that are widely used for hybrid processing in calculating predictions of the spread of COVID-19 infection.",No,"본 논문은 Covid-19 확산 예측을 위한 머신러닝 연구 동향과 방법을 체계적으로 문헌 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합 분석하는 문헌 리뷰에 해당합니다."
E-Commerce Fraud Detection Based on Machine Learning Techniques: Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506811,"The e-commerce industry’s rapid growth, accelerated by the COVID-19 pandemic, has led to an alarming increase in digital fraud and associated losses. To establish a healthy e-commerce ecosystem, robust cyber security and anti-fraud measures are crucial. However, research on fraud detection systems has struggled to keep pace due to limited real-world datasets. Advances in artificial intelligence, Machine Learning (ML), and cloud computing have revitalized research and applications in this domain. While ML and data mining techniques are popular in fraud detection, specific reviews focusing on their application in e-commerce platforms like eBay and Facebook are lacking depth. Existing reviews provide broad overviews but fail to grasp the intricacies of ML algorithms in the e-commerce context. To bridge this gap, our study conducts a systematic literature review using the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) methodology. We aim to explore the effectiveness of these techniques in fraud detection within digital marketplaces and the broader e-commerce landscape. Understanding the current state of the literature and emerging trends is crucial given the rising fraud incidents and associated costs. Through our investigation, we identify research opportunities and provide insights to industry stakeholders on key ML and data mining techniques for combating e-commerce fraud. Our paper examines the research on these techniques as published in the past decade. Employing the PRISMA approach, we conducted a content analysis of 101 publications, identifying research gaps, recent techniques, and highlighting the increasing utilization of artificial neural networks in fraud detection within the industry.",No,"본 논문은 머신러닝 기법을 활용한 전자상거래 사기 탐지에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰 연구로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 연구 공백을 분석하는 데 중점을 두고 있습니다."
Human-centric Requirements Engineering for Artificial Intelligence Software Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604547,"The surge in data availability and processing power has made it possible for Artificial Intelligence (AI) to advance at a faster rate. However, the different nature of AI systems has posed significant new challenges to Requirements Engineering (RE). Literature has shown that AI systems do not use current RE methods. It was also found that data scientists are taking the role of the requirements engineers resulting in software that does not focus on users needs. Building AI software with a human-centric approach has proven to produce more ethical, transparent, inclusive and non-bias outcomes. This research will look into adjusting current RE methodologies to fit into AI systems from a human-centric perspective. The project will aim to establish requirements specifications for human-centric AI and map them into a modeling language. A platform will be used to visually model and present requirements. Finally, I plan to conduct a case study to evaluate the modeling language. To date, I have conducted a Systematic Literature Review (SLR) to find current RE methodologies and challenges in AI and currently in the planning phase of a survey to find adopted practices in the industry.",Yes,"본 논문은 기존 요구공학 방법론을 AI 시스템에 맞게 조정하고, 인간 중심 요구사항 명세를 수립하며, 이를 모델링 언어에 매핑하는 등 구체적인 연구 계획과 방법론을 제시하고 있다. 또한 사례 연구를 통해 제안된 모델링 언어를 평가할 예정이므로 독창적인 연구 기여가 포함된 연구 논문으로 판단된다."
Emerging Trends in Realistic Robotic Simulations: A Comprehensive Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538106,"Simulation plays a pivotal role in providing safely reproducible scenarios to evaluate the ever-advancing domain of computer science and robotics. It was an essential part of the pandemic when no access to physical spaces was available. The advent of AI-powered platforms in conjunction with enhanced graphics, physics and other sensory engines attracts a new breed of interdisciplinary researchers to enter the robotic field, most notably from computer science, engineering and social sciences. Integration of ROS as a uniform middleware to deploy achieved outcomes in real practice provides an opportunity to move one step closer to the sim-to-real experiences that enables researchers to test ideas beyond the close laboratory spaces. There is a lack of comprehensive evaluation of ROS-enabled simulators, and the integration of advanced AI techniques for realistic scenario replication. This paper addresses this challenge by evaluating ROS-enabled simulators in the design and implementation of AI techniques through an in-depth systematic literature review (SLR). This SLR is guided by the research and commercial market demands, employing Population, Intervention, Comparison, Outcome, and Context (PICOC) and Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) frameworks with a major focus on Wheeled Mobile Robots (WMRs). We also highlight the increasing importance of game engines like Unity and Unreal in future of robotic simulations, especially under modelling close to real experiences. By comparing simulation platform features and capabilities, this paper offers guidance to developers and researchers, enabling them to select the most suitable platform for their projects efficiently that contradicts the commonly in use “one size fits all” approach. Finally, based on the thorough insights from the review, we identify and suggest some key future research directions in AI-enhanced realistic robotic simulations.",No,"본 논문은 ROS 기반 시뮬레이터와 AI 기법의 통합에 관한 체계적 문헌 고찰(SLR)을 수행한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 기존 연구들을 종합하고 평가하는 연구로서 연구 논문에 해당하지 않습니다."
Systematic Review on AI-Blockchain Based E-Healthcare Records Management Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9868006,"Electronic health records (EHRs) are digitally saved health records that provide information about a person’s health. EHRs are generally shared among healthcare stakeholders, and thus are susceptible to power failures, data misuse, a lack of privacy, security, and an audit trail, among other problems. Blockchain, on the other hand, is a groundbreaking technology that provides a distributed and decentralized environment in which nodes in a list of networks can connect to each other without the need for a central authority. It has the potential to overcome the limits of EHR management and create a more secure, decentralized, and safer environment for exchanging EHR data. Further, blockchain is a distributed ledger on which data can be stored and shared in a cryptographically secure, validated, and mutually agreed-upon manner across all mining nodes. The blockchain stores data with a high level of integrity and robustness, and it cannot be altered. When smart contracts are used to make decisions and conduct analytics with machine-learning algorithms, the results may be trusted and unquestioned. However, Blockchain is not always indestructible and suffers from scalability and complexity issues that might render it inefficient. Combining AI and blockchain technology can handled some of the drawbacks of these two technical ecosystems effectively. AI algorithms rely on data or information to learn, analyze, and reach conclusions. The performance of AI algorithms is enhanced through the data obtained from a data repository or a reliable, secure, trustworthy, and credible platform. Researchers have identified three categories of blockchain-based potential solutions for the management of electronic health records: conceptual, prototype, and implemented. The purpose of this research work is to conduct a Systematic Literature Review (SLR) to identify and assess research articles that were either conceptual or implemented to manage EHRs using blockchain technology. The study conducts a comprehensive evaluation of the literature on blockchain technology and enhanced health record management systems utilizing artificial intelligence technologies. The study examined 189 research papers collected from various publication categories. The in-depth analysis focuses on the privacy, security, accessibility, and scalability of publications. The SLR has illustrated that blockchain technology has the potential to deliver decentralization, security, and privacy that are frequently lacking in traditional EHRs. Additionally, the outcomes of the extensive analysis inform future researchers about the type of blockchain to use in their research. Additionally, methods used in healthcare are summarized per application area while their pros and cons are highlighted. Finally, the emphasized taxonomy combines blockchain and artificial intelligence, which enables us to analyze possible blockchain and artificial intelligence applications in health records management systems. The article ends with a discussion on open issues for research and future directions.",No,"본 논문은 체계적 문헌고찰(Systematic Literature Review, SLR)로서 기존 연구들을 종합하고 평가하는 데 중점을 두고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 새로운 연구 내용을 제시하는 연구 논문으로 보기 어렵습니다."
Cybersecurity in Financial Services: Addressing AI-Related Threats and Vulnerabilities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616498,"In the rapidly evolving landscape of financial services, the integration of Artificial Intelligence (AI) has ushered in a new era of efficiency and innovation. However, this advancement also brings forth a myriad of cybersecurity threats and vulnerabilities that pose significant risks to the integrity and reliability of financial systems. AI-related threats in financial services are not only diverse in nature, encompassing everything from AI-driven phishing attacks to sophisticated algorithm manipulation, but also present unique challenges in detection and mitigation due to their complexity and adaptability. This paper aims to address these critical issues by proposing a comprehensive cybersecurity framework specifically designed to combat AI-related threats within the financial sector. Through an extensive literature survey, the paper identifies key vulnerabilities introduced by AI technologies and reviews current defensive measures, highlighting their limitations. The proposed work introduces novel algorithms and mathematical models that enhance threat detection and vulnerability assessment, tailored for the intricate environment of financial services. Implementation results, supported by empirical data, demonstrate the efficacy of the proposed framework in identifying and mitigating AI-related cybersecurity threats, thereby significantly enhancing the security posture of financial institutions. This research not only contributes to the academic discourse on financial cybersecurity but also offers practical solutions for industry practitioners facing the challenges of AI-related threats.",Yes,"논문 초록에서 제안된 프레임워크와 새로운 알고리즘, 수학적 모델을 통해 AI 관련 위협 탐지 및 취약점 평가를 수행하는 독창적인 연구 내용을 포함하고 있음을 명확히 밝히고 있습니다. 또한 실험적 데이터로 구현 결과를 검증하여 직접적인 연구 기여를 하고 있음을 보여줍니다."
UAV Remote Sensing applications and current trends in crop monitoring and diagnostics: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179038,"Crop monitoring and diagnosis are crucial for efficient agricultural production, and unmanned aerial vehicle (UAV) Remote Sensing can assist in achieving this goal. This article offers an automated Systematic Literature Review (SLR) of UAV Remote Sensing for crop monitoring and diagnosis. This review analyzes the primary scientific applications and trends in this area using Deep Learning techniques to automatically select relevant articles and validate them through full reading. The SLR collected over 800 papers, of which 64 met the selection process. The articles selected by Deep Learning classifiers were successfully cataloged with high accuracy in pre-selecting articles for review. F1 scores of 93% were achieved in tests with unpublished examples for the classifier model. The review of the 64 primary studies reported a peak in UAV Remote Sensing applications in 2020, attributed to the increasing diffusion of precision farming practices with technological equipment. The UAV Remote Sensing application objectives included crop monitoring, pest and disease detection, yield prediction, and plant nutrition. Artificial Intelligence, particularly Machine Learning and Deep Learning, are widely used for UAV Remote Sensing analysis. The NDVI is the most applied vegetation index for crop condition assessment and monitoring. The proposed solution for automating the literature selection process for precision agriculture-related scientific articles can be used in other areas that require extensive literature review.",No,"본 논문은 UAV 원격탐사를 활용한 작물 모니터링 및 진단 분야의 기존 연구들을 체계적으로 문헌 검토한 리뷰 논문이다. 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구들을 분류하고 분석하는 데 중점을 두고 있어, 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Comparing large language model artificial intelligence tools in aid of electrical engineering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10814860,"Conversational AI (artificial intelligence) systems have broken into the digital technology sphere at the end of 2022. Many scientists, engineers, and authors have begun using them despite a lack of evidence for their applicability. Research has been conducted in several fields, including electrical engineering, on the applicability of generative AI in many fields, such as chemical engineering, programming, medicine, and even electrical engineering. Most, although not all, of the existing research focuses on applying specific AI models. This case study compares several generative AI models (ChatGPT, Bard, Bing AI, Copilot) through tasks in electrical engineering design. We qualitatively evaluated the AI responses to the questions. We have found that although AI tools are applicable and perform similarly well in general questions, their use is highly questionable in more specific cases, for example, in circuit design. Lastly, AIs perform equally unacceptably in tasks that require specific data, such as literature reviews. We conclude that the bots should be used sparingly with strict supervision and professional knowledge.",Yes,"본 논문은 여러 대형 언어 모델 AI 도구들을 전기공학 설계 과제에 적용하여 비교 평가하는 사례 연구를 수행하였으며, AI 응답의 질적 평가를 통해 구체적 적용 가능성과 한계를 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구를 단순히 요약한 것이 아니라 직접적인 실험과 분석을 통해 새로운 지식을 제공하고 있다."
A Review of Effectiveness and Efficiency Methodology of Decision Support System for Selecting Suppliers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346850,"In today's highly competitive global business environment, effective supplier selection plays a crucial role in the success and sustainability of organizations. Decision Support Systems (DSS) have emerged as powerful tools to aid in this complex process by providing valuable insights and data-driven recommendations for selecting suppliers. This research paper presents a comprehensive review of the methodologies used to assess the effectiveness and efficiency of Decision Support Systems employed in supplier selection processes. The primary objective of this study is to evaluate and compare the various methodologies applied in Decision Support Systems, focusing on their effectiveness and efficiency in assisting decision-makers during supplier selection. The research critically analyzes a range of published literature, academic papers, and case studies to highlight the strengths and limitations of different DSS methodologies in this context. The paper commences by elucidating the fundamental concepts of supplier selection and the significance of Decision Support Systems in streamlining this multifaceted process. Subsequently, a systematic review of existing literature is conducted to identify the key methodologies employed in DSS frameworks. Furthermore, the research explores the novel integration of emerging technologies, such as Artificial Intelligence, Machine Learning, and Big Data analytics, within Decision Support Systems for supplier selection. This investigation aims to discern how these advanced techniques contribute to the enhancement of decision-making precision and the overall efficiency of the selection process. By critically analyzing and comparing the effectiveness and efficiency of various Decision Support System methodologies, this research seeks to provide valuable insights for both academics and practitioners in supply chain management. The findings of this study will enable decision-makers to make informed choices regarding the adoption and customization of DSS frameworks according to their organizational context, requirements, and available resources. This research paper presents a comprehensive review of the effectiveness and efficiency of methodologies employed in Decision Support Systems for supplier selection. It not only enhances our understanding of the current state of DSS in supplier selection but also identifies potential areas for future research and development. The insights gained from this study have the potential to significantly contribute to the advancement of supplier selection processes, ultimately leading to improved operational efficiency, reduced costs, and enhanced competitive advantage for organizations across industries.",No,"본 논문은 기존 문헌과 사례 연구를 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 분석하고 정리하는 데 중점을 둔 문헌 리뷰에 해당합니다."
Advanced Coil Design for UAV Wireless Charging for Enhancing Magnetic Linkage and Improving Misalignment Tolerance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10594873,"In the rapidly evolving landscape of computer-related technologies, the synergy between artificial intelligence (AI) and the Internet of Things (IoT) has garnered increasing interest from both industry and academia. This burgeoning interest has catalyzed the development of applications at the network edge, which have now achieved considerable scale. As a vital and emergent field of research, machine learning at the network edge intertwines two pivotal themes: wireless communication and machine learning. This research area, termed edge machine learning, endeavors to harness vast quantities of mobile data from edge devices to train machine learning models. A primary challenge addressed in this research is the efficient allocation of limited communication resources amidst the data abundance at the network edge. This includes the strategic management of radio resources, with a particular focus on evaluating and utilizing data importance for optimal radio resource management and allocation. For instance, in wireless communications, the signal-to-noise ratio is often considered a critical metric for assessing data importance. This paper reviews and juxtaposes various methodologies and theories from related works, proposing a novel scheduling algorithm predicated on signal-to-noise ratio considerations. A simulation experiment, leveraging batch training characteristics of convolutional neural networks, is conducted to validate the proposed approach. The findings confirm that the strategic emphasis on data importance significantly enhances the efficiency of training data acquisition for edge machine learning systems.",No,"논문 초록은 UAV 무선 충전용 코일 설계와 관련된 제목과 달리, 실제 내용은 엣지 머신러닝에서 무선 통신 자원 관리 및 스케줄링 알고리즘에 관한 리뷰와 시뮬레이션 연구를 다루고 있다. 이는 기존 연구들을 비교하고 새로운 알고리즘을 제안하는 리뷰 및 시뮬레이션 연구로 보이며, 직접적인 독창적 실험이나 설계 연구로 보기 어렵다."
Advanced operation control in wind power plants using active wake control methods and artificial intelligence - state of research and concept for the project “SmartWind”,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770724,"State-of-the-art operation control methods for wind power plants (WPP) use Maximum Power Point Tracking (MPPT) to maximize the power production of each wind turbine individually. The resulting wake formations, however, affect wind turbines that are positioned downwind, deteriorate their energy yield and increase structural loads caused by wake turbulences. Advanced control methods utilize Active Wake Control (AWC) to increase the global power production and reduce structural loads on the wind turbine's components resulting from wake interactions between the turbines. While the basic principles of these AWC methods are understood, the dynamic set point generation presents a complex optimization problem especially with high numbers of clustered wind turbines to consider. In the international research project “SmartWind”, methods from the Artificial Intelligence (AI) domain are deployed for the set point generation. AI has so far been applied to improve wind speed as well as power production forecasting, supported the planning/layout process of WPPs and is successfully investigated to evaluate maintenance data for component fault detection. In this paper, multiple concepts describing the possible implementation of AI for the set point calculation of AWC methods in WPPs are presented. These include Deep Reinforcement Learning (DRL) agents in single and multi-agent configuration as well as regression models for the WPP control. In addition, a detailed literature review about existing implementations of AWC and the application of AI methods is provided. Finally, a real world case study motivates the application of AI-based AWC in modern WPPs as well as necessary steps towards its implementation.",Yes,"본 논문은 인공지능을 활용한 능동 와류 제어 방법의 구현 개념과 실제 사례 연구를 포함하여, 기존 연구를 바탕으로 새로운 제어 기법을 제안하고 구체적인 적용 방안을 탐구하는 독창적인 연구 내용을 담고 있다. 따라서 연구 논문에 해당한다."
A Comparison of AI-Enabled Digital Twins for DSP-based Self-Interference Cancellation in Wideband Full-Duplex Communications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9539817,"We propose the concept of a “digital twin” - namely, a data-driven simulation model of a full-duplex transceiver chain - to enable real-time computational modeling of the entire radio frequency (RF) system, including non-linearities, self-interference coupling, and other performance metrics that depend on multiple hard-to-model factors including the transmit signals and their waveforms, the peak power level, the instantaneous transmit power level, the transceiver circuit settings, and the proximity of the receiver. A machine-learning approach is explored for obtaining artificial intelligence (AI)-based digital twins of various types, including traditional (shallow) neural networks, deep belief networks, and deep convolutional networks for supervised learning, as well as reinforcement learning approaches. While several related approaches exist in the literature, they have generally been limited to simulations. By contrast, the goal of this work is to study the performance of state-of-art machine learning algorithms in an experimental framework using real-world test data obtained from a recently completed STAR front-end [1] . The proposed STAR front-end has measured bandwidths of better than 500 MHz across al-3 GHz range and provides better than 80 dB analog cancellation of self-interference. A dataset of experimental measurements of this front-end will be collected and applied to a suite of ML algorithms for self-interference cancellation that has recently appeared in the literature. The machine learning algorithms will be evaluated for performance in terms of computational complexity, latency, power consumption, and the reduction of self-interference from the transmitter into the receiver during STAR operation.",Yes,본 논문은 AI 기반 디지털 트윈을 활용하여 실제 실험 데이터를 바탕으로 머신러닝 알고리즘의 성능을 평가하는 독창적인 연구를 수행하고 있다. 기존 문헌과 달리 시뮬레이션이 아닌 실험적 프레임워크에서의 검증을 목표로 하여 직접적인 연구 기여가 포함되어 있다.
Expanding the ‘A’ in STEAM: Integrating Poetry and AI for Educational Evolution,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578751,"In this systematic literature review spanning the last five years (2018–2023), the role of poetry within the ‘A’ for Arts in STEAM (Science, Technology, Engineering, Arts, Mathematics) education is examined. Specifically, the primary purpose of this study is to delve into the integration of poetry and Artificial Intelligence into the STEAM teaching method. The research objectives which guide this study are to understand the methods and depth of including poetry in STEAM and evaluate how Artificial Intelligence (AI) can enhance poetry's role in STEAM. Using the PRISMA methodology, an extensive search was conducted in key databases such as Scopus and Google Scholar. This search identified 26 crucial papers, then analyzed qualitatively and quantitatively. Current STEAM teaching approaches focus on merging literary expressions with essential scientific disciplines. A large part of the findings points to new teaching methods that bring together poetic elements and the usual STEAM subjects. These methods increase student participation, bridge the gap between arts and sciences and encourage creative problem-solving. In the realm of STEAM education, the significance of Artificial Intelligence (AI) is growing. The findings suggest that AI can boost teaching methods centred on poetry in STEAM. AI allows for lesson adjustments based on students' engagement with poetry, creates platforms for more student participation, and offers feedback to help educators improve their teaching methods. However, there are challenges in merging the exactness of AI algorithms with the complexities of poetry, especially when trying to maintain poetry's essence. As STEAM education moves towards a more integrated model, the interplay between AI's technical abilities and the nuances of poetry calls for further scholarly research based on these findings.",No,"본 논문은 지난 5년간의 문헌을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분석하고 종합하는 데 초점을 맞추고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 통합과 평가에 해당한다."
A survey of machine learning applications in medical imaging for neurodegenerative disease diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10853556,"The increasing prevalence of brain degenerative disorders particularly Alzheimer's and Parkinson's, have incurred a dire need for effective diagnostic methodologies. With advancements in medical imaging techniques and Artificial Intelligence (AI), transformative potential exists to enhance early detection and disease classification. This review examines the recent progress made in the use of diagnostic medical imaging and artificial intelligence (AI) techniques for the early detection of Alzheimer's disease and Parkinson's disease. Despite significant progress, a comprehensive overview is lacking. The review aims to summarize and synthesize existing methods, critically evaluate their strengths and limitations, and identify emerging trends. Multimodal imaging data, combining information from different sources like MRI, PET, and SPECT, has shown progress in improving diagnostic accuracy and providing insights into disease progression. Advanced AI techniques, such as transfer learning and generative adversarial networks, have further enhanced the performance of diagnostic models. Previous studies have predominantly focused on using standalone techniques, often leaving the integration of multi-modal imaging and deep learning methodologies that could lead to more robust diagnostic frameworks. This literature review addresses this gap by synthesizing current research on medical imaging modalities, AI applications, and their implications in diagnosing Alzheimer's and Parkinson's diseases. This review seeks to connect technological innovations with clinical applications to establish more robust and dependable diagnostic methods.",No,"초록에서 해당 논문은 기존 연구들을 종합하고 평가하는 문헌 리뷰(survey)임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않는다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Explainable Intrusion Detection for Cyber Defences in the Internet of Things: Opportunities and Solutions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136827,"The field of Explainable Artificial Intelligence (XAI) has garnered considerable research attention in recent years, aiming to provide interpretability and confidence to the inner workings of state-of-the-art deep learning models. However, XAI-enhanced cybersecurity measures in the Internet of Things (IoT) and its sub-domains, require further investigation to provide effective discovery of attack surfaces, their corresponding vectors, and interpretable justification of model outputs. Cyber defence involves operations conducted in the cybersecurity field supporting mission objectives to identify and prevent cyberattacks using various tools and techniques, including intrusion detection systems (IDS), threat intelligence and hunting, and intrusion prevention. In cyber defence, especially anomaly-based IDS, the emerging applications of deep learning models require the interpretation of the models’ architecture and the explanation of models’ prediction to examine how cyberattacks would occur. This paper presents a comprehensive review of XAI techniques for anomaly-based intrusion detection in IoT networks. Firstly, we review IDSs focusing on anomaly-based detection techniques in IoT and how XAI models can augment them to provide trust and confidence in their detections. Secondly, we review AI models, including machine learning (ML) and deep learning (DL), for anomaly detection applications and IoT ecosystems. Moreover, we discuss DL’s ability to effectively learn from large-scale IoT datasets, accomplishing high performances in discovering and interpreting security events. Thirdly, we demonstrate recent research on the intersection of XAI, anomaly-based IDS and IoT. Finally, we discuss the current challenges and solutions of XAI for security applications in the cyber defence perspective of IoT networks, revealing future research directions. By analysing our findings, new cybersecurity applications that require XAI models emerge, assisting decision-makers in understanding and explaining security events in compromised IoT networks.",No,"초록에서 해당 논문은 기존 연구들을 종합적으로 검토하고 분석하는 리뷰 논문임을 명확히 밝히고 있습니다. 독창적인 실험 결과나 새로운 연구 방법론 제시는 없으며, 주로 기존 연구 동향과 향후 연구 방향을 논의하는 데 초점이 맞춰져 있습니다."
"Unveiling the Potential of Artificial Intelligence in Cooperative, Connected, and Automated Mobility (CCAM) Solutions: A Systematic Literature Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857172,"The mobility industry can transform significantly if Artificial Intelligence (AI) is included in Cooperative, Connected, and Automated Mobility (CCAM) systems. The key components and requirements for adopting AI in CCAM systems must be identified to get the maximum benefits. This research develops a conceptual model with a technical architecture containing essential technological components and a technology adoption framework comprising policy and business-related measures. The conceptual model is designed based on the key findings of a Systematic Literature Review (SLR) conducted in this research. After multiple screenings, 23 papers are analyzed based on the formulated research questions. This research gives researchers and practitioners a comprehensive overview of the critical technologies, their requirements, and the socioeconomic framework to adopt AI technologies in CCAM.",No,본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 통해 기존 연구들을 분석하고 개념적 모델과 기술 아키텍처를 제안하는 리뷰 논문입니다. 직접적인 실험이나 새로운 데이터 생성 등 독창적인 연구 결과를 제시하지 않으므로 연구 논문에 해당하지 않습니다.
Comprehensive Review of Fetal Health Monitoring using DL and ML techniques with Ultrasound and Cardiotocography data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481665,"The Artificial Intelligence (AI) has found extensive applications across various research domains, encompassing fields such as health and drug discovery. In the realm of pregnancy, there exist complications or disorders that pose a danger posed by well-being of both the fetus and its pregnant mother. The monitoring of fetal health is a fundamental aspect of prenatal care, playing a vital role in safeguarding the welfare of both the growing fetus and pregnant individuals. This thorough review article explores the crucial domain of fetal health assessment, with particular emphasis on the application of Deep Learning as well Machine Learning methodologies that functions in concert with ultrasound and cardiotocography (CTG) data. Evaluating fetal growth, development, and well-being, therefore establishing a fundamental. This study investigates the multifunctionality of Ultrasonography as a tool for examining fetal anatomy, as well as the informative capabilities of cardiotocography (CTG) in collecting crucial insights via the appraisal of the fetal heart rate patterns throughout the process of labour and delivery. This paper conducts preliminary analysis of the scholarly literature spanning the last five-years (2018-2023) to discern the prevalent methodologies, techniques, algorithms, and frameworks within Machine Learning and Deep Learning applied in multimodal calculating for the health and happiness of pregnant women.",No,"초록에서 해당 논문은 최근 5년간의 문헌을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있으며, 독창적인 연구 결과나 실험 데이터를 제시하지 않고 기존 연구들을 분석하는 데 중점을 두고 있습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Integration of Artificial Intelligence (AI) in Academic Libraries: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569288,"AI involves programming that conveys human intellect to computers to perform repetitive tasks. Most organizations have done integrations of AI; however, critical elements are not considered, resulting in negative user adoption, including unethical use and cost. This paper presents the findings from a Systematic Literature Review (SLR). The study found that old traditional operations in Libraries are somehow mostly automated using Chatbots in the form of Generative AI and humanoid robots, with concern of policies that are non-existing to guide the use of the tools for ethical reasons. Through the themes identified and discussed, recommendations are provided with key factors to consider as a guide when integrating AI in libraries. Future improvement in this research, such as developing a design model of AI integration in academic libraries, is recommended, and its applicability should be evaluated.",No,"본 논문은 체계적 문헌고찰(Systematic Literature Review)을 수행하여 기존 연구들을 종합하고 분석한 결과를 제시하고 있으며, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 해석에 초점을 맞춘 논문이다."
AI-based Malware Threat Prediction through CNN-SVM Ensemble,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10779683,"The dynamic terrain of malware attacks presents noteworthy obstacles to cyber security, necessitating for proactive and resilient detection techniques. Conventional signature-based methods struggle to keep pace with the new malware strains and obfuscation strategies. A systematic literature review was conducted to investigate existing methods for malware threat prediction and detection using machine learning and deep learning techniques. The review identified several promising approaches: convolutional neural networks, graph neural networks, and visual malware characterization achieving 95-99% accuracy on malware classification and detection tasks. However, major gaps were identified in the models’ generalizability across diverse malware types, robustness against evasion attempts, lack of interpretability due to the black-box nature of deep learning models, and limited evaluation on real-world emerging threats as opposed to controlled datasets. This project aimed to develop an AI-based threat predictive algorithm that leverages the power of deep learning and machine learning for effective malware detection and prediction. The suggested method utilises an ensemble approach that combines a convolutional neural network (CNN) for pattern recognition in malware code structures with a support vector machine (SVM) for robust decision boundaries in the feature space, thereby enhancing generalization, interpretability and adversarial resilience. By evaluating the model on the MalImg dataset, the system achieved 92.37% accuracy. Although the developed system exhibits optimal outcomes, several areas could use more improvement. This project contributes to the ongoing efforts in combating malware threats and highlights the potential of combining deep learning and traditional machine learning techniques for effective threat prediction and detection.",Yes,"논문 초록에서 제안된 CNN-SVM 앙상블 기반의 AI 위협 예측 알고리즘 개발과 성능 평가가 명확히 기술되어 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 기존 연구의 한계점을 분석하고 이를 개선하기 위한 새로운 방법론을 제시하고 있습니다."
A Systematic Review on Application of Deep Learning Techniques for Software Quality Predictive Modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200103,"Software quality prediction is the process of evaluating the software developed for various metrics like defect prediction, bug localisation, effort estimation etc. To evaluate these metrics a myriad of techniques have been developed in the literature, from manual assessment to application of machine learning and statistical testing. These methodologies, however, had lower accuracy in determining SQPMs due to their inability to model the complex relationships in the training data. With the wide emergence of deep learning, not only has the accuracy of the pre-existing models enhanced, but it has also opened doors for new metrics that could be evaluated and automated. This study performs a systematic literature review of research papers published from January 1990 to January 2019 that used deep learning to evaluate software quality prediction metrics (SQPM). The paper identifies 20 primary studies and 7 categories of application of deep learning in SQPM. Models using deep learning techniques significantly outperform other traditional methodologies in almost all studies. The concept and external threats to the models are limited, however the time taken to train these models is large. The techniques, currently predominantly applied for defect prediction, have shown promising results in other diverse software engineering fields like code search and effort estimation by modeling the source code efficiently. There is, hence, scope for incorporating deep learning further with pragmatic use and diverse application. The need to find scalable solutions, however, still persists.",No,"본 논문은 딥러닝 기법을 소프트웨어 품질 예측에 적용한 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 초점이 맞춰져 있습니다."
The Impact of AI on Teacher Roles and Pedagogy in the 21st Century Classroom,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10617236,"The integration of Artificial Intelligence (AI) into educational environments has ushered in a transformative era for teaching practices in the 21st-century classroom. This paper delves into the profound impact of AI on teacher roles and pedagogy, seeking to elucidate how educators can leverage these technological advancements to enhance the learning experience. Through an extensive review of existing literature, we identify the current landscape of AI in education, emphasizing both its promises and challenges. Our proposed framework introduces a collaborative model that positions AI as a complementary tool, augmenting rather than replacing teachers. This model is designed to cater to the diverse needs of students, fostering a personalized and adaptive learning environment. The study employs a mixed-methods approach, combining qualitative and quantitative methodologies, to investigate the practical implications of our proposed framework. We delve into the experiences of educators with AI integration, measure student performance, and assess overall satisfaction to provide a comprehensive analysis. The implementation model details the necessary infrastructure and training programs required for a seamless transition to an AI-enhanced pedagogical setting. The results highlight the effectiveness of AI in supporting teachers and enriching the learning experience. In conclusion, this paper not only underscores the potential of AI to revolutionize education but also acknowledges the ongoing challenges and underscores the imperative for educators to adapt continuously in the dynamic educational landscape. This research contributes to the discourse surrounding the symbiotic relationship between AI and education, offering insights for educators, policymakers, and researchers alike.",Yes,"논문은 AI가 교사 역할과 교수법에 미치는 영향을 탐구하며, 제안된 협력 모델과 혼합 방법론을 통해 실증적 연구를 수행하고 있다. 이는 기존 문헌 검토뿐만 아니라 독창적인 연구 결과와 실험적 분석을 포함하고 있어 연구 논문에 해당한다."
"Applications of AI-Enabled Deception Detection Using Video, Audio, and Physiological Data: A Systematic Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10681404,"Artificial intelligence-enabled deception detection is an emerging tool for identifying dishonest behavior in a wide range of applications, from security and forensics to politics and lower-risk everyday interactions, addressing the pressing need for enhanced trust and security in an increasingly digital and interconnected world. However, to date, approaches to achieve deception detection with AI have not been evaluated by application area, leaving a disconnect between approaches leveraged and their potential real-world use. Thus, this paper provides a systematic review of application areas for AI-enabled deception detection approaches following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology. Specifically, we discuss 93 articles in detail, (1) identifying common applications of automated deception detection in the literature, (2) enumerating deception detection approaches by application area, and (3) describing publicly available datasets per application area. We also identify open challenges, such as the lack of datasets that support cross-domain deception detection research. By focusing on the application areas of automated deception detection, this review helps to contextualize the surveyed literature and understand the specific challenges and requirements associated with different domains. Further, by examining various application areas, researchers can tailor their approaches and techniques to address the unique characteristics and constraints of each domain. This targeted approach increases the practical relevance and applicability of deception detection methods in real-world scenarios.",No,"본 논문은 AI 기반 기만 탐지 기술의 다양한 응용 분야를 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 평가에 해당합니다."
Deep Learning Methods of Dynamic Sign Language Translation: A Rapid Umbrella Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698113,"Countries globally are recognizing sign languages, which are used by more than 5% of national and global populations as official languages. In Africa, South Africa was the fourth country to officialize sign language in 2023. This trend combined with advances in computer vision and machine learning has resulted in an increase of scholarly literature on the translation between spoken and signed languages to bridge the language and communication divide. This has been accompanied by a high volume of published literature reviews on the subject, most of them distinguished by time and geographical contexts. Despite numerous published review papers on the subject, to date, no formal or umbrella reviews of these reviews have been performed to consolidate the similarities, contrasts, gaps, and opportunities reported in these reviews. In this paper, we conduct a first-of-its-kind, rapid systematic umbrella review of 11 literature reviews about machine translation and recognition of dynamic sign languages using machine learning. Three lenses namely, the evolution of machine learning and computer vision technologies and resulting applications to sign language translation; geographical contexts and associated applications; and the unique linguistic context of sign languages are used. We found that most reported literature has solved the sign language translation and recognition problem from a purely data-driven perspective, without due attention to specific linguistic complexities of the languages. Consequently, we found that most literature has not paid attention to proper corpora design, model design, and meaningful performance evaluation besides the generic and inherited measures of performance from spoken language processing and computer vision literature.",No,"본 논문은 기존 문헌 리뷰들을 종합하여 동적 수화 번역에 관한 연구 동향과 한계를 분석하는 우산 리뷰(umbrella review)로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 종합 및 분석에 초점을 둔 리뷰 논문에 해당합니다."
Application of Large Language Models in Cybersecurity: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767242,"The emergence of Large Language Models (LLMs) is currently creating a major paradigm shift in societies and businesses in the way digital technologies are used. While the disruptive effect is especially observable in the information and communication technology field, there is a clear lack of systematic studies focusing on the application and impact of LLMs in cybersecurity holistically. This article presents an exhaustive systematic literature review of 177 articles published in 2018-2024 on the application of LLMs and the use of Artificial Intelligence (AI) as a defensive measure in cybersecurity. This article contributes an analytical compendium of the recent research on the application of LLMs in offensive and defensive cybersecurity as well as in research on cyberethics, current legal frameworks, and research regarding the use of LLMs for cybersecurity governance. It also contributes a statistical summary of global research trends in the field. Of the reviewed literature, 68% was published in 2023. Nearly 30% of the articles originate from the USA and 11% from China, with other countries currently having significantly lower contributions to recent research. Most attention in recent research has been given to AI as a defensive measure, accounting for 27% of the reviewed literature. It was observed that LLMs have proven highly effective in phishing attack simulations and in managing cybersecurity administrative aspects, including defending against advanced exploits. Furthermore, LLMs show significant potential in the development of security software, further cementing their role as a powerful tool in cybersecurity innovation.",No,"본 논문은 177편의 기존 연구를 체계적으로 검토한 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합 분석하는 데 초점이 맞춰져 있습니다. 따라서 새로운 연구 기여보다는 연구 동향과 현황을 정리하는 리뷰 논문에 해당합니다."
Systematic Literature Review of Dialectal Arabic: Identification and Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354635,"It is becoming increasingly difficult to know who is working on what and how in computational studies of Dialectal Arabic. This study comes to chart the field by conducting a systematic literature review that is intended to give insight into the most and least popular research areas, dialects, machine learning approaches, neural network input features, data types, datasets, system evaluation criteria, publication venues, and publication trends. It is a review that is guided by the norms of systematic reviews. It has taken account of all the research that adopted a computational approach to dialectal Arabic identification and detection and that was published between 2000 and 2020. It collected, analyzed, and collated this research, discovered its trends, and identified research gaps. It revealed, inter alia, that our research effort has not been directed evenly between speech and text or between the vernaculars; there is some bias favoring text over speech, regional varieties over individual vernaculars, and Egyptian over all other vernaculars. Furthermore, there is a clear preference for shallow machine learning approaches, for the use of n-grams, TF-IDF, and MFCC as neural network features, and for accuracy as a statistical measure of validation of results. This paper also pointed to some glaring gaps in the research: (1) total neglect of Mauritanian and Bahraini in the continuous Arabic language area and of such enclave varieties as Anatolian Arabic, Khuzistan Arabic, Khurasan Arabic, Uzbekistan Arabic, the Subsaharan Arabic of Nigeria and Chad, Djibouti Arabic, Cypriot Arabic and Maltese; (2) scarcity of city dialect resources; (3) rarity of linguistic investigations that would complement our research; (4) and paucity of deep machine learning experimentation.",No,"이 논문은 방언 아랍어에 관한 기존 연구들을 체계적으로 검토하고 분석한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 연구 격차를 정리하는 데 중점을 두고 있습니다."
Possibilities of AI Algorithm Execution in GNSS,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10118529,"A large number of studies were obtained to validate the opportunity of using Artificial Intelligence (AI) algorithms in the field of the Global Navigation Satellite System (GNSS). There are two ways to achieve intelligence: one is through Machine Learning (ML) and another is through Deep Learning (DL). Most commonly, Support Vector Machine (SVM) and Convolutional Neural Network (CNN) are the important algorithms of AI which is used in the literature to enhance the position accuracy of GNSS system. Here, the literature review has been done by considering the different stages of GNSS receivers at the Radio Frequency (RF) Front End level, at the pre-correlation level, at the post-correlation level, and at the Navigation level which will provide a better understanding of the implementation of AI in this domain. The major research work is done at the post-correlation stage where different data formats like correlation outputs, National Marine Electronics Association (NMEA) data, and Receiver Independent Exchange Format (RINEX) data have been utilized. Along with that, threats and risk factors associated with the application of AI algorithms have been discussed in this paper.",No,논문 초록은 AI 알고리즘의 GNSS 분야 적용에 관한 기존 연구들을 검토하는 문헌 리뷰임을 명확히 하고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 분석에 초점이 맞춰져 있습니다.
Check It Before You Wreck It: A Guide to STAR-ML for Screening Machine Learning Reporting in Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251957,"Machine learning (ML) is a technique that learns to detect patterns and trends in data. However, the quality of reporting ML in research is often suboptimal, leading to inaccurate conclusions and hindering progress in the field, especially if disseminated in literature reviews that provide researchers with an overview of a field, current knowledge gaps, and future directions. While various tools are available to assess the quality and risk-of-bias of studies, there is currently no generalized tool for assessing the reporting quality of ML in the literature. To address this, this study presents a new screening tool called STAR-ML (Screening Tool for Assessing Reporting of Machine Learning), accompanied by a guide to using it. A pilot scoping review looking at ML in chronic pain was used to investigate the tool. The time it took to screen papers and how the selection of the threshold affected the papers included were explored. The tool provides researchers with a reliable and systematic way to evaluate the quality of reporting of ML studies and to make informed decisions about the inclusion of studies in scoping or systematic reviews. In addition, this study provides recommendations for authors on how to choose the threshold for inclusion and use the tool proficiently. Lastly, the STAR-ML tool can serve as a checklist for researchers seeking to develop or implement ML techniques effectively.",No,"본 논문은 머신러닝 연구 보고의 질을 평가하기 위한 도구(STAR-ML)를 개발하고 이를 안내하는 내용에 초점을 맞추고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문이라기보다는 도구 개발 및 방법론 안내에 해당합니다."
Cyber Security Issues and Challenges Related to Generative AI and ChatGPT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375472,"In recent years, Generative Artificial Intelligence (AI) and ChatGPT (Generative Pre-trained Transformer) models that are capable of generating realistic human-mimicked languages have gained progressive popularity. With the evolution of technology, there has been a significant increase in the availability and usage of artificial intelligence tools, such as ChatGPT and Generative AI, that will assist in shaping the future. However, this increasing popularity poses a potential risk if used inappropriately. Threats from AI pose special challenges for government, the private sector, and national security. In this paper, we address some of key concerns of significant cyber security issues and challenges related to Generative AI and ChatGPT. With careful consideration to application usage, organizations can implement appropriate security measures to mitigate these risks. We also incorporate recommendations about ChatGPT usage and its impact on society. It is important that researchers, developers, and policymakers (CIOs, CSOs) work together to mitigate these risks and to ensure that these models are used in a responsible and ethical manner.",No,"초록 내용은 Generative AI와 ChatGPT 관련 사이버 보안 이슈와 도전 과제를 개괄적으로 다루고 있으며, 구체적인 실험, 데이터 분석, 또는 독창적인 연구 결과를 제시하지 않는다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
The Implementation of Artificial Intelligence in Knowledge Management: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10502986,"Knowledge Management (KM) is crucial since it is used for managerial decisions that affect the organization’s success. To improve the quality of KM, there is an innovation that implements Artificial Intelligence (AI) for the KM process. AI is a machine learning tool that can execute human tasks, adapt to new inputs, and learn from experience. This study aims to investigate the growth of AI in KM and evaluate how AI can be applied in KM to manage information and knowledge. The study used a systematic literature review method with PRISMA and data sources from Scopus. A total of 30 articles were examined in the review analysis. The research results found that the implementation of AI in KM was already conducted on various continents and most of the previous studies discussed this topic in the General field. Furthermore, the review discovered that AI can be applied to fundamental knowledge management process, decision-making, knowledge forecasting, and knowledge exchanges. This research also indicates that the implementation of AI in KM is growing and the topic of AI research in KM continues to develop. This study provides insight into prospects for innovation and improvement by offering evaluations for the future development of AI in KM.",No,"본 논문은 인공지능과 지식관리의 적용 현황을 체계적으로 문헌 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 기존 연구들을 종합하여 분석한 내용에 해당합니다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 정리하는 목적의 논문입니다."
Demystifying Privacy and Security Issues in Potentially Harmful Mobile Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10272516,"Android system's features, such as support for peer-to-peer networking, cloud computing services, and various communication protocols and network technologies, make it a suitable platform for distributed computing systems that require efficient resource management and coordination. The widespread popularity of Android, as one of the most widely-used mobile operating systems [1], is attributed to its ability to provide users with a wide range of convenient and entertaining options through its functional apps. Nevertheless, mobile users may face a risk to their privacy and property from potentially harmful apps that can be installed on their devices. Thus, it is imperative to conduct an analysis of potentially harmful applications [2]–[10] with a sense of urgency. Our research focuses on combining Android static analysis, adversarial machine learning, and natural language processing techniques to investigate app behavior, discover vulnerabilities in Android malware detection systems, and understand software privacy policy statements. To safeguard user privacy from potentially harmful apps, we propose the following measures: (I) Investigating the vulnerability of Android malware detection systems under evolving attacks and proposing defense solutions [11]; (II) Analyzing whether Android app privacy policies meet regulatory requirements [12]; and (III) Examining the privacy concerns associated with third-party libraries in Android [13]. For (I), to investigate the vulnerability of Android malware detection (AMD) systems under evolving attacks and discover defense solutions, we propose a Heuristic optimization model integrated with Reinforcement learning framework to optimize our structural ATtack, namely HRAT [11], which is the first problem-pace structural attack designed to deceive AMD. HRAT employs four types of graph modification operations and corresponding bytecode manipulation techniques to generate executable adversarial apps that can evade detection. HRAT bridges the research gap between feature-space attacks, which generate only adversarial features to deceive machine-learning models, and problem-space attacks, which generate complete adversarial objects. Our extensive experiments demonstrate that HRAT has effective attack performance and remains robust against obfuscation methods that do not affect the app's function call graph. In addition, we propose potential defense solutions to improve the robustness of AMD against such advanced attack methods. For (II), we construct the first large-scale human-annotated Chinese Android application privacy policy dataset, namely CA4P-483 [12]. Following-a manual inspection of regulatory articles, we identified seven types of labels that are relevant to the regulatory requirements for apps' access to user data. We then designed a two-step annotation process to ensure label agreement, and our evaluation showed that our annotations achieved a Kappa value of 77.20%, indicating substantial agreement for CA4P-483. In addition, we evaluate robust and representative baseline models on our dataset and present our findings and potential research directions based on our results. Finally, we conduct case studies to explore the potential application of CA4P-483 for protecting user privacy. For (III), we propose an Automated Third-Party library Privacy compliance Checker tool, ATPChecker [13], for checking Android third-party libraries (TPL) for compliance. ATPChecker combines static analysis to identify third-party libraries and analyze host app behavior in bytecode, as well as natural language processing techniques to investigate the privacy access-related statements in their privacy policies. Subsequently, ATPChecker determines whether the identified third-party libraries satisfy regulatory requirements and whether the host apps' use of these libraries complies with regulatory requirements by cross-checking the results of the static analysis and privacy policy analysis. For future work, first, we can develop more effective and efficient attack strategies against Android malware detection systems. The utilization of HRAT is constrained to white-box settings, thereby limiting its potential application scenarios. In cases where access to the classification model in victim malware detection systems is not feasible, a substitute model can be trained using input and output data. For instance, the classification probability of victim classifiers can be used to train a model that mimics the target classifier. Afterward, the HRAT approach can be applied to deceive the substitute model and achieve similar evasion performance of victim detection systems. Secondly, an exploration of potential application scenarios for the CA4P-483 dataset. It is a challenging task to determine whether a privacy policy will share or collect user data due to the use of negative words and tones. Applying emotional analysis to identify negative statements in privacy policies is a promising direction for understanding privacy policies. Lastly, enhancing ATPChecker by analyzing the purpose of data collection and developing automatic privacy policy generation methods for third-party library developers to reduce their workload and enhance compliance with regulations.",Yes,"본 논문은 Android 악성 앱 탐지 시스템의 취약점 분석, 방어 기법 제안, 개인정보 보호 정책 데이터셋 구축 및 분석, 그리고 제3자 라이브러리의 개인정보 준수 여부 검사 도구 개발 등 독창적인 연구 방법과 실험 결과를 포함하고 있다. 이는 직접 기여하는 독창적인 연구 내용을 포함한 연구 논문에 해당한다."
Advancements in Cardiotoxicity Detection and Assessment through Artificial Intelligence: A Comprehensive Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548722,"This comprehensive review critically examines the forefront of artificial intelligence (AI) applications in cardiology, focusing on detecting and assessing cardiotoxicity and heart failure. It discusses various studies that demonstrate AI's revolutionary impact on cardiac healthcare, including deep learning for predicting pharmaceutical cardiotoxic effects, advanced AI models for identifying cardiotoxicity in stem cell-derived cardiomyocytes, and AI-enhanced imaging for monitoring cardiac complications in cancer patients. The review also explores AI systems for detecting early signs of heart failure in electrocardiograms and computational models for predicting drug interactions with hERG channels, which are vital for cardiac safety assessments. Additionally, it highlights the use of AI in preclinical research and drug safety evaluations. The reviewed studies underscore the potential of AI, especially deep learning techniques, in predicting and detecting drug-induced cardiotoxicity, offering new perspectives in assessing cardiotoxicity risks prior to drug administration and detecting early cardiac damage. However, the paper notes the necessity for further studies to validate and refine these AI models and suggests the integration of more diverse data and considerations for improved predictive performance.",No,"초록에 따르면 이 논문은 다양한 AI 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 요약하고 평가하는 데 중점을 두고 있다. 따라서 연구 논문보다는 종설 논문에 해당한다."
Detection of Gastric Cancer in Endoscopic Images Using Deep Convolutional Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763738,"In finding features of gastric cancer in endoscopic images, difficulties arise from early manifestations and variance in the appearance of lesions. The objective of this study is to design an automobile deep convolutional neural network (CNN) that would help enhance the rate of accurate diagnosis of gastric cancer. In the current work, using the dataset of endoscopic images reviewed by senior gastroenterologists, transfer learning is applied on the base of the ResNet-50 based deep CNN the result of data augmentation is extended. By evaluating the model on measures of training, validation, and testing sets, high performance has been realized as depicted by accuracy of 87. 4%, precision of 87. 5%, recall of 87%, and f1-score of 87%. Research on the comparison of the proposed method with the conventional CAD systems and machine learning models confirmed the efficiency of the deep learning method. The utilization of Grad-CAM visualizations brought interpretability in improving match between regions of focus and clinically relevant areas therefore improving the trust in the model among the clinicians. To foster efficiency, the implementation incorporated Python and deep learning tools, including TensorFlow, the model of which was trained on GPUs. Thus, this paper demonstrates the feasibility of the studied CNNs in promoting the early diagnosis of gastric cancer, suggesting the adaptation of AI tools as complementary tools for clinicians in everyday practice. Future studies should follow up on how false negative results may occur; additionally, future studies should devise better approaches on how to increase the generalizability of these models across different patients.",Yes,"본 논문은 위암 진단을 위한 딥러닝 기반의 새로운 CNN 모델을 설계하고, 이를 기존 방법과 비교 평가한 독창적인 연구 내용을 포함하고 있습니다. 데이터셋 구축, 모델 학습, 성능 평가 및 해석 가능성 분석 등 직접적인 연구 기여가 명확히 드러나 있습니다."
A Systematic Review and Analysis of Lung Disease Detection Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245744,"Artificial Intelligence (AI) advancements have astonishingly taken place in the last decade, and it is seen their way into innumerable applications from driverless cars to medical diagnosis. AI is being highly used to identify lung diseases in the present years. Its main aim is to estimate human cognition in the analysis of complicated medical data attained from diagnostic tests for lung diseases. In this review paper, 20 research papers are studied based on different techniques of machine learning as well as deep learning employed for the detection of lung disease. In addition, research papers are reviewed on the basis of diverse kinds of lung diseases, namely tuberculosis, COVID-19, pneumonia, involving lung cancer by classifying the papers based on the methods, like Random forest, Deep CNN, voting classifier, etc. Then, gaps and issues in the research identified in conventional works are enlisted. This helps the researchers in finding a solution and progress their research. The literature review examines various factors when analyzing works, including the datasets used, metrics for evaluating performance, methods employed for detection/classification, and the performance achieved by those detection methods. By analyzing the existing literary works on lung disease detection, the review identifies research challenges that can guide researchers to enhance their work and suggests future directions for improvement.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 평가에 중점을 두고 있습니다."
Future Trend: Review of Artificial Intelligence Applications in Prosthodontics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498696,"Dental implants are a popular medical solution for restoring one or more lost teeth. By being surgically inserted into the jawbone or skull, an implanted dental prosthesis may support a crown, bridge, denture, or face prosthesis. The implant is often made of titanium or a titanium alloy. The missing tooth root is replaced by surgically implanting a new one into the jawbone. Dental implants are generally regarded as a more durable and long-lasting solution than other dental prostheses and may help a person’s smile operate and look better. It is essential to employ the proper imaging methods for implants and to properly interpret the data. The preliminary objective of the paper is to investigate the impact of AI on different stages of dental implant procedures. Also, the study aims to evaluate the potential benefits of AI in terms of reduced procedural time, improved success rates and enhanced patient satisfaction. A systematic review is based on 183 papers of Dental Implant using Artificial Intelligence from 2017 to 2023. Machine learning algorithms and computer-aided design tools utilized in dental implant procedures will be analysed. Furthermore, the study will involve the development and validation of an AI-driven model for implant planning, utilizing diagnostic radiographic images. Through the implementation of advanced Deep learning algorithms this research aims to provide a comprehensive understanding of the benefits and challenges associated with AI in dental implant procedures.",Yes,"논문 초록에서 AI 기반 임플란트 계획 모델의 개발 및 검증과 심층 학습 알고리즘의 구현을 명시하고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 183편의 논문을 체계적으로 검토하는 동시에 새로운 AI 모델을 제안하는 점에서 연구 논문으로 판단됩니다."
Enhancing Smart Grid Stability Using AI Techniques: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933390,"The modern smart grid leverages advanced technologies to enhance electricity distribution's efficiency, reliability, and sustainability, enabling two-way energy and data exchange. This systematic literature review (SLR) explores how artificial intelligence (AI) addresses the challenge of smart grid stability. From 1,181 records identified via Scopus and Web of Science, 41 studies were analyzed in-depth. The SLR evaluates AI techniques such as machine learning, deep learning, neural networks, hierarchical clustering, and explainable AI across functions like fault detection, stability analysis, energy forecasting, and demand response. Findings reveal AI's role in enhancing grid stability through predictive capabilities and adaptive controls. This study contributes a taxonomy of AI applications, identifies research gaps, and outlines a future research direction. Key challenges include scalability, data privacy, and model interpretability, while opportunities lie in hybrid models, and integration with blockchain and IoT. The study provides actionable insights for researchers, policymakers, and stakeholders.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)로, 기존 연구들을 분석하고 정리하는 데 중점을 두고 있어 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 평가에 해당한다."
Effectiveness of Generative Artificial Intelligence for Scientific Content Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313167,"Generative artificial intelligence (GenAI) in general, and large language models (LLMs) in particular, are highly fashionable. As they have the ability to generate coherent output based on prompts in natural language, they are promoted as tools to free knowledge workers from tedious tasks such as content writing, customer support and routine computer code generation. Unsurprisingly, their application is also attractive to professionals in the research domain, where mundane and laborious tasks, such as literature screening, are commonplace. We evaluate Vertex AI ‘text-bison’, a foundational LLM model, in a real-world academic scenario by replicating parts of a popular systematic review in the information management domain. By comparing the results of a zero-shot LLM-based approach with those of the original study, we gather evidence on the suitability of state-of-the-art general-purpose LLMs for the analysis of scientific content. We show that the LLM-based approach delivers good scientific content analysis performance for a general classification problem (ACC =0.9), acceptable performance for a domain-specific classification problem (ACC =0.8) and borderline performance for a text comprehension problem (ACC ≈0.69). We conclude that some content analysis tasks with moderate accuracy requirements may be supported by current LLMs. As the technology will evolve rapidly in the foreseeable future, studies on large corpora, where some inaccuracies are tolerable, or workflows that prepare large data sets for human processing, may increasingly benefit from the capabilities of GenAI.",Yes,"본 논문은 Vertex AI ‘text-bison’ LLM 모델을 실제 학술 시나리오에서 평가하고, 기존 연구와 비교하여 성능을 정량적으로 분석하는 독창적인 연구 내용을 포함하고 있습니다. 따라서 직접적인 실험과 분석을 통해 새로운 지식을 제공하는 연구 논문에 해당합니다."
Trends and Challenges in AI-Driven Microservices for Cloud-Based Airline Reservation Systems: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915076,"The rapid advances in Artificial Intelligence (AI), microservices architecture, and cloud computing highly impact airlines in optimizing reservation systems. This review paper attempts to analyze the ongoing trend, challenges, and future opportunities of AI-enabled microservices toward cloud-based airline reservation systems. AI with machine learning and deep learning supports dynamic pricing, personalized services, and better demand forecasting, transforming booking, pricing, and customer experience. While microservices architecture offers scalability and flexibility, in conjunction with cloud computing, it optimizes airline demands and system performance. At the same time, challenges such as data privacy and security, scalability, interoperability, and costs are still present with adopting cloud-based microservices. Case studies are provided in this paper, as well as a thorough exposition of future directions of AI in airline reservation systems, such as the application of emerging AI techniques like reinforcement learning, evolution toward micro service architecture, and the influence of new cloud computing models such as edge and hybrid clouds. In addition, the work points out research gaps in areas such as real-time predictive analytics and security measures. Findings indicate a transforming potential of service on airline reservation systems for AI-facilitated micro services and cloud computing and provide possible suggestions for overcoming the current challenges to harness great efficiency from them.",No,"본 논문은 AI 기반 마이크로서비스와 클라우드 항공 예약 시스템에 대한 동향과 도전 과제를 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합하고 평가하는 개관 논문에 해당합니다."
Generative AI for Revolutionizing Personal Style,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932201,"The fashion business is growing quickly and is one of the main forces behind the world's economies. The everexpanding business is driving a significant rise in the application of computer science to address various issues inside it. It becomes difficult to discover a personalized wardrobe that reflects their preferences, taste and needs. Traditional Fashion Recommendation Systems (FRSs) limits their ability to scale and adapt the ever growing styles as they heavily rely on manual design. Fashion can also be so inclusive, expressive and sustainable. Appropriate recommendations given by FRS helps to enhance user satisfaction and makes it more enjoyable and accessible. Artificial Intelligence (AI) tools have revolutionized FRS enabling them to consume beyond conventional methods by taking in contextual data, user preferences and visual content for recommendations with a more individualized suggestion. Recently, Generative Adversarial Networks (GANs) have emerged as a potent technique to enhance these systems by generating diverse fashion designs with high fidelity. In this paper, a systematic review of parameters used to evaluate FRS using Generative Algorithms is discussed. Various parameters to evaluate system performance and the recommendation quality are analyzed. Detailed analysis of the input parameters, to be considered to design the efficient AI based FRS (AI-FRS) is also presented. Along with this, research gaps are explored by surveying numerous review papers. This review will help in deciding the evaluation parameters to develop and examine more efficient AI based FRS.",No,"본 논문은 생성적 AI를 활용한 패션 추천 시스템에 대한 체계적인 리뷰를 제공하며, 기존 연구들을 종합하고 평가 지표와 연구 격차를 분석하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵다."
IoT meets distributed AI - Deployment scenarios of Bonseyes AI applications on FIWARE,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8958742,"Bonseyes is an Artificial Intelligence (AI) platform composed of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms with the aim of facilitating tech and non-tech companies a rapid adoption of AI as an enabler for their business. Bonseyes provides methods and tools to speed up the development and deployment of AI solutions on low power Internet of Things (IoT) devices, embedded computing systems, and data centre servers. In this work, we address the deployment and the integration of Bonseyes AI applications in a wider enterprise application landscape involving different applications and services. We leverage the well-established IoT platform FIWARE to integrate the Bonseyes AI applications into an enterprise ecosystem. This paper presents two AI application deployment and integration scenarios using FIWARE. The first scenario addresses use cases where edge devices have enough compute power to run the AI applications and there is only need to transmit the results to the enterprise ecosystem. The second scenario copes with use cases where an edge device may delegate most of the computation to an external/cloud server. Further, we employ FIWARE IoT Agent generic enabler to manage all edge devices related to Bonseyes AI applications. Both scenarios have been validated on concrete use cases and demonstrators.",Yes,"본 논문은 Bonseyes AI 애플리케이션을 FIWARE 플랫폼에 배포하고 통합하는 두 가지 시나리오를 제시하며, 이를 구체적인 사례와 데모를 통해 검증하였다. 이는 기존 기술을 단순히 소개하는 것이 아니라, 실제 적용 및 검증을 포함한 독창적인 연구 기여로 판단된다."
Aero-Engine Blade Defect Detection: A Systematic Review of Deep Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138396,"Aero-engine blade defect detection is a crucial task in ensuring the safety and reliability of aircraft. The visual inspection of aero-engine blades is a complex process that requires extensive knowledge and experience. This paper presents a systematic literature review (SLR) of deep learning models for detecting defects in aero-engine blades. The review considers 13 primary studies, including methods and conceptual works. This is the first systematic review of deep learning models for aero-engine blade defect detection. The findings of this review demonstrate the potential of deep learning in detecting blade defects and improving the accuracy and efficiency of visual inspection. However, there is a need for more research on integrating deep learning models into practical applications and developing robust and reliable systems for defect detection. This review framework provides a comprehensive methodology for selecting and evaluating relevant studies, which researchers can use for future investigations in this area. These results should encourage further work on deep learning techniques, system integration, and testing and validation for defect detection of aero-engine blades.",No,"본 논문은 심층 학습 모델을 이용한 항공기 엔진 블레이드 결함 탐지에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이 아니라, 기존 연구들을 종합하고 평가하는 리뷰 논문에 해당한다."
The Impact of Artificial Intelligence and Internet of Things in the Transformation of E-Business Sector,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735644,"This study explores the impact of Artificial Intelligence (AI) and Internet of Things (IoT) in the transformation of E-Business Sector in South Africa. AI and IoT are beginning to shape the future of many industries globally by generating an unprecedented amount of data. In the case of South Africa, we observe that in e-business new value can be created by the ways in which transactions are enabled. In this study we use the principles and applications of AI and IoT to determine the impacts in the transformation of E-Business sector in South Africa. The objective of this study is not to reproduce experiments, but to investigate and quantify the impact AI and IoT has in the transformative process of change in the E-Business sector. This study employed a qualitative research approach and data was collected through a systematic literature review using the snowballing search method. 18 peer reviewed papers were identified and analyzed in relation to their relevance to the study.",No,"본 논문은 AI와 IoT가 남아프리카 전자상거래 부문에 미치는 영향을 조사하기 위해 기존 문헌을 체계적으로 검토한 연구로, 직접적인 실험이나 독창적인 연구 결과를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 문헌 분석에 기반한 리뷰 논문에 해당합니다."
Systematic Review of Recent ChatGPT updates on Reporting of Radiology Cases with Reference to Magnetic Resonance Imaging of the Lumbar Spine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10842881,"This systematic review aims to evaluate the recent advancements and applications of ChatGPT in reporting radiology cases, focusing on magnetic resonance imaging (MRI) of the lumbar spine. The review adheres to the PRISMA guidelines and explores how artificial intelligence (AI) can improve the accuracy, efficiency, and consistency of radiology reports. A comprehensive literature search was conducted across databases such as PubMed, Scopus, and IEEE Xplore, covering articles published from January 2022 to September 2024. The inclusion criteria were studies that examined the use of ChatGPT or similar AI models in radiology reporting, specifically focusing on MRI of the lumbar spine. Studies not involving ChatGPT or AI, non-radiology-related studies, and articles not available in full text were excluded. Data extraction and synthesis were performed in line with PRISMA guidelines. A total of 18 studies met the inclusion criteria, with 12 focusing on MRI of the lumbar spine. The findings indicate that ChatGPT shows potential in standardizing radiology reports, improving diagnostic accuracy, and reducing the time needed for report generation. However, challenges such as the necessity for clinical context, risks of over-reliance on AI, and issues regarding the interpretability of AI outputs were noted. ChatGPT and similar AI models have considerable potential in radiology reporting, especially for lumbar spine MRI. However, careful implementation, continuous updates, and rigorous validation are crucial to ensure these tools enhance rather than replace the expertise of radiologists.",No,"이 논문은 ChatGPT의 최근 업데이트와 적용 현황을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 기존 연구들을 종합하여 분석한 내용에 해당합니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합적 평가에 초점이 맞춰져 있습니다."
Data Partitioning and Storage Strategies for Artificial Intelligence and Machine Learning Applications: A Review of Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10927845,"This paper presents a review of data partitioning and storage strategies critical for optimizing the performance and scalability of Artificial Intelligence (AI) and Machine Learning (ML) applications. As the volume and complexity of data continue to grow, effective data management becomes increasingly essential for ensuring the efficiency and accuracy of AI/ML models. The review categorizes and evaluates various data partitioning techniques, including hash-based, range-based and graph-based algorithms, highlighting their strengths, weaknesses, and suitability for different types of datasets and applications. It also explores the data processing challenges with AI/ML applications, data access patterns for AI/ML algorithms; and implementation techniques and evaluation metrics. By identifying key factors that influence the choice of partitioning and storage strategies, this review provides valuable insights for researchers and practitioners seeking to enhance the performance of AI/ML systems. The review also serves as a foundation for future research and development, guiding the design of more efficient and scalable data management solutions in artificial intelligence.",No,"본 논문은 기존 데이터 분할 및 저장 전략에 대한 리뷰를 제공하는 문헌 조사 논문으로, 직접적인 독창적 연구 결과나 새로운 기법 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 종합적 고찰 논문에 해당합니다."
A Review on Bayesian Deep Learning in Healthcare: Applications and Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745083,"In the last decade, Deep Learning (DL) has revolutionized the use of artificial intelligence, and it has been deployed in different fields of healthcare applications such as image processing, natural language processing, and signal processing. DL models have also been intensely used in different tasks of healthcare such as disease diagnostics and treatments. Deep learning techniques have surpassed other machine learning algorithms and proved to be the ultimate tools for many state-of-the-art applications. Despite all that success, classical deep learning has limitations and their models tend to be very confident about their predicted decisions because it does not know when it makes mistake. For the healthcare field, this limitation can have a negative impact on models predictions since almost all decisions regarding patients and diseases are sensitive. Therefore, Bayesian deep learning (BDL) has been developed to overcome these limitations. Unlike classical DL, BDL uses probability distributions for the model parameters, which makes it possible to estimate the whole uncertainties associated with the predicted outputs. In this regard, BDL offers a rigorous framework to quantify all sources of uncertainties in the model. This study reviews popular techniques of using Bayesian deep learning with their benefits and limitations. It also reviewed recent deep learning architecture such as Convolutional Neural Networks and Recurrent Neural Networks. In particular, the applications of Bayesian deep learning in healthcare have been discussed such as its use in medical imaging tasks, clinical signal processing, medical natural language processing, and electronic health records. Furthermore, this paper has covered the deployment of Bayesian deep learning for some of the widespread diseases. This paper has also discussed the fundamental research challenges and highlighted some research gaps in both the Bayesian deep learning and healthcare perspective.",No,"본 논문은 베이지안 딥러닝의 의료 분야 적용과 한계에 대해 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않고, 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다."
The Role of Artificial Intelligence in Promoting Sustainable Business Operations and Autonomy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434654,"This empirical study examines the ways artificial intelligence (AI) is fundamentally changing sectors around the globe, with a particular emphasis on the way AI is revolutionizing autonomous and sustainable business practices. The combination of artificial intelligence (AI) with sustainability cannot be considered accidental; rather, it is a calculated reaction to the urgent need to solve environmental issues, improve productivity, and promote innovation. Fundamentally, sustainability aims to strike a balance between the environmental, social, and economic ramifications of corporate operations. Artificial intelligence (AI) is essential to establishing this balance because of its capacity to analyse enormous data volumes, extract insightful knowledge, and make deft judgments in real time. It gives organizations the ability to simultaneously enhance the standard of living for people and communities while decreasing their environmental impact, maximize resource use, and negotiate the challenges of a fast-paced global marketplace. The literature review in the research emphasizes the way artificial intelligence (AI) maximizes resource utilization, minimizes waste, and aids in environmental conservation and monitoring. It emphasizes the manner in which artificial intelligence (AI) may improve waste reduction, energy efficiency, and accurate agriculture, including autonomous procedures. Robotic process automation and predictive analytics systems powered by AI are prime examples of the operational autonomy attained in business. AI, sustainability, and autonomy work together to generate a potent blend that promotes eco-friendly behaviour and increases productivity. It makes the deployment of Internet of Things technology, smart cities, and circular economy models easier. The study recognizes difficulties and moral issues, which include data privacy and employment displacement, despite the enormous promise.",No,"초록 내용은 AI와 지속가능한 경영 및 자율성의 역할에 대한 문헌 검토와 개념적 논의를 중심으로 하고 있으며, 구체적인 실험, 데이터 분석, 또는 독창적인 연구 결과 제시는 보이지 않습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Extreme Learning Machine: A Comprehensive Survey of Theories & Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10183613,"The Extreme Learning Machine (ELM), a quick and effective approach for training single-hidden-layer feedforward neural networks, is thoroughly reviewed in this paper. The objective is to provide a systematic review and analysis of the various theories, algorithms, and applications of ELM, which have not been adequately covered in the literature. The study examines the theoretical foundations of ELM, provides an overview of various ELM algorithms, analyzes the applications of ELM in different domains, and identifies future research directions and challenges in the field of ELM. The findings suggest that ELM is a promising and effective learning algorithm with many benefits as improved learning speed, good generalisation performance, and low complexity. The implications of this research for practice and research are significant. However, more empirical studies are necessary to evaluate its performance in real-world applications, and further research is required to assess its effectiveness in different domains. Future work should focus on exploring the potential of ELM in deep learning, enhancing the interpretability of ELM models, and developing new algorithms that can improve the learning efficiency of ELM. Overall, this survey provides a valuable reference for researchers and practitioners to better understand the potential and limitations of the ELM algorithm.",No,"본 논문은 ELM에 대한 이론, 알고리즘, 응용을 종합적으로 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않는다. 따라서 기존 연구들을 체계적으로 리뷰하고 분석하는 연구 논문으로 분류된다."
Machine learning based system for the automation of systematic literature reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385372,"The paper gives an overview of a machine learning-based system developed to support systematic literature reviews (SLR). The objective of the system is to provide scientists and anyone else who gives scientific advice supporting policy development with a tool for literature search and appraisal that reduces the human effort. The structure of the system is presented along with the description of the communication between modules and data storage methods. The Kafka technology is used for inter-module communication and the system consists of several independent modules which can be easily expanded with new modules without the need to introduce significant changes. We propose to semi–automate the SLR processes by applying an active learning approach which is based on machine learning classification models and on manual screening by experts of a subset of articles. Using classification algorithms requires a numerical representations of articles. This work investigates the utility of bag of concepts approach for text representations in order to create classification models used as components of automated systematic literature review systems. The presented study uses the bag of concepts approach in which a set of concepts identified by an annotator is extended by the concepts which lie, within a given distance, on paths from the originally identified concepts to the root of the ontology tree. Experiments are performed on datasets from systematic literature reviews in the medical domain. We summarize the performance of the proposed system by evaluating the WSS@95% metrics of active learning processes for several SLR case studies.",Yes,"본 논문은 머신러닝 기반 시스템을 개발하고, 이를 체계적 문헌고찰 자동화에 적용하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 개념 기반 텍스트 표현 방법을 제안하고 실험을 통해 성능을 평가하는 등 직접적인 연구 기여가 명확합니다."
The Missing Link in Network Intrusion Detection: Taking AI/ML Research Efforts to Users,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540566,"Intrusion Detection Systems (IDS) tackle the challenging task of detecting network attacks as fast as possible. As this is getting more complex in modern enterprise networks, Artificial Intelligence (AI) and Machine Learning (ML) have gained substantial popularity in research. However, their adoption into real-world IDS solutions remains poor. Academic research often overlooks the interconnection of users and technical aspects. This leads to less explainable AI/ML models that hinder trust among AI/ML non-experts. Additionally, research often neglects secondary concerns such as usability and privacy. If IDS approaches conflict with current regulations or if administrators cannot deal with attacks more effectively, enterprises will not adopt the IDS in practice. To identify those problems systematically, our literature survey takes a user-centric approach; we examine IDS research from the perspective of stakeholders by applying the concept of personas. Further, we investigate multiple factors limiting the adoption of AI/ML in security and suggest technical, non-technical, and user-related considerations to enhance the adoption in practice. Our key contributions are threefold. (i) We derive personas from realistic enterprise scenarios, (ii) we provide a set of relevant hypotheses in the form of a review template, and (iii), based on our reviews, we derive design guidelines for practical implementations. To the best of our knowledge, this is the first paper that analyzes practical adoption barriers of AI/ML-based intrusion detection solutions concerning appropriateness of data, reproducibility, explainability, practicability, usability, and privacy. Our guidelines may help researchers to holistically evaluate their AI/ML-based IDS approaches to increase practical adoption.",No,"본 논문은 AI/ML 기반 침입 탐지 시스템의 실제 도입 장벽을 문헌 조사와 사용자 관점에서 분석하고, 설계 지침을 제안하는 리뷰 논문입니다. 직접적인 실험이나 새로운 알고리즘 개발 등 독창적인 연구 결과를 제시하지 않으므로 연구 논문에 해당하지 않습니다."
Deepfake Detection: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721302,"Over the last few decades, rapid progress in AI, machine learning, and deep learning has resulted in new techniques and various tools for manipulating multimedia. Though the technology has been mostly used in legitimate applications such as for entertainment and education, etc., malicious users have also exploited them for unlawful or nefarious purposes. For example, high-quality and realistic fake videos, images, or audios have been created to spread misinformation and propaganda, foment political discord and hate, or even harass and blackmail people. The manipulated, high-quality and realistic videos have become known recently as Deepfake. Various approaches have since been described in the literature to deal with the problems raised by Deepfake. To provide an updated overview of the research works in Deepfake detection, we conduct a systematic literature review (SLR) in this paper, summarizing 112 relevant articles from 2018 to 2020 that presented a variety of methodologies. We analyze them by grouping them into four different categories: deep learning-based techniques, classical machine learning-based methods, statistical techniques, and blockchain-based techniques. We also evaluate the performance of the detection capability of the various methods with respect to different datasets and conclude that the deep learning-based methods outperform other methods in Deepfake detection.",No,"본 논문은 112편의 기존 연구를 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구들을 종합하여 분석하는 데 중점을 두고 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Optimizing Mental Health Status Prediction Models Using Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10835491,"In contemporary society, mental health concerns are gaining heightened attention, and accurately forecasting individuals’ mental well-being has become a pivotal research focus. Traditional predictive models for mental health often struggle with issues like the complexity of data processing and insufficient prediction precision. However, the application of machine learning algorithms introduces new approaches to address these challenges. In the age of artificial intelligence, creative works are high-quality data sources for machine learning, raising crucial questions about copyright regulations. Balancing these rules to foster innovation in both cultural and technological sectors is a pressing issue. This study employs various machine learning techniques, such as support vector machines, decision trees, and deep learning algorithms, to thoroughly examine and evaluate mental health data. As artificial intelligence technology evolves rapidly and reshapes its applications, new research themes and methodologies continuously emerge within machine learning, with advances in deep learning and reinforcement learning driving the field forward. This paper not only reviews the essential techniques in the optimization process but also explores future research avenues and potential applications, aiming to provide valuable insights for ongoing research and practical efforts in mental health prediction.",No,"초록은 기존 머신러닝 기법들을 검토하고 정신 건강 예측 모델의 최적화 방법을 논의하는 리뷰 및 전망 성격이 강하며, 독창적인 연구 결과나 새로운 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 직접 기여하는 연구 논문으로 보기 어렵습니다."
Study Literature Review: Discovering the Effect of Chatbot Implementation in E-commerce Customer Service System Towards Customer Satisfaction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9920434,"Customer service plays a crucial role for a company. As an important aspect of e-commerce companies, they would be required to directly interact and try to solve customers’ problems that might occur anywhere and anytime. However, the limitation of human man hours became a barrier to overcome customers’ problems. On one hand, the rapid development of technology was predicted to replace the traditional human customer service with an Artificial Intelligence agent. On the other hand, this replacement affects the customer satisfaction. This paper performed a study literature review to discover the effect of chatbots and its impact towards customer satisfaction. In an e-commerce customer service use case, chatbots could be implemented in a number of methods. The methods implemented by chatbots are avatar-based, verbal-based, text-based, and menu-based. Research showed that text-based chatbot is the most commonly used methodology and has advanced the most, where some are implementing higher level machine learning methods, such as deep learning. The usage of such chatbot in e-commerce customer service systems will lower the cost but might also lower customer satisfaction, due to reasons such as unsatisfying answers and inhuman behavior. Research showed that even a more sophisticated chatbot doesn’t always mean higher customer satisfaction, even with high accuracy ratings. To look into customer satisfaction, this paper has identified 4 aspects of a chatbot that are relevant to customer satisfaction, which are privacy, reliability, personalization, and responsiveness. Chatbots currently excel in some of these quality measures, but require further research to effectively replace human customer service agents.",No,"본 논문은 기존 연구들을 종합하여 챗봇이 전자상거래 고객 서비스에 미치는 영향을 문헌 리뷰 형식으로 분석한 것으로, 직접적인 실험이나 새로운 연구 결과를 제시하지 않는다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
"WIP: Beyond Code: Evaluating ChatGPT, Gemini, Claude, and Meta AI as AI Tutors in Computer Science and Engineering Education",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893528,"This Work-in-Progress research paper evaluates the validity of Large Language Models (LLMs) as conversational AI tutors for computer science learning. While current engineering education literature has predominantly emphasized the rapid evolution of LLMs as conversational AI tutors for programming languages, the exploration into their effectiveness within general STEM topics remains comparatively scarce. This WIP study thus centers on evaluating the potential of LLMs to facilitate understanding of core hardware design concepts critical to computer science and engineering (CSE) education. By cross-checking the responses from generative AI chatbots to an openended CSE-based question, we aimed to uncover how LLMs, such as ChatGPT-3.5, Claude, Gemini, and Meta AI, can contribute to teaching and learning of general CSE courses instead of a specifically coding-based one. Our method involved simulating a student query on the popular debate between CISC vs. RISC related to computer architecture and analyzing the chatbots' responses. This initial collection of data served as the foundation for a continual comparative analysis aimed at determining the inherent instructional value of each LLM and its validity and reliability. To systematically assess the responses, we introduced an evaluation framework focusing on metrics, such as response accuracy, persuasiveness, and depth of explanation. The current work anticipates not only enriching our understanding of how these advanced LLMs can support general CSE education but also identifying areas where further development is needed for a more holistic integration of LLM-based chatbots in assisting student comprehension in the overarching engineering education.",Yes,"본 논문은 LLM을 활용한 AI 튜터의 교육적 유효성을 평가하기 위해 자체적으로 질문을 설정하고, 여러 모델의 답변을 비교 분석하는 독창적인 연구 방법론과 평가 프레임워크를 제시하고 있다. 이는 기존 연구를 단순히 요약하는 것이 아니라 직접 데이터를 수집하고 분석하는 연구 논문에 해당한다."
POTDAI: A Tool to Evaluate the Perceived Operational Trust Degree in Artificial Intelligence Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10663721,"There is evidence that a user’s subjective confidence in an Artificial Intelligence (AI)-based system is crucial in its use, even more decisive than the objective effectiveness and efficiency of the system. Therefore, different methods have been proposed for analyzing confidence in AI. In our research, we set out to evaluate how the degree of perceived trust in an AI system could affect a user’s final decision to follow AI recommendations. To this end, we established trustworthy criteria that such an evaluation should meet by following a co-creation approach with a multidisciplinary group of 10 experts. After a systematic review of 3,204 articles, we found that none of the tools met the inclusion criteria. Thus, we introduce the so-called “Perceived Operational Trust Degree in AI” (POTDAI) tool that is based on the findings from the expert group and the literature analysis, with a methodology that adds rigor to that employed previously to create similar evaluation tools. We propose a short questionnaire for quick and easy application, inspired by the original version of the Technology Acceptance Model (TAM) with six Likert-type items. In this way, we also respond to the need pointed out by authors such as Vorm and Combs to extend the TAM to address questions related to user perception in systems with an AI component. Thus, POTDAI can be used alone or in combination with TAM to obtain additional information on its usefulness and ease of use.",Yes,"본 논문은 AI 시스템에 대한 신뢰도를 평가하는 새로운 도구(POTDAI)를 개발하고, 이를 위해 전문가 그룹과 문헌 분석을 통해 독창적인 평가 방법론을 제시하고 있다. 이는 기존 연구를 단순히 요약한 것이 아니라 직접적인 연구 기여와 도구 개발을 포함하고 있어 연구 논문에 해당한다."
Engineering Challenges in Industrial AI,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556073,"This talk summarizes important experiences we cultivated in several projects where we developed AI methods for industrial customers like chemical production plants or gas fired power plants.One might think that applying leading edge AI methods to large quantities of industrial data will automatically yield valuable results. In our experience there are many ""traditional"" obstacles that need to be removed, before the magic can happen [2].For many use cases, data from various sources need to be integrated. For example, time series data stored in industrial Data Historians need to be labeled according to quality data stored in Laboratory Information Management Systems (LIMS). The process data like setpoints and raw material characteristics need to be contextualized using the production data, e.g. production orders, raw material lots, from ERP systems. Although many companies are currently building up enterprise data lakes, the reality in many plants still is that data is locked in separate silos and a real-time data integration requires significant efforts.Building, automating and operating a large industrial plant requires major (traditional) engineering efforts and expertise. Data scientists usually have different backgrounds and often lack knowledge, for example in control systems engineering and process engineering. In addition, many important engineering artifacts like Piping and Instrumentation Diagrams (P&ID) and process design document are either not up to date or even unavailable at all. However, in order to define relevant use cases and design powerful AI solutions, quite a high level of both traditional engineering and data science know-how is required. This means that traditional plant experts need to closely cooperate with data science experts. In the long run, software tools like ABB AbilityTM BatchInsight will be able to offer easy to use solutions for a broad class of use-cases, so that plant experts can use AI solutions without the need for additional consulting [3].Many industrial AI methods work with time series data. A larger industrial plant like a refinery can have many tens of thousands different time series. In the best case a consistent hierarchical naming schema is available and for each time series a good description exists. In reality, it can be cumbersome to identify the best set of time series related to the use case at hand [1]. In addition, data quality needs to be ensured. For example, if sensors are not properly calibrated, using this data can destroy AI models. In one of our projects, a sensor was being replaced at a certain point in time and the measured value changed from power to current. For convenience and because of the typical time pressure during daily operation, the new value was stored in the old variable. Our AI methods identified this abrupt change in system behavior, however this finding had no value for the customer.Industrial plants and their subsystems have different operating states that often need to be clearly separated. For example, if a time interval where the plant was in fault condition is accidentally included to train a reference model, the quality of this model will degrade.In summary, various challenges exist in the application of industrial AI solutions. Our talk will provide our learnings in addressing these challenges and will help to ask the following five questions before starting an industrial AI project:•Can we define a use case with sufficient business value?•Are people both with domain expertise and with expertise in AI committed to the project?•Will the communication between domain experts and AI experts be good enough?•Is all required data available for the project in sufficient quantities?•Is the quality of the data sufficient or are additional preparation and cleaning efforts required?",No,"이 논문은 여러 산업 AI 프로젝트에서 경험한 도전과제와 문제점들을 요약하고 있으며, 구체적인 독창적인 연구 방법론이나 실험 결과를 제시하지 않습니다. 따라서 직접적인 연구 기여보다는 산업 AI 적용 시 고려해야 할 사항들을 정리한 개요성 자료로 판단됩니다."
A Meta-Analysis of AI in Fraud Detection: Evaluating the Effectiveness of Different Algorithms and Data Sources,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10842759,"The widespread use of artificial intelligence (AI) techniques has changed the way fraud is found in many areas, offering better accuracy and speed compared to older methods. This meta-analysis looks at how well distinctive AI calculations and information sources work at finding tricks. The objective is to deliver a full picture of how things are done presently and discover places where more ponder is required. The think about takes the comes about of numerous diverse inquire about papers and centers on how AI can be utilized to discover tricks in areas like healthcare, managing an account, and e-commerce. A few of the foremost critical AI calculations that are looked at are machine learning models (like Back Vector Machines, Arbitrary Woodlands, and Neural Systems), strategies for finding exceptions (like Segregation Woodland and One-Class SVM), and blended approaches that utilize more than one method to urge way better comes about. To see if each calculation is sweet at finding extortion totally different circumstances, its execution measures are carefully looked at. These incorporate precision, review, F1-score, and range beneath the bend (AUC). The meta-analysis too looks into how distinctive information sources influence how well AI models work. It looks at how to utilize organized and unstructured information, money related records, client behavior designs, content information from client audits, and data from social systems and IoT gadgets approximately the setting of the information. Putting these distinctive sorts of information together is implied to create AI frameworks way better at finding tricks by catching complicated patterns and other ways of assaulting. The audit finds critical patterns in AI-based extortion location, such as the move toward real-time handling and the utilize of logical AI models to create things clearer and simpler to understand. It too talks about problems like information protection issues, uneven datasets, and the ought to keep changing models to ensure against unused dangers. In conclusion, the meta-analysis gives valuable data to individuals who need to utilize or improve AI-based trick location frameworks.",No,"이 논문은 여러 기존 연구 결과를 종합하여 AI 기반 사기 탐지 알고리즘의 효과를 평가하는 메타분석 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합적 분석에 해당합니다."
Software Defect Prediction Using Ensemble Learning: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477596,"Recent advances in the domain of software defect prediction (SDP) include the integration of multiple classification techniques to create an ensemble or hybrid approach. This technique was introduced to improve the prediction performance by overcoming the limitations of any single classification technique. This research provides a systematic literature review on the use of the ensemble learning approach for software defect prediction. The review is conducted after critically analyzing research papers published since 2012 in four well-known online libraries: ACM, IEEE, Springer Link, and Science Direct. In this study, five research questions covering the different aspects of research progress on the use of ensemble learning for software defect prediction are addressed. To extract the answers to identified questions, 46 most relevant papers are shortlisted after a thorough systematic research process. This study will provide compact information regarding the latest trends and advances in ensemble learning for software defect prediction and provide a baseline for future innovations and further reviews. Through our study, we discovered that frequently employed ensemble methods by researchers are the random forest, boosting, and bagging. Less frequently employed methods include stacking, voting and Extra Trees. Researchers proposed many promising frameworks, such as EMKCA, SMOTE-Ensemble, MKEL, SDAEsTSE, TLEL, and LRCR, using ensemble learning methods. The AUC, accuracy, F-measure, Recall, Precision, and MCC were mostly utilized to measure the prediction performance of models. WEKA was widely adopted as a platform for machine learning. Many researchers showed through empirical analysis that features selection, and data sampling was necessary pre-processing steps that improve the performance of ensemble classifiers.",No,"본 논문은 소프트웨어 결함 예측을 위한 앙상블 학습 기법에 관한 체계적 문헌 리뷰로, 기존 연구들을 종합하고 분석하는 데 초점을 맞추고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이 아니라, 기존 연구들을 정리한 리뷰 논문에 해당합니다."
A Systematic Review on Crop Leaf Disease Identification Using Machine Learning and Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392042,"World Population is increasing drastically and is expected to be around 12.3 billion in 2100. With the increase in population, it becomes important to foster the basic needs of mankind suitably. To withstand larger food requirements, getting higher crop yields is important. However, challenges like crop diseases, environmental factors like soil type, soil moisture, nutrient contents in soil, climate change, etc., affect various crop yields severely. The modern technologies can help in minimizing the adverse effects of these factors on crop yields. The advent of advanced automation techniques in Artificial Intelligence (AI) & Machine Learning (ML) can help in automated detection of crop diseases more accurately. However, use of such automated techniques face several challenges. Current study attempts to present a systematic literature review (SLR) of the existing body of knowledge in crop disease recognition & classification using automated techniques. The review followed PRISMA guidelines & searched four major databases with a comprehensive search query. 60 relevant research studies were identified after filtering through systematic inclusion & exclusion criteria for further analysis. Each research article is examined for the crop species used, the dataset used, & the methodology used. In each of the studied research papers, we looked at the performance measures utilized to assess the overall acquired findings for crop disease diagnosis & classification. It is observed that such systems have great potential to improve food security & agricultural productivity. However, more research is needed to overcome the challenges & limitations of the current methods and to ensure their validity & reliability in different settings and platforms. The readers of this study could acquire challenges faced by researchers in automated crop disease identification systems & their potential solutions to improve the performance of advanced algorithms.",No,"본 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 포함하지 않고 기존 연구들의 분석과 요약에 초점을 맞추고 있습니다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당합니다."
Investigating the Potential of AI-Driven Data Analytics for eARMS: A Survey-Based Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10823524,"The aim of this paper is on the role of AI powered data analysis in the decision-making processes of information systems such as ERP systems. We reviewed the literature, technological aspects, and case studies that pertain to the area of embedding AI analytics into ERP systems, or in any compliance with the ERP design. We consider the weights and downsides of the suggested AI analytics sales model, potential ethical issues, privacy concerns, other technical issues, and foresight research. We focus on bridging this information gap by increasing the knowledge of intersection analysis and artificial intelligence that structures ERP systems, describing how ERP systems can benefit along with the shortcomings in this area and how they can be addressed in further research. At the same time, we address the practical aspects of the implementation of AI-guided methods in everyday business activities and evaluate their applicability to various sectors. By analyzing the latest trends, we intend to add value to the existing body of knowledge by addressing emerging opportunities and challenges in the effective utilization of AI in enterprise systems. Our review also indicates the necessity of a cross-discipline agenda in order to resolve current issues and shape the growth of this fast-changing domain.",No,"본 논문은 AI 기반 데이터 분석과 ERP 시스템에 관한 기존 문헌, 기술, 사례 연구를 종합적으로 검토하는 서베이 연구로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 정리하고 분석한 리뷰 논문에 해당합니다."
Continuous Multimodal Biometric Authentication Schemes: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9360794,"Building safeguards against illegitimate access and authentication is a cornerstone for securing systems. Existing user authentication schemes suffer from challenges in detecting impersonation attacks which leave systems vulnerable and susceptible to misuse. A range of research proposals have suggested continuous multimodal biometric authentication (CMBA) systems as a reliable solution. Though contemporary authentication systems have the potential to change their current authentication scheme, there is a lack of critical analysis of current progress in the field to foster and influence practical solutions. This paper provides a systematic survey of existing literature on CMBA systems, followed by analysis to identify and discuss current research and future trends. The study has found that many diverse biometric characteristics are used for multimodal biometric authentication systems. The majority of the studies in the literature reviewed apply supervised learning approaches as a classification technique, and score level fusion is predominantly used as a fusion model. The review has determined however that there is a lack of comparative analysis on CMBA design in terms of combinations of biometric types (behavioural only, physiological only, or both), machine learning algorithms (unsupervised learning and semi-supervised learning), and fusion models. Most of the studies evaluated a CMBA system’s accuracy functionality, such as False Acceptance Rate (FAR), False Rejection Rate (FRR) and Equal Error Rate (EER). However, security, scalability and usability (user acceptance and satisfaction) are generally not addressed thoroughly even though these are key factors for system success in a real deployment. Furthermore, a CMBA system should be implemented and evaluated extensively on real data without restriction to prove that such systems are feasible.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 조사 및 종합 분석에 해당합니다."
A Survey of Machine Learning Technologies for COVID-19 Pandemic,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730228,"Intelligent technologies including machine learning and artificial intelligence are playing significant roles in human's battle against COVID-19 pandemic. Machine learning enables the machine to learn and improve on its own without being programmed in detail. Machine learning has now penetrated into many fields to help fight the epidemic. However, a specific and representative review of the contributions of machine learning is currently lacking. The purpose of this paper is to summarize several machine learning applications against COVID-19 including: i) predicting confirmed cases and trend, ii) classifying and diagnosing using ML-based images, and iii) managing medical resources. A database related to machine learning Technologies for COVID-19 is created. Moreover, a concise review is finished on the collected information by evaluating the different uses of machine learning against COVID-19. We also assemble researches in the present COVID-19 literature focused on ML-based methods in order to demonstrate a profound insight into COVID-19 related topics. Our discoveries emphasize crucial variables and available COVID-19 resources that facilitate clinical and translational research.",No,"본 논문은 COVID-19에 대한 머신러닝 기술의 적용 현황을 정리하고 평가하는 서베이(리뷰) 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하여 분석하는 데 중점을 두고 있습니다."
Comparative Analysis of Machine Learning and Deep Learning in Predicting Multiclass Obesity Levels,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10729926,"In recent years, obesity has grown exponentially. Obesity or overweight is a physical condition where there is an excessive accumulation of fat in the body, disrupting one's health. Society tends to feign ignorance to their unhealthy lifestyles which leads to many health problems including obesity. It is a type of disease that can cause many chronic diseases, such as heart disease, diabetes, and much more. Therefore, we propose the idea of using AI to build a system that will serve as preventive measures which serve as the purpose of this research. Meaning that identifying obesity risk will be much easier and will help raise people awareness. Another aim of this paper is not only will readers be able to learn about obesity and its causing factors, but readers will also gain in-depth knowledge about machine learning and deep learning algorithms. Diving deep into the AI algorithms especially reading related works, we found interesting research by Elias Rodriguez et al., where machine learning performs better than deep learning in terms of KNN and MLP algorithms. This piques our interest. Therefore, this paper will further study this phenomenon. For this research, implementation of machine learning and deep learning algorithms widely used in prediction is involved. The algorithms used are K-Nearest Neighbor (KNN) and Multi-Layer Perceptron (MLP). The experiment shows that during certain circumstances MLP has achieved above 89% accuracy. This shows that the model can receive the dataset and predict obesity without any issue. Furthermore, the model can now be used as a preventive measure (early detection) against obesity or overweight.",Yes,논문 초록에서 머신러닝과 딥러닝 알고리즘을 직접 구현하고 실험하여 비만 예측 정확도를 평가하는 독창적인 연구 내용을 포함하고 있음을 확인할 수 있습니다. 이는 기존 연구를 참고하되 새로운 실험과 분석을 수행한 연구 논문에 해당합니다.
An Intelligent Cyber Security System Approach using Machine Learning Techniques on KDD-99 Dataset,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763794,"Internet is used in everywhere in our day-to-day life & it led to rise in cyberattacks. As people are rely on digital platform, cybercriminals get opportunities to explore their vulnerabilities & grow cybercrime so there is need to developed robust cyber security system is paramount. This paper covers comprehensive approach to enhanced cybersecurity with the help of machine learning by specially focused on well-known KDD99 dataset. In this paper, introduction section describes significance of cybersecurity, its different types & potential of machine learning to detect & mitigate cyber-attack. In review of literature, Literature related to machine learning using KDD99 dataset by highlighting existing research & methods are given. Paper mainly focused on proposed model using KDD99 dataset & its structure that integrates machine learning classification algorithm & benchmark dataset for intrusion detection is extensively review that also includes overview, types of attack, features & various attack categories like DOS, R2L, U2R & Probing are discussed. For calculating performance of the model, Naïve bayes, DT, RF, SVM, LR and GB classifier is used. Performance of all machine learning classifier is calculated by using accuracy parameter. Amongst them random forest machine learning classifier gives outperforming result over all classifier that showcasing potential of these technique in real world cybersecurity applications. This developed system helps in contributing to safeguard the digital infrastructure.",Yes,"논문은 KDD-99 데이터셋을 활용한 머신러닝 기반 사이버 보안 시스템 모델을 제안하고, 여러 분류기들의 성능을 비교 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구를 리뷰하는 데 그치지 않고 직접적인 실험과 성능 분석을 수행한 연구 논문으로 판단된다."
A Review on Improvement in Detection of Cyberattacks Using Artificial Intelligence for the Grid Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542612,"The integration of artificial intelligence and machine learning technologies into critical infrastructures, such as smart grids, has raised significant concerns regarding cybersecurity. This paper explores the dual role of artificial intelligence and machine learning in both enhancing and challenging cybersecurity within smart grid systems. By analysing the current stateof-the-art research and technology, the utilisation of artificial intelligence and machine learning to fortify cybersecurity defences while addressing potential vulnerabilities. The the emergence of cyber threats targeting Internet-of-Things-based smart grids is highlighted and solutions to mitigate these risks are proposed. Through a comprehensive review of literature, the efficacy of artificial intelligence driven cybersecurity measures in detecting and preventing cyberattacks are evaluated. Additionally, challenges associated with implementing these solutions in smart grid environments, such as data complexity and computational requirements are taken into account. The findings underscore the critical importance of ongoing research and innovation to ensure the resilience of smart grid cybersecurity.",No,"초록에서 본 논문은 기존 연구들을 종합적으로 검토하고 평가하는 리뷰 논문임을 명확히 밝히고 있으며, 독창적인 실험 결과나 새로운 연구 방법론 제시는 포함되어 있지 않다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Techniques and Equipment for Automated Pupillometry and its Application to Aid in the Diagnosis of Diseases: A Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202608,"This work aims to investigate, by means of a Systematic Literature Review, to evaluate the current state of the use of artificial intelligence in automated pupillometric technology and its application in helping to diagnose diseases, to identify the methods and equipment used and propose case new equipment based on computer vision is feasible. We also investigated the accuracy of methodologies and equipment that use computerized pupilometry to identify pathologies or disorders, as well as the viability and usability of existing pupilometers. In this sense, creating a pupilometer capable of stimulating and varying wavelengths, providing an interface to preview the exam, and embedding the classification algorithms is a great challenge. In this systematic review of the literature, we consider publications from the last ten years (2010 - 2020) indexed by seven solid scientific databases. The review identified a vast amount of work on pupillometry; however, a small amount related to the construction and viability of a pupilometer with an embedded system, easy to use and with a preview interface. Having identified this, we propose a new methodology for the construction of the pupilometer as well as the algorithm for extracting the characteristics through pupilometry.",No,본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 평가하고 새로운 방법론을 제안하는 리뷰 논문입니다. 직접적인 실험 결과나 독창적인 연구 데이터가 포함되어 있지 않아 연구 논문으로 보기 어렵습니다.
A Review of Recent Trends in Blockchain Consensus Algorithms: Artificial Intelligence-Based Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10460688,"Blockchain has emerged as an important technology, offering safe, decentralized, and transparent platforms for recording and validating transactions. Blockchain technology consist of several main components, i.e., consensus algorithm. The consensus algorithm guarantees that all participating nodes in a blockchain network agree over the data control. Traditional consensus methods, such as Proof of Work (PoW) and Proof of Stake (PoS), present issues in terms of energy consumption and attack vulnerability. To overcome these constraints, there has been a rising interest in incorporating Artificial Intelligence (AI) techniques, especially deep learning, into blockchain consensus algorithms. We highlight blockchain fundamentals in this article, while also stressing the importance of the consensus algorithm. Furthermore, we address the most recent advancements in blockchain consensus methods in both performance-based and reputation-based paradigm, emphasizing the use of AI inside these decentralized systems. The use of AI, especially deep learning, in consensus algorithms has the potential to overcome the limitations of previous techniques. Blockchain networks may improve its performance by employing AI capabilities. However, incorporating AI into blockchain consensus algorithms is having its own challenges. Therefore, we also highlight several issues related with AI-based techniques in blockchain consensus algorithms, such as dealing with the quality and variety of data utilized by consensus algorithms and maintaining the openness of the AI models. In addition to those challenges, we propose a future direction for the AI-based approach in blockchain, which includes merging the mechanisms of performance-based and reputation-based consensus algorithms to incorporate the merits of both methods.",No,본 논문은 블록체인 합의 알고리즘과 인공지능 기반 접근법에 대한 최근 동향을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 실험이나 독창적인 연구 결과를 제시하기보다는 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다.
A review on Deep Learning in thyroid ultrasound Computer-Assisted Diagnosis systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708866,"Ultrasound is one of the most used imaging techniques for assessing and evaluating thyroid lesions. Indeed, it shows a good performance in terms of discrimination between benign and malignant thyroid nodules. Diagnosis by ultrasound is, however, not as easy as it seems and depends strongly on the experience of the radiologists. To help physician and radiologists to better diagnose, many Computer-Assisted Diagnosis (CAD) systems have been developed. These systems are based on image processing techniques and on machine learning. They represent effective and useful tools allowing doctors to have a second opinion far from human subjectivity. Among the machine learning techniques, the Deep Learning has recently made rapid progress in interpreting medical imaging and has demonstrated an impressive efficiency. Various CAD systems treating ultrasound images of the thyroid have widely use it since then. This paper reviews the most recent research works on the CAD systems for analyzing ultrasound images of the thyroid to diagnose the benign or malignant nature of the thyroid nodules. The CAD systems studied in this paper are based on the Deep Learning. We present a brief description of the CAD systems based on the Deep Leaning. Specifically, we describe the data collection and the CNN implementation. We report also the results obtained in these studies and highlight the limitations of such studies. This literature review is aimed at researchers but also at physician who are interested in CAD tools in ultrasound images of the thyroid gland and can represent a state of the art for all those interested in the classification of medical imaging.",No,초록에서 해당 논문은 기존 연구들을 종합하여 CAD 시스템과 딥러닝 적용 현황을 정리하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험 기여가 포함된 연구 논문이 아닙니다.
Exploring the Intersections between the Metaverse and Web3 Emerging Technologies,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10367245,"The advancements of the metaverse has validated a new world of possibilities with Web2 and Web3 technologies that are increasingly immersive, interconnected, spatial and decentralized. One of the key Web3 technologies that is intersecting with the metaverse is blockchain technology, that has now imbued metaverse worlds and platform as what are now identified as Layer-1 and Layer-2 metaverse, and in tandem the intersecting of decentralized Web3 technologies including but not limited to metaverse currencies and cryptocurrencies, non-fungible tokens and tokenization, marketplaces, digital wallets, and blockchain main and side-chain networks. In combination with interoperable and interconnected avatars, the metaverse is evolving beyond phygital experiences and asset architecting into financial experiences and asset trading. This research paper investigates the intersections between the metaverse and Web3 emerging technologies by firstly conducting a systematic literature review of the metaverse and its intersection with emerging technologies which currently has a dearth in academic and research literature despite the developments of the Web3 metaverse and its Layer-1 Layer-2 protocol. The review is followed by pilot-testing the metaverse platforms intersecting with Web3 technologies in real-world, particularly on the use of buy-and-sell, trading and item ownership under decentralized finance and financial technology. Finally, the research assesses the resulting metaverse based on user reviews and experiences on the feasibility of Web3 metaverse. The intersection of Web3 and the metaverse is still an early adoption of IR4.0 emerging technologies, yet the convergence of the metaverse, blockchain technologies and advancements in artificial intelligence is ultimately the precursor to the future Web4.",Yes,본 논문은 체계적인 문헌 검토와 실제 메타버스 플랫폼의 파일럿 테스트를 통해 Web3 기술과 메타버스의 교차점을 직접 조사하는 독창적인 연구를 수행하고 있다. 또한 사용자 리뷰와 경험을 바탕으로 Web3 메타버스의 실현 가능성을 평가하는 등 연구 기여가 명확하다.
Zero-BertXGB: An Empirical Technique for Abstract Classification in Systematic Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845770,"Abstract classification in systematic reviews (SRs) is a crucial step in evidence synthesis but is often time-consuming and labour-intensive. This study evaluates the effectiveness of various Machine Learning (ML) models and embedding techniques in automating this process. Five diverse datasets are utilized: Aceves-Martins (2021), comprising 1,258 excluded and 230 included abstracts on the utilization of animal models in depressive behaviour studies; Bannach-Brown (2016), with 896 excluded and 73 included abstracts focusing on the methodological rigour of environmental health systematic reviews; Meijboom (2021), containing 599 excluded and 32 included abstracts on the retransitioning of Etanercept in rheumatic disease patients; Menon (2022), with 896 excluded and 73 included abstracts on environmental health reviews; and a custom Clinical Review Paper Abstract (CRPA) dataset, featuring 500 excluded and 50 included abstracts. A significant research gap in abstract classification has been identified in previous literature, particularly in comparing Large Language Models (LLMs) with traditional ML and Natural Language Processing (NLP) techniques regarding scalability, adaptability, computational efficiency, and real-time application. Addressing this gap, this study employs GloVe for word embedding via matrix factorization, FastText for character n-gram representation, and Doc2Vec for capturing paragraph-level semantics. A novel Zero-BertXGB technique is introduced, integrating a transformer-based language model, zero-shot learning, and an ML classifier to enhance abstract screening and classification into “Include” or “Exclude” categories. This approach leverages contextual understanding and precision for efficient abstract processing. The Zero-BertXGB technique is compared against other prominent LLMs, including BERT, PaLM, LLaMA, GPT-3.5, and GPT-4, to validate its effectiveness. The Zero-BertXGB model achieved accuracy values of 99.3% for Aceves-Martins2021, 92.6% for Bannach-Brown2016, 85.7% for Meijboom2021, 94.1% for Menon2022, and 98.8% for CRPA. The findings indicate that the Zero-BertXGB model, alongside other LLMs, can deliver reliable results with minimal human intervention, enhancing abstract screening efficiency and potentially revolutionizing systematic review workflows.",Yes,"본 논문은 다양한 데이터셋을 활용하여 새로운 Zero-BertXGB 기법을 제안하고, 이를 기존의 여러 LLM 모델과 비교 평가하는 독창적인 연구를 수행하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문에 해당한다."
Taxonomy of Quality Assessment for Intelligent Software Systems: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10320363,"The increasing integration of AI software into various aspects of our daily lives has amplified the importance of evaluating the quality of these intelligent systems. The rapid proliferation of AI-based software projects and the growing reliance on these systems underscore the urgency of examining their quality for practical applications in both industry and academia. This systematic literature review delves into the study of quality assessment metrics and methods for AI-based systems, pinpointing key attributes and properties of intelligent software projects that are crucial for determining their quality. Furthermore, a comprehensive analysis of this domain will enable researchers to devise novel methods and metrics for effectively and efficiently evaluating the quality of such systems. Despite its importance, this area of development is still relatively nascent and evolving. This paper presents a systematic review of the current state of the taxonomy of quality assessment for AI-based software. We analyzed 271 articles from six different sources that focused on the quality assessment of intelligent software systems. The primary objective of this work is to provide an overview of the field and consolidate knowledge, which will aid researchers in identifying additional areas for future research. Moreover, our findings reveal the necessity to establish remedial strategies and develop tools to automate the process of identifying appropriate actions in response to abnormal metric values.",No,"본 논문은 체계적 문헌 리뷰(Systematic Literature Review)를 수행하여 기존 연구들을 종합하고 분류하는 데 중점을 두고 있으며, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않는다. 따라서 연구 논문이라기보다는 기존 연구를 정리하고 분석한 리뷰 논문에 해당한다."
Systematic Literature Review: An Investigation Towards Finding Constructs For Performance Prediction of Students in an Online Engineering Course,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996249,"The use of technology in the field of engineering education has been the most common intervention, especially in the post-pandemic era. Most universities have plans to continue the blended approach to education in the future. This decision has to be an evaluated decision and the analysis of a student's performance serves as an input to take the decision. The other advantage of analysis include early prediction of student performance which can help the instructors to provide timely interventions and help the students to improve their performance. Thus identification of constructs that reflect student engagement and performance in a course delivered in online mode is very essential. This literature review attempts to bring forward the constructs used by various researchers that reflect student engagement and performance. The review is situated in the context of engineering education delivered in online mode. The identification of constructs is significant and helps to build machine learning models for predicting the performance of the students. Standard Systematic Literature Review(SLR) methods defined in literature including citation searching and hand searching were carried out to identify the constructs that have been in use. A list of constructs used by researchers in the literature, that capture students' attention and performance are identified and presented in this review. The identified constructs include students' interaction with content, students' interaction with peers, demographic factors, and the academic records of the student. These validated constructs are proposed to integrate with the Learning Management System(LMS) and use the feature for early prediction of student failures.",No,"본 논문은 기존 연구들을 체계적으로 검토하여 학생 성과 예측에 사용되는 구성요소들을 정리한 문헌 리뷰 논문입니다. 직접적인 실험이나 새로운 연구 결과를 제시하는 것이 아니라, 기존 연구들을 종합하여 분석하는 데 초점이 맞춰져 있어 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Artificial Intelligence in Computerized Adaptive Testing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458219,"Artificial intelligence (AI) is increasingly used to provide customized and efficient e-learning, job search, and career development assistance to students and workers. Both students and jobseekers encounter assessments several times throughout their career and during job searches. Organizations now employ computerized adaptive testing (CAT), a computer-administered assessment that serves questions based upon the ability of a test taker. CAT aims to provide personalized assessments to test takers to accurately estimate their proficiency with respect to a latent trait (e.g., general intelligence and personality characteristic) that is not directly observable. There are several challenges in CAT, such as estimating the latent traits of an individual, generating questions, and question selection. Furthermore, these challenges become more complex as the number of latent trait dimensions being measured increases or if item responses are categorical rather than binary (e.g., using a 1 to 5 scale versus true or false). Traditional approaches employ psychometric and statistical models to make estimations. However, many approaches using machine learning, deep learning, and other AI techniques have emerged to provide better performance. In this paper, we provide a technique-oriented review of AI applications in CAT, and highlight the advantages, limitations, and future challenges in this problem area. We also reconcile different terms and notations used across psychometrics and AI to assist future research and development.",No,"본 논문은 AI를 활용한 컴퓨터 적응형 평가(CAT)에 대한 기술 중심의 리뷰 논문으로, 기존 연구들을 정리하고 장단점 및 향후 과제를 제시하는 내용입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Optimizing Article Screening and Information Extraction: A Hybrid Approach with GeminiAI and Vector Database,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740208,"The exponential growth of scientific literature poses a significant challenge to researchers, resulting in redundancy in R&D due to inefficient review mechanisms. Manual literature reviews are time-consuming and resource-intensive, particularly when screening abstracts and titles, highlighting the need for innovative solutions to optimize the review process. This study introduces a three-step methodology using the GeminiAI model to streamline literature reviews: (1) Initial Screening, (2) Abstract Detail Extraction, and (3) Final Integration, with a Vector Database enabling efficient semantic searches in PDF files. In the first phase, GeminiAI achieved an accuracy of 88.66% in evaluating titles and abstracts based on specific inclusion and exclusion criteria, demonstrating its capability to filter relevant literature efficiently. The second phase enhanced this process by extracting key details, such as research-related modalities, thereby significantly reducing the pool of relevant papers. In the final step, the integration of the Vector Database with GeminiAI excelled, achieving an 80% similarity score in extracting detailed information, which greatly facilitated review writing and minimized manual effort. The user-friendly website designed for this purpose enables seamless paper uploads, with the Vector Database automatically extracting relevant details to streamline the workflow and accelerate innovation. This approach underscores the power of AI in optimizing literature reviews, reducing manual screening time and resources, and mitigating the risk of overlooking critical information. The entire system, including source code and supplementary materials, is available on our publicly accessible GitHub repository. https://github.com/mammona/ai-powered-litreview.git",Yes,"본 논문은 GeminiAI 모델과 벡터 데이터베이스를 활용한 문헌 스크리닝 및 정보 추출 방법론을 제안하고, 그 성능을 수치로 평가하는 등 직접적인 연구 기여와 실험 결과를 포함하고 있다. 따라서 독창적인 연구 내용을 담은 연구 논문에 해당한다."
Advancing Systems Integration and Administration: Harnessing Artificial Intelligence for Enhanced Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10840382,"This paper explores the pivotal role of Systems Integration and Administration in AI/Security, emphasizing their importance amidst rapid technological advancements. Traditional cybersecurity measures are increasingly inadequate against evolving threats, necessitating the integration of AI-driven solutions. While maneuvering through the complexity of today's systems, a clear trend of more effective integration and good administration becomes even more prominent as the years go by. Through the careful exploitation of the processes and the analysis, the specialists draw attention to the great complementarity of Systems Integration and Administration to technological breakthroughs. Systems integration helps organizations to turn fragmented processes into a systematic approach for more effective use of resources and to foster innovation. Through a comprehensive literature review, this study elucidates the critical role of Systems Integration and Administration in facilitating the seamless operation of complex systems, particularly when augmented by AI technologies such as machine learning and deep learning. The integration of AI enhances security measures, enabling organizations to detect and mitigate threats in real time. The convergence of Systems Integration, Administration, Artificial Intelligence, and Security offers transformative potential, bolstering defenses against cyber threats while fostering innovation and efficiency. Through interdisciplinary collaboration, organizations can harness the power of AI to navigate the evolving threat landscape with confidence, contributing to a more secure and resilient future.",No,"본 논문은 기존 문헌을 종합적으로 검토하는 문헌 리뷰 연구로 보이며, 직접적인 실험, 데이터 분석, 또는 새로운 방법론 제시와 같은 독창적인 연구 기여가 명확히 드러나지 않습니다. 따라서 연구 논문보다는 개념적 고찰에 가까운 것으로 판단됩니다."
"Approaches, Applications, and Challenges of Using Sentiment Analysis for False Information Detection: A Systematic Literature Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857547,"In today's digital age, the prevalence of false information on the internet, spanning from fake news to fake user accounts, poses a pressing concern that undermines the reliability of online information. Such misinformation can have harmful consequences, such as promoting biased ideologies, swaying public opinion, and inciting unrest. Implementing sentiment analysis technology in false information detection significantly impacts combating misinformation on digital platforms. It analyzes textual content in real time, which is crucial for effectively identifying and mitigating false information such as fake news, fake reviews, fake accounts, and fake websites on digital platforms. This paper explores sentiment analysis's approaches, applications, and challenges for false information detection, providing insights for future research and application. A systematic literature review was conducted using the PRISMA methodologies to identify and evaluate relevant studies. The results of this paper contribute to understanding the current state of sentiment analysis in combating misinformation. This paper provided the common and available applications of sentiment analysis, such as fake news detection, fake review detection, fake social media account detection, and more. Approaches that can be taken to achieve these applications include the machine learning approach, neural network approach, hybrid approach, and more. Moreover, this work highlights the need for more advanced algorithms and ongoing research to address challenges such as sarcasm detection, bot account identification, and effectively interpreting informal language and slang in texts.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 종합하고 분석한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 과제를 정리하는 데 중점을 둔 논문이다."
Machine Learning and Mathematical Models for Prediction of Structural Aging Process,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158652,"This paper is a thorough and organized overview of the literature on the development of mathematical models to evaluate aging and the necessity of machine learning techniques in many failure prediction applications. It discusses the important elements of the aging phenomena, such as the mechanical, electrical, and thermal aging processes and how they affect the system’s efficiency. Throughout the research approach, the classification of the aging process and a description of its mathematical modeling are included as state-of-the-art of aging models, performance, and its evaluations. Whereas, in today’s world, the adaptation and importance of different machine learning (ML) methods like deep learning (DL), decision trees (DT), conventional neural networks (CNN), support vector machines (SVM), regression analysis, and artificial neural networks (ANN) are described briefly which helps to increase the efficiency of modeling of aging process. In order to give a comprehensive review of the many ML models utilized in this domain, the research where ML models are compared based on a qualitative evaluation of durability, quality, efficiency, and its rapid performance is specifically explored in this work. The analysis results of ML models provide a detailed overview of the various methodologies within the context of a review. As a result, the research reveals that using ML and artificial intelligence techniques is much more efficient than using simple mathematical modeling. The development of forecasting analysis offering excellent efficiency and economical solutions was significantly assisted by the use of ML techniques to simplify the complicated mathematical equations describing physical phenomena of structural aging. Due to the enormous advantages and capabilities of ML, researchers are applying ML techniques and combining them with its other methods in an effort to find prediction models that are more precise and effective. Researchers and engineering managers can use this survey as a reference to help them select the best machine ML technique for their particular prediction problem.",No,"초록에서 본 논문은 기존 연구들을 종합적으로 정리하고 비교하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험, 모델 개발을 포함하지 않고 문헌 조사를 중심으로 한 개관임을 알 수 있습니다."
"Recent Advancement in Sleep Technologies: A Literature Review on Clinical Standards, Sensors, Apps, and AI Methods",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905578,"This is a literature review paper covering state-of-the-art sleep technologies to measure sleep and clinical sleep disorders. This paper addresses an interdisciplinary audience from a variety of subdomains in engineering and medicine. We reviewed 120 scientific papers, 15 commercial mobile apps, and 4 commercial devices. We selected the papers from scientific publishers including Institute of Electrical and Electronics Engineers (IEEE), Nature, Association for Computing Machinery (ACM), Proceedings of Machine Learning Research, Journal of Informatics in Health and Biomedicine, Plos One, PubMed, and Elsevier and Nature digital libraries. We used Google Scholar with keywords including “sleep monitoring”, “sleep monitoring technologies”, “non-contact sleep monitoring”, “mobile apps for sleep monitoring”, “AI in sleep technologies”, and “automated sleep staging.” The manuscript reviews sleep technologies, including sleep lab technologies such as polysomnography and consumer sleep technologies categorized as ambient room sensors, wearable sensors, bed sensors, mobile apps, and artificial intelligence. We primarily focused on validation and comparison studies of the reviewed technologies. The manuscript also provides an overview of several clinical datasets for sleep staging and taxonomizes the different learning methods. Finally, the manuscript offers our insights and recommendations about the application of the reviewed sleep technologies.",No,"본 논문은 기존 연구들을 종합하여 정리한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 현황과 동향을 소개하는 데 중점을 두고 있습니다."
Towards Human-Centered Explainable AI: A Survey of User Studies for Model Explanations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316181,"Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how human-computer interaction (HCI) and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI.",No,"이 논문은 97편의 기존 연구를 체계적으로 검토하고 분류하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 사용자 연구 방법론을 종합하는 데 중점을 두고 있습니다."
Combinatorial Analysis of Deep Learning and Machine Learning Video Captioning Studies: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10413461,"Recent improvements formulated in the area of video captioning have brought rapid revolutions in its methods and the performance of its models. Machine learning and deep learning techniques are both employed in this regard. However, there is a lack of tracing the latest studies and their remarkable results. Although several studies have been proposed employing the ML and DL algorithms in different other areas, there is no systematic review utilizing the video captioning task. This study aims to examine, evaluate, and synthesize the primary studies into a thorough Systematic Literature Review (SLR) that provides a general overview of the methods used for video captioning. We performed the SLR to determine the research problems under which machine learning models were preferred over the deep learning models and vice versa. We collected a total of 1,656 studies retrieved from four electronic databases; Scopus, WoS, IEEE Xplore, and ACM, based on our search string from which 162 published studies passed the selection criteria related to one primary and two secondary research questions after a systematic process. Moreover, insufficient data collection and inefficient comparison of results are common issues identified during the review process. We conclude that the 2D/3D CNN for video feature extraction and LSTM for caption generation, METEOR and BLEU performance evaluation tools, and MSVD dataset are most frequently employed for video captioning. Our study is the pioneer in comparing the implementation of ML and DL algorithms employing the video captioning area. Thus, our study will accelerate the critical assessment of the state-of-the-art in other research fields of video analysis and human-computer interaction.",No,"본 논문은 비디오 캡셔닝 분야에서 머신러닝과 딥러닝 연구들을 체계적으로 문헌 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하고 평가하는 데 초점을 맞추고 있습니다. 따라서 새로운 연구 기여를 포함한 연구 논문으로 보기 어렵습니다."
Striking Ethical Balance in AI-TAI: Promoting Academic Integrity through AI-Powered Tools,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531521,"Artificial intelligence (AI) is rapidly changing the landscape of academic integrity. AI-powered tools can be used to detect plagiarism, prevent cheating, and promote responsible research practices. However, there are also concerns about the ethical implications of using AI for academic integrity. This paper reviews the research on AI-powered academic integrity tools. It discusses the advantages and disadvantages of these tools, as well as the concerns that have been raised about their use. The paper also provides recommendations for how AI can be used to promote academic integrity in a responsible and ethical way. The conclusion of the paper states that AI-powered tools can be a valuable tool for promoting academic integrity. However, it is important to use these tools in a responsible and ethical way. This means being aware of the limitations of these tools, and using them in conjunction with other measures to promote academic integrity.The paper also calls for further research on the use of AI for academic integrity. This research should focus on the ethical implications of using these tools, as well as their effectiveness in detecting plagiarism and preventing cheating.",No,초록 내용은 AI 기반 학문적 정직성 도구에 대한 기존 연구를 검토하고 장단점 및 윤리적 문제를 논의하는 리뷰 논문으로 보입니다. 독창적인 연구 결과나 실험적 기여가 포함되어 있지 않아 연구 논문으로 판단하기 어렵습니다.
AI in Cybersecurity Education- A Systematic Literature Review of Studies on Cybersecurity MOOCs,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156050,"Machine learning (ML) techniques are changing both the offensive and defensive aspects of cybersecurity. The implications are especially strong for privacy, as ML approaches provide unprecedented opportunities to make use of collected data. Thus, education on cybersecurity and AI is needed. To investigate how AI and cybersecurity should be taught together, we look at previous studies on cybersecurity MOOCs by conducting a systematic literature review. The initial search resulted in 72 items and after screening for only peer-reviewed publications on cybersecurity online courses, 15 studies remained. Three of the studies concerned multiple cybersecurity MOOCs whereas 12 focused on individual courses. The number of published work evaluating specific cybersecurity MOOCs was found to be small compared to all available cybersecurity MOOCs. Analysis of the studies revealed that cybersecurity education is, in almost all cases, organised based on the topic instead of used tools, making it difficult for learners to find focused information on AI applications in cybersecurity. Furthermore, there is a gab in academic literature on how AI applications in cybersecurity should be taught in online courses.",No,"본 논문은 사이버보안 MOOC에 관한 기존 연구들을 체계적으로 문헌 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 분석하는 데 초점이 맞춰져 있습니다."
AI for Automated Thoracic Disease Assessment from X-Ray Imaging: a Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296683,"With the increasing availability of digital X-ray imaging, artificial intelligence (AI) has emerged as a promising tool for automating the assessment of thoracic diseases. The objective of this study is to systematically review the artificial intelligence (AI) and deep learning methods proposed for the automated assessment of thoracic diseases from chest X-ray images. A thorough search of the relevant literature was conducted, and studies that met the inclusion criteria were critically reviewed. Information on the datasets, model architectures, evaluation metrics, and results was extracted. Convolutional neural networks are prevalent, achieving a state-of-the-art classification performance. Recent studies have explored more complex tasks such as disease localization, segmentation, and report generation. The multitask and multimodal approaches are promising. Challenges related to the data, evaluations, and clinical adoption were identified. This study prevails that there is a significant progress in using deep learning for automated chest X-ray analysis. Further research is needed to validate these models in real-world settings and to facilitate their integration into clinical workflows.",No,이 논문은 흉부 X선 영상에서 흉부 질환 평가를 위한 AI 기법들을 체계적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들을 종합하고 분석하는 데 중점을 두고 있습니다.
"Analysis the relationship of physiological, environmental, and cow milk productivity using AI",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7936165,"The dairy cattle productivity is very depending on the quality of their environment and physiological aspect. Hence, the purpose of the paper is to looking for the relationship model of physiological, environmental and milk productivity by using artificial intelligence (AI). The model will be useful for the user to decide the best cow treatment in order to gain the best milk production. The research is started with literature review and early survey of cattle physiological, environment factors and milk productivity. The next step is measuring the environment data (temperature, wind speed, and relative humidity) and measuring physiological aspect (heart rate, body temperature) correlated with milk productivity in 500 pairs of data. All the data are collected and stored into the database and then trained and validated using Back Propagation Neural Network (BPNN) with Genetic Algorithm (GA) optimization. The initial BPNN architectures are selected in 2 hidden layer, delta bar delta learning rule, sigmoid transfer function and epoch 10000. As a result, the system successfully developed an intelligent tool to predict milk production in any levels of environment and physical condition. Based on sensitivity analysis, the relative humidity, heart rate, environment and cow body temperature are categorized in strong impact, beside that are in weak impact on milk production.",Yes,"본 논문은 인공지능 기법을 활용하여 생리적, 환경적 요인과 우유 생산성 간의 관계 모델을 개발하는 독창적인 연구를 수행하였다. 데이터 수집, 모델 학습 및 검증 과정을 포함한 실질적인 연구 방법론과 결과를 제시하고 있어 연구 논문에 해당한다."
A Systematic Review of Feature Selection Techniques in Software Quality Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959566,"Background: Feature selection techniques are important factors for improving machine learning models because they increase prediction accuracy and decrease the time to create a model. Recently, feature selection techniques have been employed on software quality prediction problems with different results and no clear indication of which techniques are frequently used.Objective: This study aims to conduct a systematic review of the application of feature selection techniques in software quality prediction and answers eight research questions.Method: The review evaluates 15 papers in 9 journals and 6 conference proceedings from 2007 to 2017 using the standard systematic literature review method.Results: The results obtained from this study reveal that the filter feature selection method was the most commonly used in the studies (60%) and RELIEF was the most employed among this method, and a limited number of studies employed an ensemble method. Several studies used public datasets available in the PROMISE software project repository (60%). Most studies focused on software defect prediction (classification problem) using area under curve (AUC) as a primary evaluation measure, whereas only two studies focused on software maintainability prediction (regression problem) using mean magnitude of relative error (MMRE) as a primary evaluation measure. All selected studies performed k-fold cross-validation to evaluate model accuracy. Individual prediction models were mostly employed and ensemble models appeared only in three studies. Naive Bayes was the most investigated among individual models, whereas Random forest was the most investigated among ensemble models.Conclusion: Feature selection techniques used by selected primary studies have a positive impact on the performance of the prediction models. Further, both ensemble feature selection method and ensemble models have the ability for increasing prediction accuracy over single methods or individual models and have reported improvement in the prediction accuracy; however, the application of these techniques in software quality prediction is still limited.",No,"본 논문은 기존 연구들을 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 여러 연구의 결과를 종합하여 분석하고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 평가에 초점이 맞춰져 있다."
The Detection of E-Commerce Manipulated Reviews Using GPT-4,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346848,"In recent years, the exponential growth of e-commerce has transformed consumer purchasing behavior, with online reviews playing a crucial role in shaping buying decisions. Unfortunately, the prevalence of manipulated reviews has become a significant challenge, undermining the integrity of consumer feedback and eroding trust in e-commerce platforms. This research paper focuses on developing an advanced solution to address the issue of manipulated reviews using the innovative language model, GPT-4. The primary objective of this study is to investigate the effectiveness of GPT-4 in identifying and flagging manipulated reviews within the context of e-commerce platforms. GPT-4 is a state-of-the-art language model with superior natural language processing capabilities, making it an ideal candidate for automated review analysis. The research methodology encompasses a large-scale data collection process, where diverse e-commerce reviews are gathered from various platforms. To simulate real-world scenarios, manipulated reviews are artificially injected into the dataset, representing different degrees of sophistication. The dataset is then annotated by expert reviewers to establish ground truth labels for comparison. Next, GPT-4 is fine-tuned using transfer learning to specialize in detecting manipulated reviews. The fine-tuning process involves exposing the model to both genuine and manipulated review samples, allowing it to learn patterns and features indicative of manipulation. The fine-tuned model's performance is then evaluated using various metrics, including precision, recall, F1 score, and accuracy, against the ground truth dataset. The results of the experiment demonstrate the efficacy of GPT-4 in distinguishing between authentic and manipulated reviews. GPT-4 showcases remarkable accuracy and robustness in detecting increasingly sophisticated manipulation techniques, outperforming previous iterations of language models and traditional detection methods. Furthermore, the research delves into the characteristics of manipulated reviews that render them distinguishable from genuine reviews. These insights aid in understanding the vulnerabilities of e-commerce platforms to manipulation and offer valuable information for future research and platform enhancement efforts. The implications of this study are significant for e-commerce businesses, consumers, and platform administrators alike. By leveraging GPT-4's detection capabilities, e-commerce platforms can implement more robust review validation systems, ensuring the authenticity and reliability of consumer feedback. This, in turn, fosters greater trust among consumers, leading to informed purchasing decisions and improved overall user experience. In conclusion, this research paper contributes to the existing body of knowledge by showcasing the potential of GPT-4 in combatting manipulated reviews and strengthening the integrity of e-commerce platforms. With the rise of AI-driven technologies, the proposed approach presents a promising step towards a more transparent and trustworthy e-commerce ecosystem.",Yes,"본 논문은 GPT-4를 활용하여 전자상거래 조작 리뷰를 탐지하는 방법을 제안하고, 데이터 수집, 모델 미세조정, 성능 평가 등 독창적인 연구 과정을 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
PsychSynth: Advancing Mental Health AI Through Synthetic Data Generation and Curriculum Training,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773545,"The number of Mental-health help seekers are on rise over recent years, but the medical practitioners are limited due to which the healthcare system is heavily loaded and thereby the outcome is not promising. Advanced AI Technology is expected to ease this problem, however completely relying on the tools is reported to not work with the help seekers. Hence technology to assist help seekers and allow the healthcare system to screen the needy ones from the large pool of distress candidates should make the system efficient a nd effective.H owever for establishing the AI supportive system, two challenges exists: one is the large pool of dataset with diversity, and other is the AI model to respond to a context-aware situations. Expecting large dataset for training model from medical community needs further infrastructure support to make it digitally available with embedded annotations. Relying patiently for the dataset will miss the opportunity to serve the distress patients. Hence synthetic dataset generation and its validation from medical experts is an alternative for training robust and reliable model. Besides, context-aware curriculum inspired AI based summarizer model is found appropriate to adopt for this use-case where relevant features meant for diagnosing the problem is extracted from the improvised input text. The proposed curriculum trained AI model helps in transforming the improvised text inputs fed from the distress individuals to a summarized version representing domain expert form, embedded with symptoms related features for further classification. The synthetic data-set g eneration through OpenAI's GPT-40 models and Nemotron models are further evaluated with BERT based classifier m odels a nd curriculum based AI model. The training of the classifier m odels are also evaluated for synthetic and real-world dataset, which was scrapped from Reddit forum. Around 800 stream of real-world posts were evaluated from the medical experts and their findings related to sympotoms and annotations were employed to fine-tune the classifier and summarizer m odel. It was found t hat the fine-tuned models and training of BERT models from the merged dataset composed of synthetic ones with the medical practitioners annotated dataset were found to perform better than others. The summarizer model fetching shorter version of domain expert output enhanced the classification accuracy by 5 % for the real-world data. The effort is a step towards developing AI assistant to screen large posts of submissions from distress individuals and arrange for the necessary connects for the needy ones with the medical experts. The models and pruned datasets are made freely available for further usage to the researchers community.",Yes,본 논문은 정신 건강 AI를 위한 합성 데이터 생성과 커리큘럼 학습 기반 모델 개발 및 평가를 포함한 독창적인 연구 내용을 다루고 있습니다. 실제 의료 전문가의 평가를 통한 데이터셋 구축과 모델 성능 향상 실험을 수행하여 직접적인 연구 기여를 하고 있음을 알 수 있습니다.
Advancements in Sign Language Recognition: A Comprehensive Review and Future Prospects,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670380,"Sign language (SL) is a vital mode of communication, bridging the gap between the hearing impaired and hearing communities. However, SL, despite its paramount importance, has received relatively limited attention from researchers. Its unique structural characteristics, distinct from those of natural languages, present novel challenges that require innovative solutions. Remarkable technological advancements, notably in Artificial Intelligence (AI) and machine learning, offer promising avenues for automated Sign Language translation Systems (SLTS). This review study addresses the crucial need for a comprehensive synthesis of existing research by systematically examining and evaluating the progress made in SLTS. By analyzing 58 research papers, with a particular emphasis on the most frequently cited papers from each year up to 2023, we shed light on the field’s current state, identifying key advancements and challenges. This review followed a systematic approach based on clear guidelines. The methodology involved defining research questions, formulating queries, selecting studies based on clear criteria, and extracting pertinent information to address the research objectives. This review found that deep learning techniques, such as convolutional and recurrent neural networks, have shown high accuracy in sign language recognition, and their performance in recognizing the variety of signs has steadily improved over time. Additionally, integrating non-manual features has proven pivotal in enhancing recognition accuracy. Future research should refine advanced deep learning models and integrate non-manual features to improve system accuracy and applicability. These ongoing advancements hold the potential to revolutionize communication and break down barriers for individuals who rely on sign language as their primary mode of communication.",No,"이 논문은 기존 연구들을 종합하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 연구 동향을 정리하는 종합적 고찰에 해당합니다."
"Utilization of Deep Learning and Machine Learning Algorithms in Detection, Segmentation, and Classification of Brain Metastasis: A Systematic Literature Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10923420,"One-tenth to around a quarter of cancer patients lose their lives to Brain Metastasis (BM) globally. Accurate neuroimaging of BM is essential to encourage early detection, decide treatment options, and increase survival rates. This systematic literature review (SLR) examines the utilization of machine/deep learning algorithms and imaging modalities in the detection, segmentation, and classification of BM. The study adheres to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis Statement) and meticulously evaluates 32 relevant articles spanning from 2018 to 2023. The investigation explores scholarly databases including Google Scholar, SCOPUS, Web of Science, PubMed, SCISPACE, IEEE, Science Direct, Hindawi, and EBSCOhost to pull out relevant studies. The four aspects viz. employment of various imaging modalities, utilization of machine/deep learning algorithms, evaluation of performance metrics, and investigation of software tools for the identification of BM considered in this study. The review identifies Convolutional Neural Networks (CNN) as the most frequently utilized algorithm with the highest AUC-ROC (0.98) and demonstrates its efficacy for BM detection. Furthermore, out of 32, 15 articles highlight the significance of contrast-enhanced T1weighted images (T1CE/ T1c). This SLR unearths the transformative impact of machine/deep learning algorithms and emphasizes the importance of imaging modalities for enhancing the accuracy of diagnostics.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)로, 기존 연구들을 종합하여 분석한 내용이며, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당합니다."
Deep Reinforcement Learning for Containerized Edge Intelligence Inference Request Processing in IoT Edge Computing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10268016,"Edge intelligence (EI) refers to a set of connected systems and devices for artificial intelligence (AI) data collected and learned near the data collection site. The EI model inference phase has been improved through edge caching technologies such as intelligent models (IMs). IM inference across heterogeneously distributed edge nodes is worthy of discussion. The present focuses on software-defined infrastructure (SDI) and introduces a containerized EI framework for a mobile wearable Internet-of-Things (IoT) system. This framework, called the containerized edge intelligence framework (CEIF), is an inter-working architecture that allows the provisioning of containerized EI processing intelligent services related to mobile wearable IoT systems. CEIF enables dynamic instantiation of the inference services of AI models that have been pre-trained on clouds. It also accommodates edge computing devices (ECDs) running the container virtualization technique. Dynamic AI learning policies can also help with workload optimization, thereby reducing the response time of the requests of the EI inference. To stall the rapid increase in user workload when inferring the collected data for analysis, we then propose a deep q-learning algorithm in which the container cluster platform learns the varying user workload at the location of each ECD. The requests of the EI inference are scaled with the learned value and are processed successfully without overloading the ECD. When evaluated in a case study, the proposed algorithm enabled scaling of the processing requests of the EI inference in a containerized EI system while minimizing the number of instantiated container EI instances. The EI inference's requests are completed in an under-loaded container EI cluster system.",Yes,"논문은 컨테이너화된 엣지 인텔리전스 프레임워크(CEIF)를 제안하고, 딥 Q-러닝 알고리즘을 통해 EI 추론 요청을 동적으로 처리하는 방법을 연구하여 직접적인 알고리즘 설계 및 성능 평가를 포함하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문에 해당한다."
Artificial Intelligence Applications in Quality Management System: A Bibliometric Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707340,"This paper presents a systematic literature review and bibliometric analyses of publications in the field of Quality Management System (QMS) by authors who applied artificial intelligence (AI) in ISO 9001:2015 audits. Scopus-indexed papers that were published from 1998 to 2021 were evaluated based on the research publication metrics made available by Scopus. From the 142 extracted Scopus-indexed publications in September 2021, 109 or 76 percent of the publications remained after Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) procedure. Analyses and visualizations using VOSviewer reveal research productivity, affiliation and collaboration networks in various countries, and the corresponding relationship between research networks in the field of AI-enabled QMS. Findings reveal that QMS is leaning towards sustainability, big data, and applied technological innovations.",No,본 논문은 인공지능을 활용한 품질경영시스템 분야의 문헌을 체계적으로 검토하고 계량서지학적 분석을 수행한 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들의 분석과 종합에 초점이 맞춰져 있어 연구 논문으로 보기 어렵습니다.
Machine Learning Techniques for Sentiment Analysis of COVID-19-Related Twitter Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035946,"On Twitter, COVID-19 is a highly discussed topic. People worldwide have used Twitter to express their viewpoints and feelings during the pandemic. Previous research has focused on particular topics such as the public’s sentiment during the lockdown, their opinion on governmental measures, or their stance towards COVID-19 vaccines. However, until today, there is no comprehensive overview that presents possible areas of application for sentiment analysis of COVID-19 Twitter data. Therefore, this study reveals how sentiment analysis can provide relevant insights for managing the pandemic by applying a behavioral and social science lens. In this context, our systematic literature review focuses on machine learning-based sentiment analysis techniques and compares the best-performing classification algorithms for COVID-19-related Twitter data. We performed a search in five databases, which are: IEEE Xplore DL, ScienceDirect, SpringerLink, ACM DL, and AIS Electronic Library. This search resulted in 40 papers published between October 2019 and January 2022 that used sentiment analysis to evaluate the public opinion on COVID-19-related topics, which we further investigated. Our research indicates that the best performing models in terms of accuracy are ensemble models that comprise various machine learning classifiers. Especially BERT and RoBERTa models provide the most promising results when fine-tuned on Twitter data. Our study aims to combine machine learning-based sentiment analysis and insights from social and behavioral science to provide decision-makers and public health experts with guidance on the application of sentiment analysis in the fight against the spread of COVID-19.",No,"본 논문은 COVID-19 관련 트위터 데이터에 대한 감성 분석 기법을 체계적으로 문헌 검토한 연구로, 기존 연구들을 종합하고 비교하는 데 중점을 두고 있다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여보다는 기존 연구의 분석과 정리에 초점이 맞춰져 있다."
"Machine Learning Algorithm for Malware Detection: Taxonomy, Current Challenges, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068497,"Malware has emerged as a cyber security threat that continuously changes to target computer systems, smart devices, and extensive networks with the development of information technologies. As a result, malware detection has always been a major worry and a difficult issue, owing to shortcomings in performance accuracy, analysis type, and malware detection approaches that fail to identify unexpected malware attacks. This paper seeks to conduct a thorough systematic literature review (SLR) and offer a taxonomy of machine learning methods for malware detection that considers these problems by analyzing 77 chosen research works related to malware detection using machine learning algorithm. The research investigates malware and machine learning in the context of cybersecurity, including malware detection taxonomy and machine learning algorithm classification into numerous categories. Furthermore, the taxonomy was used to evaluate the most recent machine learning algorithm and analysis. The paper also examines the obstacles and associated concerns encountered in malware detection and potential remedies. Finally, to address the related issues that would motivate researchers in their future work, an empirical study was utilized to assess the performance of several machine learning algorithms.",No,본 논문은 77개의 기존 연구를 체계적으로 검토하고 머신러닝 기반 악성코드 탐지 방법에 대한 분류체계를 제시하는 리뷰 논문으로 보입니다. 직접적인 독창적 연구 결과나 새로운 알고리즘 개발보다는 기존 연구 분석과 평가에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
A Comparative Analysis between AI Generated Code and Human Written Code: A Preliminary Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825958,"In today’s world where generative artificial intelligence has almost become an integral part of the coding, new challenges must be faced. Therefore, evaluating software bugs in both human written and AI generated code can be useful for the developers. A comparative analysis of these two coding practices is not only helpful for the decisions taken by the developers, it will also assist to give a direction on how to improve the quality of AI driven code. Currently, researchers have leveraged the role of software metrics to compare human written code and AI generated code as these metrics have long been utilized for software bug and vulnerability prediction by the researchers. They also analyzed the secure coding practices in terms of the number of bugs found in both AI and human written code. Our study is an extension of the current works as this study focuses on a set of metrics and a set of bugs as identified by some static analyzer tools. Investigating these two coding practices from different angles can help to reveal unknown relationships and factors that further can be analyzed to improve code quality of recent AI tools. Therefore, the main objective of our work is to identify the relationships between software metrics and bugs in AI generated code and human written code to compare and contrast the coding profiles of the two approaches. This will offer developers critical knowledge to enhance their strategies in mitigating potential bug risks across different coding methodologies. We have utilized top-rated Java solutions to 90 LeetCode problems, generated corresponding AI Java solutions to them, and utilized various static analysis tools to collect metrics and bugs. In this study, we found that two software metrics CountLineCodeDecl and CountLineCodeExe are positively correlated with the bug DLS_DEAD_LOCAL_STORE and the metric AvgCyclomatic is related to the bug AvoidLiteralsInIfCondition in both human written and AI generated code. These findings provide developers with critical insights into potential bug risks, enabling more effective mitigation strategies across different coding methodologies.",Yes,"본 논문은 AI 생성 코드와 인간 작성 코드의 소프트웨어 메트릭과 버그 간의 관계를 실험적으로 분석하여 새로운 인사이트를 제공하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 정적 분석 도구를 활용한 데이터 수집과 비교 분석을 통해 직접적인 연구 기여를 하고 있음을 알 수 있습니다."
Deep learning for target-dependent sentiment classification in social media,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643214,"There is currently a revolution in developing deep learning models for improving performance of many machine learning systems. This revolution has been expanded to sentiment analysis (opinion mining) as a promising research area. Throughout this paper, we swaged in various recent works that are performed for developing sentiment analysis systems by exploiting capabilities of deep learning models. We introduce a comprehensive literature review on predicting sentiments expressed towards particular topics (targets) in micro-blogs (such as tweets). This micro-specialization is referred to as target-dependent sentiment classification. To make our work more comprehensive, we evaluated also two more deep learning models that have not been used before in this research direction. Experimental results are shown along with summaries and discussions to emerge significance of developing deep learning based models in improving accuracy of target-dependent sentiment classification. Our findings highlighted some gaps that can be filled in future research and illustrates that there is a room for improvement.",Yes,"논문 초록에서 기존 연구들을 종합적으로 리뷰하는 동시에, 이전에 사용되지 않은 두 가지 딥러닝 모델을 평가하고 실험 결과를 제시한 점으로 보아 독창적인 연구 기여가 포함되어 있다. 따라서 단순 리뷰 논문이 아니라 직접적인 연구 결과를 포함한 연구 논문으로 판단된다."
Application of Artificial Intelligence Vision Technology in Chinese Fencing Teaching,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918801,"Nowadays, with the development of Artificial Intelligence (AI) and its wide application in life practice, based on the cross-application of computer algorithms and related equipment technology, it has been widely studied in many fields. AI to promote physical education teaching and training has also become one of the essential projects of scientific physical education. Based on this, this research supports Chinese Fencing technology teaching with AI vision. First, this paper studies the development status of artificial intelligence visual technology to promote physical education, analyzes its system framework, establishes a model of AI visual technology to promote physical education, and finally carries out experiments. Then, this paper aims to use AI vision technology to evaluate the Chinese Fencing motion attitude specification problem and to explain the working principle of its additional correction of wrong actions and improvement of motion attitude specification.",Yes,논문은 AI 시각 기술을 활용하여 중국 펜싱 교육에 적용하는 모델을 구축하고 실험을 수행하는 등 구체적인 연구 방법과 결과를 포함하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 담고 있는 연구 논문으로 판단된다.
"A Review on Educational Games Design, Development and Effectiveness Measurement",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225880,"This paper qualitatively reviews game design, development and the effectiveness of serious games. Firstly, we analyzed the factors that need to be considered for effectiveness of serious games to achieve student learning outcomes, engagement and motivation. Then, we reviewed the game design and development methodology or framework in the current literature. The review indicates that there is a limited systematic connection between learning theory and serious game development methodology. Majority of research studies focus on student motivation and engagement but only few studies focus on learning outcomes. Moreover, the motivation and engagement are measured manually rather than using real time tools. Additionally, the current serious games designs are mostly static in nature and need to incorporate artificial intelligence and machine learning to make serious games dynamic to handle various scenarios.",No,"본 논문은 기존 문헌을 정성적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 한계를 분석하는 데 중점을 두고 있습니다."
Deep Learning Architectures for Improving Effectiveness of Covid Detection – A Pilot Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9675714,"Covid 19 has disrupted people’s lives all over the world. It has also resulted in serious infection, if not predicted at an initial stage. Accurate diagnosis is critical for early diagnosis and to provide appropriate treatment at an earlier stage. Therefore, this work aims at developing a deep learning-based diagnosis system to identify onset of covid at an early stage. In the proposed system a review of existing work in literature and need for covid diagnosis using Artificial Intelligence (AI) techniques is analyzed and presented. Further, three deep learning architectures viz., Convolutional Neural Network (CNN), Deep features from pretrained network classified using Support Vector Machine (SVM) classifier and Transfer learning using Resnet 101 architecture are experimented as part of this pilot study. A publicly available Covid CT image dataset is used to evaluate the proposed system. A considerable improvement in relevant performance metrics is observed in Transfer learning using RESNET 101 architecture. Henceforth the proposed system could be used as an aid to clinicians to accelerate covid 19 diagnosis and provide the necessary treatment.",Yes,"본 논문은 Covid 조기 진단을 위한 딥러닝 기반 진단 시스템을 개발하고, 세 가지 딥러닝 아키텍처를 실험하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
User‟s Opinion Analysis Towards Unified Payment Interface (UPI) Transactions Using Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568859,"Unified Payment Interface (UPI) has revolutionized the way digital transactions are conducted in India. As a fast and convenient payment system, it has gained immense popularity among users. This paper presents a comprehensive analysis of users' opinions and sentiments towards UPI transactions using Artificial Intelligence Natural Language Processing (NLP) techniques. The study explores the vast amount of textual data available on social media platforms, customer reviews, and surveys to gain insights into how users perceive and experience UPI transactions. The research methodology involves data collection, preprocessing, sentiment analysis, and topic modeling using advanced NLP algorithms and AI techniques. Furthermore, topic modeling techniques such as Latent Dirichlet Allocation (LDA) are applied to identify key topics and themes in user opinions. This helps in categorizing and summarizing the various aspects of UPI transactions that users discuss most frequently. Common topics may include security, ease of use, transaction speed, and user interface design. The application of Artificial Intelligence and NLP enables the development of sentiment and topic models that provide valuable insights into users' opinions. The findings of this research reveal the current trends in user sentiments towards UPI transactions, enabling stakeholders to make data-driven decisions. It also sheds light on areas that require attention and improvement, such as security measures, user education, and transaction reliability. Overall, this study showcases the power of Artificial Intelligence and NLP in analyzing user opinions and offers valuable insights for the continuous development and enhancement of the Unified Payment Interface system.",Yes,"본 논문은 인공지능과 자연어처리 기법을 활용하여 UPI 거래에 대한 사용자 의견을 분석하는 독창적인 연구를 수행하고 있으며, 데이터 수집부터 감성 분석, 주제 모델링까지 구체적인 연구 방법론과 결과를 제시하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Leveraging Edge Machine Learning for Energy-Efficient Communication in IoT Networks for Carbon-Neutrality,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10595019,"In the rapidly evolving landscape of computer-related technologies, the synergy between artificial intelligence (AI) and the Internet of Things (IoT) has garnered increasing interest from both industry and academia. This burgeoning interest has catalyzed the development of applications at the network edge, which have now achieved considerable scale. As a vital and emergent field of research, machine learning at the network edge intertwines two pivotal themes: wireless communication and machine learning. This research area, termed edge machine learning, endeavors to harness vast quantities of mobile data from edge devices to train machine learning models. A primary challenge addressed in this research is the efficient allocation of limited communication resources amidst the data abundance at the network edge. This includes the strategic management of radio resources, with a particular focus on evaluating and utilizing data importance for optimal radio resource management and allocation. For instance, in wireless communications, the signal-to-noise ratio is often considered a critical metric for assessing data importance. This paper summarizes various methodologies and theories from related works, proposing a novel scheduling algorithm predicated on signal-to-noise ratio considerations. A simulation test, leveraging batch training characteristics of convolutional neural networks, is conducted to validate the proposed approach.",Yes,"논문 초록에서 새로운 스케줄링 알고리즘을 제안하고, 이를 검증하기 위한 시뮬레이션 테스트를 수행했다고 명시하고 있어 독창적인 연구 내용이 포함된 연구 논문임을 알 수 있습니다. 또한 기존 연구들을 요약하는 데 그치지 않고, 구체적인 방법론을 개발하고 평가한 점에서 직접 기여가 있다고 판단됩니다."
Advanced Fraud Detection Using Machine Learning Techniques in Accounting and Finance Sector,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10568756,"Monetary fraud, which is a deceptive method for getting cash, has turned into a typical issue in organizations and associations as of late. Customary techniques like manual checks and reviews aren't extremely precise, are costly, and consume most of the day. Attempting to get cash by lying. With the ascent of simulated intelligence, approaches based on machine learning have become more well known. can be utilized shrewdly to track down fraud by dissecting an enormous number of monetary exercises information. Thus, this work attempts to give a systematic literature review (SLR) that ganders at the literature in a systematic manner. reviews and sums up the exploration on machine learning (ML)-based fraud recognizing that has proactively been finished. In particular, the review utilized the Kitchenham strategy, which depends on clear systems. It will then, at that point, concentrate and rundowns the significant pieces of the articles and give the outcomes. Considering the Few investigations have been finished to accumulate search systems from well-known electronic information base libraries. 93 pieces were picked, examined, and integrated in light of measures for what to incorporate and what to forget about. As the monetary world gets more confounded, robbery is turning into a more serious issue in the accounting and finance industry. Fraudulent activities cost cash, yet they likewise make it harder for individuals to trust monetary frameworks. To stop this danger, we want further developed ways of tracking down fraud straightaway. This theoretical gives an outline of how machine learning strategies are utilized to further develop fraud detection in accounting and finance.",No,"이 논문은 머신러닝 기반 사기 탐지에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰(SLR) 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 요약에 초점이 맞춰져 있습니다."
A Comparative Analysis of the Latest Trends in Stock Market Forecasting,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911642,"Forecasting Stock market prices is ongoing research with researchers aspiring to develop models that supersede previous accuracy rates. Historically, statistical models like the GARCH and ARIMA models have been used for predictive analytics tasks, however, the volatility and complex nature of stock market data make it difficult to complete prediction tasks with such models. Various machine learning models like the Random Forest algorithm and Support Vector Machine have also been used for stock market prediction, at times achieving significant prediction accuracy in some tasks. However, this may not be sufficient. Of late researchers are experimenting with deep learning algorithms for stock market prediction with these remarkably improving accuracy rates. In this study, we conduct a comparative analysis of prominent stock price forecasting models to identify the best-performing model. By using a systematic review of the literature, we evaluate the top-performing models in terms of prediction accuracy, adaptability, and robustness. Results show that variations of the ensemble LSTM model outperform other models. Additionally, we observe that models with knowledge retention generally outperform traditional machine learning approaches in stock market prediction tasks. Our findings have significant implications for developing effective stock market prediction systems and contributing to the growth of AI-driven financial analytics.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 비교 분석하는 문헌 리뷰 연구로 보이며, 직접적인 독창적인 실험이나 새로운 모델 개발에 대한 기여가 명확하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Maturity Assessment of Appraisal Processes in the AI Age: Ongoing Framework and Measuring Method,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10826084,"The increasing volume of generated data and archives raises pertinent questions regarding the effectiveness of traditional archival appraisal methods, which largely depend on human expertise. As automation and artificial intelligence (AI) become prevalent in various sectors, the field of archiving stands on the brink of significant transformation.This paper explores the integration of AI within archival appraisal processes, framed within the context of the Maturity Assessment for Appraisal (MAA) project (2023-2025). The MAA seeks to evaluate the defensibility, stability, and appropriateness of current appraisal practices while assessing the readiness of records for automated appraisal. Employing exploratory qualitative research, the study outlines a systematic approach that includes a literature review, testing of a maturity model, and consultations with archival professionals. The MAA encompasses six key dimensions: principles, vision/strategic framework, compliance, methodology, tools, and criteria, providing a structured framework for assessing the maturity of appraisal practices in the AI age. Preliminary results highlight the potential benefits of AI in enhancing appraisal efficiency and effectiveness, paving the way for more informed and defensible archival decisions. Some applied use cases are already started and primarily derived results will be highlighted",Yes,"본 논문은 AI를 활용한 기록 평가 프로세스의 성숙도 평가를 위한 체계적 접근법과 측정 방법을 제시하며, 문헌 검토와 전문가 상담을 통해 새로운 평가 모델을 테스트하는 탐색적 질적 연구를 수행하고 있다. 이는 기존 연구를 종합하는 수준을 넘어 직접적인 연구 방법론 개발과 적용 사례를 포함한 독창적인 연구 기여로 판단된다."
A Systematic Literature Review of the Emerging Technologies used in Securing Healthcare Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10846068,"This study investigates the unification of blockchain technology and artificial intelligence (AI) models to improve the security and privacy of healthcare records. As healthcare systems increasingly depend on digital technologies, the protection of sensitive patient information has become a critical challenge. Blockchain's decentralized, immutable ledger offers robust data integrity and transparency, while AI techniques provide powerful tools for predictive analytics and decision-making. However, combining these two technologies introduces substantial challenges, including scalability issues, privacy concerns, and the requirement for regulatory compliance. This study systematically reviews existing literature to identify key challenges and promise solutions in the amalgamation of blockchain and AI within healthcare. We evaluate the applicability, objectives, techniques, and security measures of selected studies to assess their relevance and contribution to the field. Our findings reveal that while significant progress has been made, gaps remain in areas such as the alignment of these technologies with regulatory frameworks, the development of privacy-preserving AI methods, and the protection of AI models from adversarial attacks. The paper concludes by proposing a set of research questions and future directions to address these challenges, with the aim of advancing the secure and ethical integration of blockchain and AI in healthcare.",No,"본 논문은 기존 문헌을 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 현황과 향후 연구 방향을 제안하는 데 중점을 두고 있다."
Metabolite Profiling of Herbal Medicines using an Artificial Intelligence Approach-A Concise Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911248,"Artificial intelligence (AI) is being developed extremely swiftly in medicine. A structured review should be conducted to comprehend the current status of AI models since they have been introduced in Traditional medicine to Investigate and classify the present applications of AI in TM. Following the technique suggested by the Joanna Briggs Institute, a thorough scoping review was conducted. To find papers on AI and CAM, three databases were searched: PubMed, Embase, and Cochrane Library. Studies published only in English since 2000 are taken into account. Those without mention of traditional medicine or AI methods were also disqualified, as were studies that were not peer-reviewed. All pertinent studies were located using a comprehensive search technique. This review demonstrated the potential use of AI models in TM; nevertheless, to develop AI in TM in the era of digital health, future research should concentrate on validating and verifying the models by carrying out a sizable clinical investigation.",No,"본 논문은 인공지능을 활용한 전통 의학 분야의 연구 동향을 정리한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하여 분석한 문헌 고찰에 해당합니다."
Solid waste classification based on AI: A review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10924493,"Waste management is becoming an intriguing problem all over the world. waste’s volume is becoming more and more huge due to the urbanization and industrialization. This issue impacts all components of the environment, including air, water, and land. This alarming situation needs some solutions which should be rapid, urgent and innovative. For this reason, several solutions have been proposed to reduce the huge amount of waste globally. In fact, recycling may be an efficient tool, especially when the sorting process is done automatically. As this is one of the main goals of the circular economy, many researchers have focused on developing automatic solutions to address environmental problems. Recently, with rapid advancement of Artificial Intelligence (AI), different solutions were developed in order to improve accuracy and save time. Many investigations on AI techniques have been made in order to include machine learning, deep learning and ensemble approaches. This paper reviews AI based approaches for waste classification while emphasizing their real world scope. In addition, it highlights the most popular dataset used in order to validate the efficiency of the developed solutions. The main goal of this paper is to provide an overview of the existing techniques of AI helping researches to built a more accurate solution for a cleaner environment.",No,"본 논문은 AI 기반의 폐기물 분류에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않고, 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다."
Deep Learning Approaches for Similarity Computation: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10584318,"The requirement for appropriate ways to measure the similarity between data objects is a common but vital task in various domains, such as data mining, machine learning and so on. Driven by abundant real-world applications, many well-known similarity (distance) metrics are proposed to measure the pairwise similarity of data pairs, e.g., graph edit distance for graphs and dynamic time warping for time series. However, many similarity metrics suffer from the high time complexity. More specifically, most of the well-known similarity metrics often need quadratic time or even much more time to compute the ground truth similarity and some of them are proven to be NP-hard. With the development of deep learning techniques, there is an emerging research trend on the learning for similarity computation on various data types in the field of database (DB) and data mining, which is quite different with the metric learning studies in the machine learning (ML) literature. Specifically, the studies in the ML focus on the learning for semantic similarity in specific tasks, which is implicitly indicated by the training data, on the data in the feature space. While the studies in the DB literature usually consider the learning for well-defined similarity metrics (e.g., graph edit distance) on the data objects (e.g., graphs), such that it can benefit the similarity computation on data in terms of multiple aspects, such as computation time, metric quality and search heuristic, and the learned representation of data can also be naturally fed to downstream tasks. This survey paper provides a comprehensive review of similarity computation learning on several data types, including set, sequence and graph. Moreover, we first classify the learning-based approaches in terms of their learning target into three categories, i.e., similarity learning, cost matrix learning and search heuristic learning. Then we detail some representative approaches for each category on every data type, and analyze some key features that are utilized by these approaches. Finally, we discuss some challenges and future directions towards the learning for similarity learning on these data types.",No,"이 논문은 다양한 딥러닝 기반 유사도 계산 방법들을 종합적으로 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분류하고 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 체계적 리뷰에 해당합니다."
Optimization of Energy Storage Systems with Renewable Energy Generation and Consumption Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756498,"This work provides a comprehensive systematic review of optimization techniques using artificial intelligence (AI) for energy storage systems within renewable energy setups. The primary goals are to evaluate the latest technologies employed in forecasting models for renewable energy generation, load forecasting, and energy storage systems, alongside their construction parameters and optimization methods. The review highlights the progress achieved, identifies current challenges, and explores future research directions. Despite the extensive application of machine learning (ML) and deep learning (DL) in renewable energy generation, consumption patterns, and storage optimization, few studies integrate these three aspects simultaneously, underscoring the significance of this work. The review encompasses studies from Web of Science, Scopus, and Science Direct up to December 2023, including works scheduled for publication in 2024. Each study related to renewable energy storage was individually analyzed to assess its objectives, methodology, and results. The findings reveal useful insights for developing AI models aimed at optimizing storage systems. However, critical areas need further exploration, such as real-time forecasting, long-term storage predictions, hybrid neural networks for demand-based generation forecasting, and the evaluation of various storage scales and battery technologies. The review also notes a significant gap in research on large-scale storage systems in Brazil and Latin America. In conclusion, the study emphasizes the need for continued research and the development of new algorithms to address existing limitations in the field.",No,"초록에서 본 논문은 기존 연구들을 종합적으로 검토하는 체계적 문헌 리뷰임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않는다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당한다."
School evaluation and artificial intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10372877,"Assessment in education has evolved over time and has established new ways of obtaining information about students’ academic progress. However, the advent of artificial intelligence, such as ChatGPT, has posed challenges in the assessment process, as students can use these technologies to solve questions and tasks without studying. This research focused on recommending alternative educational resources for assessment, considering the pros and cons of ChatGPT and other AI. A systematic literature review was conducted and resources such as written tests, Kahoot!, Quizlet, Mentimeter and Nearpod were identified and evaluated in the tool designed in Microsoft Excel to evaluate their effectiveness. The results showed that the written test and the Plickers tool were the most effective, followed by ClassTools, Flip, Kahoot!, Quizlet, Mentimeter and Nearpod, in addition a list of activities that can be used to assess knowledge through the rubric is shown, among the most important are: oral lessons, exhibitions, open houses, speeches, case studies, debates and observation of participation, because these types of work encourage the speaker to prepare and master the subject. Traditional assessment in the educational field has faced challenges with the advent of artificial intelligence because AI’s ability to generate tasks and answers without students having to read, write or study poses a challenge to ensure the actual acquisition of knowledge and skills. This research recommends educational resources to assess through digital tools or educational activities that allow for authentic assessment of learning and limit reliance on AI.",Yes,"본 논문은 인공지능의 등장에 따른 교육 평가의 문제점을 분석하고, 대체 평가 도구와 활동을 체계적으로 문헌 검토 및 도구 평가를 통해 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Explainable Text Classification Techniques in Legal Document Review: Locating Rationales without Using Human Annotated Training Text Snippets,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020626,"US corporations regularly spend millions of dollars reviewing electronically-stored documents in legal matters. Recently, attorneys apply text classification to efficiently cull massive volumes of data to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs of legal matters, it also faces a perception challenge: amongst lawyers, this technology is sometimes looked upon as a ""black box."" Put simply, no extra information is provided for attorneys to understand why documents are classified as responsive. In recent years, explainable machine learning has emerged as an active research area. In an explainable machine learning system, predictions or decisions made by a machine learning model are human understandable. In legal ‘document review’ scenarios, a document is responsive, because one or more of its small text snippets are deemed responsive. In these scenarios, if these responsive snippets can be located, then attorneys could easily evaluate the model’s document classification decisions – this is especially important in the field of responsible AI. Our prior research identified that predictive models created using annotated training text snippets improved the precision of a model when compared to a model created using all of a set of documents’ text as training. While interesting, manually annotating training text snippets is not generally practical during a legal document review. However, small increases in precision can drastically decrease the cost of large document reviews. Automating the identification of training text snippets without human review could then make the application of training text snippet-based models a practical approach. This paper proposes two simple machine learning methods to locate responsive text snippets within responsive documents without using human annotated training text snippets. The two methods were evaluated and compared with a document classification method using three datasets from actual legal matters. The results show that the two proposed methods outperform the document-level training classification method in identifying responsive text snippets in responsive documents. Additionally, the results suggest that we can automate the successful identification of training text snippets to improve the precision of our predictive models in legal document review and thereby help reduce the overall cost of review.",Yes,"본 논문은 인간이 주석을 단 텍스트 스니펫 없이도 법률 문서 내에서 반응성 텍스트 스니펫을 자동으로 찾아내는 두 가지 기계 학습 방법을 제안하고, 실제 데이터셋을 통해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 확장하고 새로운 방법론을 제시하는 연구 논문에 해당한다."
AI Orthopraxy: Towards a Framework for That Promotes Fairness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462167,"This paper introduces the term AI Orthopraxy as the correct practice of AI and a framework that aims to unify some aspects associated with AI ethics. These include standards, legal, and measures of fairness. We draw from existing tools that have been peer-reviewed by academics and discussed in recent literature to provide a mechanism for assessing the level by which a model or AI technology follows the correct practices of ethical AI. This paper describes a preliminary, ongoing, study and shows the early stages of a prototype framework, including a visual representation of the level of AI Orthopraxy of a model using hive plots. This work can potentially help create fair and trustworthy AI built upon the core tenets of accountability, transparency, and fairness. One of the current limitations is that it requires validation of peers that are willing, able, and trained to evaluate an AI model or technology using standards and other novel frameworks.",No,"본 논문은 AI 윤리와 공정성에 관한 개념적 프레임워크를 제안하고 초기 프로토타입을 소개하는 예비 연구로 보입니다. 직접적인 실험 결과나 독창적인 연구 데이터보다는 기존 도구와 문헌을 종합하여 개념을 정립하는 데 중점을 두고 있어, 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
A Physiologically Informed Three-Dimensional Convolutional Neural Network to Aid Diagnosis of Non-Small Cell Lung Cancer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389384,"Lung cancer can be divided into two main categories, i.e., either small cell lung cancer (SCLC) or Non-Small Cell Lung Cancer (NSCLC), which accounts for 85% of lung cancers. NSCLC is a disease wherein malignant cancer cells form in the lung tissue. The five-year survival rate decreases as the NSCLC cancer becomes more advanced, from 40% for stage I to only 1% for stage IV; thus, a vital challenge to overcome is the early detection of NSCLC to achieve a higher chance of survival. As sonographers and radiologists are faced with the problem of being overworked with many time-consuming duties, it is important to consider new systems that could help in improving their effectiveness and efficiency while maintaining a clinically acceptable level of accuracy and reliability. Artificial Intelligence (AI) paves the way to allow computerized algorithms to perform tasks that would normally require only expert human involvement. In this report, a new AI-driven system is proposed to aid radiologists in the diagnosis of NSCLC, thus improving its specificity by design, while minimizing false positives as much as possible. It is worth noting that many clinical sub-fields have already started using AI to aid decision making, such as in breast cancer screening. Computed Tomography (CT) scans contain crucial volumetric, morphological lung tissue-related information that is only partly used to inform a diagnosis of NSCLC. In CT scans, each voxel can be converted into its corresponding Hounsfield Unit (HU) values, which are used to estimate tissue density; via this complementary information, areas of abnormal density or high attenuation can be identified, thus leading an AI-driven model to achieve a higher reliability in aiding diagnosis of NSCLC. Therefore, a three-dimensional AI-driven system may lead to an earlier detection of NSCLC, by complementing radiologists' expertise, experience, and decision making. This study leverages a thresholding-based segmentation algorithm to identify the region of interest defining the lungs by analysing each voxel's HU. Subsequently, an AI-driven model is proposed to analyse all CT scans of a patient to identify NSCLC earlier. This model is based on a three-dimensional convolutional neural network, trained on volumetric information of 90 images and validated on 40 images. The CT scan dataset has metadata for all patients and allows to compare the classification models predictions to the known or ground-truth diagnostic labels of the patients. The model led to 100% classification accuracy and an area under the precision-recall curve of 1 (100%) over 130 subjects (70 patients with NSCLC, 60 healthy controls). Further work is underway to train, validate, and test the model on a larger dataset and assess its performance more comprehensively, and such minimum sample sizes will be determined statistically based on a power analysis, so that the results will be confidently comparable against the clinical diagnostic gold-standard labels.",Yes,"본 논문은 3차원 합성곱 신경망을 활용하여 비소세포폐암(NSCLC) 진단을 돕는 AI 모델을 제안하고, 이를 CT 영상 데이터에 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
"NG-ICPS: Next Generation Industrial-CPS, Security Threats in the Era of Artificial Intelligence, and Open Challenges With Future Research Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10735364,"The complexity of next-generation industrial cyber-physical systems (NG-ICPSs) is increasing due to the integration of machine-embedded sensors, cyber-infrastructure, and physical processes, which calls for the new intelligent operation mechanisms to achieve system-level objectives. Although NG-ICPS has proliferated in many applications, such as advanced manufacturing, intelligent transportation, smart homes, etc., and achieved remarkable results. But these applications are susceptible to many problems and new security threats are some of them that goes beyond the scope of traditional communication and network security, due to the tight integration of cybers and physical systems. For redressal of this, several traditional authentication and data privacy schemes have been used in the recent past, but somehow, they did not satisfy the need for this emerging technology, due to their complex verification and validation processes. Recently, artificial intelligence (AI), machine learning (ML), and deep learning (DL) enabled authentication and data preservation techniques had shown remarkable results to address the security problems of this technology at the system/client side and server side cost-effectively. Given that, in this article, we present a comprehensive survey of the current literature on NC-ICPS technology security threats and their countermeasures, with a focus on AI, ML, and DL-enabled techniques. We evaluate these techniques by identifying their advantages and disadvantages compared to traditional authentication and data preservation methods. In addition, we discussed the review articles published on this topic to acknowledge their contributions and limitations, because most of them cover a specific part of security concerns of this technology, and unable to present the true picture of all problems under one shallow. Building on this, we addressed the gaps in the literature by highlighting the open security challenges of NG-ICPS technology and suggesting potential future research directions, considering the capabilities of AI, ML, and DL-enabled algorithms. Finally, we compared this article sectionwise with rival review articles to claim its novelty followed by the question of reviewers, editors, students, and readers why this article is needed in the presence of these articles and what are its distinctive factor that makes this article different from them.",No,이 논문은 NG-ICPS 기술의 보안 위협과 AI 기반 대응 기법에 대한 포괄적인 문헌 조사 및 기존 연구들의 한계와 향후 연구 방향을 제시하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구들의 종합과 분석에 중점을 두고 있다.
An LLM-Driven Chatbot in Higher Education for Databases and Information Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10706931,"Contribution: This research explores the benefits and challenges of developing, deploying, and evaluating a large language model (LLM) chatbot, MoodleBot, in computer science classroom settings. It highlights the potential of integrating LLMs into LMSs like Moodle to support self-regulated learning (SRL) and help-seeking behavior. Background: Computer science educators face immense challenges incorporating novel tools into LMSs to create a supportive and engaging learning environment. MoodleBot addresses this challenge by offering an interactive platform for both students and teachers. Research Questions: Despite issues like bias, hallucinations, and teachers’ and educators’ resistance to embracing new (AI) technologies, this research investigates two questions: (RQ1) To what extent do students accept MoodleBot as a valuable tool for learning support? (RQ2) How accurately does MoodleBot churn out responses, and how congruent are these with the established course content? Methodology: This study reviews pedagogical literature on AI-driven chatbots and adopts the retrieval-augmented generation (RAG) approach for MoodleBot’s design and data processing. The technology acceptance model (TAM) evaluates user acceptance through constructs like perceived usefulness (PU) and Ease of Use. Forty-six students participated, with 30 completing the TAM questionnaire. Findings: LLM-based chatbots like MoodleBot can significantly improve the teaching and learning process. This study revealed a high accuracy rate (88%) in providing course-related assistance. Positive responses from students attest to the efficacy and applicability of AI-driven educational tools. These findings indicate that educational chatbots are suitable for integration into courses to improve personalized learning and reduce teacher administrative burden, although improvements in automated fact-checking are needed.",Yes,"본 논문은 LLM 기반 챗봇 MoodleBot의 설계, 개발, 배포 및 평가를 포함한 독창적인 연구를 수행하였으며, 사용자 수용도와 정확도 평가를 통해 실험적 데이터를 제시하고 있다. 이는 기존 연구를 단순히 요약한 것이 아니라 직접적인 연구 기여를 포함한 연구 논문임을 보여준다."
"A Systematic Review of AI-Driven and Quantum-Resistant Security Solutions for Cyber-Physical Systems: Blockchain, Federated Learning, and Emerging Technologies",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10927942,"Cyber-Physical Systems (CPS) form the foundation of critical infrastructures, integrating computing, networking, and physical processes across sectors such as energy, healthcare, and manufacturing. As CPS become increasingly interconnected, they face sophisticated cyber threats that challenge traditional security measures. This systematic review explores next-generation cybersecurity solutions designed to protect CPS, focusing on emerging technologies like Artificial Intelligence (AI), blockchain, quantum-resistant cryptography, and federated learning (FL). We conducted a comprehensive literature search across IEEE Xplore, Scopus, and Web of Science, to identify studies published between 2010 and 2024. The review covers studies that examine the role of AI in anomaly detection and automated threat responses, blockchain for decentralized communication and identity management, and quantum-resistant cryptographic techniques as a safeguard against future quantum-enabled attacks. Federated learning, which enables collaborative learning across CPS devices while preserving data privacy, is also evaluated for its distributed security applications. The findings indicate that while AI enhances real-time security, it remains vulnerable to adversarial attacks. Blockchain offers robust security but faces scalability challenges, and federated learning requires stronger safeguards against model poisoning.",No,"주어진 논문은 체계적 문헌 고찰(Systematic Review)로, 기존 연구들을 종합하여 분석한 내용에 초점을 맞추고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험, 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Should Robots Replace Teachers? Mobilisation of AI and Learning Analytics in Education,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9697300,"The purpose of this study was to determine how artificial intelligence (AI) impacts education. The study's focus was restricted to the use and impact of artificial intelligence in administration, education, and learning, and was founded on a narrative and framework created via exploratory research for assessing AI. The use of literature review as a research strategy and methodology helped in accomplishment of study's goal, which was achieved making use of a qualitative research technique. AI is an area of research in which computers, robots, and other technologies are programmed to exhibit human-like intelligence, as characterised by cognitive skills such as learning and adaptation, as well as decision-making capabilities. According to the findings, AI has been widely embraced and used in a number of ways by organisations, especially educational institutions, in a range of circumstances and settings. AI began with computer technology, progressed to intelligent online and web-based education, and finally, embedded computer systems were used to perform instructional tasks autonomously or collaboratively with teachers, in conjunction with other technologies such as humanoid robots and web-based chatbots. These platforms have allowed instructors to improve the overall quality of their instructional work while also improving their efficiency and speed for administrative duties such as evaluating students' work. However, since the systems are based on machine learning and adaptability, curriculum and materials can be customised to meet the unique requirements of individual students, resulting in improved absorption and retention and a more pleasant overall learning experience.",No,"본 논문은 AI가 교육에 미치는 영향을 문헌 리뷰와 탐색적 연구를 통해 서술하는 개관적 연구로, 직접적인 실험이나 새로운 데이터 수집, 독창적인 연구 결과를 제시하지 않는다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
College Enquiry Chatbot,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10156978,"Today's widespread use of smartphones is proof that technology is always evolving. Nowadays, artificial intelligence is crucial to numerous industries, including manufacturing, human resources, and customer service. There are numerous chatbots that help people discover solutions to their questions. As a result, we are developing an AI-powered chatbot that can address all questions about colleges. It serves as an intelligence tool with an emphasis on higher education. This artificially intelligent machine will respond to queries from users regarding matters related to higher education. The information is kept in the chatbot's database so that it can be used to spot trends and decide how to respond to questions. The chatbot for college inquiries was developed using an NLP system that evaluates questions and comprehends messages. People react to each other differently based on emotions and attitude. Because chatbots must go by rules just like humans do, they will communicate with customers in a courteous and correct manner. Students can ask the chatbot any questions at any time of the day or night, and they will receive a prompt and accurate response. A chatbot can respond to thousands of users simultaneously. Chatbot can work 24 ×7 without getting tired. It has minimal errors, which increases productivity.",No,"초록 내용은 AI 챗봇 개발에 대한 개요와 기능 설명에 집중되어 있으며, 구체적인 연구 방법론, 실험 결과, 또는 독창적인 연구 기여에 대한 언급이 부족합니다. 따라서 본 논문은 연구 논문보다는 개발 보고서나 기술 소개에 더 가깝다고 판단됩니다."
A review on Student Performance Prediction using Educational Data mining and Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9768773,"Predicting student’s performance helps all stakeholders of education system to plan and take appropriate measures. Increase in the number of higher educational institutes and adoption to online and blended learning has enabled collection of large amounts of data. Data Mining and Artificial Intelligence tools can be successfully used on this data to predict student performance to provide required insights to the stakeholders. This paper focusses a systematic literature review on use of data mining and AI tools for Student’s Performance Prediction. Using Critical review of available literature authors have proposed a combinatorial model for student performance prediction using some techniques like Decision Tree, Random Forest, Genetic Algorithm, Artificial Neural Networks, etc.",No,"본 논문은 학생 성과 예측에 관한 기존 연구들을 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 문헌의 종합과 분석에 초점을 맞추고 있다."
The Future of Cyber Defence: ML and DL Enhanced Firewalls for Intrusion Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10838028,"Given the increasing frequency of network-based threats, there is a critical need for robust intrusion detection systems (IDS) to safeguard digital assets. Traditional rule-based IDS often struggle to counter emerging cyber threats, highlighting the demand for more adaptive solutions. This study introduces the Next Generation Firewall (NGFW), an advanced firewall system enhanced by machine learning (ML) and deep learning (DL) techniques. By integrating ML algorithms like Support Vector Machines (SVM), Random Forests, and Deep Neural Networks (DNN) with DL models such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), the NGFW significantly improves detection flexibility, effectiveness, and accuracy. Our comprehensive literature review demonstrates the substantial impact of ML and DL on IDS performance. The NGFW's approach includes systematic data collection, preprocessing, feature extraction, and rigorous model evaluation using metrics like accuracy, precision, and recall. Experimental results show that CNNs excel in detecting malicious activities with minimal false positive rates. Real-world testing further validates the NGFW's capacity to strengthen cybersecurity measures. The classification report reveals impressive performance, with precision, recall, and F1-scores exceeding 97% in each category and an overall accuracy of 97.29%. These results underscore the potential of ML and DL to advance intelligent cybersecurity solutions and enhance the reliability of intrusion detection systems.",Yes,"논문은 ML과 DL 기법을 활용한 차세대 방화벽 시스템을 제안하고, 실험을 통해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 또한, 구체적인 데이터 처리와 모델 평가 과정을 기술하며 실험 결과를 제시하고 있어 연구 논문에 해당한다."
A Systematic Review on Deep Learning Models for Sleep Stage Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776965,"Sleep is a key aspect of the body's recuperation, memory integration and consolidation processes, as well as a vital part of overall health. Early discovery of sleep related disorders can help to prevent subsequent sleep problems and measuring sleep stages can be as useful tool for sleep research. Sleep analysis is based on expert led imagery and is sensitive to inter and intra-observer variability. Supporting specialists with such a Programmable Diagnostic Tool (PDT) equipped with different machine learning algorithms are useful for diagnosis of sleep problems. But used need to be educated with the technology and it will be difficult in practice. This research paper presents a systematic review of 20 studies that used various deep learning models to classify sleep stages using polysomnography datasets. According to this paper, the majority of the research is based on convolutional neural network (CNN) over electroencephalography (EEG) datasets to classify sleep stages and obtained great performance. The accuracy of feature extraction and the classification algorithms are evaluated. This study also shows that deep learning model namely CNN is effective in terms of classification accuracy. For deep learning techniques to be completely implemented as a workable PDT for scoring sleep stages in medical applications, other Polysomnography data must be included in addition to EEG datasets. This review paper covers methodologies from the recent decade, focusing the use of deep learning models in combination with some other polysomnography data like EMG, EOG, ECG and EEG for scoring of sleep-stages.",No,"본 논문은 20개의 연구를 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구들을 종합하여 분석하고 평가하는 데 중점을 두고 있습니다. 따라서 연구 논문보다는 체계적 문헌고찰에 해당합니다."
Heart Failure Prediction System Using Machine Learning Techniques: A Comprehensive Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581269,"This review paper employs machine learning methodologies to conduct a comprehensive investigation into heart failure prediction systems. The study scrutinizes various databases, machine learning algorithms, and pre-processing techniques applied in the realm of heart failure prediction. It encompasses an array of preprocessing methods, feature extraction techniques, and machine learning and deep learning approaches, ranging from logistic regression, decision trees, random forests, K-NN, SVM, isolation forests, CNN, to RNN. Additionally, exploratory data analysis techniques are incorporated into the literature review. The challenges associated with feature engineering, class disparity, and the application of machine learning technologies are thoroughly addressed. Sources such as the UCI Machine Learning Repository, GitHub, Kaggle, and other reputable publications and datasets are utilized for presenting and analysing results. The datasets under scrutiny cover diverse facets of heart failure factors, including different causes, reasons across various age groups, genetic influences, preventive measures, and more. The study concludes that machine learning algorithms provide a viable avenue for identifying causes and predicting heart failure. It asserts that machine learning models evolve and enhance their proficiency over time, adapting to emerging patterns in heart failure data.",No,"본 논문은 다양한 머신러닝 기법과 데이터셋을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구들을 분석하고 요약하는 데 중점을 두고 있다. 따라서 연구 논문보다는 문헌 리뷰에 해당한다."
Exploring Innovative Approaches for Software Development Risk Assessment and Management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10730152,"The success of software development projects is significantly impacted by various risk factors, both predictable and unpredictable. This literature review aims to explore and evaluate innovative risk assessment and management techniques in software development. Using the Systematic Literature Review (SLR) method, this study reviews existing research on new software development risk assessment and management methods. This extensive review highlights various approaches, tools, and strategies for identifying, evaluating, and mitigating risks in software projects. The results underscore the importance of creative solutions in tackling risk management challenges. Practitioners are encouraged to adopt innovative methods for risk assessment and management as the field of software development evolves. Furthermore, this study recommends that researchers continue to investigate emerging trends and challenges in risk management. Emphasis is placed on developing hybrid models that combine traditional and innovative approaches, examining risk management practices in new technologies such as AI and machine learning, and addressing cultural and organisational barriers to adopting new techniques. Collaboration and knowledge sharing between practitioners and researchers are expected to improve risk management practices in software development significantly.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 통해 기존 연구들을 종합하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 새로운 연구 내용을 제시하는 연구 논문으로 보기 어렵습니다."
The Effect of Artificial Intelligence on Cybersecurity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111151,"Cyber-attacks present a greater threat to individuals and the security of the modern state in the globalized world. Artificial intelligence offers several tools that can be used to improve cybersecurity. Throughout this article, we can observe many benefits and disadvantages of artificial intelligence in cybersecurity at national and international levels. Artificial intelligence makes new opportunities for value creation in businesses, sectors, societies, and the public possible. Research has shown that technology is important in many facets of life. This reason has contributed to its incorporation in many enterprises and sectors. The uses of AI are too many to list. The following study looks at how artificial intelligence (AI) is used in cybersecurity. A developing idea in the technological sector is cybersecurity. Information technology has been incorporated into many enterprises. Public and private companies and organizations have had to demand stronger security measures because of this problem. Cyberspace has expanded in response to the need to secure not only personal data but official info as well as technology that is presently accessible. AI was already observed to impact cybersecurity on a broad scale significantly. Because of this, machine learning has been strongly incorporated into modern cybersecurity-related technology. The research study does a literature review and investigates the general effects of AI on cybersecurity.In our results, we have obtained positive findings in attacks with artificial intelligence and have found certain results when obtaining information on attacks using AI. In conclusion, it is necessary to advance in artificial intelligence since the increasing volume and complexity of attacks at the international level require more resources to face them. Cybercriminals will also use artificial intelligence to attack individuals, state infrastructure, and systems.",No,"초록에서 본 논문은 인공지능과 사이버보안의 관계에 대한 문헌 리뷰와 일반적인 영향 조사에 초점을 맞추고 있으며, 독창적인 실험 결과나 새로운 연구 방법론을 제시하지 않습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
"An Overview on Deep Leaning Application in Coronavirus (COVID-19):Diagnosis, Prediction and Effects",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591921,"Recently, the coronavirus 2019 or COVID-19, which originated in China, has spread to other countries' population. It is critical to evaluate an automated detection system for rapid alternative prediction and diagnosis in order to reduce the impact of COVID-19. Because of the constant increase in cases, there are fewer COVID-19 available kits than are required for testing in hospitals. Deep learning methods are evolving to provide outstanding performance in the medical field. Deep learning inspired by brain structure is referred to as machine learning. This paper provides an overview of COVID-19’s detection applications based on deep learning. Furthermore, a comprehensive review of the literature on deep leaning in COVID-19 disease has been illustrated. The proposed research study shows that in spite of presence of issues in medical database, where the transfer method can be used effectively.",No,"초록에서 이 논문은 COVID-19 진단 및 예측을 위한 딥러닝 적용에 대한 개요와 문헌 리뷰를 제공한다고 명시하고 있어, 직접적인 독창적 연구 결과보다는 기존 연구들의 종합적 정리에 초점이 맞춰져 있습니다. 따라서 새로운 실험이나 연구 기여가 포함된 연구 논문으로 보기 어렵습니다."
Deep Learning Based Object Oriented Software Estimation: A Case Of Reusability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525948,"The software development stages supported by deep learning techniques in view of the collection of software engineering promises, it is essential to see the current software engineering requirements helped through deep learning, by seeing it various supports experts find the feasible to use deep learning information to deliver commercial software by considering functional and nonfunctional requirements. A Reusability assessment model for object oriented design particularly for design phase of development life cycle has been developed in this research paper with the help of deep learning technique. The developed system relates the Reusability orientated diagram qualities with software maintenance requirements and also connects feasibility with Reusability. No such model has been currently available in the literature review that measures Reusability of OO diagram through bringing practicality into consideration utilizing deep learning. The proposed model overcomes any barrier among object orientated outline qualities, practicality and Reusability. Proposed efforts works in order to improve the overall software design quality by considering the quality key attributes especially reusability of software design at initial stage of development life cycle. Finally in view of the above truth proposed model has been developed and validated with the help of class diagrams.",Yes,"본 논문은 딥러닝 기법을 활용하여 객체지향 설계 단계에서 재사용성 평가 모델을 개발하고 이를 검증한 연구로, 독창적인 연구 내용과 직접적인 기여가 포함되어 있다. 따라서 연구 논문에 해당한다고 판단된다."
Deep Reinforcement Learning for AoI Minimization in UAV-Aided Data Collection for WSN and IoT Applications: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589619,"Deep reinforcement learning (DRL) has emerged as a promising technique for optimizing the deployment of unmanned aerial vehicles (UAVs) for data collection in wireless sensor networks (WSNs) and Internet of Things (IoT) applications. With DRL, UAV trajectory can be optimized, optimal data collection points can be determined, sensor node transmissions can be scheduled efficiently, and irregular traffic patterns can be learned effectively. In view of the significance of DRL for UAV-assisted IoT research in general and, more specifically, its use for time-critical applications, this paper presents a review of the existing literature on UAV-aided data collection for WSN and IoT applications related to the application of DRL to minimize the Age of Information (AoI), a recent metric used to measure the degree of freshness of transmitted information collected in data-gathering applications. This review aims to provide insights into the state-of-the-art techniques, challenges, and opportunities in this domain through an extensive analysis of a sizable range of related research papers in this domain. It discusses application areas of UAV-assisted IoT, such as environmental monitoring, infrastructure inspection, and disaster response. Then, the paper focuses on the proposed works, their optimization objectives, architectures, simulation libraries and complexities of the various DRL-based approaches used. Thereafter discussion, challenges, and some opportunities for future work are provided. The findings of this review serve as a valuable resource for researchers and practitioners, guiding further advancements and innovations in the field of DRL for UAV-aided data collection in WSN and IoT applications.",No,"본 논문은 DRL을 활용한 UAV 기반 데이터 수집 분야의 기존 연구들을 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 문헌 분석과 정리, 향후 연구 방향 제시에 중점을 두고 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Assessing AI Readiness Across Organizations: The Case of UAE,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225386,"several recent business case studies showed the benefits of adopting Artificial Intelligence (AI) technologies in business, which encouraged many organizations to think seriously in adopting AI to re-innovate existing business processes or entire business models. Though, the challenge of having a clear framework to guide companies in such an AI transformation process is missing. This paper aims to investigate conceptual frameworks sued to assess AI readiness in organizations using popular existing theories and frameworks from the Information Systems (IS) discipline such as Technology-Organizations-Environment (TOE) framework, Diffusion of Innovation theory (DOI), and socio-technical Design. Followed by a review of the current theories in depth and propose an extended framework that will consider the socio-technical elements based on the local context of the UAE. A mixed methods research approach is proposed to test and validate the framework and answer the research questions. The expected results from applying the proposed framework will help academics and practitioners to identify the critical factors needed to have a successful implementation of AI strategies in the UAE.",Yes,"본 논문은 기존 이론과 프레임워크를 검토하고 이를 확장한 새로운 개념적 프레임워크를 제안하며, 이를 검증하기 위한 혼합 방법 연구 접근법을 제시하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문에 해당한다."
"Development Status, Frontier Hotspots, and Technical Evaluations in the Field of AI Music Composition Since the 21st Century: A Systematic Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10570408,"In recent years, “Artificial Intelligence (AI)” has become a focal point of discussion. AI music composition, an interdisciplinary field blending computer science and musicology, has emerged as a prominent area of research. Despite rapid advancements in AI music creation technology, there remains a dearth of comprehensive surveys addressing the core technologies within this domain. To address this gap, this study conducted a comprehensive search across multiple databases spanning a 23-year period (2000–2023) on the topic of “AI music composition.” Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) standard for literature screening, the study systematically organized the development status, frontier hotspots, and technical evaluations of the field. Drawing from literature data, the study verified Price’s Law, Lotka’s Law, and Bradford’s Law—three scientific productivity laws—while summarizing the current landscape from four perspectives: authors, organizations, countries, and journals. Subsequently, utilizing VOSviewer and CiteSpace, two technical software tools, the study conducted an in-depth analysis consisting of four steps: clustering, time zone, burst words, and high-frequency referenced literature. The study presented the evolution trajectory of frontiers and hotspots through visualization maps. Finally, building upon quantitative statistical insights, the study qualitatively expanded research efforts by organizing and evaluating the latest AI music generation algorithm technologies. The systematic literature analysis, both quantitative and qualitative, aims to furnish researchers and practitioners in related fields with systematic references.",No,"본 논문은 AI 음악 작곡 분야의 기존 연구들을 체계적으로 검토하고 분석한 체계적 문헌 리뷰(Systematic Review)로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합하여 정리한 리뷰 논문에 해당합니다."
Deep Learning for Automated Face Mask Detection: Enhancing Public Health Measures,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10672697,"The COVID-19 pandemic has emphasized the critical importance of face masks in mitigating the transmission of respiratory infections. In this research paper, we explore the role of deep learning in automated face mask detection systems, aiming to enhance public health measures and safety protocols. Through a comprehensive literature survey, we review advancements in AI technology, particularly in the context of detecting mask-wearing individuals. Our methodology involves dataset preparation and training of convolutional neural network (CNN) architectures for binary classification tasks. Results demonstrate the effectiveness of deep learning models in accurately identifying individuals wearing face masks, with deeper architectures showcasing superior performance in terms of loss reduction and accuracy. We conclude that automated face mask detection systems offer invaluable support in enforcing mask-wearing regulations, especially in densely crowded regions and large gatherings. Furthermore, these systems provide valuable data for informing public health policies and resource allocation. This study highlights the transformative potential of deep learning in addressing current and future public health crises, emphasizing the centrality of technology in safeguarding community health and well-being.",Yes,"본 논문은 딥러닝 기반의 얼굴 마스크 자동 감지 시스템을 위한 데이터셋 준비와 CNN 모델 학습 등 구체적인 연구 방법론과 실험 결과를 포함하고 있어, 독창적인 연구 내용을 직접적으로 다루고 있다. 따라서 연구 논문에 해당한다."
Software Product Quality Metrics: A Systematic Mapping Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336003,"In the current competitive world, producing quality products has become a prominent factor to succeed in business. In this respect, defining and following the software product quality metrics (SPQM) to detect the current quality situation and continuous improvement of systems have gained tremendous importance. Therefore, it is necessary to review the present studies in this area to allow for the analysis of the situation at hand, as well as to enable us to make predictions regarding the future research areas. The present research aims to analyze the active research areas and trends on this topic appearing in the literature during the last decade. A Systematic Mapping (SM) study was carried out on 70 articles and conference papers published between 2009 and 2019 on SPQM as indicated in their titles and abstract. The result is presented through graphics, explanations, and the mind mapping method. The outputs include the trend map between the years 2009 and 2019, knowledge about this area and measurement tools, issues determined to be open to development in this area, and conformity between conference papers, articles and internationally valid quality models. This study may serve as a foundation for future studies that aim to contribute to the development in this crucial field. Future SM studies might focus on this subject for measuring the quality of network performance and new technologies such as Artificial Intelligence (AI), Internet of things (IoT), Cloud of Things (CoT), Machine Learning, and Robotics.",No,"본 논문은 소프트웨어 제품 품질 지표에 관한 기존 연구들을 체계적으로 정리하고 분석하는 체계적 맵핑 연구(Systematic Mapping Study)로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 현황을 종합하는 리뷰 성격이 강합니다."
Big Data Governance Challenges Arising From Data Generated by Intelligent Systems Technologies: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10839423,"The exponential growth of intelligent systems technologies, including Artificial Intelligence (AI), Internet of Things (IoT), Machine Learning (ML), and Smart Connected Products, has intensified the difficulties of data governance. Organizations adopting these technologies face challenges in managing the volume, variety, and velocity of data while ensuring quality, security, compliance, and ethical integrity. This study investigates the state of big data governance (BDG) programs in organizations leveraging intelligent systems technologies, using a systematic literature review guided by PRISMA. A synthesis of insights from 74 peer-reviewed articles, conferences, and industry reports reveals both the significant benefits of BDG such as enhanced decision-making, operational efficiency, and regulatory compliance and persistent challenges, including fragmented governance frameworks, limited scalability, data quality issues, and ethical concerns. To address these gaps, this study proposes the big data governance maturity assessment model (BDG MAM), a novel framework developed to assess and enhance the maturity of BDG programs. The BDG MAM evaluates governance maturity across four key dimensions: people, process, data, and technology. It provides a structured roadmap for organizations to benchmark governance practices, prioritize improvements, and implement effective strategies. The model was validated through a pilot study conducted with a public higher learning institution, demonstrating its practical applicability and effectiveness in real-world scenarios. By bridging theoretical insights with practical implementation, this study advances academic discourse and provides practitioners with a robust approach to navigate the challenges of modern data governance practices, ensuring sustainable and effective management of heterogeneous data environment.",Yes,"본 논문은 체계적 문헌고찰을 통해 기존 연구를 종합하고, 빅데이터 거버넌스 성숙도 평가 모델(BDG MAM)이라는 새로운 프레임워크를 제안 및 실제 기관에서 파일럿 검증을 수행하여 독창적인 연구 기여를 포함하고 있다. 따라서 단순 리뷰가 아닌 직접적인 연구 결과와 모델 개발이 포함된 연구 논문으로 판단된다."
Bridging Decentralized AI and Blockchain: Challenges and Solutions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10859075,"The integration of Deep Learning and Blockchain presents an opportunity to decentralize artificial intelligence, reducing the risks associated with centralized AI models. This paper proposes a novel approach using a consensus protocol that leverages Deep Learning as Proof of Useful Work. The proposed protocol aims to address key challenges related to scalability, security, and efficiency in decentralized AI systems. To validate the protocol, a series of simulations were conducted to analyze reward distribution in networks with both honest and Byzantine nodes. Additionally, an alternative solution using smart contracts was explored, highlighting potential gas cost issues on Layer-1 networks. A Layer-2 implementation is suggested as a scalable alternative to minimize gas costs. The findings demonstrate that a decentralized AI system is achievable with carefully designed protocols and reward mechanisms, though further research is needed to address outstanding challenges.",Yes,"논문은 새로운 합의 프로토콜을 제안하고, 이를 검증하기 위한 시뮬레이션을 수행하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 스마트 계약과 레이어-2 구현 방안도 탐구하여 실질적인 문제 해결에 기여하고 있어 연구 논문에 해당한다."
Classification and Detection of Breast Cancer Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10532879,"The Cancer is the second-leading cause of death for women aged 20–59 worldwide and very few men. Compared to other cancers, breast cancer kills more people. According to Cancer.org, 13% of American women are at risk of having this cancer, and approximately 80% of them advance, which decreases recovery and treatment success. Deep learning methods for BC detection have been successful thanks to AI. This improves early diagnosis, increasing patient survival. This publication provides a detailed review of the deep learning-based BC diagnosis literature. It aims to help practitioners and researchers understand this domain's issues and trends. This article reviews looks at deep learning methods for breast cancer detection. Next, we examine and synthesize the latest AI-based breast cancer diagnostic studies using multiple breast DL modalities. We also provide a complete overview of breast-cancer imaging datasets, emphasizing their importance in AI-driven algorithms and deep learning model training. The investigation found that the CNN is the most widely used and accurate BC diagnosis model. Also, accuracy measures are the key way to evaluate such models. To provide a complete reference for breast cancer imaging researchers from the details of the researchers works. we can say that The performance of breast cancer detection is influenced by three factors: (1) the efficacy of the CAD system, (2) the characteristics of the population under analysis, and (3) the proficiency of the radiologists utilizing the system. CAD can assist in detecting microcalcifications, which may serve as potential indicators of",No,"초록에서 이 논문은 기존 연구들을 종합하고 분석하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않고 있다. 따라서 본 논문은 연구 논문이 아닌 문헌 리뷰에 해당한다."
Acute Lymboplastic Leukemia Detection Challenges and Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10060916,"-Leukemia is considered as the type of cancer that occurs on the blood cells. It leads to enlargement of the cells and become immature cell, and then it affects the other cells also. In recent times, the testing laboratories have consumed more time to detect as well as to diagnosis the Acute Lymboplastic Leukemia Detection (ALL), which is tedious to detect by humans. Moreover, the manual models and the existing models are also costly and time-consuming too. To overcome from these issues, various Computer Aided Diagnosis (CAD) techniques for analyzing of blood smear microscopic images have been designed by utilizing deep learning and machine learning techniques. Thus, this paper intends to give the literature review for detecting the acute lymboplastic Leukemia with the help of various development approaches. It is also illustrating the wide range of existing studies regarding acute lymboplastic Leukemia detection using machine or deep learning, performance analysis, and its challenges. Since the former methodologies exist with some downside factors, the review paper is helpful for future research work Consequently, the literature review is followed by fundamental prerequisite of acute lymboplastic Leukaemia detection, techniques for forecasting the disease, distinct data sources utilization, analyzing with diverse validating metrics, implementation tools, and also exhibits the information in terms of challenges and motivation towards the future direction.",No,"초록에서 해당 논문은 기존 연구들을 종합하여 정리한 문헌 리뷰(review)임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 본 논문은 연구 논문이 아닌 체계적 문헌 고찰에 해당한다."
Deep Cardiovascular Clinical Decision Support and Control System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346453,"Since the past decade, Internet of Health Things (IoHT) and artificial intelligence (AI) have continued to revolutionize the healthcare sector by enabling remote patient monitoring and the development of diagnostic tools. The availability and vast amount of data and its variety gathered from patient, encouraged researchers and increased their interest in developing healthcare solutions. In this research work, we present literature studies related to cardiovascular diseases and how machine learning can be used in clinical applications for prediction, diagnosis, and prognosis to help specialists to choose the appropriate treatment decision for a patient's specific clinical condition. In addition a new architecture is proposed for monitoring and assisting in cardiac diseases diagnosis. Using ADS1298 chip, the board is able to measure the ECG signal, analyse it, and determine whether a person have heart disease (cardiomyopathy) or not. In case of heart failure, our prototype triggers an alarm by sending a message in order to report the patient's condition to doctor and to clinical center. The measure healthcare parameters are collected from different e-health sensors platform connected to raspberry Pi card. We executed our model on 90,000 instances from PhysioNet MIT-BIH Arrhythmia Diagnostic ECG database. The dataset is annotated in five different categories (normal, atrial premature, premature ventricular contraction, fusion of ventricular and normal beat, fusion beat of paced and normal) in accordance with the EC57 standard. Our proposed CNN model gives good result in terms of detection and accuracy. We achieved an average accuracy of 98.64%.",Yes,"본 논문은 심혈관 질환 진단을 위한 새로운 시스템 아키텍처를 제안하고, 실제 ECG 데이터셋을 이용해 CNN 모델을 개발 및 평가하여 98.64%의 정확도를 달성한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Review and Empirical Analysis of Machine Learning-Based Software Effort Estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538131,"The average software company spends a huge amount of its revenue on Research and Development (R&D) for how to deliver software on time. Accurate software effort estimation is critical for successful project planning, resource allocation, and on-time delivery within budget for sustainable software development. However, both overestimation and underestimation can pose significant challenges, highlighting the need for continuous improvement in estimation techniques. This study reviews recent machine learning approaches employed to enhance the accuracy of software effort estimation (SEE), focusing on research published between 2020 and 2023. The literature review employed a systematic approach to identify relevant research on machine learning techniques for SEE. Additionally, comparative experiments were conducted using five commonly employed Machine Learning (ML) methods: K-Nearest Neighbor, Support Vector Machine, Random Forest, Logistic Regression, and LASSO Regression. The performance of these techniques was evaluated using five widely adopted accuracy metrics: Mean Squared Error (MSE), Mean Magnitude of Relative Error (MMRE), R-squared, Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE). The evaluation was carried out on seven benchmark datasets: Albrecht, Desharnais, China, Kemerer, Mayazaki94, Maxwell, and COCOMO, which are publicly available and extensively used in SEE research. By carefully reviewing study quality, analyzing results across the literature, and rigorously evaluating experimental outcomes, clear conclusions were drawn about the most promising techniques for achieving state-of-the-art accuracy in estimating software effort. This study makes three key contributions to the field: firstly, it furnishes a thorough overview of recent machine learning research in software effort estimation (SEE); secondly, it provides data-driven guidance for researchers and practitioners to select optimal methods for accurate effort estimation; and thirdly, it demonstrates the performance of publicly available datasets through experimental analysis. Enhanced estimation supports the development of better predictive models for software project time, cost, and staffing needs. The findings aim to guide future research directions and tool development toward the most accurate machine learning approaches for modelling software development effort, costs, and delivery schedules, ultimately contributing to more efficient and cost-effective software projects.",Yes,본 논문은 최신 머신러닝 기법을 활용한 소프트웨어 노력 추정에 대해 체계적인 문헌 리뷰와 함께 다섯 가지 ML 기법을 적용한 실험적 비교 분석을 수행하여 직접적인 실험 결과와 새로운 통찰을 제공하고 있다. 따라서 단순 리뷰 논문이 아니라 독창적인 실험 연구가 포함된 연구 논문으로 판단된다.
Sentimental Analysis based on Machine Learning Technique,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511973,"In an age defined by the unprecedented proliferation of online content, the ability to discern sentiments within textual data has become integral across a spectrum of applications, spanning from business intelligence to the dynamic realm of social media monitoring. This study conducts a thorough exploration of sentiment analysis methodologies, placing a primary focus on the practical application of machine learning algorithms tailored to the demands of today's generation.The research conducts a comparative analysis to evaluate the effectiveness of diverse algorithms, including support vector machines, neural networks, and ensemble methods. By exploring proposed strategies to address these challenges, the study sheds light on their implications focusing on accuracy and robustness of sentiment analysis models in the present-day landscape.A comprehensive analysis compares diverse paradigms, such as SVM & neural networks, assessing their effectiveness amidst the challenges of navigating sarcasm, decoding context-dependent sentiments, and understanding language subtleties.Twitter, as a platform, presents unique challenges for sentiment analysis, marked by the prevalence of slang and misspellings, compounded by the 140-character limitTwo predominant strategies, the approach based on knowledge and machine learning, are deployed for extracting sentiments from textual data. This research delves into the machine learning approach, specifically applied to analyse Twitter posts about electronic products such as mobiles and laptops. Focusing on sentiment analysis within a specific domain elucidates the impact of domain-specific information on sentiment classification.This study introduces a novel feature vector designed for the classification of tweets into positive and negative sentiments, offering insights into public opinions about electronic products. The utilization of a machine learning framework enhances the precision and granularity of sentiment analysis in the dynamic landscape of Twitter, allowing for a more nuanced understanding of user sentiments within the electronic products domain.However, a primary challenge faced by sentiment analysis lies in the dearth of adequately labelled data within the realm of NLP. Deep learning models stand out because of their inherent capacity for automatic learning.This research paper mentions recent advancements showcasing the integration of deep learning models, such as deep neural networks, convolutional neural networks, among others, aimed at resolving various challenges within sentiment analysis. This includes tasks such as classification of sentiments, addressing cross-lingual complexities, textual and visual analysis, and the examination of product reviews, among others.",Yes,"논문 초록에서 다양한 머신러닝 알고리즘을 적용하고, 새로운 특징 벡터를 제안하며, 특정 도메인(전자제품)에 대한 감성 분석을 수행하는 등 독창적인 연구 내용과 실험적 기여가 포함되어 있음을 확인할 수 있습니다. 따라서 본 논문은 직접 기여하는 연구 논문에 해당합니다."
AI-Enabled Trust in Distributed Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10224236,"Cybersecurity, as a crucial aspect of the information society, requires significant attention. Fortunately, the concept of trust, originating from the field of sociology, has been under extensive research in order to enhance cybersecurity by evaluating the trustworthiness of nodes with artificial intelligence (AI) techniques in distributed networks (DNs). However, the scalability issues faced by AI-enabled trust hinder its integration with the DNs. Currently, there is a lack of a comprehensive review article that explores the current state of AI-enabled trust development applications. This paper aims to address this gap by providing a review of the state-of-the-art AI-enabled trust in DNs. This review focuses on the concept of trust and how it can be facilitated through AI, particularly utilizing machine learning and deep learning methods. Additionally, the paper provides a comprehensive comparison and analysis of three key domains in the field of AI-enabled trust: trust management (TM), intrusion detection system (IDS), and recommender systems (RS). Some open problems and challenges that currently exist in the field are manifested, and some suggestions for future work are presented.",No,"본 논문은 AI 기반 신뢰 기술에 대한 종합적인 리뷰를 제공하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석하는 리뷰 논문에 해당합니다."
Telemedicine Framework in COVID-19 Pandemic,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007389,"The COVID-19 pandemic coincided with the growth and ripeness of several digital methods, such as Artificial Intelligence (AI) (including Machine Learning (ML) and Deep Learning (DL)), internet of things (IoT), big-data analytics, Software Defined Network (SDN), robotic technology, and blockchain, etc. resulting in an experience chance for telemedicine advancement. In several nations, a telemedicine platform based on digital technology has been built and integrated into the clinical workflow in a variety of modes, including many-to-one, one-to-many, consultation mode, and practical-operation modes. These platforms are practical, efficient, and successful for exchanging epidemiological data, facilitating face-to-face interactions between patients or healthcare professionals over long distances, lowering the risk of disease transmission, and enhancing patient outcomes. This article provides a Systematic Literature Review (SLR) to call attention to the most recent advancements in evaluating COVID-19 data utilizing various methodologies such as ML, DL, SDN, and IoT. The number of studies on ML and DL provided and reviewed in this article has proven a considerable effect on the prediction and spreading of COVID-19. The main goal of this study is to show how ML, DL, IoT, and SDN may be used by researchers to provide significant solutions for authorities and healthcare statements to lessen the influence of pestilence. This report also includes many novel strategies for raising the prevalent telemedicine use.",No,"본 논문은 COVID-19 기간 동안의 원격의료 프레임워크와 관련된 기존 연구들을 체계적으로 문헌 검토(SLR)한 내용으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 분석에 초점을 맞추고 있다."
A Systematic Review on Recently Developed Models for Supply Chain Financial Risk Evaluation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10134018,"The financing cost of Supply Chain Management (S CM) is decreased by the emergence of supply chain finance. There is a deficiency in risk in terms of strategy and practice due to the growth of a diversified supply chain financial area. Numerous automated risk assessment frameworks are developed using deep learning and machine learning techniques in recent years to address these issues. Hence, industries are faced with the supply chain sustainability risk that is emerged from various resources. In existing research works, the field of supply chain finance is very limited due to the lack of integration. However, it does not address the small and medium sized enterprise which is used in the real time economy. The purpose of this survey is to conduct a literature review on estimating the financial risks associated with SCMs. This study includes various literature works considered for the financial risk evaluation, where the evaluation of deep learning techniques, performance analysis and their challenges while performing the financial risk evaluation are researched. However, the conventional techniques are presented with diverse challenges in estimating financial risks, and this review paper is highly useful for future research work. This research analyzes the estimation techniques, dataset utilized for evaluating the financial risks, implementation tools and performance metrics involved in the evaluation models. Moreover, various research gaps in the existing research works are explored which helps to show the enhanced performance of the supply chain financial risk evaluation model. Here, it explores various financial risk analyses which will be helpful for future research. Finally, the overall challenges and future works on the financial risk evaluation models are discussed to support future works.",No,"본 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(survey) 논문으로, 직접적인 독창적 연구 결과나 새로운 모델 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합하고 분석하는 리뷰 논문에 해당합니다."
Towards MLOps: A Framework and Maturity Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582569,"The adoption of continuous software engineering practices such as DevOps (Development and Operations) in business operations has contributed to significantly shorter software development and deployment cycles. Recently, the term MLOps (Machine Learning Operations) has gained increasing interest as a practice that brings together data scientists and operations teams. However, the adoption of MLOps in practice is still in its infancy and there are few common guidelines on how to effectively integrate it into existing software development practices. In this paper, we conduct a systematic literature review and a grey literature review to derive a framework that identifies the activities involved in the adoption of MLOps and the stages in which companies evolve as they become more mature and advanced. We validate this framework in three case companies and show how they have managed to adopt and integrate MLOps in their large-scale software development companies. The contribution of this paper is threefold. First, we review contemporary literature to provide an overview of the state-of-the-art in MLOps. Based on this review, we derive an MLOps framework that details the activities involved in the continuous development of machine learning models. Second, we present a maturity model in which we outline the different stages that companies go through in evolving their MLOps practices. Third, we validate our framework in three embedded systems case companies and map the companies to the stages in the maturity model.",Yes,"이 논문은 MLOps 도입을 위한 프레임워크와 성숙도 모델을 제안하고, 이를 세 개의 사례 기업에 적용하여 검증하는 독창적인 연구를 수행하고 있다. 따라서 기존 문헌 검토뿐만 아니라 새로운 모델과 실증적 검증을 포함한 직접적인 연구 기여가 포함되어 있다."
ICT Enabled TVET Education: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446159,"Economies of the world and workforce level of skills are changing rapidly. Industrial revaluations and knowledge economies have raised the demand for a knowledgeable and digitally equipped skilled workforce. Technical and Vocational Education and Training (TVET) is an educational stream that provides vocational skills to youth and produces a skilled workforce for the industry. The role of Information and Communication Technologies (ICT) in the TVET pedagogy, training delivery, teachers training, curriculum, and labs is need of the day to meet current era challenges. In the TVET education training cycle, the level of ICT enablement is required to be reviewed. This Systematic Literature Review (SLR) evaluates literature for innovation of ICT in TVET education for the last ten years. This study aims to identify and present ICT-based technology innovations, research, and applications used in TVET training cycle system components/functional areas to identify gaps for future research directions/agenda. This gap identification will help (i) TVET training institutes to upgrade their teachers, curriculum, labs and, equipment, (ii) policymakers to devise technology-oriented skilled workforce development policies, (iii) to provide guidelines to industry, researchers, and academics to focus on gaps to address future challenges of workforce development. Based on the search string, we found 2,445 relevant documents and after applying quality assurance and inclusion/exclusion criteria, finally, 134 documents were selected for the study and analyzed. The result of this systematic literature review identifies that ICT technologies and application dispersion into TVET training cycle system components/functional areas is very low particularly in monitoring and evaluation, career guidance and job placement, trainee's assessment, and teacher's training. TVET Technology index suggests that much focus is needed on IoT, Robotics, Data Science, Artificial intelligence, cloud computing, and other similar technology induction to all TVET training.",No,"본 논문은 ICT가 TVET 교육에 어떻게 적용되고 있는지에 대한 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 초점이 맞춰져 있습니다."
Advancements and Challenges in Visual-Only Simultaneous Localization and Mapping (V-SLAM): A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10624826,"Visual Simultaneous Localization and Mapping (V-SLAM) is a technique used by autonomous navigation systems that leverages visual sensors, such as cameras, to create a map of an unknown environment while simultaneously tracking the sensor's location. While most reviews focus on SLAM technology in-corporating multiple sensors, this paper comprehensively reviews the historical evolution and recent advancements in visual-only SLAM algorithms. The main objectives are to address challenges faced by traditional SLAM methods and highlight the need for a visual-only approach. This research study has conducted a systematic literature review, highlighting the integration of deep learning for enabling enhanced feature extraction and semantic understanding. The research findings show significant progress from early feature-based methods like Mono-SLAM to advanced algorithms like ORB-SLAM3, incorporating robust loop closure and global optimization. The proposed model is then evaluated using benchmark datasets like KITTI and TUM RGB-D, further these advancements demonstrate improvements in accuracy and efficiency. This review offers unique insights and future research directions, benefiting researchers and practitioners.",No,"본 논문은 V-SLAM 분야의 기존 연구들을 체계적으로 정리하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 알고리즘 제안보다는 기존 연구의 발전과 도전 과제를 종합적으로 다루고 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Data Migration between on prim to Cloud using Generative AI to Reduce Costing And Overheads,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616354,"“Cloud data migration” describes the method of transferring digital data to new cloud services or garage structures. Data have to be transferred in a way that guarantees it remains available, secure, and uncompromised. Businesses might also easily adjust to new needs and trends in era by way of transferring their statistics, apps, or workloads to the cloud. Scalability, adaptability, and optimising charges are all made feasible by this. Generative AI can help us evaluate existing applications and provide insights into how they can be optimized for the target cloud environments. In the ever-evolving world of technology, Generative AI (GenAI) has emerged as a transformative force, poised to revolutionize various industries and domains, including cloud migration. As the global landscape of migration continues to shift, GenAI is prepared to play an essential role in streamlining processes, enhancing accuracy, and improving overall migration outcomes. The review present paper investigates the utilisation of generative AI in order to facilitate the transfer of data from on-premises to the cloud. While highlighting the potential of generative AI to automate transfer operations, it tackles obstacles such as economic implications and security concerns. Migration techniques, the benefits of cloud migration, and pertinent literature are examined in this study, which also provides a summary of key findings and recommendations for future research.",No,"초록에서 제시된 내용은 주로 기존 연구와 기술을 검토하고, 생성형 AI의 활용 가능성을 논의하는 리뷰 논문에 가깝습니다. 독창적인 실험 결과나 새로운 방법론 개발에 대한 직접적인 기여가 명확히 드러나지 않습니다."
Application of Artificial Intelligence Methods in Carotid Artery Segmentation: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038684,"The carotid artery is one of the most important blood vessels that supply blood to the brain. If thrombus occurs, it may cause cerebral ischemic stroke and endanger life. Carotid intima-media thickness and stability of carotid plaque are essential indicators for predicting stroke, which can be measured through medical image segmentation. Therefore, automatic and accurate carotid artery image segmentation and measurement of carotid intima-media thickness and the area and volume of carotid plaque are of great significance for stroke risk prediction and treatment. However, due to the complex shape of the carotid artery and the characteristics of carotid artery imaging, the traditional methods (such as threshold methods, region growth methods) can not segment the carotid artery very well. In recent years, researchers have taken artificial intelligence (traditional machine learning and deep learning) as a critical research method for carotid artery segmentation and extensive research has been performed with satisfactory results. In this paper, we present a comprehensive review of carotid artery segmentation using artificial intelligence methods. We first briefly introduce medical image processing methods and artificial intelligence methods. And then, review and summarize the application of artificial intelligence segmentation methods in carotid artery segmentation (including carotid lumen, media-adventitia, lumen-intima, and plaques). Finally, the challenges of current artificial intelligence methods in carotid artery segmentation are analyzed.",No,"본 논문은 인공지능 방법을 이용한 경동맥 분할 연구를 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
A Systematic Literature Review on AI-Based Methods and Challenges in Detecting Zero-Day Attacks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10668871,"The detection of zero-day attacks remains one of the most critical challenges in cybersecurity. This systematic literature review focuses on the various AI-based methods employed for detecting zero-day attacks, identifying both the strengths and weaknesses of these approaches. By critically evaluating existing literature, this review provides new insights and highlights the gaps that future research must address. The findings suggest that while artificial intelligence, particularly machine learning, offers promising solutions, there are significant challenges related to data availability, algorithmic complexity, and real-time application. This review contributes to the field by providing a comprehensive analysis of current AI-driven methods and proposing future research directions to enhance zero-day attack detection.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 분석하는 문헌 리뷰로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 평가에 중점을 둔 논문입니다."
A Synthesis of Green Architectural Tactics for ML-Enabled Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554775,"The rapid adoption of artificial intelligence (AI) and machine learning (ML) has generated growing interest in understanding their environmental impact and the challenges associated with designing environmentally friendly ML-enabled systems. While Green AI research, i.e., research that tries to minimize the energy footprint of AI, is receiving increasing attention, very few concrete guidelines are available on how ML-enabled systems can be designed to be more environmentally sustainable. In this paper, we provide a catalog of 30 green architectural tactics for ML-enabled systems to fill this gap. An architectural tactic is a high-level design technique to improve software quality, in our case environmental sustainability. We derived the tactics from the analysis of 51 peer-reviewed publications that primarily explore Green AI, and validated them using a focus group approach with three experts. The 30 tactics we identified are aimed to serve as an initial reference guide for further exploration into Green AI from a software engineering perspective, and assist in designing sustainable ML-enabled systems. To enhance transparency and facilitate their widespread use and extension, we make the tactics available online in easily consumable formats. Wide-spread adoption of these tactics has the potential to substantially reduce the societal impact of ML-enabled systems regarding their energy and carbon footprintCCS CONCEPTS• Software and its engineering → Designing software; Software architectures; • Social and professional topics → Sustainability; • Computing methodologies → Machine learningLay Abstract: Machine learning (ML) is a technology field that wants to provide software with functionality similar to humanlike intelligence, e.g., for understanding text or describing images. However, creating and using systems with ML needs a lot more computing power than non-ML systems, which is bad for the environment. Companies therefore need concrete advice on how they can create ML systems that are environmentally sustainable. In this paper, we provide a catalog of 30 green architectural tactics for these systems. An architectural tactic is a high-level design technique to improve software quality, in our case environmental sustainability. To achieve this, we analyzed 51 scientific papers and later discussed with 3 experts to improve and extend our catalog. If many companies start using these tactics, the energy footprint of systems with ML can be greatly reduced.",Yes,본 논문은 51편의 기존 연구를 분석하고 3명의 전문가와의 검증을 통해 ML 시스템의 환경 지속 가능성을 높이기 위한 30가지 건축 전술을 새롭게 제안하고 체계화한 연구이다. 이는 기존 연구를 종합하는 동시에 독창적인 설계 지침을 제시하는 직접적인 연구 기여로 판단된다.
A Semantic Approach to Emotion Recognition Using IBM Watson Bluemix tone Analyzer and translator Language,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9012970,"Artificial intelligence has been a far-flung goal of computing since the conception of the computer, but we may be getting closer than ever with new cognitive computing models. Personal Assistant Agents (PAAs) can assist users to deal with the task of selecting news items and making decisions. The term cognitive computing is typically used to describe AI systems that aim to simulate human thought. Sentiment analysis and emotion detection that aim to build intelligent systems able to recognize and interpret human emotions. Emotions are considered a very important area because they impact interactions, thinking and behaviors. IBM Watson is one of the most famous company, among others, a lot of services for Natural Language Processing. The IBM Watson Developer cloud provides a library of cognitive services as REST APIs which are available on IBM Bluemix. A machine learning information retrieval tool and building on related work in this area which suggests a powerful correlation between the people, emotions, attitude and cognitive processes which shows more documentation that profiling and predicting user's identity is feasible. We introduce an E-ANRS as a solution to problem of cold-start by using IBM Bluemix server for first time, which provides two services are (language translator and tone analyzer). The experimental results obtained from research are, evaluation of RS which gives three parameters being precision (86%), recall (87%), and F1-score (86%). In another side we have two ways to measure accuracy of emotion for our model by using EEG and Self-Assessment-Manikin techniques. By the use of EEG signals as (attention and meditation), electrical activity of neurons within the brain EEG is used. The results of IBM service Language translation and tone analyzer accuracy for 40 tests are 42%. The main objective of this work is to demonstrate the feasibility of a translation-based approach to emotion recognition in texts.",Yes,"논문은 IBM Watson Bluemix의 톤 분석기와 번역기를 활용한 감정 인식 모델을 제안하고, 정밀도, 재현율, F1-score 등의 실험 결과를 제시하며 직접적인 연구 기여를 포함하고 있다. 또한 EEG와 자기 평가 기법을 사용해 모델의 정확도를 측정하는 등 독창적인 연구 내용을 담고 있다."
Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622073,"In today's legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review - where they are typically assessed for relevancy or privilege - the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing `responsive' electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.",Yes,"본 논문은 설명 가능한 예측 코딩(explainable predictive coding) 개념과 방법을 제안하고, 실제 법률 문서 검토 데이터를 사용한 예비 실험 결과를 보고하고 있어 독창적인 연구 내용을 포함하고 있다. 따라서 연구 논문에 해당한다."
"Recent Advances in WSN-Based Indoor Localization: A Systematic Review of Emerging Technologies, Methods, Challenges, and Trends",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10772114,"Indoor localization (IL) systems are crucial for enhancing operational efficiency, safety, and user experience by allowing precise tracking of objects, robots, or individuals within various environments such as healthcare, retail, and industrial sectors. Despite their increasing importance, there remains a notable deficiency in the literature, particularly concerning systematic reviews that consolidate findings from experimental research. This work fills this crucial gap by rigorously assessing the advancements and challenges faced by Wireless Sensor Network (WSN)-based IL systems, with a focused examination of experimental studies conducted over the past five years. It delves into both radio frequency (RF) and non-RF technologies, critically evaluating a spectrum of localization methods including fingerprinting, geometric mapping, proximity, and dead-reckoning. It systematically evaluates the advantages, limitations, and current solutions of each method, based on their citation metrics and prevalence in scholarly literature. Furthermore, the paper explores innovative performance enhancement techniques, including the integration of machine learning and the hybridization of multiple technologies, to demonstrate significant improvements in IL functionalities. It also identifies and analyzes key trends, such as the choice of technologies for specific methods, typical network density configurations, and accuracy enhancements achieved through different approaches. Research gaps are highlighted, including the need for advancements in machine learning for offline and edge computing, standardization of sensor components, and improvements in interoperability and energy efficiency. The paper concludes by proposing strategic future research directions, outlining a roadmap for advancing IL research and development in this rapidly evolving field.",No,"본 논문은 최근 5년간의 실험 연구를 체계적으로 검토하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 수행한 내용이 포함되어 있지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하고 평가하는 데 중점을 두고 있습니다."
Notice of Removal: Hybrid intelligent trail to search engine answering machine: Squat appraisal on pedestal technology (hybrid search machine),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7253955,"Arched type Swing in loom of information retrieval system is observed with record progression to information fetch, to knowledge data processing, to intelligent information progression. Subsequent processing machines like document retrieval, text summarization, search engines, rule based machines, expert systems have been developed. These machines have dedicated performance with retrieval measure in particular dimension. Machine learning methods have facilitated reasoning machine with ability like humans. Still a corner in research argues highly intelligent time constraint fact seeking real world information processing machine. Hybrid technology is integration of optimized approaches at various levels of information processing. We proposed a hybrid search answer machine with four techniques of optimization “question reformulation” (from user-intent, profile), “search method” (semantic concept, context, machine learning), “answer presentation” (ranking algorithm), “decision support ” (comparative analysis to choose best techniques to retrieve results). Data corpus is heart of any IR system large dataset facilitates good search which argue to distributed data and computing. Intelligence is reformation proceeds that excel our time and dataset. The machine is designed to facilitated updatable training dataset for fact seeking knowledge acquirement “it trains over data”. Muti-agent model distributed search methodology is proposed. In precise Hybrid extraction of “hybrid models” is performed. Semantic context (concept based) user profiled; best machine learning, decision supportive multiagent distributed search system is proposed. This paper gives underlying technologies overview, with examinations of 30 papers is done as with recent review of technology advancement. The review outcomes are orderly placed with 3 research query answering. The outputs of query structure a trail to search engine answering machine. We facilitate research done by scholars on technology perspective we integrate them to draw a sketch of hybrid search answering. In domain “a point of reference” concepts of research are studied, with comparative views on advance in IR. We identify the benchmark of research methods blueprint and explore space of research in area of intelligent machine implementation.",No,초록은 여러 기존 연구와 기술들을 통합하여 하이브리드 검색 시스템의 개념과 기술적 개요를 제시하는 리뷰 및 개념적 논의에 초점이 맞춰져 있습니다. 직접적인 실험 결과나 독창적인 연구 기여 내용이 명확히 포함되어 있지 않아 연구 논문으로 보기 어렵습니다.
A Trend of AI Conference Convergence in Similarity: An Empirical Study Through Trans-Temporal Heterogeneous Graph,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049682,"Publishing the research works on academic publications is an important part of the scientific process. Since the development of computer science research is very fast, researchers tend to publish the research works in a fast way, such as conferences whose review processes are faster than the journals. In the past decades, one conference usually focuses on a specific research field and the topic or method overlap between conferences is low. We have noticed that, in recent years, some topics or methods which were once studied in a small number of specific research fields have become popular in many other fields. Naturally, we come up with two research questions: (1) Do the conferences indeed become similar? and (2) How do conferences become similar? In this paper, we first use a trans-temporal heterogeneous graph network to model academic conferences in recent 20 years. Due to the large number of conferences, we categorize these conferences into 6 research fields for brevity. Then, we first quantitatively and qualitatively assess “Do the research fields become similar?” and then focus on exploring “How do research fields become similar?”. From the result, we find the reason for the research fields in computer science become similar is that AI becomes pervasive and researchers tend to apply the machine learning methods to different application fields. Since the methods become universal between different research fields, researchers should pay more attention to advanced information in other fields to motivate more interdisciplinary works. To assist the researchers to explore related interdisciplinary advanced information, it is crucial to measure the cross-field impact of papers using the citation information and recommend the paper which has a high cross-field impact on the related researchers. As for the newly published papers which do not have any citations, we also propose a cross-field impact prediction model to recommend the cutting-edge research works to related researchers accurately. Experiments conducted on real-world datasets verify the effectiveness of the proposed method.",Yes,"본 논문은 트랜스-템포럴 이기종 그래프 네트워크를 활용하여 컴퓨터 과학 분야 내 학술대회 간 유사성 변화를 분석하고, 교차 분야 영향력 예측 모델을 제안하는 등 독창적인 방법론과 실험 결과를 포함하고 있다. 이는 직접적인 연구 기여를 담은 연구 논문으로 판단된다."
Application of Artificial Intelligence in Clinical Diagnosis of Children with Autism Spectrum Disorders,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730989,"In recent years, the incidence of autism spectrum disorder has been increasing, traditional diagnostic tools such as the autism diagnostic observation scale - revised”, “the autism diagnostic interview scale”, and intervention measures such as applied behavior analysis, structured education, social story has been applied to clinical, but the early accurate diagnosis and treatment of ASD still faces various challenges. With the development of computer technology, electronic engineering, statistics and other disciplines, clinical medicine has entered the era of intelligence. The application of artificial intelligence in the medical field provides broad prospects for the timeliness and accuracy of ASD diagnosis and the improvement of its core symptoms. The problems of ASD rehabilitation caused by insufficient medical resources and high treatment costs are expected to be solved by the intervention of artificial intelligence. Artificial intelligence (AI) is becoming more and more prominent in the efficient, high-precision and high-stability clinical diagnosis of autism. This paper reviews the literature in recent years, and discusses the prediction, early recognition and diagnosis of autism, as well as the research on auxiliary intervention.",No,"초록 내용은 인공지능의 자폐 스펙트럼 장애 진단 및 치료에 대한 기존 연구들을 종합적으로 검토하는 문헌 리뷰임을 나타내고 있으며, 독창적인 연구 결과나 실험 데이터에 대한 직접적인 언급이 없다. 따라서 본 논문은 새로운 연구를 직접 수행한 연구 논문이라기보다는 기존 연구를 정리한 리뷰 논문으로 판단된다."
A Systematic Literature Review on Machine Learning Algorithms for Human Status Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829739,"Human status detection (HSD) is important to understand the status of users when interacting with various systems under different conditions. Recently, although various machine learning algorithms have been applied to analyze and detect human status, there are no guidelines to utilize machine learning algorithms to analyze physical, cognitive, and emotional aspects of human status. Therefore, this study aimed to investigate measures, tools, and machine learning algorithms for HSD by applying a systematic literature review method. We followed the preferred reporting items for systematic reviews and meta-analysis (PRISMA) model to answer three research questions related to the research objective. A total of 76 articles were identified using two hundred keyword combinations addressing topics under HSD in the fields of human factors and human-computer interaction (HCI). The results showed that research on HSD becomes important in industrial systems, focusing on how intelligent systems based on machine learning (ML) differ from earlier generations of automated systems, and what these differences necessarily imply for HCI to design and evaluation. The tools used to collect data for HSD on different parameters are broadly discussed. Recent HSD studies seem to focus on cognitive load and emotion, whereas prior studies have focused on the detection of physical effort. This research assists domain researchers in identifying HSD approaches using different ML algorithms that are suitable for use in their research.",No,"이 논문은 머신러닝 알고리즘을 활용한 인간 상태 감지 연구에 대한 체계적 문헌 고찰(Systematic Literature Review)을 수행한 것으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석한 리뷰 논문입니다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 정리하는 데 초점이 맞춰져 있습니다."
How affective computing could complement and advance the quantified self,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999285,"This paper primarily relates to the engineering of affective (emotional) computing (AC) capabilities in machines and also considers how such developments may be used to advance the quantified self (QS) paradigm. Our work will provide a literature and technology review providing some general insights and discussion around AC applications, technologies, tools, platforms and alternative controls/interfaces. Specific focus from a quantified self perspective will also be addressed with applied literature references from the affective sciences research community. Readers new to the field will take away a solid understanding of what AC is, how it is impacting on new software innovations and how it relates to QS. Those with existing expertise will gain new research insights, learn about new research projects and software development platforms and will also learn about applied AC and QS research.",No,"초록에서 본 논문은 주로 기존 문헌과 기술에 대한 리뷰 및 개념적 논의를 제공하는 것으로 보이며, 독창적인 연구 결과나 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단하기 어렵습니다."
A Systematic Literature Review on Machine Learning and Laboratory Techniques for the Diagnosis of African swine fever (ASF),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10220551,"African swine fever (ASF) is a virulent infectious disease of pigs. It can infect domestic and wild pigs, causing severe economic and production losses. The virus can be spread through live or dead pigs and through pork products. Since there is currently no vaccine or treatment method, it poses a major challenge and threat to the pig industry once it breaks out. The results of the investigation show that most existing solutions use laboratory tests to diagnose possible ASF cases. In addition, various machine learning (ML) techniques have been used in the past to diagnose ASF. However, historical review of recent years shows that laboratories have difficulty diagnosing ASF with the required accuracy due to a lack of correlation between causes and effects. Lack of accuracy and incorrect ASF diagnoses by laboratories have proven to be a major problem for pig welfare. Consequently, misdiagnosis of ASF disease can result in severe direct and indirect economic losses to farmers, especially farmers whose income is derived primarily from pig production. While several other researchers have proposed the use of ML for ASF diagnosis, the application of cause-effect relationships between specific viruses and symptoms for ASF diagnosis is still missing. In this systematic literature review, we examine the methods, limitations, and approaches in the existing literature from ML and laboratories for ASF diagnosis. In this review, we evaluate the performance of ML and laboratory techniques for ASF diagnosis. In addition, we compare the performance of the techniques of ML with other statistical approaches such as causal ML and computer vision for ASF diagnosis. In addition, the strengths and weaknesses of ML and laboratory techniques for ASF diagnosis were summarized. A thorough search of relevant databases was performed, and the selected studies were examined using predefined inclusion and exclusion criteria. Nevertheless, the study also indicates an area for improvement, such as the accuracy of ASF diagnosis. The study recommends the use of Causal Reasoning with ML to develop a causal ML model capable of establishing relationships between viruses and symptoms to improve the accuracy of the ASF disease. The application of causal ML is presented as an alternative solution for laboratory diagnosis of ASF, which contributes to the field of the study. In addition, further research could investigate the possible characteristics of ASF, including virus variants originating from the ASF family. The review could provide essential information on ASF datasets based on the interpretation of results obtained from the use of appropriate samples and validated tests in combination with the information from laboratory tests of ASF disease epidemiology, scenario, clinical signs, and lesions produced by different virulence. This review concludes that more studies are needed for improving the accuracy and implementation of the causal ML model for ASF diagnosis in real-time surveillance systems.",No,"이 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 포함하지 않고 기존 연구들의 방법과 한계, 성과를 종합적으로 분석하는 데 초점을 맞추고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 평가에 해당합니다."
Cognitive Science: Towards a Multi-Disciplinary Sense for solving a complex mechanism,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541726,"Cognitive Science is a complex mechanism of solving a problem based on the impact of behavior, analysis of interpretation, studying mental representations functioning of the neurons at the level of brain, perceiving and sensing an object: a living: nature & nurture, a deeper understanding of psychological aspects, relating mind to the brain, handling emotional sense with Intelligent quotient, a meta-analysis study of automation, handling the issues dealing with biasedness due to lack of optimal Hypothesis, correlating to develop ontological representations and illustrating the language of thought on empirical evidence, observational phenomena and applying technologies of Deep learning, machine learning, statistics subset to Artificial Intelligence for optimality. This comprehensive literature study navigates through the historical evolution and interdisciplinary dimensions of cognitive science, spanning from ancient Greek philosophy to its contemporary amalgamation with neuroscience, psychology, linguistics, and artificial intelligence. Examining theoretical paradigms like formal logic, rules, concepts, analogies, images, and connectionism, the study elucidates their pivotal roles in deciphering the mind's intricate representations and computations. Focused on the challenges encountered by cognitive science, the study underscores the integration of analytics, foundational concepts, and diverse methodologies such as statistical tools, machine learning, and prescriptive analytics. A Structured outlined working procedure showcases the application of cognitive science principles to enhance student performance, conducting and evaluating surveys, statistical analysis, machine learning models, and association rule mining. The methodology progresses systematically from data collection and persona analysis to prediction hypothesis formulation, mismatch analysis, marks prediction, rule-based classification, association rule mining, data analytics, and exploratory data analysis. Signifying the importance of big data, the study recommends cognitive computing concepts for intervention measures, encompassing cognitive behavioral therapy, gaming approaches, and neurological interventions, thereby contributing to the practical implementation of cognitive science in student performance analysis and intervention.",No,"초록은 주로 기존 연구와 이론들을 종합하고 문헌 연구를 수행한 내용으로 보이며, 직접적인 실험 결과나 독창적인 연구 기여가 명확하게 제시되어 있지 않습니다. 따라서 본 논문은 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
The Effect of Adversarial Machine Learning Attack on Agriculture Field and Food Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756330,"Machine Learning and Deep Learning models had enabled several smart applications such as classification and recognition of objects. Smart machine and deep learning models can provide effective solutions to real life problems in various sectors such as agricultural and food security. Agriculture plays a significant role in food security where it supplies nations with the needed nutritious. Food security matters for nations as it ensures continuity and sufficiency of food and nutritious for humans. Machine and Deep learning models are exposed to a cybersecurity attack that is called the adversarial machine learning attack. This attack employes different methods and different algorithms to hamper the performance of the models. The attack initiates a noise attack over the images, videos, audios, text data that is being processed by the model. This results in the model misfunctioning and misclassifying the data which reduces the number of correctly classified samples and increase the number of wrongly classified samples by the models. The interesting aspect of the attack is that the human eye cannot detect the change of the data such as images. The data/images look identical before and after the adversarial attack. Only the models can detect the change in pixels and therefore the performance of the model degrades. This research paper provides a comprehensive literature review over the most common and widely used algorithms of the adversarial attack. The paper explores into the critical issue of food security, shedding light on its significance in global contexts. It highlights the potential consequences of adversarial attacks, emphasizing their unfavorable impact on food security and production systems. By drawing attention to these challenges, the paper calls for urgent measures to safeguard food security infrastructure against emerging cyber threats.",No,"본 논문은 적대적 머신러닝 공격에 대한 기존 연구들을 종합적으로 검토하는 문헌 리뷰 논문으로 보이며, 직접적인 실험이나 새로운 알고리즘 제안 등 독창적인 연구 결과를 포함하고 있지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
IT/OT challenges and opportunities to improve cyber resiliency for utilities: a review paper,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10863572,"As utilities increasingly rely on interconnected digital systems, the integration of IT and OT has become crucial for ensuring cybersecurity. However, this integration presents significant challenges due to differing priorities, governance models, and the inherent complexities of disparate technologies. This paper evaluates the effectiveness of the report’s proposed strategies, which are grounded in the National Institute of Standards and Technology (NIST) Cybersecurity Framework, to mitigate these challenges. The review identifies the report’s comprehensive approach to presenting risks and recommendations, alignment with existing cybersecurity practices, and its practical implications for utility companies. By comparing the report’s content with existing literature, this paper assesses its contributions to the field and suggests areas for further research, particularly the need for empirical validation of the recommended strategies. This study proposes solutions including the use of automated tools for continuous monitoring, blockchain for data integrity, and AI-driven supply chain risk management to address these vulnerabilities. Additionally, we explore the potential of machine learning for predictive analytics, quantum-resistant encryption, and advanced network segmentation to secure critical infrastructure. This research contributes to the academic discourse by providing a comprehensive framework that utilities can adopt to mitigate risks and leverage technological opportunities for improved security.",Yes,"본 논문은 기존 문헌과 보고서를 비교 분석하고, 자동화 도구, 블록체인, AI 등 구체적인 기술적 해결책을 제안하며, 사이버 보안 향상을 위한 종합적 프레임워크를 제시하는 등 독창적인 연구 기여를 포함하고 있다. 따라서 단순 리뷰를 넘어 새로운 통찰과 적용 방안을 제공하는 연구 논문으로 판단된다."
A Systematic Literature Review of Machine Learning Applications for Port's Operations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314756,"In transportation systems, there is a need of Machine Learning (ML) to create intelligent solutions for many issues. In port's studies, there has been a long focus on optimization and simulation modeling, but still, ML served in building more complex decision support systems for better management of port operations. To evaluate the contributions in the application of ML for port operations, we conducted this Systematic Literature Review (SLR) in order to select and characterize the most relevant papers. This paper, reports the applied research protocol and its results, and highlights the gaps that could be addressed in future works.",No,"이 논문은 머신러닝이 항만 운영에 어떻게 적용되었는지를 체계적으로 문헌을 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 중점을 둔 논문입니다."
A Review on the Applications of Machine Learning and Deep Learning Algorithms for Image Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933346,Modern image recognition has experienced dramatic improvements because of Machine Learning and Deep Learning algorithms together. This study investigates CNNs and SVMs for recognition enhancement while reviewing image recognition technologies extensively through literature. This paper demonstrates how the applications of healthcare and security systems and social media analysis influence society. New developments in technology have not resolved multiple unresolved obstacles such as data bias as well as computational complexity and privacy concerns together with real-time processing restrictions. The latest image processing techniques include ViTs alongside GANs and Few-Shot Learning but developers need to achieve better results in future improvements. The main goal of this research examines present-day advancements in ML and DL with a review of their capabilities as well as constraints before recommending future study paths to overcome problems encountered today. This paper evaluates both the future potential and benefits alongside drawbacks of ML and DL models applied to image recognition.,No,"이 논문은 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 현황과 문제점, 향후 연구 방향을 제안하는 데 중점을 두고 있다."
Spatio-Temporal Crime HotSpot Detection and Prediction: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187772,"The primary objective of this study is to accumulate, summarize, and evaluate the state-of-the-art for spatio-temporal crime hotspot detection and prediction techniques by conducting a systematic literature review (SLR). The authors were unable to find a comprehensive study on crime hotspot detection and prediction while conducting this SLR. Therefore, to the best of author's knowledge, this study is the premier attempt to critically analyze the existing literature along with presenting potential challenges faced by current crime hotspot detection and prediction systems. The SLR is conducted by thoroughly consulting top five scientific databases (such as IEEE, Science Direct, Springer, Scopus, and ACM), and synthesized 49 different studies on crime hotspot detection and prediction after critical review. This study unfolds the following major aspects: 1) the impact of data mining and machine learning approaches, especially clustering techniques in crime hotspot detection; 2) the utility of time series analysis techniques and deep learning techniques in crime trend prediction; 3) the inclusion of spatial and temporal information in crime datasets making the crime prediction systems more accurate and reliable; 4) the potential challenges faced by the state-of-the-art techniques and the future research directions. Moreover, the SLR aims to provide a core foundation for the research on spatio-temporal crime prediction applications while highlighting several challenges related to the accuracy of crime hotspot detection and prediction applications.",No,"이 논문은 체계적 문헌 고찰(Systematic Literature Review, SLR)로서 기존 연구들을 종합하고 평가하는 데 중점을 두고 있으며, 직접적인 독창적 연구 결과나 새로운 실험, 모델 제안 등은 포함하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 분석하는 리뷰 논문에 해당합니다."
Deep Learning for Text Based Emotion Classification from Social Media,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10462735,"Affective Computing is the study of systems that can recognize underlying human emotions. To be able to detect this, the systems usually have a sensor that captures the features of the input to evaluate it further. Based on this idea, we tried to explore several state-of-art machine learning methods and deep learning methods that are used to classify emotions. However, recognizing emotions is relatively a daunting task for an un-social computer. There are several techniques to model the emotions from text, some implement machine learning, others take advantage of the deep learning technology. Therefore, we evaluated several models of Machine Learning methods and recent Deep Learning methods to make a comparison with previous related works. The dataset used in this research was from social media. Recognizing emotions from social media provides some non-verbal insights for the readers. The experiment demonstrates that deep learning models outperform several previous results and machine learning models on emotion recognition tasks. The results demonstrate that the model trained with BERT achieved the accuracy of 93.75%. Moreover, Love and Joy class is relatively challenging to distinguish.",Yes,"논문 초록에서 여러 최신 머신러닝 및 딥러닝 모델을 평가하고, 소셜 미디어 데이터를 활용해 감정 분류 성능을 실험한 결과를 제시하고 있어 직접적인 연구 기여가 포함되어 있다. 또한 BERT 모델을 사용해 높은 정확도를 달성한 점을 명확히 기술하고 있어 독창적인 연구 내용으로 판단된다."
Mental Health Analytics: A Comparative Exploration of Machine Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837048,"Machine learning methods for checking mental health: findings, understanding, and how well the models work. In short, this study explored how machine learning tools can be used to assess mental health by analyzing a large amount of information about people, including their backgrounds, lifestyles, and social behaviors. Using a Boosting model, a Decision Tree Classifier, and a logistic regression model helped us thoroughly evaluate how well the models performed. The Boosting model worked the best, reaching an accuracy of 81.75% Next came the Decision Tree Classifier with an accuracy of 80.69%, and the logistic regression model had an accuracy of 79.63% Besides checking how well the models work, the research also looked at ways to improve the models so they can be better understood. It focused on finding the important factors that greatly affect mental health results. We used techniques called hyperparameter tuning and cross-validation to make sure the models are strong and can work well on new data. The suggested model could help find people who are likely to develop different mental health issues. Feature importance analysis can give useful guidance to doctors, and future treatment plans in behavioral medicine can be created based on these predictions. AI models can help show the link between life, lifestyle choices, and high stress levels. This paper aims to help improve fast AI learning for mental health evaluation and highlight concerns with using data-related methods to assess social and health problems.",Yes,"본 논문은 머신러닝 모델을 활용하여 정신 건강 평가에 직접적으로 기여하는 연구를 수행하였으며, 모델 성능 비교, 하이퍼파라미터 튜닝, 교차 검증 등 독창적인 실험과 분석을 포함하고 있다. 또한, 중요한 요인 도출과 임상적 활용 가능성 제시를 통해 연구적 기여가 명확하다."
A Literature Review on Cybersecurity Risks and Challenges Assessments in Virtual Power Plants: Current Landscape and Future Research Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10792889,"Virtual Power Plants (VPPs) have emerged as dynamic solutions, harnessing the distributed resources such as solar panel and batteries. However, the complicated interconnectedness of VPPs exposes them to unique cybersecurity vulnerabilities, distinguishing them from traditional smart grids. This systematic literature review primarily focuses on the cybersecurity challenges VPPs face, such as data breaches and data flow manipulation. The study explores these unique risks based on proposed solutions from the existing literature like federated learning, intrusion detection systems, blockchain integration, and machine learning by analyzing their strengths, weaknesses, and applicability to VPP cybersecurity. Additionally, regulatory barriers and opportunities like cost-savings are discussed, with a focus on ensuring the secure future of VPPs through machine and deep learning and strong security policies. This paper contributes significantly to the field by identifying VPP cybersecurity risks, evaluating potential solutions, and proposing future research directions.",No,"본 논문은 기존 문헌을 체계적으로 검토하고 현재 연구 동향과 미래 연구 방향을 제시하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Neural Networks and Deep Learning: Innovations and Applications in Soft Computing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829173,"The Paper explores different aspects of deep learning techniques and neural networks in the fields of healthcare, time-series forecasting, agriculture, and other relevant sectors through soft computing. The objective of the report is to examine the significant potential and applications of neural networks. While machine learning techniques called neural networks (NN) are utilized for audio, picture, and the processing of natural languages, it has huge implications in robotics where several machine learning algorithms are significantly required. Improvements in deep machine learning are also investigated in the article specifically in the fields of drug construction, genetics, recognizing faces, farming, biological medicine, biological informatics, medical treatment, natural language, multimedia analysis of data, and mobility prediction. However black box challenges and privacy concerns with data are often ignored in this research. The paper also outlines deep learning-driven data mining techniques for precisely predicting, displaying, evaluating, and classifying data. The report has also emphasized on addressing the potential privacy concerns and challenges associated to information representation. By reviewing recent literature sources about neural network techniques, the study proposes innovation strategies and potential improvement areas for the development of advanced technologies like deep learning and neural networks while addressing the key privacy concerns in data processing.",No,초록에서 본 논문은 기존 문헌을 검토하고 신경망과 딥러닝의 응용 및 혁신을 개괄적으로 설명하는 리뷰 성격의 보고서로 보입니다. 독창적인 연구 결과나 실험적 기여에 대한 언급이 없으므로 직접 기여하는 연구 논문으로 판단하기 어렵습니다.
Internet of Things Intrusion Detection: A Deep Learning Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308293,"The Internet of Things (IoT) is a shifting paradigm that allows the integration of billions of devices with the Internet. With its wide range of application domains, including smart cities, smart homes, and e-health, the IoT has created new challenges, particularly security threats. Traditional security solutions, such as firewalls and intrusion detection systems, need amending to fit the new networking paradigm. Given the recent advances in machine learning, we investigated the use of deep learning algorithms for anomaly detection. The IoT collects a massive amount of data from the environment, and deep learning is based on a set of algorithms striving for the data. Intrusion detection systems are used to expose network threats and are an effective means of protecting network assets. Anomaly detection is a conventional intrusion detection approach that separates normal and abnormal network traffic using statistical, rule-based, or machine learning models. Of the machine learning models, deep learning is a neural network algorithm that has provided breakthroughs in domains such as object and voice recognition. However, there are limitations in applying deep learning to network anomaly detection. This paper proposes a novel anomaly detection framework based on unsupervised deep learning algorithms for revealing network threats. Our research explores the applicability of deep learning to detect anomalies by evaluating the use of Restricted Boltzmann machines as generative energy-based models against Autoencoders as non-probabilistic algorithms. The study provides an in-depth analysis of unsupervised deep learning algorithms. The simulations studies show ≈ 99% detection accuracy, which is significantly improved compared to the closely related work.",Yes,"논문 초록에서 제안된 새로운 비지도 학습 기반 딥러닝 이상 탐지 프레임워크와 Restricted Boltzmann 머신 및 오토인코더의 적용 및 성능 평가를 다루고 있어, 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 알 수 있다."
Quantifying Confounding Bias in AI Generated Art: A Case Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658705,"In recent years, artificial intelligence (AI) generated art has become very popular. From generating art works in the style of famous artists like Paul Cezanne and Claude Monet to simulating styles of art movements like Ukiyo-e, a variety of creative applications have been explored using AI. However, there has been very little focus concerning the ethical implications of AI generated art. Can AI model artists' styles without stereotyping them? Does AI do justice to the socio-cultural nuances of art movements? In this work, we take a first step towards analyzing these issues. Leveraging directed acyclic graphs to represent potential processes of art creation, we propose a simple metric to quantify confounding bias due to the lack of modeling the influence of art movements in learning artists' styles. As a case study, we consider the popular cycleGAN model and analyze confounding bias across various genres. The proposed metric is more effective than state-of-the-art outlier detection method in understanding the influence of art movements in artworks. We also highlight how the proposed metric can aid in determining authenticity of artworks. We hope our work triggers discussions related to ethical implications of AI generated art.",Yes,"본 논문은 AI 생성 예술에서 발생하는 교란 편향을 정량화하는 새로운 지표를 제안하고, 이를 cycleGAN 모델에 적용하여 분석하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 지표의 효과를 기존 방법과 비교 평가하는 실험적 기여도 포함되어 있어 연구 논문에 해당한다."
The conception of a large-scale Systems Engineering environment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9256709,"With the rise of artificial intelligence, it is time to shape the systems engineering tooling environment for the future. In the last decade, we have seen several emerging technologies that will potentially have a great impact in complex systems. These new technologies are expected to cause a disruptive impact not only in the products but also in to the tools used across the whole product life cycle. For this reason, is imperative to perform a critical review of the current systems engineering tooling ecosystem. This assessment should also map the open research problems that could prevent the complete integration of the new technologies into the systems engineering framework. This paper proposes a new architecture for a system engineering environment to operate in large scale projects. The objective of this research is twofold: it will first identify the capabilities for the next generation platform, and secondly, it will evaluate how artificial intelligence applications can be integrated in compliance with DO-330. The concept developed by this research will drive tool design recommendations enabling the use of artificial intelligence driven applications in a systems engineering tooling ecosystem.",Yes,"논문은 새로운 시스템 엔지니어링 환경 아키텍처를 제안하고, 차세대 플랫폼의 기능을 식별하며 인공지능 통합 방안을 평가하는 등 독창적인 연구 내용을 포함하고 있다. 이는 기존 도구 생태계에 대한 비판적 검토와 새로운 설계 권고안을 제시하는 연구 논문으로 판단된다."
Regulatory Compliance and User Trust: Balancing Innovation and Security in AI-Driven Online Payment Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10763096,"To enhance user experience and security, integrating AI bots into online payment systems offers both potential and challenges. This study evaluates the complicated world of AI-powered online payment systems, stressing the main issues raised and innovation solutions recommended. Main challenges include maintaining the trust of users in AI, cybersecurity guidelines will implement to safeguard private financial data from growing threats and enhancing system efficiency to handle high transaction volumes while maintaining accuracy and speed. With the advancements in the technology and the usage of the Artificial intelligence usage the users are very comfortable in trusting the use of various features in doing online transactions as well. We al are aware of this these days that going to the banks for the smallest of the activity is becoming hassle but with the help of all these features the KYC can also be done online using the applications and the use of AI. The major concerns such as user's trust, cybersecurity, system efficiency and regulatory compliances are being resolved by number of approaches to solve the issues with the help of AI tools, such as predictive data used for improving user experience, NLP serving as customer service with AI capabilities, and ML algorithms for fraud identification and prevention. This investigation concerns artificial intelligence (AI) and how such policies can be implemented to optimize users' management and legal requirements adherence. It further carries out the literature review regarding the methods of analytics integration into the bot's evolution and transformations within the payment systems. The complete roadmap is given to the stakeholders how to confront the challenges of Artificial Intelligence as well as exploit its potentials to create safe, effective and simple-to-use electronic transactions. The main objective of the study is giving the opportunities, challenges, and solutions regarding usage of Artificial Intelligence in online payment processing.",No,"초록 내용은 AI 기반 온라인 결제 시스템의 문제점, 기회, 그리고 해결책에 대한 문헌 검토와 정책적 제언에 초점을 맞추고 있으며, 독창적인 실험 결과나 새로운 연구 방법론에 대한 직접적인 기술이 부족합니다. 따라서 본 논문은 기존 연구를 종합하고 분석하는 리뷰 성격에 더 가깝다고 판단됩니다."
Landscape of Requirements Engineering for Machine Learning-based AI Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9719885,"Techniques and practices in RE are not well researched, although problems and the research challenges on requirements engineering (RE) for machine learning-based systems (MLS) are evaluated via empirical case studies. A systematic literature review of RE for MLS was conducted to guide practitioners and researchers to design and research effective RE for ML systems and software. We identified 32 papers. Although many studies have been recently conducted, problem statements and research challenges remain. Future studies should include the monitoring requirements for concept drifts and how domain experts collaborate with ML experts and engineers.",No,"본 논문은 체계적 문헌 리뷰(Systematic Literature Review)를 수행하여 기존 연구들을 종합하고 분석한 결과를 제시하고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 포함하고 있지 않습니다. 따라서 새로운 연구 내용을 직접 제시하는 연구 논문으로 보기 어렵습니다."
Deep Learning for Crowd Counting: Addressing Crowd Density with Advanced Methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10690493,"Counting individuals in densely populated urban areas is essential for applications ranging from event planning to public safety. Traditional crowd counting methods face challenges in accurately estimating crowd density due to complex dynamics, varying lighting conditions, and occlusions. This research explores the potential of advanced deep learning techniques, focusing on Faster R-CNN, YOLO, and SSD models, to address these challenges. Real-world datasets, including the Shanghai Dataset and a Custom Crowd Counting Dataset, are employed to comprehensively evaluate model performance. The introductory section underscores the importance of precise crowd counting in contemporary urban scenarios, highlighting limitations of traditional methods and motivating the adoption of deep learning techniques. The literature survey provides a comprehensive review of crowd counting approaches, tracing the evolution from early handcrafted features to advanced deep learning models. This paper presents the subsequent section, include a comprehensive analysis of assessment metrics, like Mean Absolute Error, the Mean Squared Error, the true positive rate and F1 score for each model. YOLO emerges as the top performer, displaying the lowest MAE and MSE, along with the highest precision, recall, and F1-score. Pictorial representations and metric comparisons further illustrate the capabilities of the models. The conclusion summarizes insights gained from the study, emphasizing YOLO's superiority in accurately estimating crowd density. Practical implications for event management, public safety, and urban planning are discussed, positioning the research contribution within the broader context of advancing deep learning applications. In summary, this research bridges the gap between theoretical advancements and practical applications in crowd counting. By exploring advanced deep learning models and conducting a comparative analysis, valuable insights are provided, paving the way for continued innovations in crowd counting methodologies. The study indicates the exponential rising field of deep learning tecnolgies, showcasing the potential for enhanced accuracy and reliability in estimating crowd density for real-world applications.",Yes,"논문은 기존 방법들의 한계를 극복하기 위해 Faster R-CNN, YOLO, SSD 등 최신 딥러닝 모델을 적용하고, 실제 데이터셋을 활용해 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 평가 지표를 통해 모델 성능을 비교 분석하며 실질적인 기여를 하고 있음을 보여준다."
"Artificial Intelligence in Cellular-V2X Networks: A Survey on Dynamic Spectrum Allocation, Traffic Prediction, and Anomaly Detection",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10774806,"As cellular-vehicle-to-everything (C-V2X) networks gain importance within emerging intelligent transportation systems, providing effective, secure network management is crucial. This survey paper focuses on the opportunities for applying Artificial Intelligence (AI) to improve the performance of C-V2X networks. AI-assisted techniques for C-V2X network management are investigated, including Deep Q-learning techniques for dynamic spectrum allocation, Machine Learning for improved traffic prediction, and anomaly detection algorithms for improving C-V2X network security. The survey reviews existing research in these domains, providing an overview of current advancements, analyzing the strengths and limitations of AI-driven solutions, and highlighting potential areas for further enhancement using AI approaches. The paper also explores methods for evaluating AI-based solutions compared to traditional non-AI methods. Additionally, it identifies key challenges and outlines future research opportunities in the application of AI to C-V2X network management.",No,본 논문은 기존 연구들을 종합하여 AI 기법들이 C-V2X 네트워크에 어떻게 적용되는지 개관하는 서베이 논문입니다. 독창적인 실험 결과나 새로운 연구 기여보다는 기존 연구의 분석과 향후 연구 방향 제시에 중점을 두고 있습니다.
Enhancing Engineering Cost Risk Management Through Artificial Intelligence-Based Warning and Control Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795935,"With the growing importance of engineering cost risk management in the construction sector, the integration of artificial intelligence (AI) technology has emerged as a crucial tool for enhancing the effectiveness of risk alerts and control measures. This research endeavors to delve into the utility of AI in managing engineering cost risks and to suggest appropriate warning and control approaches. By conducting a thorough literature review and case studies, the study identified and compared three AI models: the Long Short-term Memory (LSTM) Network, Support Vector Machine (SVM), and Random Forest (RF), evaluating their performance against key metrics such as Cost Variance Ratio (CVR), Schedule Performance Index (SPI), and Risk Response Efficiency (RRE). The research methodology encompassed data collection and preprocessing, model training and validation, and comprehensive performance assessment. The findings revealed that the LSTM model achieved a minimum CVR of 1.1%, a maximum SPI of 1.03, and a remarkable risk response efficiency as fast as 12.5 minutes. The study concludes that LSTM significantly enhances the precision of engineering cost risk warnings, offering a novel insight and robust technical foundation for project management in the construction industry.",Yes,"본 논문은 AI 모델을 활용하여 공학 비용 위험 관리를 개선하는 독창적인 연구를 수행하였으며, LSTM, SVM, RF 모델의 성능을 비교 평가하는 실험적 방법론과 결과를 제시하고 있다. 이는 기존 연구를 단순히 요약한 것이 아니라 새로운 데이터 분석과 모델 적용을 통해 직접적인 연구 기여를 포함하고 있음을 보여준다."
Advancements in Waste Segregation Through Machine Learning and Integrating AI for Sustainable Waste Management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882258,"Management of waste is very important for sustainable development, mostly in urban areas where population growth and industrial activity are on the rise leading to the generation of huge volume. Proper segregation and collection of waste are essential because it helps in recycling and reduce the amount that piles up as a dump, thus saving several resources. This paper reviews the state of the art in waste segregation methodologies and how different approaches are employed to segregate wastes using machine learning algorithms and robotics use-case scenarios with Internet of Things (loT) devices. This paper offers a systematic review synthesising the key elements of segregation in recent literature, comparing datasets for research and examining various ways to measure it. In addition, we suggest improvements to existing systems by the incorporation of multi-modal data processing and real-time adaptive methods. Our results indicate that there have been some major advances in terms of graph processing, but building and deploying a real-time system is still too difficult. The paper concludes with recommendations for future research directions, emphasizing the need for more robust datasets and the integration of AI-driven adaptive systems for dynamic waste management.",No,"본 논문은 기존 연구들을 종합하고 비교하는 체계적 문헌 리뷰(review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 향후 연구 방향을 제안하는 논문에 해당합니다."
Deep Learning in the Fast Lane: A Survey on Advanced Intrusion Detection Systems for Intelligent Vehicle Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10582439,"The rapid evolution of modern automobiles into intelligent and interconnected entities presents new challenges in cybersecurity, particularly in Intrusion Detection Systems (IDS) for In-Vehicle Networks (IVNs). This survey paper offers an in-depth examination of advanced machine learning (ML) and deep learning (DL) approaches employed in developing sophisticated IDS for safeguarding IVNs against potential cyber-attacks. Specifically, we focus on the Controller Area Network (CAN) protocol, which is prevalent in in-vehicle communication systems, yet exhibits inherent security vulnerabilities. We propose a novel taxonomy categorizing IDS techniques into conventional ML, DL, and hybrid models, highlighting their applicability in detecting and mitigating various cyber threats, including spoofing, eavesdropping, and denial-of-service attacks. We highlight the transition from traditional signature-based to anomaly-based detection methods, emphasizing the significant advantages of AI-driven approaches in identifying novel and sophisticated intrusions. Our systematic review covers a range of AI algorithms, including traditional ML, and advanced neural network models, such as Transformers, illustrating their effectiveness in IDS applications within IVNs. Additionally, we explore emerging technologies, such as Federated Learning (FL) and Transfer Learning, to enhance the robustness and adaptability of IDS solutions. Based on our thorough analysis, we identify key limitations in current methodologies and propose potential paths for future research, focusing on integrating real-time data analysis, cross-layer security measures, and collaborative IDS frameworks.",No,"본 논문은 최신 침입 탐지 시스템에 대한 심층적인 조사와 분류를 제공하는 서베이(조사) 논문으로, 기존 연구들을 종합하고 분석하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문에 해당하지 않습니다."
Analyzing and assessing the security-related defects,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542332,"The use of the Internet has become an integral part of everyone's life. Due to this, the introduction of virus and other malicious crackers is increasing everyday. This in turn leads to the introduction of defects which adversely affect the security. Thus, protecting vital information in this cyber world is not an easy task. We need to deal with security related defects to ensure failure free and smooth functioning of the software. Thus, in this paper, we intend to study and analyze various aspects of security-related defects by analyzing the defect reports available in various open-source software repositories. Besides this, prediction models can also be constructed which can be used by researchers and practitioners to predict various aspects of security - related defects. Such prediction models are especially beneficial for large-scale systems, where testing experts need to focus their attention and resources to the problem areas of the system under development. Thus, application of software prediction models in the early phases of the software life cycle contributes to efficient defect removal and results in delivering more reliable and better quality software products. Empirical studies lack the use of proper research methodology and thus result in reporting inconsistent results. This study will review the sequence of steps followed in the research process for carrying empirical and replicated studies. The steps include (a) literature survey and definition of variables (b) data collection (c) report findings using statistical and machine learning techniques (d) analyzing performance measures for evaluating the performance of the predicted models and (e) interpretation of the obtained results for developing a software prediction model. These steps are explained with the help of experimental public domain data set. In addition, the paper provides an overview of repositories for mining software engineering data, tools for analyzing this data and various categories of machine learning methods. It also discusses existing research avenues and provides future research directions in this area.",Yes,"논문 초록에서 보안 관련 결함에 대한 분석과 예측 모델 구축을 위한 연구 방법론을 제시하고 있으며, 공개된 데이터셋을 활용한 실험적 연구를 수행하고 있음을 명확히 밝히고 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 나타냅니다."
A Survey of Machine Learning Applications for Power System Analytics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8783340,"Recent advances in computing technologies and the availability of large amounts of heterogeneous data in power grids are opening the way for the application of state-of-art machine learning techniques. Compared to traditional computational approaches, machine learning algorithms could gain an advantage from their intrinsic generalization capability, by also providing accurate short-term power flow forecasts from distributed measurement units, with greater computational efficiency and scalability. Several studies in the literature investigated the use of suitable machine learning models to address different issues in the field of power grid operation and management. Furthermore, the ongoing transition towards smart grids is generating new research opportunities for the real-time application of machine learning algorithms in power systems. In this paper, a literature survey on the application of different machine learning techniques in power systems is presented and critically reviewed, by evaluating the main advantages and criticalities of each technique for the selected applications.",No,"본 논문은 다양한 머신러닝 기법의 전력 시스템 적용에 대한 문헌 조사를 수행한 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합하고 비평하는 리뷰 논문에 해당합니다."
A Framework for Explainable Text Classification in Legal Document Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005659,"Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Over the past two decades, attorneys have been using a variety of technologies to conduct this exercise, and most recently, parties on both sides of the `legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, text classification also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a black box Put simply, very little information is provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In legal `document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if text classification can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, text classification can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.",Yes,"논문은 법률 문서 검토에서 설명 가능한 텍스트 분류를 위한 프레임워크를 제안하고, 실제 법률 데이터에 적용한 실험 결과를 보고하고 있어 독창적인 연구 내용과 실험적 기여가 포함되어 있다. 따라서 연구 논문에 해당한다."
"Green Internet of Vehicles (GIoV): Applications, Awareness, Technologies and Challenges",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10245493,"Internet of Vehicle (IoV) set the key pillar for fifth generation (5G) automobiles. This is possible due to master stroke played by the Internet of Things (IoT), which provides the smartness in all devices i.e. ahead of 5G (A5G) networks. A revolution is started in electrical power and energy techniques (EPETs) to provide spotless scattered energy for justifiable global profitable growth. Internet of Things (IoT) is at the head of this makeover convey potentiality, such as synchronized watching capability, circumstantial alertness and intellectual property, manage and cyber security to renovate the existing EPET into smart virtual-enabled EPTS, which is more well-organized, protected, consistent durable and justifiable. Moreover, computerized the electric control ecology with IoT enhances the strength transparency, finest management of scattered generation, vanishes energy wastage, and produce savings. It is evaluated to have 42 billion IoT devices by the year 2025. Presently, carbon releases and electronic dissipate (e-waste) are notable ultimatums in the Information & communication automations in (ICT) sector. The goal of this paper is to furnish insights on green IoV (GIoV) applications, operations, awareness, and ultimatums to a jack-of- all-trades of wireless technologies. We gather various well organized facilitators, architectonics, ecological impacts, mechanizations, energy models, and blueprints, thus a reviewer can find a broad range of GIoV understanding. In this paper, several well organized underlying energy structures, and freeware based data congestions handling techniques are considered as providers of GIoV. Energy replicas of IoV devices are presented in terms of transmissions, motivation process, and stable power dissipation and produced power by harvesting processes for superb power assigning methodologies. Problems related to ever increasing data in IoT networks can be solved by integrating artificial intelligence (AI) along with machine learning (ML) models in IoT web-work. IoT has a major influence on EPETs and suggest various chances for magnification and progress. There are numerous ultimatums with the disposal of IoT for EPETs. Feasible solutions entail to be flourished to conquer these ultimatums to guarantee pursued growth of IoT for EPETs. The improvements in computing intellect potentials can open out an intuitive IoT structure by reproducing biological neurotic structures with empiric computing, pouring and distributed analysis counting at the border and tool levels. This assessment paper furnishes an analysis of the role, influences and provocations of IoV in convert electric powered and vitality systems.",No,"초록에서 본 논문은 Green Internet of Vehicles (GIoV)의 응용, 인식, 기술 및 도전과제에 대한 종합적인 리뷰 및 평가를 제공하는 평가(assessment) 논문임을 명확히 하고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구들을 정리하고 분석하는 리뷰 논문에 해당합니다."
Review of Deep Learning Techniques for Improving the Performance of Machine Reading Comprehension Problem,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121015,"The amazing research of Artificial Intelligence is natural language processing (NLP) and the mesmerizing field in NLP is machine reading comprehension (MRC). MRC alleviates the efforts of making machines behave like a human as it helps information accessing in natural language by developing Question answering systems. MRC is summarized as a task to read a piece of text, understand it, and answer the related question of the text. Reading text can be cloze style reading (fill in the blanks from the text) as well as open style reading (separate question) and understanding the piece of text as well as the query is accomplished by contextual representation and Attention mechanism. In the MRC literature, various methodologies have been used for extracting answers from the given text including primitive methods to the deep learning methods to have a step towards deploying machine intelligence. The introduction of deep learning and large datasets in the recent few years has encouraged the success of MRC. This paper gives a recent review of MRC models based on deep learning, datasets on which they have been evaluated, and also their word representations.",No,"본 논문은 최신 딥러닝 기반 기계 독해 모델과 데이터셋, 단어 표현 방법에 대한 리뷰를 제공하는 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 정리한 문헌 고찰 논문이다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Quantitative Measurement of Bias in AI-Generated Content: A Comprehensive Narrative Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10732696,"Corporations, institutions, and individuals increasingly use artificial intelligence (AI) to make decisions and predictions that shape many aspects of human lives. Furthermore, individuals and organizations use AI to generate articles, blog posts, social media posts, or books, which can be substantial in various scenarios. However, this AI-generated content is often subject to biases that have unintended consequences on human lives during decision-making. The ability of AI models and systems to sustain and augment biases is a growing issue. Therefore, this literature examines how bias in AI-generated content can impact society, the sources of bias, and the quantitative methods used for measuring the bias. The literature also reviews real-world scenarios where quantitative bias measures in AI-generated content have been implemented successfully.",No,"본 논문은 AI 생성 콘텐츠의 편향에 관한 기존 연구들을 종합적으로 검토하는 문헌 리뷰로, 직접적인 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Recent Research on AI in Games,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148327,"Games tend to have the properties of vast state space and high complexity, making them excellent benchmarks for evaluating various techniques, including AI ones. Techniques utilized in games capable of making them more attractive, immersive, smarter etc. can all be considered to be certain forms of game AI. Considering there are few reviews on the more recent work in the game AI field from the perspective of essential applications, in this paper, we make a systematic review of typical research from 2018 on three application fields of game AI: believable agents in non-player characters research, game level generation in procedural content generation, and player profiling in player modeling. We also provide a timeline of game AI history to give the readers a clearer picture of the game AI field. Moreover, general game AI and hybrid intelligence for games are discussed.",No,"초록에서 본 논문은 2018년 이후의 게임 AI 분야 연구를 체계적으로 리뷰하는 논문으로, 기존 연구들을 종합하고 정리하는 데 중점을 두고 있다. 따라서 직접적인 독창적 연구 결과나 새로운 기여를 포함한 연구 논문이라기보다는 리뷰 논문에 해당한다."
The Role of Artificial Intelligence in Future Rehabilitation Services: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015010,"Artificial intelligence technologies are considered crucial in supporting a decentralized model of care in which therapeutic interventions are provided from a distance. In the last years, various approaches have been proposed to support remote monitoring and smart assistance in rehabilitation services. Comprehensive state-of-the-art of machine learning methods and applications is presented in this review. Following PRISMA guidelines, a systematic literature search strategy was led in PubMed, Scopus, and IEEE Xplore databases. The search yielded 519 records, resulting in 35 articles included in this study. Supervised and unsupervised machine learning algorithms were identified. Unobtrusive capture motion technologies have been identified as strategic applications to support remote and smart monitoring. The main tasks addressed by algorithms were activity recognition, movement classification, and clinical status prediction. Some authors evidenced drawbacks concerning the low generalizability of the results retrieved. Artificial intelligence-based applications are likely to impact the delivery of decentralized rehabilitation services by providing broad access to sustained and high-quality therapy. Future efforts are needed to validate artificial intelligence technologies in specific clinical populations and evaluate results reliability in remote conditions and home-based settings.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)로, 기존 연구들을 종합하여 인공지능 기술의 재활 서비스 적용 현황을 정리한 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고, 기존 연구들을 분석하는 데 중점을 두고 있습니다."
A Systematic Literature Review of Intelligent Tutoring Systems With Dialogue in Natural Language,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186073,"Intelligent tutoring systems (ITSs) are computer programs that provide instruction adapted to the needs of individual students. Dialog systems are computer programs that communicate with human users by using natural language. This paper presents a systematic literature review to address ITSs that incorporate dialog systems and have been implemented in the last twenty years. The review found 33 ITSs and focused on answering the following five research questions. a) What ITSs with natural language dialogue have been developed? b) What is the main purpose of the tutoring dialogue in each system? c) What are the pedagogical features of the teaching process performed by the ITSs with natural language dialogue? d) What natural language understanding approach does each system employ to understand students’ utterances? e) What evidence exists related to the evaluation of ITSs with natural language dialogue? The results of this review reveal that most ITSs are directed toward science, technology, engineering, and mathematics (STEM) domains at the university level, and the majority of the selected ITSs implement the expectations and misconceptions tailored approach. Furthermore, most ITSs use dialog to help students learn how to solve a problem by applying rules, laws, etc. (the apply level in Bloom’s taxonomy). With regard to the instructional approach, the selected ITSs help students write correct explanations or answers for deep questions; assist students in problem solving; or support a reflective dialogue motivated by either previously provided content or the result of a simulation. Additionally, we found empirical evaluations for 90.91% of the selected ITSs that measure the learning gains and/or assess the impacts of different tutoring strategies.",No,"이 논문은 지난 20년간 발표된 지능형 튜터링 시스템에 관한 문헌을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 평가에 초점을 맞추고 있다."
Reviewing Fault Diagnosis Methods in Electric Drives: Power Subsystem and Electrical Machine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225122,"This paper provides a review of popular fault diagnosis methods in the power subsystem and electrical machine of electric drives. The review aims to explore the different approaches used to detect faults in these components and the most common types of faults encountered in electric drives. The paper first provides an overview of the power subsystem and electrical machine and their respective functions within the electric drive system. It then proceeds to discuss the different types of faults that can occur in these components, including overvoltage, undervoltage, overcurrent, and short circuits. The review evaluates the various fault diagnosis methods available for these components, including model-based, signal-based, and artificial intelligence-based approaches. Model-based approaches use mathematical models of the system to detect and diagnose faults. Signal-based approaches rely on the analysis of measured signals to detect changes in system behavior that indicate a fault. Artificial intelligence-based approaches use machine learning algorithms to identify patterns in data and detect faults. Model-based approaches are highly accurate but can be computationally intensive. Signal-based approaches are less computationally intensive but may be less accurate in certain situations. Artificial intelligence-based approaches are versatile but require significant amounts of data for training. This review provides a comprehensive overview of the current state-of-the-art in fault diagnosis methods for the power subsystem and electrical machine of electric drives. The review concludes by suggesting potential areas for future research to further improve the accuracy and efficiency of fault diagnosis methods for electric drives.",No,초록에서 이 논문은 기존의 고장 진단 방법들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 기여를 포함한 연구 논문으로 보기 어렵습니다.
Current Research Themes and Future Research Needs on Making AI's Energy Consumption Efficient: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10775966,"While Artificial Intelligence (AI) offers significant benefits, it also poses challenges to environmental sustainability due to increased energy consumption and resulting carbon emissions. Consequently, optimizing AI's energy efficiency has become a critical priority in the scientific community. This article aims to explore current research on enhancing AI's energy efficiency by reviewing 32 relevant literatures. The current themes include the general relevant topic, the impact of AI on the environment, the tools, and the method to reduce carbon emissions. Key areas for future research include comprehensive carbon emission calculations for all hardware configurations, standardizing methodologies for assessing carbon footprints, evaluating carbon emissions across different AI algorithms, developing energy-efficient models for AI systems amidst increasing device penetration, establishing a taxonomy for measuring AI infrastructure energy efficiency applicable across diverse urban settings, and improving energy efficiency in accelerator components such as GPUs, CPUs, and FPGAs.",No,"본 논문은 32편의 기존 문헌을 검토하여 AI 에너지 효율성 관련 연구 동향과 향후 연구 필요성을 제시하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않고, 기존 연구를 종합 분석하는 데 중점을 두고 있습니다."
Advanced Techniques in Semiconductor Defect Detection and Classification: Overview of Current Technologies and Future Trends in AI/ML Integration,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696150,"This review evaluates advancements and future trends in semiconductor defect detection methods, which are critical for enhancing electronic components' efficiency and reliability. As semiconductor devices shrink and become more complex, the accuracy of defect detection becomes crucial. This paper traces the evolution from manual inspections to the use of advanced technologies such as automated vision systems, artificial intelligence (AI), and machine learning (ML). It discusses various defects like crystallographic errors, surface anomalies, and chemical impurities that affect device functionality and longevity, emphasizing the need for precise identification. The shift to ML and deep learning (DL) represents a significant move towards more adaptive, accurate, and faster detection methods. The paper outlines challenges like the miniature scale of modern devices, high costs of advanced imaging technologies, and the need speed in mass production. It identifies a critical gap between current technological capabilities and industry needs, particularly in scalability and processing throughput. Future research directions are suggested to close these gaps, including enhancing AI computational efficiency, developing new materials for better imaging contrast, and integrating these technologies seamlessly into production lines. This synthesis of current technologies and exploration of future trends aims to advance the dialogue and development of more effective defect detection and classification methods, leading to the production of more reliable semiconductor devices.",No,"본 논문은 반도체 결함 검출 기술의 현황과 미래 동향을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않고 기존 연구들을 요약 및 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기술 동향과 향후 연구 방향 제시에 초점을 맞추고 있습니다."
Integrating Safety in VANETs: A Taxonomy and Systematic Review of VEINS Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10710343,"Vehicular Ad-Hoc Networks (VANETs) play an essential role in road safety through Vehicle-to-Vehicle Communications and Vehicle-to-Infrastructure communications. In this paper, we offer a survey of the state-of-the-art literature about VEINS tool set which is an extraordinary vehicle and network simulation framework for VANET researchers. This paper identifies and classifies the existing research into a comprehensive taxonomy that we called Applications, Solutions and Networks so as to provide an organized survey of safety-related VEINS-based literature. This review can be used by the research community to understand where gaps exist in the literature of particular real-world applicability or integration with emerging technologies, as well as socio-economic factors associated with deployment of VEINS-based safety applications. In addition, we evaluate the VEINS framework including what it provides (e.g., better simulation accuracy), safety testing is more comprehensive as well as other technologies that are used may have like in our case 5G and AI. We also refer to its affordability, growth-ability and adaptiveness along with real-time data analytics commented the author. The constraints of VEINS are also discussed, for instance the bridge from simulation to reality, computational complexity and problems with regard to emerging technologies integration. We discuss future research directions to further enrich the potential of VEINS by incorporating Beyond-5G technologies, cutting-edge AI algorithms, blockchain (BC) for communication security and reliability, semi-virtual/hybrid simulation environments and a wider range of V2X communications. It includes case studies and applications which illustrate how general safety scenarios (e.g., collision avoidance, emergency vehicle prioritization), road hazard detection can be simulated using VEINS highlighting the importance of this approach for practicing engineers. In all, the wide-ranging review would be useful to researchers and practitioners working toward ensuring a secure and efficient vehicular network design.",No,"본 논문은 VEINS 시뮬레이션 도구를 활용한 기존 연구들을 체계적으로 분류하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 문헌의 종합과 분석에 중점을 두고 있습니다."
Harnessing Transformers for Detecting Adverse Drug Reaction and Customized Causality Explanation using Generative AI,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392249,"Pharmacovigilance plays an important role in monitoring safety of pharmaceutical products. There is an abundance of unstructured data available in social media and online reviews that can provide valuable insights into adverse drug reaction (ADR). This research paper aims to explore the efficient techniques of pharmacovigilance in identifying ADRs from unstructured data available in social media and reviews. This research work proposes the use of a transformer-based model for ADR classification, followed by named entity recognition (NER) to identify the causality between drugs and adverse effects. Furthermore, the output of NER is provided as a customized prompt to the generative AI model to generate a user interpretable explanation. Enhancing the accuracy of NER is carried out using the SciBERT model, which is specifically designed to capture drug and effect names. By utilizing this combination of techniques, the research aims to improve the efficiency and effectiveness of pharmacovigilance processes, enabling earlier detection and reporting of ADRs. To our knowledge, previous works have not explored the application of generative AI based customized explanation of causality. Patients, healthcare professionals, regulatory agencies, and pharmaceutical companies can benefit from the timely and accurate identification of ADRs, allowing for improved drug safety monitoring, better decision-making regarding drug usage, and the development of proactive measures to mitigate risks.",Yes,"본 논문은 transformer 기반 모델과 SciBERT를 활용한 ADR 분류 및 인과관계 인식, 그리고 생성형 AI를 이용한 맞춤형 설명 생성 기법을 제안하는 독창적인 연구 내용을 포함하고 있다. 기존 연구와 차별화된 새로운 방법론을 제시하며, 실제 약물 부작용 탐지 및 설명에 기여하는 연구 논문으로 판단된다."
An Intelligent Semantic Vector Search Model for Grading and Assessing Students,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500811,"The introduction of technology-based learning is attracting considerable attention. With its ability to train learners, and assess and grade their academic work, this platform is a critical component of educational teaching and learning. Most importantly, the learning process is dependent on the human cognitive process in terms of how we learn, perceive, think, and interpret information. Therefore, technology-based teaching and learning is a crucial aspect of Artificial Intelligence (AI) technology. As AI evolves, researchers are producing innovative works based on AI. However various data search-retrieval mechanisms in instructional teaching models are struggling to comprehend and understand both structured and unstructured data as most data stores are conversant with structured data. The need for unstructured data as a component of decision-making is growing rapidly. In support of this, such data must be available in the appropriate structured format. In this study, we examine models of cognitive performance using semantic vector search on learner's psychology, and how the two relate throughout the learning process. We comparatively evaluated the application of two core artifacts: context similarity and content awareness. By comparative analysis, the result suggests that semantic search with content similarity and awareness in vector databases ensures an adaptive and intelligent search retrieval. This approach is considered the best for grading and assessing academic work while improving intuitive cognitive abilities.",Yes,"논문 초록에서 제시된 연구는 학습자의 심리와 인지 성과를 분석하기 위해 의미 벡터 검색 모델을 개발하고, 이를 학습 평가 및 채점에 적용하는 새로운 방법론을 제안하고 있습니다. 이는 기존 연구와 차별화된 독창적인 AI 기반 교육 평가 모델을 제시하는 연구 논문으로 판단됩니다."
Botnet Detection: A Review of Machine Learning and AI Strategies,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724496,"Botnets, networks of infected devices controlled by malicious actors, pose a significant threat to cybersecurity. The sophistication of botnet attacks has escalated, making their detection increasingly challenging. This review paper delves into the realm of machine learning (ML) and artificial intelligence (AI) strategies for botnet detection. It presents an analysis of the evolution of botnets and the corresponding development in detection techniques. The paper categorizes various ML and AI methodologies, examining their effectiveness, adaptability, and efficiency in identifying botnet activities. Key strategies include supervised and unsupervised learning, deep learning, and reinforcement learning, each tailored to specific aspects of botnet behavior. The review also highlights the challenges and limitations of current approaches, such as the need for extensive datasets, the dynamic nature of botnets, and the trade-off between accuracy and computational efficiency. The paper concludes with future research directions, emphasizing the integration of AI with other cybersecurity measures to enhance the robustness and resilience of botnet detection systems. This comprehensive overview aims to provide insights into the current state of botnet detection and encourage further advancements in this critical field of cybersecurity.",No,"초록에서 본 논문은 기존 연구들을 종합하고 분석하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않고, 기존 연구 동향과 방법론을 정리하는 데 중점을 두고 있습니다."
Harnessing Machine Learning for APTs Detection and Mitigation in Large-Scale Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829149,"A significant and ongoing cyber security concern is the presence of Advanced Persistent Threats (APTs), especially in large-scale networks. APTs are known for their lengthy lifespan, stealth, and sophistication. As a result, they frequently elude conventional security measures, calling for more flexible and durable solutions. Machine learning (ML) has become a potent instrument for improving these sophisticated threats’ detection and response. This review paper investigates the use of machine learning (ML) to counteract advanced persistent threats (APTs), emphasizing critical techniques including anomaly detection, behavioural analysis, and threat intelligence integration. We investigate how successfully different machine learning algorithms-such as deep learning models, supervised, unsupervised, and reinforcement learning approaches-identify and react to APT activities. The paper also discusses important obstacles to overcome when using machine learning (ML) for APT detection, such as problems with data quantity and quality, attacker evasion strategies, scalability issues, and more. We highlight the advantages and disadvantages of existing MLbased techniques in practice using real-world examples. The report also addresses future areas for research, highlighting the necessity of improved model robustness, adaptive learning capabilities, and cooperative efforts to remain ahead of developing cyber threats. With an emphasis on both the state of the art and potential directions for future research, this review seeks to give readers a thorough grasp of how machine learning (ML) can be used to fortify cyber security defences against advanced persistent threats (APTs) in large-scale network systems.",No,"본 논문은 머신러닝을 활용한 APT 탐지 및 대응 기법에 대한 리뷰 논문으로, 기존 연구들을 종합하고 분석하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여가 포함되어 있지 않아 연구 논문으로 보기 어렵습니다."
AI-Blockchain Systems in Aerospace Engineering and Management: Review and Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983700,"Artificial Intelligence (AI) and Blockchain technologies are increasingly used for various industrial applications. AI offers self-learning capabilities that can be used to automate certain industrial applications. On the other hand, blockchain provides decentralized system capabilities to ensure transaction security, transparency, and auditability. The aerospace industry is struggling with several challenges that the above-mentioned technologies can help resolve. This paper adopts an exploratory literature review to evaluate potential use cases for AI and Blockchain in aerospace engineering and management. This research shows that blockchain technologies can help with records management especially concerning aircraft maintenance, repair, and overhaul operations. In addition, AI provides significant capabilities in predictive fault detection, guidance systems, and optimization of flight paths. Nevertheless, most of these capabilities still require experimental validation and compliance with regulatory requirements.",No,"본 논문은 AI와 블록체인 기술의 항공우주 공학 및 관리 분야 적용 가능성을 문헌 리뷰 방식으로 탐색한 연구로, 직접적인 실험이나 독창적인 연구 결과를 제시하지 않고 기존 연구들을 종합하여 논의하고 있다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Analysis of Deep Neural Networks for Human Activity Recognition in Videos—A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530410,"From the past few decades, Human activity recognition (HAR) is one of the vital research areas in computer vision in which much research is ongoing. The researcher's focus is shifting towards this area due to its vast range of real-life applications to assist in daily living. Therefore, it is necessary to validate its performance on standard benchmark datasets and state-of-the-art systems before applying it in real-life applications. The primary objective of this Systematic Literature Review (SLR) is to collect existing research on video-based human activity recognition, summarize, and analyze the state-of-the-art deep learning architectures regarding various methodologies, challenges, and issues. The top five scientific databases (such as ACM, IEEE, ScienceDirect, SpringerLink, and Taylor & Francis) are accessed to accompany this systematic study by summarizing 70 different research articles on human activity recognition after critical review. Human activity recognition in videos is a challenging problem due to its diverse and complex nature. For accurate video classification, extraction of both spatial and temporal features from video sequences is essential. Therefore, this SLR focuses on reviewing the recent advancements in stratified self-deriving feature-based deep learning architectures. Furthermore, it explores various deep learning techniques available for HAR, challenges researchers to face to build a robust model, and state-of-the-art datasets used for evaluation. This SLR intends to provide a baseline for video-based human activity recognition research while emphasizing several challenges regarding human activity recognition accuracy in video sequences using deep neural architectures.",No,"본 논문은 기존 연구들을 체계적으로 수집, 요약, 분석하는 체계적 문헌 리뷰(Systematic Literature Review)로서, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하는 데 목적이 있다. 따라서 새로운 연구 기여를 포함한 연구 논문으로 보기 어렵다."
Predictive Maintenance in Healthcare System: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155123,"Medical devices are a vital component of healthcare systems, the advantages they may give continue to grow as they are crucial for the safe and effective prevention, diagnosis, treatment, and rehabilitation of illnesses and diseases. Therefore, it is critical to maintain them in good operating order to ensure optimum availability, minimal failures, and guarantee patients’ and users’ safety. The stages involved in medical devices regulation and management are complex, but they are necessary to ensure their quality, safety, and compatibility with the settings in which they are used. Medical equipment complexity has increased due to technological advancement and the traditional maintenance strategies do not meet the needs of today’s healthcare organizations. Therefore, integrating information technology, social networking technologies, digitization and management of medical devices, and the use of big data technologies and Machine Learning (ML) techniques has the potential to significantly improve healthcare services. Integrating autonomous and intelligent systems where data and sophisticated data analytics may be employed led to enhanced equipment data collecting via the deployment of information and communication technologies, notably intelligent devices. With this advancement came an increase in Predictive Maintenance (PdM) solutions. PdM has become a commonly used approach, described as a set of procedures used to evaluate the condition of equipment and predict future failures. These estimations are then utilized to schedule maintenance activities through smart scheduling of maintenance procedures, which aids in preventing or at least minimizing the impacts of unanticipated failures. The purpose of this article is to present a Systematic Literature Review (SLR) exploring and reviewing prior research on the subject of PdM and the developments of this method, particularly in the medical field. In addition to supporting new research projects in the PdM sector, this paper offers a good foundation for understanding PdM approaches, their key findings, problems, and potential. This review focuses on two scientific databases from which a substantial number of articles dedicated solely to PdM in the medical field have been retrieved for analysis. Our research led us to conclude that, despite the many potential benefits of predictive maintenance in the medical field, the concept is still being under-exploited and faces many obstacles.",No,"본 논문은 Predictive Maintenance에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰(SLR) 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 초점을 맞추고 있습니다."
"Online Extremism Detection: A Systematic Literature Review With Emphasis on Datasets, Classification Techniques, Validation Methods, and Tools",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383220,"Social media platforms are popular for expressing personal views, emotions and beliefs. Social media platforms are influential for propagating extremist ideologies for group-building, fund-raising, and recruitment. To monitor and control the outreach of extremists on social media, detection of extremism in social media is necessary. The existing extremism detection literature on social media is limited by specific ideology, subjective validation methods, and binary or tertiary classification. A comprehensive and comparative survey of datasets, classification techniques, validation methods with online extremism detection tool is essential. The systematic literature review methodology (PRISMA) was used. Sixty-four studies on extremism research were collected, including 31 from SCOPUS, Web of Science (WoS), ACM, IEEE, and 33 thesis, technical and analytical reports using Snowballing technique. The survey highlights the role of social media in propagating online radicalization and the need for extremism detection on social media platforms. The review concludes lack of publicly available, class-balanced, and unbiased datasets for better detection and classification of social-media extremism. Lack of validation techniques to evaluate correctness and quality of custom data sets without human interventions, was found. The information retrieval unveiled that contemporary research work is prejudiced towards ISIS ideology. We investigated that deep learning based automated extremism detection techniques outperform other techniques. The review opens the research opportunities for developing an online, publicly available automated tool for extremism data collection and detection. The survey results in conceptualization of architecture for construction of multi-ideology extremism text dataset with robust data validation techniques for multiclass classification of extremism text.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 비교하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 한계점 분석에 중점을 두고 있습니다."
Using Artificial Intelligence and Mathematical Modeling for Advancement of Gold Nanotechnology in Therapeutic Biophysics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9587879,"We will examine and critique the future uses of artificial intelligence (AI) and mathematical modelling in medical applications, with a particular emphasis on their interactions with gold nanotechnology. There have been significant advances in the use of artificial intelligence and mathematical modelling to medical biophysics. This particular methodology assists with the advancement of nanotechnology-related study projects. There have been many papers on this subject. Now it is time to collaborate and study all of these papers in order to evaluate the progress achieved in nanotechnology as a result. Theoretical and clinical data is reviewed in order to comprehend what is present-day and new. To provide more explanation as to variable interaction, AI and mathematical modelling are used to track the specified parameters and defined equations. This commentary covers the synthesis and production of gold nanoparticles using the Turkevich and Brust and Schiffrin one-pot procedure. Results obtained indicate that the size, shape, and overall functionality of gold nanoparticles directly impact the synthetic characteristics. The light-absorbing, wavelength, and optical density properties of the gold nanoparticle vary based on the features of the gold nanoparticle. Using the appropriate nanoparticle size (depending on the wavelength of light) enables more light absorption inside the nanoparticle. Transmission electron microscopy (TEM) and Fourier transform infrared radiation (FT-IR) spectroscopy are used to examine the cellular uptake and cytotoxicity of nanoparticles. Optimizing nanoparticle efficiency for precision cancer therapy is essential to maximizing treatment effectiveness. Manipulated nano-probes are employed in gold nanoparticle-based therapy in order to control tumour treatment. Nanoparticle sensors have the ability to collect a variety of different images and assists with diagnostics and therapeutic imaging techniques. Direct findings will assist push additional understanding and development in medical biophysics research, using AI and mathematical modelling, in biophysical gold nanoparticle technology applications.",No,"초록 내용은 기존 연구들을 종합하고 평가하는 리뷰 또는 논평 성격으로 보이며, 직접적인 실험 결과나 독창적인 연구 기여가 명확히 제시되어 있지 않습니다. 따라서 본 논문은 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Revolutionizing Healthcare Intelligence Multisensory Data Fusion with Cutting-Edge Machine Learning and Deep Learning for Patients’ Cognitive Knowledge,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616464,"The study explores the recent advancements in healthcare that have witnessed an unprecedented integration of multisensory data, ranging from wearable devices to medical imaging and electronic health records. This paper explores the transformative impact of cutting-edge machine learning algorithms and deep learning algorithms in processing and interpreting this multisensory data, offering unparalleled insights into patients’ cognitive well-being. The review delves into the types and challenges of multisensory data, evaluating performance of supervised and supervised machine learning models for disease detection and pattern recognition respectively. Additionally, it scrutinizes the importance of deep learning applications that are based on convolutional neural networks as well as recurrent neural networks for medical image classification. The paper also mentions the importance of the integration of multisensor data fusion with these advanced algorithms, emphasizing real-time monitoring systems and ethical considerations. Summaries from research data shows effectiveness of remote patient care and predictive analysis and personalized treatment plans. The exploration of future directions and opportunities underscores the prime importance for further advancements in explainable AI, collaborative learning, and edge computing, consolidating the vision of a healthcare paradigm revolutionized by intelligent data fusion and cutting-edge technologies.",No,"초록은 최신 기술과 알고리즘의 적용 현황과 미래 방향을 종합적으로 검토하는 리뷰 논문으로 보이며, 독창적인 연구 결과나 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 직접 기여하는 연구 논문으로 판단하기 어렵습니다."
Bio-signals Collecting System for Fatigue Level Classification*,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340350,"Fatigue is a risk factor that reduces quality of life and work efficiency, and threatens safety in a high-risk environment. However, fatigue is not yet precisely defined and is not a quantified concept as it relies on subjective evaluation. The purpose of this study is to manage risks, improve mission efficiency, and prevent accidents through the development of machine learning and deep learning based fatigue level classifier. Acquiring true fatigue levels to train machine learning and deep learning fatigue classifier may play a fundamental role. Aims of this study are to develop a bio-signal collecting device and to establish a protocol for capturing and purifying data for extracting the true fatigue levels accurately. The bio-signal collection system gathered visual, thermal, and vocal signals at the same time for one minute. The true fatigue level of the subjects is classified through the Daily Multidimensional Fatigue Inventory and physiological indicators related to fatigue for screening the subjective factors out. The generated dataset is constructed as a DB along with the true fatigue levels and is provided to the research institutions. In conclusion, this study proposes a research method that collects bio-signals and extracts the true fatigue levels for training machine learning and deep learning based fatigue level classifier to evaluate the fatigue of healthy subjects in multi-levels.",Yes,"본 논문은 피로 수준 분류를 위한 생체 신호 수집 장치 개발과 데이터 수집 프로토콜 구축이라는 독창적인 연구 내용을 포함하고 있으며, 이를 바탕으로 머신러닝 및 딥러닝 기반 분류기 학습에 필요한 데이터셋을 생성하는 연구를 수행하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Performance Comparison of Machine Learning and Deep Learning While Classifying Driver’s Cognitive State,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721374,"Driver fatigue is a major cause of the road accidents that occur throughout the globe. It has been observed that among total number of accidents, 20% are contributed from driver fatigue. Acknowledging the existing data it is clear that a notification system for driver fatigue is of at most importance. Over the past a large number of strategies have been tested out and among them EEG based systems have shown to be the most accurate and reliable to estimate driver's cognitive state. The direct relation of brain activity to EEG signal explains its high accuracy in a fatigue detection system. Current researches in machine learning as well as deep learning have shown a new perspective in EEG data analysis. This work proposed a highly accurate, EEG based driver fatigue classification system which can reduce the rate of fatigue related road accidents using machine learning and deep learning algorithms. The results showed that the relative power of theta, alpha, beta and delta showed significant correlation to driver fatigue. The selected features were trained and evaluated using 20 well established classifiers in the field of driver fatigue. Among all the classifiers tested, the Fine Tree, Subspace KNN, Fine Gaussian SVM, and Weighted KNN were performed to the highest accuracy levels. Different performance metrics are used for this work and Deep Autoencoder and KNN are identified as the best suitable Deep learning and Machine Learning Algorithms for driver fatigue prediction with an accuracy of 99.7% and 99.6 % respectively.",Yes,"본 논문은 EEG 데이터를 이용한 운전자 피로 분류 시스템을 제안하고, 다양한 머신러닝 및 딥러닝 알고리즘을 적용하여 성능을 비교한 독창적인 연구 내용을 포함하고 있다. 또한, 여러 분류기들의 성능 평가 결과를 제시하며 직접적인 연구 기여를 하고 있음을 알 수 있다."
Effects of annotation granularity in deep learning models for histopathological images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8983158,"Pathological is crucial to cancer diagnosis. Usually, Pathologists draw their conclusion based on observed cell and tissue structure on histology slides. Rapid development in machine learning, especially deep learning have established robust and accurate classifiers. They are being used to analyze histopathological slides and assist pathologists in diagnosis. Most machine learning systems rely heavily on annotated data sets to gain experiences and knowledge to correctly and accurately perform various tasks such as classification and segmentation. Generally, annotations made in pathology-related datasets have inherited annotation methods from natural scene images. This work investigates different granularity of annotations in histopathological data set including image-wise, bounding box, ellipse-wise, and pixel-wise to verify the influence of annotation in pathological slide on deep learning models. We design corresponding experiments to test classification and segmentation performance of deep learning models based on annotations with different annotation granularity. In classification, state-of-the-art deep learning-based classifiers perform better when trained by pixel-wise annotation dataset. On average, precision, recall and F1-score improves by 7.87%, 8.83% and 7.85% respectively. Thus, it is suggested that finer granularity annotations are better utilized by deep learning algorithms in classification tasks. Similarly, semantic segmentation algorithms can achieve 8.33% better segmentation accuracy when trained by pixel-wise annotations. Our study shows not only that finer-grained annotation can improve the performance of deep learning models, but also help they extract more accurate phenotypic information from histopathological slides. The accurate and spatially precise acquisitions of phenotypic information can improve the reliability of the model prediction. Intelligence systems trained on granular annotations may help pathologists inspecting certain regions and features in the slide that were mainly used to calculate the prediction. The compartmentalized prediction approach similar to this work may contribute to phenotype and genotype association studies.",Yes,"본 논문은 다양한 주석 세분화 수준이 딥러닝 모델의 병리학적 영상 분석 성능에 미치는 영향을 실험적으로 평가하는 독창적인 연구를 수행하고 있습니다. 또한, 분류 및 분할 작업에서 주석의 세분화가 모델 성능에 미치는 구체적인 영향을 정량적으로 분석하여 새로운 지식을 제공합니다."
Accounting Information System Adoption Model for Small and Medium-Sized Enterprises in Northern Ghana,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9617111,"Accounting Information Systems (AIS) are essential tools for Small and Medium-sized Enterprises (SMEs). To continuously stay in business, businesses must manage their accounting efficiently. The study will investigate SME AIS adoption in Northern Ghana by critically evaluating different adoption models, as the SMEs are considered as the backbone of the Ghana's economy. Previous research data were gathered using popular educational databases. To collect data and generate research findings about the objective, direct content analysis and summative content analysis were used. According to the review, there have been very few previous studies on SMEs AIS adoption with a maximum dimension of three. They are either technology, organization or environment-based or human, organization, and technology-based. Combining the Technology-Organization-Environment (TOE) and Human-Organization-Technology (HOT-Fit) models yields the Human-Organization-Technology-Environment (HOTE) model. The HOTE model is a new model for SMEs AIS adoption in Ghana. This study fills a gap in the literature by broadening the range of factors influencing SMEs' adoption of AIS.",Yes,본 논문은 기존 연구들을 분석하고 새로운 통합 모델(HOTE 모델)을 제안하여 SMEs의 회계정보시스템 도입에 대한 이해를 확장하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다.
Defect Detection in Industrial Soldering Processes Using Machine Learning: A Critical Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10909447,"As electrical devices take on more life-critical roles, such as in autonomous driving, ensuring the quality of solder joints during production becomes increasingly important. Recently, there has been a growing interest in using machine learning techniques for this purpose. However, current research lacks a comprehensive overview that categorizes and analyzes relevant studies based on their specific intervention points within the production process. This literature review aims to examine and evaluate research coverage along three dimensions: intervention points in the process, non-destructive testing methods, and machine learning techniques employed. For this review, 112 conference papers and journal articles published since 2010 were selected from three databases using the PRISMA methodology. These publications were classified into the three dimensions previously mentioned, summarized, and analyzed. Furthermore, the literature core is critically evaluated to identify research gaps and limitations. The analysis shows that most studies focus on solder joint control, with few addressing intervention points in solder paste and component placement. Visual imaging and neural networks are the dominant techniques for non-destructive testing and machine learning, respectively. Despite a variety of literature that uses high-performance neural networks, meeting industrial detection standards often requires tolerating high false alarm rates. The findings contribute to structuring existing research and identifying research needs, particularly in validating these systems and integrating data from various testing methods and intervention points.",No,"본 논문은 기존 연구들을 분류하고 분석하는 문헌 리뷰(literature review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 현황과 한계를 정리하는 데 중점을 두고 있습니다."
Comprehensive Survey of Event Extraction Methods in Natural Language Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10864060,"Event Extraction (Event Extraction) plays an essential role in NLP (Natural Language Processing) by automating the identification and extraction of events from unstructured textual data. This paper provides a comprehensive overview of event extraction, highlighting its significance in various domains such as security, law, and financial analysis. The paper categorizes event extraction into closed-domain and open-domain approaches. It also talks about various techniques like argument identification, event trigger identification, as well as event type classification. It explores methodologies including machine learning, pattern matching, & deep learning, emphasizing their application in different domains. Evaluation metrics to evaluate the effectiveness of EE systems are explored, including precision, recall, as well as the F1 score. Through a systematic review of the literature, the paper underscores the advancements, challenges, and future directions in EE, positioning it as a critical component in extracting actionable knowledge from textualsources.",No,"주어진 논문은 이벤트 추출 방법에 대한 포괄적인 개요와 문헌 리뷰를 제공하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 방법론 제시는 포함되어 있지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석하는 리뷰 논문에 해당합니다."
AI-Enhanced Firewalls: Empowering Intrusion Detection with ML and DL,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912147,"Given the increasing frequency of network-based threats, strong intrusion detection systems (IDS) are essential for safeguarding digital assets. Traditional rule-based IDS often struggle to keep up with evolving cyber threats, highlighting the need for more adaptive solutions. This study introduces a Next Generation Firewall (NGFW), which integrates machine learning and deep learning techniques to improve IDS performance. By employing ML algorithms like Support Vector Machines (SVM), Random Forests, and Deep Neural Networks (DNN), alongside DL models such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), the NGFW significantly enhances detection capabilities, flexibility, and accuracy. A comprehensive review of the literature demonstrates the effectiveness of ML and DL in IDS. The proposed approach involves systematic data collection, preprocessing, feature extraction, and model evaluation using metrics such as accuracy, precision, and recall. Experimental results show that CNNs achieve particularly low false-positive rates when detecting malicious activities. Additional real-world testing further validates the NGFW’s role in strengthening cybersecurity defenses. The classification report achieved high precision, recall, and F 1 -scores above 97 percent across all categories, with an overall accuracy of 97.29 percent. These findings advocate for further research into intelligent cyber defenses, emphasizing the value of ML and DL in developing more adaptable and resilient IDS.",Yes,"논문 초록에서 ML과 DL 기법을 활용한 차세대 방화벽(NGFW)을 제안하고, 데이터 수집부터 모델 평가, 실험 결과까지 직접 수행한 연구임을 명확히 밝히고 있습니다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문임을 의미합니다."
Olive Disease Classification Based on VGGNET and Fine-Tuning Process,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10708475,"Agriculture serves as a fundamental pillar of Tunisia’s economy, providing employment and livelihood for a large segment of the population, thereby making it an essential sector for the country. Nevertheless, the sector faces challenges such as reduced agricultural output and quality due to factors like pesticide usage, water scarcity, climate change, and notably, plant diseases. These plant diseases represent one of the most significant threats to global agricultural productivity and quality. Traditional methods of plant disease detection heavily rely on expert identification, which can quickly cause delays in field management and crop disease control. The best strategy to manage these diseases and reduce crop loss is to put protective measures in place as soon as possible. To this purpose, machine learning and deep learning models-based method for the detection and classification of olive leaf disease is developed in order to improve the speed and precision of disease classification. In this study, we explore how machine learning and deep learning techniques may be used to improve agricultural practices and solve these issues. We specifically review the state-of-the-art literature on the use of deep learning and machine learning algorithms in diverse agricultural applications. We also presented a novel method for categorizing olive leaf diseases, by integrating deep learning architectures, VGG16, and fine tuning. Also, in this approach, we simulated a total of two distinct deep models utilizing pre-trained convolutional neural network architectures (VGG19 and VGG16) and along with four machine learning classifiers (RF, SVM, KNN and DT). In addition, we simulated a novel method for categorizing olive leaf diseases, by integrating deep learning architectures, VGG16, and fine tuning, our results findings showed that VGG16 with fine-tuning classifier is the best method for classification of healthy and diseased plant leaves. It achieves a testing accuracy score of 98% to beat the rest of the architectures.",Yes,"논문은 올리브 잎 질병 분류를 위해 VGG16과 VGG19 기반의 딥러닝 모델과 머신러닝 분류기를 활용한 새로운 방법을 제안하고, 실험을 통해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Automated Indicator of Atrial Fibrillations Risk Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799108,"Atrial fibrillation (AF) is a multifactorial arrhythmia linked to common cardiovascular diseases associated with classical cardiovascular risk factors. Although awareness and enhanced detection of AF have improved over the past decade as the incidence and prevalence of AF increases, the trends of applying machine learning techniques in the diagnosis still lack precision. This paper includes a review of the literature on the most common ML algorithms implemented to detect the risk of atrial fibrillation (AF). In this review, studies of AF published in academic journals between 2016 and 2021 will be evaluated to analyze the application strategies, the adopted techniques in the field, and the investigated research issues related to applying ML algorithms. The results of the review indicated that great progress has been made in terms of applying ML algorithms have been procured; however large variation can be seen between studies and countries. It was also found that most studies focused on further developing the identification, prevention, and risk separation of AF using machine learning techniques. Additionally, studies have indicated that technological and methodological advances in AF diagnosis made great progress throughout the years helping to prevent future prevention. The CNN algorithm has the main impact for the automated atrial fibrillation indicator using machine learning, as it has proven to be the most effective in terms of detection. Consequently, this research paper will review the most common ML algorithms in the research field, their characteristics, datasets, tools, and techniques used to distinguish atrial fibrillation risks using ML algorithms.",No,초록에서 본 논문은 기존 연구들을 종합하여 머신러닝 알고리즘의 적용 현황과 특징을 리뷰하는 문헌고찰(review)임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하는 연구 논문에 해당하지 않습니다.
Ethical and Sustainability Considerations for Knowledge Graph based Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939282,"Artificial Intelligence (AI) and Machine Learning (ML) are becoming common in our daily lives. The AI-driven processes significantly affect us as individuals and as a society, spanning across ethical dimensions like discrimination, misinformation, and fraud. Several of these AI & ML approaches rely on Knowledge Graph (KG) data. Due to the large volume and complexity of today's KG-driven approaches, enormous resources are spent to utilize the complex AI approaches. Efficient usage of the resources like hardware and power consumption is essential for sustainable KG-based ML technologies. This paper introduces the ethical and sustainability considerations, challenges, and optimizations in the context of KG-based ML. We have grouped the ethical and sustainability aspects according to the typical Research & Development (R&D) lifecycle: an initial investigation of the AI approach's responsibility dimensions; technical system setup; central KG data analytics and curating; model selection, training, and evaluation; and final technology deployment. We also describe significant trade-offs and alternative options for dedicated scenarios enriched through existing and reported ethical and sustainability issues in AI-driven approaches and research. These include, e.g., efficient hardware usage guidelines; or the trade-off between transparency and accessibility compared to the risk of manipulability and privacy-related data disclosure. In addition, we propose how biased data and barely explainable AI can result in discriminating ML predictions. This work supports researchers and developers in reflecting, evaluating, and optimizing dedicated KG-based ML approaches in the dimensions of ethics and sustainability.",No,"본 논문은 KG 기반 머신러닝의 윤리적·지속가능성 고려사항과 관련된 개념적 논의와 가이드라인을 제시하고 있으며, 구체적인 실험 결과나 새로운 알고리즘 개발 등 직접적인 연구 기여 내용은 포함하지 않은 것으로 보입니다. 따라서 독창적인 연구 결과를 담은 연구 논문으로 판단하기 어렵습니다."
DCGAN for Synthetic Data Augmentation of Cervical Cancer for Improved Cervical Cancer Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482312,"Medical diagnosis and treatment are greatly aided by biomedical image analysis. Deep learning model training is difficult, nevertheless, due to the scarcity of labelled medical images. For creating synthetic biological images to supplement the training data, Generative Adversarial Networks (GANs) have shown good promise. In this research paper, we focus on the application of GANs for biomedical image augmentation. Specifically, we investigate and compare the performance of a prominent GAN architecture: Deep Convolutional GAN (DCGAN) is a variant of GAN specifically designed for image generation tasks. We assess the generated images based on their quality, diversity, and preservation of biomedical aspects using different evaluation metrics. We also used a classification process to compare the classifier on real and synthetic augmented data. Our test findings show that DCGAN is capable of producing realistic synthetic biomedical images. The results of this work advance knowledge of GANs for biomedical image enhancement and offer guidance on choosing the DCGAN designs for tasks requiring medical image analysis. Researchers and practitioners can increase the diversity and quantity of training data by utilizing GAN-based augmentation strategies, which will enhance the performance and generalization of deep learning models in biomedical applications.The primary objective of this study is to evaluate the effectiveness of DCGAN in generating synthetic microscopic biomedical images of cervical cancer that can augment the training data for deep learning classification models. We aim to assess the quality, diversity, and preservation of biomedical features in the generated images.The structure of this study is as follows: we begin by brief introduction of Cervical cancer, GAN in general and DCGAN. Section 2: lays down review of the related literature. Section 3: presents the methodology of the study. Section 4: presents the results and discussion from the study. Section 5: summaries the finding and concludes with prospective future applications.",Yes,"논문은 DCGAN을 활용하여 자궁경부암 생물학적 이미지의 합성 데이터를 생성하고, 이를 통해 분류 모델의 성능 향상을 평가하는 독창적인 연구를 수행하고 있다. 또한, 생성된 이미지의 품질과 다양성, 생물학적 특성 보존 여부를 다양한 평가 지표로 분석하는 등 직접적인 연구 기여가 포함되어 있다."
Deep Learning in Barcode Recognition: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681298,"The use of deep learning (DL) for barcode recognition and analysis has achieved remarkable success and has attracted great attention in various domains. Unlike other barcode recognition methods, DL-based approaches can significantly improve the speed and accuracy of both barcode detection and decoding. However, after almost a decade of progress, the current status of DL-based barcode recognition has yet to be thoroughly explored. Specifically, summaries of key insights and gaps remain unavailable in the literature. Therefore, this study aims to comprehensively review recent applications of DL methods in barcode recognition. We mainly conducted a well-constructed systematic literature review (SLR) approach to collect relevant articles and evaluate and summarize the state of the art. This study’s contributions are threefold. First, the paper highlights new DL approaches’ applicability to barcode localization and decoding processes and their potential to either reduce the time required or provide higher quality. Second, another main finding of this study signifies an increasing demand for public and specific barcode datasets that allow DL methods to learn more efficiently in the big data era. Finally, we conclude with a discussion on the crucial challenges of DL with respect to barcode recognition, incorporating promising directions for future research development.",No,"본 논문은 딥러닝 기반 바코드 인식에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들의 동향과 한계를 요약하는 데 중점을 두고 있다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Logistic Regression based Sentiment Analysis System: Rectify,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577296,"The detection of various reactions using computer vision, machine learning, and artificial intelligence is a rapidly growing field of research. In this paper, we present a sentiment analysis model based on the Python, NLTK (natural language toolkit) libraries, and machine learning algorithms that can detect multiple reactions from the public on a single platform. The proposed Sentiment Analysis System deals with the very usual problem faced by different companies, manufacturers, and sellers about knowing the customer review for their product. The model classifies the public reviews and the normal text as positive, negative, or neutral. This proposed system is categorised into two parts, one being a field-specific sentiment analysis and the other being a generalised system that can judge any particular word or sentence to be either positive, negative, or neutral. The proposed model is also capable of representing a huge dataset (input as a csv file) in the form of a graph, which can be easily understood by the desired person, and the graphical representation is possible with the Matplotlib library of Python. The model's performance is evaluated using several metrics, such as precision, recall, and others. The model's accuracy and efficiency make it a promising tool for sentiment analysis, which can be used by anyone in any field. In conclusion, the system can work for any specific field provided the data set, and the generalized way can help out with random sentences, and the model’s potential can be boosted with some further research.",Yes,"논문은 Python과 NLTK 라이브러리, 머신러닝 알고리즘을 활용한 감성 분석 모델을 제안하고 있으며, 모델의 성능 평가 지표를 제시하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문으로 판단된다."
Sharing is Caring! Joint Multitask Learning Helps Aspect-Category Extraction and Sentiment Detection in Scientific Peer Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651781,"The peer-review process is the benchmark of research validation. Peer-reviewed texts are the artifacts via which the editors/chairs decide the inclusion/exclusion of a paper in a journal or conference proceedings. Hence it is important for the editors/chairs to carefully analyze the peer-review text from various aspects of the paper (e.g., novelty, substance, soundness, etc.), identify the underlying sentiment of the reviewers, and thereby validate the informativeness of the reviews before making a decision. With the rise in research paper submissions, the current peer-review system is experiencing an unprecedented information overload. Sometimes it becomes stressful for the chairs/editors to make a reasonable decision within the stringent timelines. Here in this work, we attempt an interesting problem to automatically extract the aspect and sentiment from the peer-review texts. We design an end-to-end deep multitask learning model to perform aspect extraction and sentiment classification simultaneously. We show that both these tasks help each other in the predictions. We achieve encouraging performance on a recently released dataset of peer-review texts. We make our codes available for further research11https://www.iitp.ac.in/~ai-nlp-ml/resources.html#aspect-category-sentiment.",Yes,"본 논문은 동시 다중 작업 학습 모델을 설계하여 피어 리뷰 텍스트에서 측면 추출과 감정 분류를 자동화하는 독창적인 연구를 수행하고 있습니다. 또한, 새로운 데이터셋에 대해 성능을 평가하고 코드를 공개하는 등 직접적인 연구 기여가 포함되어 있습니다."
Harnessing Multimodal AI for Creative Design: Performance Evaluation of Stable Diffusion and DALL-E 3 in Fashion Apparel and Typography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698398,"In recent years, multimodal AI (Artificial Intelligence) models have exhibited promising capabilities in generating diverse forms of creative content. This review paper critically evaluates and compares the performance of two prominent multimodal models, Stable Diffusion and DALL-E, in the context of fashion apparel and typography design generation. The evaluation methodology encompasses both human judgment and automatic evaluation metrics, including CLIP Score, Image Reward, and Inception Score, to provide a comprehensive assessment of the generated outputs. Through a systematic analysis of the results, this paper highlights the strengths and limitations of each model, shedding light on their respective abilities to capture design nuances, maintain coherence, and exhibit creativity. Additionally, the review discusses the impact of training data, model architecture, and hyperparameters on the quality and diversity of generated designs. The findings of this review contribute to a deeper understanding of the capabilities of multimodal AI models in creative design tasks and offer insights into their potential applications in the fashion and design industries. This paper aims to guide researchers and practitioners in selecting suitable models for specific design generation tasks while fostering advancements in multimodal AI research.",No,"초록에서 해당 논문은 두 AI 모델의 성능을 평가하고 비교하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구를 종합적으로 분석하는 데 중점을 두고 있다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
A Systematic Literature Review of Machine Learning Techniques Deployed in Agriculture: A Case Study of Banana Crop,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861649,"Agricultural productivity is the asset on which the world’s economy thoroughly relies. This is one of the major causes that disease identification in fruits and plants occupies a salient role in farming space, as having disease disorders in them is obvious. There is a need to carry genuine supervision to avoid crucial consequences in vegetation; otherwise, corresponding vegetation standards, quantity, and productiveness gets affected. At present, a recognition system is required in the food handling industries to uplift the effectiveness of productivity to cope with demand in the community. The study has been carried out to perform a systematic literature review of research papers that deployed machine learning (ML) techniques in agriculture, applicable to the banana plant and fruit production. Thus; it could help upcoming researchers in their endeavors to identify the level and kind of research done so far. The authors investigated the problems related to banana crops such as disease classification, chilling injuries detection, ripeness, moisture content, etc. Moreover, the authors have also reviewed the deployed frameworks based on ML, sources of data collection, and the comprehensive results achieved for each study. Furthermore, ML architectures/techniques were evaluated using a range of performance measures. It has been observed that some studies used the PlantVillage dataset, a few have used Godliver and Scotnelson dataset, and the rest were based on either real-field image acquisition or on limited private datasets. Hence, more datasets are needed to be acquired to enhance the disease identification process and to handle the other kind of problems (e.g. chilling injuries detection, ripeness, etc.) present in the crops. Furthermore, the authors have also carried out a comparison of popular ML techniques like support vector machines, convolutional neural networks, regression, etc. to make differences in their performance. In this study, several research gaps are addressed, allowing for increased transparency in identifying different diseases even before symptoms arise and also for monitoring the above-mentioned problems related to crops.",No,"본 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들의 내용을 종합하고 비교 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여를 포함한 연구 논문으로 보기 어렵습니다."
Enhancing Cybersecurity with Artificial Neural Networks: A Study on Threat Detection and Mitigation Strategies,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10533152,"This paper reviews the role of artificial neural networks (ANNs) in enhancing cybersecurity measures. It delves into research methodologies employed to detect security breaches, potential threats, and spam, among other cybersecurity challenges. The study further investigates the literature to gauge the efficacy of using AI methodologies in pinpointing system attacks, showcasing the potential of AI in safeguarding various data types, be it academic or industrial, and preventing system vulnerabilities. ANNs, with their predictive capabilities and experiential learning, emerge as advanced security protection and threat detection tools. Additionally, the research delves into developing a hybrid multi-agent system that leverages deep learning for identifying and mitigating cyberattacks, aiming to shed light on the current risks in information systems and propose effective countermeasures.",Yes,"논문 초록에서 인공신경망을 활용한 위협 탐지 및 완화 전략에 대한 구체적인 연구와 하이브리드 다중 에이전트 시스템 개발을 다루고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 단순 문헌 리뷰를 넘어 새로운 시스템 개발과 적용을 시도한 점이 연구 논문으로 판단되는 근거입니다."
Automated Generation of Narrative Sleep Reports Utilizing Portable Electroencephalogram Data Through ChatGPT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628709,"Sleep is a very important activity, but many people do not know their own sleep conditions. A sleep test personalizes sleep quality assessment and detects potential sleep disorders by measuring biological signals. The rise in sleep-related issues has necessitated the development of automated testing methods. Machine learning plays a pivotal role in interpreting sleep data and determining sleep stages. However, the generation of detailed reports and tailored recommendations still demands expert intervention. Automating the report generation to provide personalized sleep insights is a crucial and desired step for the future of sleep healthcare. Recently emerged Generative AI, such as ChatGPT, has attracted considerable attention in recent years. It can generate new sentences and images from input data. In this study, we investigate the practicality and applicability of using ChatGPT to generate narrative sleep reports for sleep test. In our proposed method, GPT-4 receives the information about the sleep habits of the participants and the sleep assessment automatically summarized by the rule-based algorithm. In the evaluation, we used in-home sleep EEG data obtained from 100 subjects by S'UIMIN inc. The generated reports were evaluated by experienced technicians and physicians on a 5- point scale for medical correctness and appropriateness as informative reports. The results of the evaluation showed that 60 % of the reports were the acceptable or above range in both aspects. While more than half of the results were judged to be above the acceptable range, differences between the generative AI and humans were also identified. Whereas humans comment on semantically weighted important findings such as medication and subjective insomnia, ChatGPT tends to make broad, shallow and flat comments on the input data. These facts suggest that although practical report generation only using generative AI is at present not easy, generative AI is a promising tool for improving the efficiency of physicians and technicians work.",Yes,"본 논문은 ChatGPT를 활용하여 휴대용 뇌파 데이터를 기반으로 수면 보고서를 자동 생성하는 방법을 제안하고, 실제 데이터를 이용한 평가를 수행하여 그 실용성을 검증하는 독창적인 연구 내용을 포함하고 있습니다. 따라서 기존 연구를 단순히 요약한 것이 아니라 새로운 방법론과 실험 결과를 제시하는 연구 논문에 해당합니다."
Uncertainty Quantification for Trusted Machine Learning in Space System Cyber Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9697520,"In recent years, the Aerospace Corporation has been developing machine learning systems to detect cyber anomalies in space system command and telemetry streams. However, to enable the use of deep learning in such high consequence environments, the models must be trustworthy. One aspect of trust is a model’s ability to accurately quantify the uncertainty of its predictions. Although many deep learning models output what seem to be confidence scores, current academic research has repeatedly shown that models often return high confidence even when very wrong and are unable to diagnose and respond appropriately to out-of-distribution inputs. This can result in catastrophic overconfidence when models are faced with adversarial inputs or concept drift. Even on routine inputs, without reliable uncertainty quantification, it is hard for human-machine teaming to take place as humans cannot trust the model’s reported confidence score. In short, all models are wrong sometimes, but models which know when they are wrong are considerably more useful. To this end, The Aerospace Corporation conducted a literature review and implemented current state of the art methods, including deep ensembles and temperature scaling for confidence calibration, to accurately quantify the uncertainty of deep learning model predictions. We further incorporated and tested these techniques within the existing cyber defense model framework for more trustworthy cyber anomaly detection models. We show that not only are these techniques successful, they are also easy to implement, extensible to many applications and machine learning model variants, and provide interpretable results for a wide audience. From this, Aerospace recommends further adoption of such techniques in high consequence environments.",Yes,"본 논문은 기존 연구를 검토하고 최신 기법들을 구현하여 우주 시스템 사이버 보안에서 딥러닝 모델의 불확실성 정량화를 개선하는 독창적인 연구를 수행하였음을 명확히 밝히고 있습니다. 또한, 이러한 기법들을 기존 사이버 방어 모델에 통합하고 테스트한 결과를 제시하여 직접적인 연구 기여를 포함하고 있습니다."
Detecting Frauds and Payment Defaults on Credit Card Data Inherited With Imbalanced Class Distribution and Overlapping Class Problems: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423008,"Credit card payments are one popular e-payment option apart from cash payments. Recent reports show that credit card fraud and payment defaults are increasing annually and are alarming. Thus, researchers have attempted various machine learning techniques to address these two challenges. However, they are challenged to mitigate the two major problems inherited in credit card data: (i) imbalanced class distribution and (ii) overlapping classes. Mitigating these problems shall effectively detect credit card frauds and payment defaults, thus benefiting card issuers and holders. Hence, this paper aims to develop a systematic review using PRISMA to identify and compare various credit card datasets, machine learning techniques, and evaluation metrics. Subsequently, we provide recommendations for handling these two problems. We extracted research papers from 2016 to 2023 from ScienceDirect, Springer, Association and Computing Machinery (ACM), and IEEE databases. The papers shall be included if written in English and published in peer-reviewed and indexed journals or conference proceedings. Finally, 87 papers were selected based on the eligibility criteria. Based on our findings, the European and Taiwan datasets are widely used in the research community. However, most researchers focus on tackling imbalanced class distribution rather than two problems together. We recommended to the research community the application of deep learning, ensemble learning, and sampling methods to effectively detect fraud and payment defaults on credit card datasets that inherit the two problems. In evaluating the machine learning algorithms, we recommend using metrics that can separately evaluate the algorithms’ performance in detecting frauds/payment defaults and normal transactions.",No,"본 논문은 다양한 연구들을 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 비교와 평가에 중점을 두고 있다."
Comparison of mobile interaction management products using systematic literature review method and a new product suggestion,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093372,"Because of innovations and improvements in technology, the use of smartphones that make it easier for users to work has become widespread. At this point, companies can reach their customers more easily and can communicate continuously. Once mobile applications are created, the system infrastructure needs to be improved in response to changing needs and demands to actively retain registered users and continually capture their insights. In this case, a dynamic framework that will create user profiles in a mobile application and provide services according to different user needs. In this study, the main features of the mobile interaction management applications on the market and other features they provide to create a loyal user base have been evaluated using the Systematic Literature Review (SLI) method and the necessary gaps have been discussed. In order to acquire loyal mobile-app user, Machine Learning support system is proposed as solution.",Yes,"본 논문은 기존 모바일 상호작용 관리 제품들의 주요 특징을 체계적 문헌고찰 방법으로 평가하고, 발견된 갭을 논의하며, 이를 보완하기 위한 머신러닝 지원 시스템을 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 단순 리뷰가 아닌 새로운 제품 제안과 해결책을 제시하는 연구 논문으로 판단된다."
Robotics and Artificial Intelligence in the Hotel Industry: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785257,"The paper provides an overview of robotics, and artificial intelligence (AI) research in the realm of hotel industry. In recent years hotel industry has incorporated many technological tools, such as smart sensors, chat bots, service robots, AI embedded equipments in their day to day operations. A comprehensive literature review from 1984 to 2021 evaluated 551 papers. The information extracted is analyzed to understand the work done in this field in terms of publications, authorship, subject area, source, keywords and citations. Findings suggest that the research work in AI and robotics has increased significantly in the hotel business.",No,"본 논문은 호텔 산업에서의 로봇공학과 인공지능 관련 기존 연구들을 체계적으로 문헌 검토한 리뷰 논문입니다. 직접적인 독창적 연구 결과나 실험, 데이터 분석을 제시하기보다는 기존 연구들을 종합하여 분석하는 데 초점이 맞춰져 있습니다."
Integration of AI with the Cybersecurity: A detailed Systematic review with the practical issues and challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10073040,"Not only have there been a lot more cyberattacks in recent years, but they have also gotten much more advanced. Therefore, developing a cyber-resilient strategy is of utmost significance. In the event of a cyberattack, traditional security measures are insufficient to prevent data leaks. Cybercriminals have mastered the use of cutting-edge methods and powerful tools for data intrusion, hacking, and assault. Fortunately, the use of artificial intelligence (AI) technologies has the creation of intelligent models for securing systems against attackers. AI technologies can quickly advance to meet complicated problems, making them useful as fundamental cybersecurity tools. AI-based solutions can provide effective and robust cyber defense capabilities to identify malware attacks, network intrusions, phishing and spam emails, data breaches, and alert security incidents when they occur. This paper provides a summary of existing research regarding the advantages of AI in cybersecurity and a review of the impact that AI has had on cybersecurity.",No,초록에서 본 논문은 기존 연구들을 요약하고 AI가 사이버보안에 미친 영향을 검토하는 체계적 문헌 리뷰임을 명확히 하고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않은 리뷰 논문으로 판단됩니다.
Cloudsuggest: Enhancing E-Commerce with Personalized Recommendations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486797,"This review explores the impact of cloud computing on the development of artificial intelligence (AI) in the ecommerce sector, focusing on the advantages and challenges of their integration. It also examines how cloud-based AI technologies have transformed customer assistance, inventory management, fraud detection, and supply chain optimization. The paper also explores potential advancements and business opportunities that could result from the integration of AI and cloud computing in e-commerce. Literature has the potential to transform and offer an escape from daily life, with systems for book recommendations using algorithms like collaborative filtering, content-based, and association criteria. These systems provide personalized suggestions based on users’ preferences and reading histories, enhancing the reading experience in the digital age. A strong technique for exact suggestions is created by fusing collaborative filtering with neural networks to merge content. Collaboration and real-time dynamics are emerging themes to address information overload and enhance user experiences in online applications. These trends enable collaborative and dynamic recommendations across devices and the cloud, optimizing suggestions by recognizing various data distributions. The Intelligent Device-Cloud Parameter Request Model (IDEAL) is proposed to address this issue by evaluating request revenue with little resource consumption, enabling adaptive and revenue efficient device-cloud communication.",Yes,"논문 초록에서 IDEAL 모델과 같은 구체적인 새로운 기법을 제안하고 있으며, 클라우드 기반 AI 기술과 개인화 추천 시스템을 통합하는 독창적인 연구 내용을 포함하고 있다. 이는 단순 리뷰가 아닌 직접적인 연구 기여를 담고 있음을 보여준다."
Deep Learning for Human Affect Recognition: Insights and New Developments,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598999,"Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field. In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.",No,"본 논문은 2010년부터 2017년까지 발표된 연구들을 리뷰하는 문헌 조사 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구 동향과 적용 사례를 종합적으로 분석하는 데 초점을 맞추고 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Automated Training Data Construction using Measurements for High-Level Learning-Based FPGA Power Modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9988835,"Machine Learning (ML) is the process of developing Artificial Intelligence (AI) in computers, where the generated models are trained using appropriate learning algorithms and training data. For many machine learning techniques, especially the ones related to supervised methods, the construction of the training data highly affects the quality and accuracy of the derived model. In this paper w e present and evaluate an automated training set construction methodology where data is synchronously collected from both hardware and software. The complete design and data flow including the interaction between software and hardware, are thoroughly described. As a direct application, this work targets the construction of an FPGA-based circuit power modeling for subsequent early power estimation. The constructed Artificial Neural Network (ANN) model is trained using real measurement data sets extracted using a dedicated in-house designed and implemented generation and acquisition platform. The designated application falls under the power optimization area, becoming nowadays a major concern for most digital hardware designers, particularly in early design phases and especially in limited power budget systems. The power optimization approach in context can be extended in order to support online power management.",Yes,"논문은 FPGA 전력 모델링을 위한 자동화된 학습 데이터 구축 방법론을 제안하고, 실제 하드웨어 측정 데이터를 이용해 인공신경망 모델을 훈련하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Anomaly Detection in Industrial Machinery Using IoT Devices and Machine Learning: A Systematic Mapping,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10318838,"Anomaly detection is critical in the smart industry for preventing equipment failure, reducing downtime, and improving safety. Internet of Things (IoT) has enabled the collection of large volumes of data from industrial machinery, providing a rich source of information for Anomaly Detection (AD). However, the volume and complexity of data generated by the Internet of Things ecosystems make it difficult for humans to detect anomalies manually. Machine learning (ML) algorithms can automate anomaly detection in industrial machinery by analyzing generated data. Besides, each technique has specific strengths and weaknesses based on the data nature and its corresponding systems. However, a large portion of the existing systematic mapping studies on AD primarily focus on addressing network and cybersecurity-related problems, with limited attention given to the industrial sector. Additionally, the related literature do not cover the challenges involved in using ML for AD in industrial machinery within the context of the IoT ecosystems. Therefore, this paper presents a systematic mapping study on AD for industrial machinery using IoT devices and ML algorithms to address this gap. Our primary objective is to investigate the use of ML models for anomaly detection within an industrial setting, particularly within IoT ecosystems. The study comprehensively evaluates 84 relevant studies spanning from 2016 to 2023, providing an extensive review of AD research. Our findings identify the most commonly used algorithms, preprocessing techniques, and sensor types. Additionally, this review identifies application areas and points to future challenges and research opportunities.",No,"이 논문은 산업용 기계의 이상 탐지에 관한 기존 연구들을 체계적으로 정리하고 평가하는 체계적 맵핑 연구로, 직접적인 독창적 실험이나 새로운 알고리즘 개발 등의 연구 기여를 포함하지 않습니다. 따라서 기존 연구들을 종합하고 분석하는 리뷰 논문에 해당합니다."
An Analysis of the Importance of the Artificial Intelligence on the Information System of Police Forces,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141006,"The objective of this paper is to present an analysis of the importance of Artificial Intelligence in Information System of the Police Forces (ISPF) as a tool to improve the quality of public service in general, and in the internal procedures management, in particular. The evolution in the information and communication technology systems has brought many challenges related to operational, strategic and management paradigms. For one side, not only do this, the police benefit themselves from improvements in efficiency, effectiveness and economy procedures, as well as, the citizen benefits from more simplified and faster treatment. For the other side, the introduction of the artificial intelligence by the police will get information faster in the fight against crime and it can act effectively and efficiently. Methodologically, this article focuses on a literature review with the main focus on how artificial intelligence applied can be a very important complement from the perspective of management, as well as an exploratory analysis of the systems already applied, reflecting good practices in the field of forces police officers used in the different services and police forces in a comparative way. The result of the investigation shows the importance of AI in the criminal sphere, where it is possible to use this tool as a preventive measure that guides police and criminal justice decisions and strategies. Highlighting innovative methods that can greatly increase the efficiency of information exchange between law enforcement authorities.",No,"본 논문은 인공지능의 중요성에 대한 문헌 리뷰와 탐색적 분석을 중심으로 작성되었으며, 직접적인 실험이나 새로운 연구 결과를 제시하지 않고 기존 사례와 이론을 종합하는 데 초점을 맞추고 있습니다. 따라서 독창적인 연구 내용이나 새로운 기여가 포함된 연구 논문으로 보기 어렵습니다."
Bioacoustics Monitoring of Wildlife using Artificial Intelligence: A Methodological Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985209,"Artificial intelligence (AI) is a broad computing science that has attracted significant attention in the ecological sector because of its problem-solving, deciding, and pattern recognition capabilities. Because of the large number of datasets available across spatiotemporal scales that may be used for machine learning and interpretation, bioacoustics wildlife monitoring is essential in the performance of AI techniques. Although several studies have enforced AI algorithms into the wildlife ecology, the future of this developing method in wildlife acoustic monitoring is unknown. In this study, we performed a scientific literature review covering 20 papers from 2015 and March 2022 to evaluate its application and advise future demands. During this time, we observed a considerable increase in the use of AI approaches in wildlife acoustic monitoring. Overall, bird species (N=12) received the most attention, followed by amphibians (N=5) and mammals (N=3)), even though their operations are diversifying. Among the AI learnings used in bioacoustics wildlife monitoring, a convolutional neural network was highly accurate in terms of performance, had more advantages, and was replicated in multiple articles than other classification methods. Reviewing previously used AI algorithms in bioacoustics research is expected to aid in understanding the trends and identifying gaps in automatic wildlife monitoring.",No,"본 논문은 2015년부터 2022년까지 발표된 20편의 연구를 대상으로 한 문헌 리뷰로, 직접적인 실험이나 새로운 연구 결과를 제시하지 않고 기존 연구 동향과 AI 알고리즘의 적용 현황을 분석하는 데 중점을 두고 있습니다. 따라서 독창적인 연구 내용이나 새로운 실험 결과를 포함한 연구 논문으로 보기 어렵습니다."
Leveraging Artificial Intelligence and Machine Learning for Disease Diagnosis in Healthcare,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895901,"This research paper explores the transformative potential of Artificial Intelligence (AI) and Machine Learning (ML) in disease diagnosis in healthcare systems. It reviews the current landscape of AI and ML technologies, their integration into diagnostic processes, and their impact on accuracy, efficiency, and patient outcomes. The paper also discusses challenges, ethical considerations, and prospects of AI and ML in disease diagnosis. The study evaluates the performance of a system integrating pulse measurements and user studies to assess its effectiveness in improving user engagement and satisfaction. The results suggest that the proposed system can enhance voice assistant capabilities and contribute to users' overall well-being through continuous health monitoring. Voice assistants can provide virtual healthcare services, such as answering health questions, providing information about symptoms, and offering general health advice. Random Forest (RF) outperforms Logistic Regression (LR) and Support Vector Machine (SVM) with an accuracy rate of 98%.",Yes,"논문 초록에서 제안된 시스템의 성능 평가와 다양한 알고리즘(Random Forest, Logistic Regression, SVM) 비교를 통해 직접적인 실험 및 분석 결과를 제시하고 있어 독창적인 연구 내용이 포함된 연구 논문으로 판단됩니다. 또한, AI와 ML 기술을 활용한 질병 진단의 효과를 검증하는 구체적인 연구가 수행된 점이 명확합니다."
"Conversational AI in Higher Education: Opportunities, Challenges, and Ethical Considerations",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10629104,"Artificial Intelligence, in particular conversational artificial intelligence (AI), is revolutionizing higher education. The commercialization and popularization of these tools has catapulted its adoption. It offers personalized student learning, support to academic and professional staff, and the streamlining administrative tasks. This rapidly developing technology promises to significantly influence the higher educational landscape. An interpretive synthesis of the current application of conversational artificial intelligence in higher education was guided by a state-of-the-art literature review. The aim of the research study was to evaluate academics perspective of conversational AI use and its perceived benefits and challenges, in four different countries (South Africa, Hungary, Lebanon, and Wales). It was found that less than half of the respondents employed conversational AI in teaching, whereas most of the respondents utilized it for research support. The preference for using conversational artificial intelligence tools in research rather than in teaching—particularly among younger academics and those favoring remote working environments—suggests a future trajectory where AI could become more central in academic research than in traditional teaching methods. Furthermore, it was found that significantly more of those who do not use conversational AI for teaching, prefer teaching face to face, whereas those using conversational AI to enhance their teaching, most were neutral about their preferred mode of delivery. Despite some of the concerns raised by some academics, most viewed it its numerous potential advantages for teaching and research, positively. The study, however, did raise concerns regarding the ethical integration and adoption of these technologies in academic settings.",Yes,"본 논문은 4개국 학자들의 관점을 조사하여 대화형 AI의 활용 현황과 인식, 장단점 및 윤리적 문제를 분석한 연구로, 직접 데이터를 수집하고 해석한 독창적인 연구 내용을 포함하고 있다. 따라서 단순 문헌 리뷰가 아닌 실증적 연구 결과를 제시하는 연구 논문에 해당한다."
Identifying Functional and Non-functional Software Requirements From User App Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795770,"Mobile app developers are always looking for ways to use the reviews (provided by their app’s users) to improve their application (e.g., adding a new functionality in the app that a user mentioned in their review). Usually, there are thousands of user reviews that are available for each mobile app and isolating software requirements manually from such as big dataset can be difficult and time-consuming. The primary objective of the current research is to automate the process of extracting functional requirements and filtering out non-requirements from user app reviews to help app developers better meet the wants and needs of their users. This paper proposes and evaluates machine learning based models to identify and classify software requirements from both, formal Software Requirements Specifications (SRS) documents and Mobile App Reviews (written by users) using machine learning (ML) algorithms combined with natural language processing (NLP) techniques. Initial evaluation of our ML-based models show that they can help classify user app reviews and software requirements as Functional Requirements (FR), Non-Functional Requirements (NFR), or Non-Requirements (NR).",Yes,본 논문은 사용자 앱 리뷰에서 기능적 요구사항과 비기능적 요구사항을 자동으로 추출하는 머신러닝 및 자연어처리 기반 모델을 제안하고 평가하는 독창적인 연구를 수행하고 있다. 이는 기존 연구를 바탕으로 새로운 방법론을 개발하고 실험을 통해 성능을 검증한 연구 논문에 해당한다.
Semantic Communication System Based on Semantic Slice Models Propagation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954279,"Traditional communication systems treat messages’ semantic aspects and meaning as irrelevant to communication, revealing its limitations in the era of artificial intelligence (AI), such as communication efficiency and intent-sharing among different entities. Through broadening the scope of the traditional communication system and the AI-based encoding techniques, in this manuscript, we present a novel semantic communication system, which involves the essential semantic information exploration, transmission and recovery for more efficient communications. Compared to other state-of-the-art semantic communication-related works, our proposed semantic communication system is characterized by the “flow of the intelligence” via the propagation of the model. Besides, the concept of semantic slice-models (SeSM) is proposed to enable flexible model-resembling under the different requirements of the model performance, channel situation and transmission goals. Specifically, a layer-based semantic communication system for images (LSCI) is built on the simulation platform to demonstrate the feasibility of the proposed system and a novel semantic metric called semantic service quality (SS) is proposed to evaluate the semantic communication systems. We evaluate the proposed system on Cityscapes and Open Images datasets, resulting in averaged 10% and 2% bit rate reduction over JPEG and JPEG2000, respectively. In comparison to LDPC, the proposed channel coding scheme can averagely save 2dB and 5dB in AWGN channel and Rayleigh fading channel, respectively.",Yes,"논문은 기존 통신 시스템의 한계를 극복하기 위해 새로운 의미 기반 통신 시스템과 개념(semantic slice-models)을 제안하고, 이를 시뮬레이션 플랫폼에서 구현 및 평가한 독창적인 연구 내용을 포함하고 있다. 또한, 새로운 의미적 지표(semantic service quality)를 도입하고 실험 결과를 제시하여 직접적인 연구 기여를 하고 있음을 보여준다."
Trends in Cybersecurity Management Issues Related to Human Behaviour and Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698626,"The number of organisational cybersecurity threats continues to increase every year as technology advances. All too often, organisations assume that implementing systems security measures like firewalls and anti-virus software will eradicate cyber threats. However, even the most robust security systems are vulnerable to threats. As advanced as machine learning cybersecurity technology is becoming, it cannot be solely relied upon to solve cyber threats. There are other forces that contribute to these threats that are many-a-times out of an organisation's control i.e., human behaviour. This research article aims to create an understanding of the trends in key cybersecurity management issues that have developed in the past five years in relation to human behaviour and machine learning. The methodology adopted to guide the synthesis of this review was a systematic literature review. The guidelines for conducting the review are presented in the review approach. The key cybersecurity management issues highlighted by the research includes risky security behaviours demonstrated by employees, social engineering, the current limitations present in machine learning insider threat detection, machine learning enhanced cyber threats, and the underinvestment challenges faced in the cybersecurity domain.",No,"본 논문은 체계적 문헌고찰(Systematic Literature Review)을 통해 최근 5년간의 사이버보안 관리 이슈 동향을 종합한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구를 분석하고 요약하는 성격이 강하다."
Can Unlabelled Data Improve AI Applications? A Comparative Study on Self-Supervised Learning in Computer Vision,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306184,"Artificial Intelligence (AI) represents a highly investigated area of study at present and has already become an indispensable component within an extensive range of business models and applications. One major downside of current supervised AI approaches lies in the need of numerous annotated data points to train the models. Self-supervised learning (SSL) circumvents the need for annotation, by creating supervision signals such as labels from the data itself, rather than requiring experts for this task. Current approaches mainly include the use of generative methods such as autoencoders and joint embedding architectures to fulfil this task. Recent works present comparable results to supervised learning in downstream scenarios such as classification after SSL-pretraining. To achieve this, typically modifications are required to suit the approach for the exact downstream task. Yet, current review works haven't paid too much attention to the practical implications of using SSL. Thus, we investigated and implemented popular SSL approaches, suitable for downstream tasks such as classification, from an initial collection of more than 400 papers. We evaluate a selection of these approaches under real-world dataset conditions, and in direct comparison to the supervised learning scenario. We discuss SSL's potential to take up with supervised learning, as well as the influence of the right training methods. Furthermore, we also introduce future directions for SSL research, as well as current limitations in real-world applications.",Yes,"논문은 400편 이상의 기존 연구를 조사하고, 여러 자기지도학습(SSL) 방법을 실제 데이터셋 조건에서 구현 및 평가하는 실험적 연구를 포함하고 있다. 이는 단순 리뷰가 아니라 직접적인 실험과 비교 분석을 통해 독창적인 연구 기여를 하는 논문임을 보여준다."
Smart City Digital Twin Platform Architecture for Mobility and Transport Decision Support Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825075,"Addressing mobility and transport problems is nowadays of paramount importance for any city due to the increasing urbanization. Traffic congestion, pollutant emissions, energy consumption are some of the problems related to urban mobility. Therefore, there is the need of tools able to support decision-makers in studying, evaluating, and planning sustainable urban evolutions. A few open-source and proprietary solutions are available requiring on-premises installations, large effort, and providing limited capabilities to actually handle real-time data (from data spaces, and standards). Moreover, they are limited in terms of analytic integration and do not offer automatic generation of suggestions. In practice they do not manage the explosion of complexity regarding computational and storage/models aspects. For these reasons, this paper presents a comprehensive architecture for a Smart City Digital Twin platform, specifically designed to support mobility and transportation decision-making through advanced what-if analysis and optimization. The platform, integrated within the Snap4City system, enables real-time data processing and complex analytics to create virtual urban environments for evaluating potential infrastructure changes. Through microservice architecture, the platform supports massive data ingestion, scenario creation, and predictive modelling, facilitating both short-term and long-term planning. The solution leverages artificial intelligence (AI), machine learning (ML), and reinforcement learning (RL) to optimize city operations and suggest actionable insights, aiding city planners in strategic and tactical decisions. This architecture has been validated through implementations in Italian cities, demonstrating scalability and flexibility to accommodate diverse urban needs and improve traffic flow, energy efficiency, and environmental impact. This work has been performed in the context of OPTIFaaS Flagship of CN MOST, the National Centre for Sustainable Mobility in Italy, and for CN HPC Big Data and Quantum Computing, ICSC.",Yes,"논문은 스마트 시티 디지털 트윈 플랫폼 아키텍처를 제안하고, 이를 통해 실시간 데이터 처리, 복잡한 분석, AI/ML 기법을 활용한 최적화 및 의사결정 지원을 수행하는 독창적인 연구 내용을 포함하고 있다. 또한, 이 아키텍처가 실제 이탈리아 도시들에 적용되어 검증된 점에서 직접적인 연구 기여가 명확하다."
Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8294186,"Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.",No,"이 논문은 딥러닝의 적대적 공격에 관한 기존 연구들을 종합적으로 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 리뷰하는 성격이 강합니다."
Enhancing Real-Time Object Detection With Advanced YOLOv9 and OpenCV in Python,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10836635,"Object detection is a foundation process in computer vision having widespread applications in autonomous driving, medical diagnostics and security monitoring. Recent advancements and development in field of machine learning, particularly deep learning had dynamically improved object detection accuracy. However, the selection of an optimal algorithm remains challenging due to the diverse characteristics of available techniques. This paper depicts the comprehensive review of object detection, covering commonly used data sets, evaluation metrics and both traditional and deep learning-based algorithms. The study introduces a Python-based method for real-time object detection, utilizing deep learning architectures like Convolutional Neural Networks (CNNs) and Single Shot MultiBox Detector (SSD). Built on the open-source library OpenCV, the proposed system demonstrates high adaptability and ease of use through a transfer learning strategy. Experimental results validate the system's effectiveness in achieving accurate, real-time detection across various scenarios, including different lighting conditions and object occlusions. The study also discusses the current challenges, particularly small object detection, and outlines future research directions in the field.",No,"논문 초록은 기존 알고리즘과 기술에 대한 종합적인 리뷰와 Python 기반 구현 방법을 소개하는 내용에 중점을 두고 있으며, 독창적인 연구 결과나 새로운 알고리즘 개발에 대한 직접적인 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 연구 논문보다는 기술적 리뷰 및 구현 사례에 가까운 것으로 판단됩니다."
"Adequate Testing Unmanned Autonomous Vehicle Systems - Infrastructures, Approaches, Issues, Challenges, and Needs",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912634,"Development of autonomous vehicles is seen as a solution to many of today’s societal and everyday issues such as traffic congestion, road accidents, and air pollution. Unmanned autonomous vehicles are complex systems and testing such systems raises many challenges. This paper, written in a position paper to review main aspects of testing of unmanned autonomous vehicles, from test requirements and needs to test modes, approaches and methods, platforms, and infrastructures. Artificial intelligence especially machine learning, is part of such systems. We show throughout the paper the existing interplay between testing and artificial intelligence as testing could take benefit from artificial intelligence and machine learning, but also artificial intelligence approaches need to be validated. In addition, challenges, issues, and needs are discussed, and future research directions are highlighted.",No,"본 논문은 무인 자율주행차 시스템 테스트에 관한 주요 측면들을 종합적으로 검토하는 포지션 페이퍼로, 기존 연구와 현황을 정리하고 향후 연구 방향을 제시하는 내용에 초점이 맞춰져 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵습니다."
Automated Malware Detection Based on a Machine Learning Algorithm,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366550,"Malware detection relies on the discriminative power of machine learning to identify new variants of malware samples. Automated malware detection, driven by machine learning algorithms, has garnered significant recognition for its capability to detect previously unknown malware. In recent years, various machine learning techniques have exhibited promise in enhancing malware detection. These techniques facilitate the analysis of substantial datasets, the identification of intricate patterns, and the detection of emergent threats, surpassing conventional signature-based methods. This paper offers an overview of these machine learning techniques and their potential to enhance the precision and efficiency of malware detection systems. The objective is to conduct a comprehensive literature review, analyze selected research papers, and present a taxonomy of machine learning methods for malware detection. The study delves into the intersection of malware and machine learning within the cybersecurity domain, encompassing the taxonomy of malware detection and the classification of machine learning algorithms. Moreover, the taxonomy is employed to evaluate the latest algorithms and perform an exhaustive analysis of machine learning approaches. Additionally, this paper discusses the challenges related to the application of machine learning in malware detection. Machine learning techniques provide a robust toolkit for the development of more effective and efficient malware detection systems. Continued research in this field is imperative to mitigate the ever-growing malware threat.",No,"초록에서 본 논문은 기존 연구들을 종합적으로 검토하고 분류하는 문헌 리뷰 및 분류 체계 제시에 초점을 맞추고 있으며, 독창적인 실험 결과나 새로운 알고리즘 개발에 대한 직접적인 연구 기여 내용은 포함되어 있지 않다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Artificial Intelligence Techniques for Early Prediction of Neonatal Jaundice,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10866866,"The In this case, the achievement of the utilization of artificial intelligence (AI) methods for early identifying neonatal jaundice, which is frequent in babies, is reviewed in this paper. Specifically, while presenting data about AI models and their successes, we found that deep learning approaches demonstrated high levels of accuracy in early risk assessment of neonates with jaundice burdens - before such manifestations of path gnomic symptoms or dangerous levels of bilirubin increase. An early prediction of neonatal jaundice is vital since it enables health practitioners to make early assessments and referrals and introduces treatment and prevention measures hence preventing the occurrence of complications, limiting invasive procedures, and alleviating the families' stress and expenses. With that being said, the potential application of the findings of this research can be generalized to improved and healthier lives of infants in the future, with AI technologies as one of the primary protectors of their health from early stages of their lives. We also mention limitations of the present study, directions for future work, and novel developments in applying big data analytics to optimize neonatal care: data quality remains an essential factor to consider during the analytic processes to ensures that the findings of the research will also be generalizable across different populations, as well as the ethical considerations for the integration of AI in neonatology healthcare.",No,"초록에서 본 논문은 기존 AI 기법들의 성과를 리뷰하는 내용으로 보이며, 직접적인 독창적 연구 결과나 새로운 실험, 모델 개발에 대한 언급이 없습니다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 논문에 더 가깝다고 판단됩니다."
Knowledge Graph Based Medical Chatbot building,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10353415,"To have a good and unproblematic life without any health risks, it is very important to get medical advice on any health-related problems. However, getting medical advice incurs costs. Chatbots are AI/ML based software which may be trained with a lot of inquiries and responses and match users’ inquiries against a large repository of evidence-based medical data to provide simple answers. To reduce the healthcare costs and improve accessibility of medical knowledge a medical chatbot can be built using ML and NLP techniques. In the existing system of such chatbots several databases are connected together using join statements making it more complicated to access the data. The medical knowledge is vast and varied hence giving many disadvantages to use fixed schema. In this paper we have proposed a knowledge graph based method of chatbot creation for the healthcare field. When the patient enters the symptoms, they are suffering from, then chatbot evaluates and based on the evaluation recognizes the disease. The basic idea of our work is to build a chatbot which can evaluate the symptoms and using this evaluation, rank the possible disease which the patient could be suffering from. The chatbot gets its knowledge from a knowledge graph built on an extensive TigerGraph database. The data from the TigerGraph database can be accessed using different analysis queries. The chatbot will be considered profitable only when it can diagnose all kinds of disease and provides the necessary advice measures to be taken. The knowledge graph used here can be improvised by including results from lab tests to arrive at a better diagnosis and increase the precautions by including the medicines to be taken. The chatbot can only be as smart as the knowledge graph database and hence the evaluations made must be checked with the medical professional. The chatbot has good accuracy in predicting the disease the user is suffering from.",Yes,"본 논문은 의료 챗봇을 구축하기 위해 지식 그래프 기반의 새로운 방법을 제안하고, 증상 평가 및 질병 예측 알고리즘을 개발하여 챗봇의 정확도를 검증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Techniques for Evaluating the Robustness of Deep Learning Systems: A Preliminary Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672592,"Machine Learning algorithms are currently being applied to a huge diversity of systems in various domains, including control systems in the industry, medical instruments, and autonomous vehicles, just to name a few. Systems based on deep learning models have become extremely popular in this context, and, like regular machine learning algorithms, are susceptible to errors caused by noisy data, outliers, or adversarial attacks. An error of a deep learning model in a safety-critical context can lead to a system failure, which can have disastrous consequences, including safety violations. In this paper we review the state of the art in techniques for evaluating the reliability (in lato sensu) of deep learning models, identify the main characteristics of the methods used and discuss research trends and open challenges.",No,"본 논문은 딥러닝 시스템의 강인성 평가 기법에 대한 기존 연구들을 정리하고 논의하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 현황 분석과 연구 동향 소개에 초점이 맞춰져 있습니다."
Systematic Literature Review: Anomaly Detection in Connected and Autonomous Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771587,"This systematic literature review provides a structured and detailed overview of research on anomaly detection for connected and autonomous vehicles, focusing on the Artificial Intelligence methods employed, training approaches, and testing and evaluation techniques. The initial database search identified 2,160 articles, of which 203 were included in this review after rigorous screening and assessment. This study revealed that the most commonly used anomaly detection techniques employed are deep learning networks such as LSTM, CNN, and autoencoders, alongside one-class SVM. Most detection models were trained using real-world operational vehicle data, although anomalies, such as attacks and faults, were often injected artificially into the datasets. The models were evaluated primarily using five key evaluation metrics: recall, accuracy, precision, F1-score, and false positive rate. The most frequently used set of evaluation metrics for detection models were accuracy, precision, recall, and F1-score. The review makes several recommendations to improve future work related to anomaly detection models. It recommends providing comprehensive assessment of the anomaly detection models and emphasise the importance to share models publicly to facilitate collaboration within the research community and enable further validation. Recommendations also include the need for benchmarking datasets with predefined anomalies or cyberattacks (with comprehensive threat modelling) to test and improve the effectiveness of the proposed anomaly detection models. Future research should focus on the deployment of anomaly based detection in vehicles to evaluate their performance in real-world driving conditions, and explore systems using communication protocols beyond CAN, such as Ethernet and FlexRay.",No,"이 논문은 체계적인 문헌 리뷰(Systematic Literature Review)로, 기존 연구들을 종합하고 분석하는 데 중점을 두고 있어 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 향후 연구 방향을 제시하는 리뷰 논문에 해당합니다."
Using AI-SPedia to Study Saudi Universities’ Research Outputs in the Artificial Intelligence Field,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10067833,"Saudi Arabia’s Vision 2030, developed in 2016, is not limited to diversifying the economy and reducing the dependence on the hydrocarbon sector. Artificial intelligence (AI) is among the main strategic priorities of Vision 2030. Saudi Arabia is planning to be the world leader in technology by putting AI at the heart of the country’s development and growth. This study looked at AI related research outputs produced by Saudi Arabia to calculate the growth rate of AI research over the years and to measure how much of this research is covered by altmetrics, which basically capture all kinds of research mentions in many online platforms, such as social merlia. We used the AI-SPedia knowledge base repository, which accommodates AI research around the world with all the details, including bibliometric and altmetric indicators. After running the appropriate SPARQL code, we retrieved about 4,433 AI publications that are published by Saudi organizations. This study showed that the growth rate of AI research produced by Saudi Arabia increased from 28.23 % before 2016 to 47.07 % over time. This big jump was attributed mainly to Saudi Vision 2030 and the funding by Saudi government sectors, such as the Ministry of Education. This growth will have a great impact on the quantity of AI research and the scientific community’s tangible work in the AI domain. Moreover, this paper shows that the AI research outputs from the 10 most productive Saudi universities have an average of around 31.74 % of altmetric coverage. This low altmetric coverage indicates a need for suitable mechanisms to promote the dissemination of AI research through the most popular online platforms.",Yes,"본 논문은 AI-SPedia 데이터베이스를 활용하여 사우디아라비아의 AI 연구 산출물을 분석하고, 연구 성장률과 알트메트릭스 커버리지를 측정하는 독창적인 연구를 수행하였다. 이는 기존 데이터를 기반으로 한 실증적 분석을 포함하고 있어 연구 논문에 해당한다."
Predicting Early Phase of Type 2 Diabetic by Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371843,"Deep Neural Network with prediction is the one of the main deep learning technologies which has been used by many researches for early prediction of Type 2 Diabetics (T2D). For the prediction of the T2D, the taxonomy with the components are proposed with Data, Prediction processing and Display (DPD). Those components are evaluated for the better performance of the system and are validated with the different parameters for the early diagnosis of the T2D. The system being proposed has the higher accuracy for the prediction of the T2D and early detection of the diabetics in different age group in comparison to research paper reviewed and with current findings. It also helps to diagnose the diabetics in the patients. The critical analysis of the literature review of the latest published research paper available on the T2D and on deep learning has better accuracy for the prediction of T2D. On basis of the analysis, an effective system for T2D based on Deep Neural Network (DNN) has been developed in the system that can predict the diabetics in the early stage.",Yes,"논문 초록에서 제안된 시스템이 기존 연구와 비교하여 더 높은 정확도를 보이며, 딥 뉴럴 네트워크 기반의 새로운 예측 모델을 개발하고 검증한 내용을 포함하고 있어 독창적인 연구 기여가 있다고 판단된다. 따라서 연구 논문에 해당한다."
Deep Learning-Based Earthquake Prediction Technique Using Seismic Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185869,"Earthquakes are natural disasters that can cause severe damage to human life and infrastructure. Therefore, accurate earthquake prediction is crucial for disaster prepared-ness and risk reduction. Recently, machine learning techniques have shown promise in earthquake prediction. In this paper, we present a comprehensive study on the application of machine learning techniques for earthquake prediction. We first review the existing literature on earthquake prediction using machine learning techniques, including neural networks. We then propose a machine learning approach for earthquake prediction, based on analyzing seismic data. The proposed approach uses a convolutional neural network to extract relevant features from the seismic data, and a long short-term memory network to predict the probability of an earthquake. We evaluate the performance of the proposed approach on earthquake datasets from different regions and demonstrate its high accuracy in earthquake prediction. Our study provides a new perspective on earthquake prediction using machine learning techniques and highlights the potential of deep learning approaches for improving earthquake prediction. The proposed approach can be used in conjunction with existing earthquake prediction methods to provide more accurate and reliable predictions, which can help mitigate the potential impact of earthquakes on human life and infrastructure.",Yes,"논문은 기존 연구를 검토한 후, 지진 데이터를 분석하여 지진 예측을 위한 새로운 딥러닝 기반 접근법을 제안하고 그 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
A Comprehensive Study on Non-sequence and Sequence modeling word vector embedding approach for clinical text Named Entity Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10481577,"Human languages are hard to interpret and can not be understood by the computer. Thus, teaching a computer to understand human language is a difficult endeavor that has only lately been made possible by the application of Natural Language Processing (NLP) combined with the recent advancements and developments in fields such as Deep Learning (DL) with a manifold improvement in Recurrent Neural Network (RNNs) and the use of Word Embeddings. The language modeling and feature learning method in NLP known as “word embedding” maps vocabulary to actual number vectors leveraging products like word2vec, GloVe, and fastText.NLP can help us create more effective deep-learning models to solve language problems. Statistical analysis, machine learning methods, and deep learning all benefit from the improved word and phrase processing that natural language processing (NLP) techniques provide.NLP turned unstructured text data into more structured data that expert systems could easily modify and evaluate. Using embeddings, it is possible to handle textual input more quickly and effectively while building robust deep-learning models. Studies have been able to successfully verify the improvements obtained as a result of the application of DL techniques and models for tackling classes of problems related to Biological Named Entity Recognition (BioNER), with impressive and promising outcomes. Various ML-based NLP tasks now routinely use deep learning (DL), which removes the requirement for task-specific feature engineering based on in-depth domain expertise and facilitates the identification of salient aspects. Deep learning techniques frequently employ neural network (NN) architecture, which can automatically deduce patterns from vector data and pick the most pertinent features. Currently, the neural network most frequently used for NLP tasks is called Long Short-Term Memory (LSTM).The discovery of long-term connections between medical entities is performed using LSTM, a specific type of Recurrent Neural Network (RNN), which also increases training accuracy overall. This chapter entails a comprehensive study of the non-sequence and sequence modeling embedding approach for clinical text corpus named entity classification and a comparative analysis on previously adopted approaches. We compared the resulting precision and other metrics such as the recall as well as the F1-Score of our model to those of other currently available models for a number of gene and protein entity categories. Our proposed approach obtains an F1-Score of 77.34% in 16 epochs. We have worked on a relatively smaller gold standard corpus and word embedding that has a slight impact on the result. Overall, due to the complexity of the bio-medical named entities, our proposed architecture has greatly improved entity extraction identification and classification, although there is still potential for improvement.",Yes,"본 논문은 임상 텍스트 명명된 개체 분류를 위한 비순차 및 순차 모델링 임베딩 접근법에 대한 종합적인 연구와 기존 방법들과의 비교 분석을 수행하며, 자체 모델의 성능 지표(F1-Score 77.34%)를 제시하고 있다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 나타낸다."
Threat Modeling AI/ML With the Attack Tree,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10752529,"The pervasive use of AI assistant systems and machine learning-based applications in various fields and everyday life has significantly shifted. However, this shift is not without its challenges. The emergence of security threats, various attacks, and vulnerabilities in this domain has not only questioned their use but also sparked the interest of security experts and researchers, underlining the urgency and importance of this topic. However, a comprehensive and systematic research endeavor is yet to be undertaken on threat modeling based on violating basic tenets of information security on the various components of a machine learning system and evaluating their security risks. This lack of comprehensive threat modeling for each violation of a machine learning system’s confidentiality, integrity, availability, and privacy for various attacks and their risk analysis is a significant gap in the field. This article aims to bridge this gap by proposing a simple, efficient, and time-saving approach to evaluate potential attacks and their security risks by utilizing the attack tree and a risk analysis method in the Adversarial Machine Learning (AML) field. One of the most important steps in determining the overall risk of the attack is evaluating the risk attached to each node in an attack tree. A systematic approach that includes describing the system architecture and identifying its assets under various operational environment scenarios is also outlined in this paper. This approach can also offer crucial insights to security experts, aiding them in understanding and mitigating potential threats and risk analysis in AML systems. To ensure the validity and reliability of our findings, we have conducted a thorough and rigorous review of academic papers, summarizing different threats and attacks and their root cause analysis.",Yes,"본 논문은 AI/ML 시스템의 보안 위협을 체계적으로 모델링하고 공격 트리와 위험 분석 방법을 제안하는 독창적인 연구 내용을 포함하고 있다. 또한, 시스템 아키텍처 설명과 자산 식별 등 구체적인 방법론을 제시하며, 학술 논문 리뷰를 통해 근거를 마련한 점에서 연구 논문으로 판단된다."
An empirical review of Machine Learning models for Energy Optimizations in IoT Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307631,"The Internet of Things (IoT) has transformed how users interact with their devices and enabled the accumulation of enormous quantities of data from various sources. There has been an increase in energy consumption due to the expanding use of IoT devices, resulting in environmental concerns and higher operational costs. The use of Machine Learning (ML) algorithms to optimize energy consumption in IoT networks is widespread for different scenarios. This paper empirically analyzes the most widely used machine learning models for energy optimization in IoT networks. The researchers evaluate the efficacy of various ML models regarding energy efficiency, precision, and scalability. The review includes a summary of the most recent research in this field and a comparison of different algorithms, including Decision Trees, Random Forests, Support Vector Machines, Neural Networks, and Deep Learning methods. In addition, this paper discusses the difficulties and limitations of ML-based energy optimization in IoT networks, including data acquisition and processing, model selection, and scalability levels. Finally, this paper presents recommendations for future research and prospective machine-learning applications in energy optimization for IoT networks. This review emphasizes the potential of machine learning (ML) models to reduce energy consumption in IoT networks. It provides researchers and practitioners with insight into this field under real-time scenarios.",No,"본 논문은 다양한 머신러닝 모델을 에너지 최적화에 적용한 기존 연구들을 종합적으로 검토하고 비교하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 분석과 요약에 중점을 두고 있습니다."
Fuzzy Machine Learning: A Comprehensive Framework and Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496855,"Machine learning draws its power from various disciplines, including computer science, cognitive science, and statistics. Although machine learning has achieved great advancements in both theory and practice, its methods have some limitations when dealing with complex situations and highly uncertain environments. Insufficient data, imprecise observations, and ambiguous information/relationships can all confound traditional machine learning systems. To address these problems, researchers have integrated machine learning from different aspects and fuzzy techniques, including fuzzy sets, fuzzy systems, fuzzy logic, fuzzy measures, fuzzy relations, and so on. This article presents a systematic review of fuzzy machine learning, from theory, approach to application, with the overall objective of providing an overview of recent achievements in the field of fuzzy machine learning. To this end, the concepts and frameworks discussed are divided into five categories: 1) fuzzy classical machine learning; 2) fuzzy transfer learning; 3) fuzzy data stream learning; 4) fuzzy reinforcement learning; and 5) fuzzy recommender systems. The literature presented should provide researchers with a solid understanding of the current progress in fuzzy machine learning research and its applications.",No,초록에서 이 논문은 기존 연구들을 체계적으로 정리하고 개념과 프레임워크를 분류하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않은 것으로 판단됩니다.
A Systematic Review on Crop-Yield Prediction through Unmanned Aerial Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689838,"In recent years, with the increase in big data technologies the Machine learning is an important decision support tool, smart farming observes the behavior of climate change over the crops to predict the yield of the crop, whether the yield will be high or low in certain factors. A systematic literature review has been performed for the purpose of identifying the extent of research work in the field of crop yield prediction using machine learning based approaches. This study aims to perform comparative analysis over the existing approaches for the sake of identifying the most frequently reported issue in the crop yield prediction along with the approaches which are most suitable for the resolution of these issues. Study interprets, evaluate and identify the available researches related to the explicit field. This paper presents the significance of crop yeild prediction and how developing countries are adopting machine learing algorithms. The total 37 research articles has been selected over which the SLR is documented. The study shows that RF, SVm and CNN is most common algorithms for crop yeild prediction.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 비교 분석하는 체계적 문헌고찰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 평가에 중점을 두고 있습니다."
Deep-learning based Buck-boost Converter for Laptop Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138855,"The growth of power electronics increase rapidly in the recent past compared to other technical work. A converter may be the heart of power electronics. It has applications like PV, home inverters, battery charging, etc. There are a lot of issues related to the converters and one of them is the output DC voltage without ripples. In other words, the early steady-state output is the requirement for the converter. As the early steady-state achieves, the converter output is close to the ideal one. In the literature, there were different models used to achieve it. But, unfortunately, the machine-learning technique literature is very few, as the working of machine-learning with power electronics is a newer one. In this paper, we propose a deep-learning-based buck-boost converter model that will achieve the buck level or boost level with early steady-state. The training of the model is achieved by using the PID controller by acquiring the data from the PID controller. The input voltage considered for the simulation is 12V. The output voltage is suitable for laptop charging when the model is working in boost mode and are suitable for the USB port of the laptop when working in buck mode. The simulation results validate our deep-learning model. The results were far better than the results achieved from the PID controller. Existing evaluation measures i.e settling time and overshoot are used to validate the proposed model. The results clearly show the achievement of an early-steady state and getting stability earlier with the proposed model. The problem of output voltage ripples is almost solved (in an ideal case, there are no output ripples and in our case, the ripples are less than the tenth part of one percent ripples. There is a table used to show the promising results compared to the PID controller results.",Yes,"본 논문은 딥러닝 기반의 벅-부스트 컨버터 모델을 제안하고, PID 컨트롤러 데이터를 이용해 모델을 학습시키며 시뮬레이션 결과를 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
A Review Transformation Customer Service with Customer Relationship Management (CRM) Based Chatbots,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10462818,"Chatbots have become an increasingly important solution in digital transformation in various business sectors. The role of chatbots not only provides convenience in accessing information but also plays a role in interacting with customers, creating personalized marketing, and developing artificial intelligence technology. The main objective of this study is to identify the gaps in current knowledge about the factors that influence customer satisfaction in interactions with chatbot services and develop a reliable approach to evaluate customer satisfaction. The study utilizes a systematic literature review (SLR) methodology to identify, assess, and analyze existing research on a specific question, providing a concise summary of the literature and insights into current knowledge, essential for translating evidence, identifying research gaps, and establishing frameworks, with evaluations conducted rigorously and consistently following a predetermined search strategy for completeness and scientific validity. The results show that there are a number of factors that have a significant impact on customer satisfaction after the experience with chatbots, such as the ability to collect sensitive user information, possible errors in the interpretation of user intent, the capability to reduce customer wait times and improve customer experience, and the importance of understanding user queries, although these similar factors have more significant impacts than reliability and quality conversation. In addition, there is a growing trend in the industry, where chatbot technologies are becoming increasingly important solutions in providing more efficient services and interactions between companies and customers.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 통해 기존 연구들을 요약하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 연구 갭 도출에 중점을 두고 있습니다."
Artificial Intelligence Based Automated Appliances in Smart Home,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537773,"Smart home and Artificial Intelligence (AI) expertise are quickly evolving, and a variety of smart home appliances incorporating AI have improved occupant quality of life. Although some studies looked at how artificial intelligence could be used in smart homes, few publications looked at how literature and products could be integrated. The advancement in smart home applications with technology development and the relevant literature are articulated in this paper. The functionalities and various procedures of AI are defined through literature and product reviews. This paper describes AI based gesture identification system through which the various appliances in home are controlled to make it ON or OFF without the use of Internet service. Hardware implementation is made for few appliances and based on the image captured through the gesture notation is evaluated using AI based algorithms and analyzed for controlling the appliance successfully. This system will be much helpful for the disabled or aged people, who can’t control the switch physically and comfortably.",Yes,"논문 초록에서 AI 기반 제스처 인식 시스템을 하드웨어로 구현하고, 이를 통해 가전제품을 제어하는 독창적인 연구 내용을 포함하고 있음을 알 수 있습니다. 또한, AI 알고리즘을 활용한 평가 및 분석이 이루어져 직접적인 연구 기여가 있다고 판단됩니다."
Medical Recommender Systems: a Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508695,"Medical recommender systems are applications in the field of health. These systems use Artificial Intelligence techniques to provide personalized recommendations to healthcare professionals and patients, based on available and relevant patient information. Software engineering is essential in developing medical recommender systems, as these systems must be accurate, reliable, and secure for use in clinical settings. This work presents a Systematic Literature Review based on the Kitchenham and Charters guide, in order to explore the Artificial Intelligence techniques used in this type of system, which can be incorporated or improved by software developers who participate in this type of project. Twelve primary studies were selected, where mainly machine learning approaches were identified (algorithms based on decision trees, neural networks, Bayesian classifiers and clustering such as k-means), matrix approaches, based on rules, among others. Precision, Recall, and Root Mean Square Error (RMSE) were the main measures used to evaluate the performance of these systems. Finally, the studies propose always increasing the sample size of the tests carried out, including relevant patient information such as social networks and clinical information, as well as exploring other algorithms and approaches that allow improving the results of the recommendation.",No,"이 논문은 의료 추천 시스템에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합하고 분석한 리뷰 논문에 해당합니다."
"A Comprehensive Review of AI Techniques for Resource Management in Fog Computing: Trends, Challenges, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643044,"Fog computing (FC), extending cloud services to the network edge, has emerged as a key paradigm for low-latency applications like the Internet of Things (IoT). However, efficient resource management, task scheduling, and load balancing pose challenges in fog environments. This review surveys recent research efforts aimed at addressing these challenges and optimizing FC performance. We conducted a systematic analysis of relevant research papers on FC published in reputable academic databases. The review focused on studies published between 2019 and 2024 and emphasized artificial intelligence based studies exploring resource management, task scheduling, and load balancing techniques within the FC domain. The review identifies a diverse range of techniques applied to optimize FC performance. These include machine learning (ML) and deep learning (DL) for resource allocation, heuristic algorithms for task scheduling, and nature-inspired meta-heuristics for load balancing. The review evaluates the strengths and limitations of these approaches, highlighting their impact on metrics like latency, energy consumption, and Quality of Service (QoS). This review demonstrates the significant progress made in optimizing FC through innovative techniques. ML and meta-heuristics have emerged as promising approaches for resource management, task scheduling, and load balancing, respectively. However, challenges persist in areas like real-world implementation complexities and ensuring service quality across geographically distributed fog networks. Future research directions are identified, emphasizing the need for further exploration of these challenges and the integration of emerging technologies like deep reinforcement learning for enhanced FC performance.",No,논문 초록에서 본 연구는 기존 연구들을 체계적으로 분석하고 정리한 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 기여를 포함한 연구 논문이 아니라 기존 연구의 종합적 고찰에 해당합니다.
Analysis of Compatibility in Open Source Android Mobile Apps,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233032,"Non-functional requirements (NFRs) form an intrinsic part of any software system. Compatibility between versions or different platforms of a software product is a form of NFRs. In this paper, we have studied Compatibility in open-source mobile apps. We are interested in understanding the different aspects of mobile incompatibility, their frequency of occurrence from a user perspective, and how much effort developers have spent on it. We have conducted a study on 40 randomly selected open-source mobile apps from the Google Play Store and have analyzed 258,056 commits (extracted from their version control system) to identify incompatibility issues in apps. We have also studied 205,847 reviews to identify and categorize compatibility requirements from user reviews. Both app commits and app reviews were processed by a pipeline of Natural Language Processing steps. We evaluated the efficiency of four Machine Learning classifiers to analyze compatibility. This was done by classifying commit messages and analyzing user reviews. We observed that the Logistic Regression classifier produced the best overall results. For the same data set, we classified compatibility types. In that case, the Support Vector Machine classifier performed marginally better over the other classifiers. Addressing the relative effort spent on compatibility, we found that 3.16% of the developer's effort is dedicated to compatibility issues. At the same time, we observed that 4.30% of user reviews report compatibility issues in apps. In conclusion, we see more demand for future research on (i)the gap between the time spent by the developers and the frequency of occurrence of compatibility issues, and (ii) the degree of responsiveness on actual user concerns.",Yes,"본 논문은 오픈소스 모바일 앱의 호환성 문제를 분석하기 위해 40개 앱의 커밋과 사용자 리뷰 데이터를 수집하고, 자연어 처리 및 머신러닝 기법을 적용하여 직접적인 연구를 수행하였다. 이는 기존 연구를 요약하거나 리뷰하는 것이 아니라, 독창적인 데이터 분석과 실험 결과를 포함한 연구 논문에 해당한다."
FEEL: FEderated LEarning Framework for ELderly Healthcare Using Edge-IoMT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024778,"Recent advancements in artificial intelligence (AI) and IoT technology have revolutionized the healthcare industry by providing effective remote healthcare. Furthermore, with the aging of the world’s population, remote health monitoring and recommendations are becoming imperative to provide cost-effective healthcare solutions for improving the quality of life of our senior citizens. The explosive growth of wearable sensors (IoT sensors) and health bands has facilitated the interconnection among patients and caregivers to enable assisted living by leveraging AI techniques. This work proposes an end-to-end connected smart home healthcare system (FEEL) for elderly people. Our proposed framework addresses the main challenges of the Internet of Medical Things (IoMT) system namely, the scarcity of labeled data and user’s diverse needs. The major contributions of the work are: 1) few-shot learning-enabled novel federated learning (FL) framework for health data and context information analysis and recommendation; 2) user and context-based knowledge graph (UKG) to represent and model health parameters and environmental impacts on recommendations; 3) deep learning architecture for activity monitoring and location estimation of the users; and 4) edge-fog-IoMT collaborative framework to collect, store, and share medical recommendations while protecting the privacy of the users. FEEL is specifically beneficial for elderly homes where several aged people stay together and require constant care. We aim to develop a novel AI module where along with the health parameters, the social context of the home can be augmented to provide an accurate and improved healthcare service. FEEL has been evaluated for three tasks, namely: 1) activity monitoring and location estimation; 2) fall detection; and 3) medical recommendations for unusual health conditions. A customized wearable device has been used to collect, store, and send health-related parameters. The experimental evaluation demonstrates promising accuracy (F1 score 0.86–0.94 range) for the tasks and outperforms the baselines by a significant margin ( ≈10 %–16%).",Yes,"논문은 노인 건강 관리를 위한 연합 학습 기반의 새로운 AI 프레임워크(FEEL)를 제안하고, 사용자 맞춤형 건강 모니터링 및 추천 시스템을 개발하여 실험적으로 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 기술적 기여와 실험 결과를 통해 기존 방법 대비 성능 향상을 입증하고 있어 연구 논문에 해당한다."
Bias Amplification to Facilitate the Systematic Evaluation of Bias Mitigation Methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10744554,"The future of artificial intelligence (AI) safety is expected to include bias mitigation methods from development to application. The complexity and integration of these methods could grow in conjunction with advances in AI and human-AI interactions. Numerous methods are being proposed to mitigate bias, but without a structured way to compare their strengths and weaknesses. In this work, we present two approaches to systematically amplify subgroup performance bias. These approaches allow for the evaluation and comparison of the effectiveness of bias mitigation methods on AI models by varying the degrees of bias, and can be applied to any classification model. We used these approaches to compare four off-the-shelf bias mitigation methods. Both amplification approaches promote the development of learning shortcuts in which the model forms associations between patient attributes and AI output. We demonstrate these approaches in a case study, evaluating bias in the determination of COVID status from chest x-rays. The maximum achieved increase in performance bias, measured as a difference in predicted prevalence, was 72% and 32% for bias between subgroups related to patient sex and race, respectively. These changes in predicted prevalence were not accompanied by substantial changes in the differences in subgroup area under the receiver operating characteristic curves, indicating that the increased bias is due to the formation of learning shortcuts, not a difference in ability to distinguish positive and negative patients between subgroups.",Yes,"본 논문은 편향 완화 방법의 효과를 체계적으로 평가하기 위한 두 가지 편향 증폭 접근법을 제안하고, 이를 실제 AI 모델과 사례 연구에 적용하여 성능을 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Analysis of Android Malware Detection (AMD)To Recognisecryptoware Usingdeep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525925,"Android has surpassed as the most important mobile operating system, taking a considerably larger share of the global market. Android malware poses severe dangers to mobile device security and the services it supports as its volume and sophistication continue to rise. As a result, there is significant interest in applying machine learning to improve the detection of android malware. The main cause of this is a dearth of knowledge, resources, and techniques for detecting android malware.This is mostly caused by a dearth of knowledge, resources, and techniques for detecting android malware. Using classic statistical techniques to forecast malware detection has usually led to the development of underwhelming detection models, leaving a number of questions unresolved. Machine learning has become the industry standard for identifying android malware in massive data sets thanks to the simple availability of advanced computational resources. The Machine Learning (ML) and Deep Learning (DL) models for Android malware detection are examined in this extensive literature study.This article review’s two main subheadings are AMD using ML and DL in data mining. Last but not least, DL approach is the best Android malware detection for productive outcomes. It can be seen that a variety of inputs were utilised to investigate autonomous feature extraction algorithms and forecasting techniques. DL techniques improve the processing efficiency of large real-time data sets. Recent DL research has revealed the most efficient hybrid processing techniques for data samples from Android apps.The output of DL and ML algorithms is evaluated using the dataset acquired from the Drebin project. In addition to being frequently used by researchers, the Drebin samples are also openly accessible. Three measures are used to assess the performance of ML and DL algorithms: recall, F1-score, and weighted F-measure (WFM). MATLAB was used to implement these metrics in their calculation and application. The J48, Heterogeneous Information Network (HIN), Gated Recurrent Unit (GRU), and Deep Belief Network (DBN)algorithms were used to compare the outcomes to well-known ML and DL techniques. The proposed method displayed better performance in differentiating android malware detection when compared to the other analysed methods.",Yes,"본 논문은 Android 악성코드 탐지를 위해 머신러닝과 딥러닝 기법을 적용하고, Drebin 데이터셋을 사용하여 여러 알고리즘의 성능을 비교 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Meta-Learning in Supervised Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029537,"In the present digital era, a popular use of Machine learning is knowledge mining from big data. Machine learning is the sub-branch of Artificial Intelligence (AI) that extracts rules automatically from Big Data for decision-making to build expert systems. Meta-Learning is a sub-branch of machine learning, which uses machine learning classifiers that learns to map and combine predictions and information of data of other ML-models in the field of ensemble-learning. Meta-learning helps us to select the best/right learning algorithms to solve a particular problem. It maps from the meta-data of other machine learning algorithms by evaluating it on different datasets. In this paper, we have presented very recent state-of-the-art research works on meta-learning. We have categorized meta-learning on supervised learning data sets into three categories: (1) Task Independent Recommendation, (2) Configuration Space Design, and (3) Configuration Transfer, and reviewed the recent works on each category.",No,초록에서 본 논문은 메타러닝에 관한 최근 연구들을 정리하고 분류하는 리뷰 논문으로 보입니다. 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구들을 요약하고 분류하는 데 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Machine Learning - Imaging Applications in Transport Systems: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389341,"Transport systems are fundamental to supporting economic growth, and the need for smarter, safer, more efficient and climate neutral transport systems continues to grow. Maintenance and operation of transport infrastructure is expensive and has many difficulties. In recent years, the application of machine learning to solve problems has been pursued with varying success rates. Many open problems still remain at this stage. This paper provides a review of deep learning applications in transport systems. Multiple deep learning applications are discussed e.g. railway safety, self-driving cars, pedestrian crossing and traffic light detection. Reviewed papers are evaluated in terms of challenges, contribution, weakness, research gaps. Key research questions for future study are proposed: performance optimization, data set improvement and the need for research on real-time performance on edge devices.",No,"본 논문은 다양한 딥러닝 응용 사례를 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 평가하는 문헌 리뷰에 해당합니다."
Computer Vision Intelligence Test Modeling and Generation: A Case Study on Smart OCR,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685196,"AI-based systems possess distinctive characteristics and introduce challenges in quality evaluation at the same time. Consequently, ensuring and validating AI software quality is of critical importance. In this paper, we present an effective AI software functional testing model to address this challenge. Specifically, we first present a comprehensive literature review of previous work, covering key facets of AI software testing processes. We then introduce a 3D classification model to systematically evaluate the image-based text extraction AI function, as well as test coverage criteria and complexity. To evaluate the performance of our proposed AI software quality test, we propose four evaluation metrics to cover different aspects. Finally, based on the proposed framework and defined metrics, a mobile Optical Character Recognition (OCR) case study is presented to demonstrate the framework's effectiveness and capability in assessing AI function quality.",Yes,"본 논문은 AI 소프트웨어 기능 테스트 모델을 제안하고, 3D 분류 모델과 평가 지표를 새롭게 도입하여 스마트 OCR 사례에 적용한 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구를 단순히 정리한 리뷰가 아니라 직접적인 연구 기여가 있는 논문으로 판단된다."
Big Data Monetization: Platforms and Business Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9476385,"The main goal of our PhD project is to propose a methodology that can be widely applied to determine the monetary value of a Datum in a Big Data environment. In order to define our problem, we started by investigating how monetization in those environments have been suggested in academic literature. By observing the results, it was possible to conclude that little attention has been given to the dimension Value in academic studies related to Big Data when compared to researches directed to the other three classical dimensions (Volume, Velocity and Variety). More specifically, in terms of economic value, studies are even scarcer, and the existing ones do not share a common view on how to measure this value. Bearing that in mind, we have drawn our hypothesis that it would be possible to develop a data-driven methodology (quantitative methods based on artificial intelligence) that could highlight the monetary value of a data asset (value dimension) taking into account the different applications, contexts and scenarios related to those data. We applied a formal process of systematic literature review based on the methodology suggested by Kitchenham to find out what methods have been applied to determine the relevance and value of data in these environments and if these methods are based, in any way, on information theory. The results showed that, in spite of the progress made on the topics of Big Data and the application of analytical methodologies over the last decades, there is no method based on data that is widely used to determine the value of a datum in a Big Data environment. If, on the one hand, monetization in Big Data environments is still a field that needs to be better explored in academic literature, on the other hand, these intangible assets, i.e., data, grow exponentially and are more and more present in the corporate world. It highlights the opportunity to develop studies in search of standards that can be widely accepted and used to this end. We have now finished 12 moths on this research (1st year).",No,"본 논문은 빅데이터 환경에서 데이터의 화폐적 가치를 평가하는 방법론을 제안하기 위한 문헌 조사와 연구 계획을 설명하고 있으며, 아직 직접적인 독창적 연구 결과나 실험적 기여를 포함하고 있지 않습니다. 따라서 현재 시점에서는 연구 논문으로 보기 어렵습니다."
A review on Brain Tumor Detection using Deep Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10134594,"Artificial Intelligence has in the last 10 years drastically transformed the globe. Owing to its ability to manage vast volumes of data, deep Learning (DL) a subsection of Artificial Intelligence and machine learning, especially in the healthcare area, has proven exceptional accomplishments. New developments in DL based techniques such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), Generative adversarial neural networks, and many more that have shown exceptional potential for inaccurate categorization of brain cancers. The major goal of this paper is mainly to critically examine the previous identification and classification efforts of brain tumors using MRI (Magnetic Resonance Imaging) data. In this work, we review the best current algorithms for deep learning applications in Brain tumor classification problem contexts and utilization. The most significant findings that have been documented thus far are addressed, and finally, the research’s present hurdles are also highlighted. The conclusion discusses the pros and limitations of deep neural networks. In the time ahead using the results reported in this paper, researchers will be able to evaluate recent studies extensively and gain a knowledge of how different DL algorithms function.",No,"논문 초록에서 본 연구는 기존의 뇌종양 탐지에 관한 딥러닝 기법들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않고, 기존 연구들을 비평적으로 분석하는 데 중점을 두고 있습니다."
EvauleBlock: Evaluating Answers in E-Learning Applications with Enhanced Security and Intelligence through DRL and Blockchain Integration,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10718561,"Evaluating the candidate answers during online examination by a human examiner is wearisome task. Along with that, security concerns also associated in the E-Learning environment in terms of false data injection and modification attacks respectively. The state of the art work, either focus on designing intelligent and robust question answering model using Machine Learning (ML)/Deep Learning (DL) algorithm, or designing a secure framework for E-Learning environment using blockchain. To neutralize that issue, we design a secure and intelligent model for E-Learning environment using Deep Reinforcement Learning (DRL) and blockchain methodology respectively named EvauleBlock. The EvauleBlock composed of four major processes such as data assortment, data pre-processing, intelligent answer evaluation, and blockchain storage. In data assortment stage, the training of the proposed DRL algorithm named Multi Agent Deep Deterministic Policy Gradient (MADDPG) is taken place by crawling the related question answering data from the various websites. Followed by, we have performed pre-processing for the candidate written answers using adequate pre-processing steps. Once the answers are pre-processed, it is then provided to the MADDPG algorithm which employs four agents for validating the candidate answers by checking the semantic, syntactic, keyword, and context similarity scoring respectively. At last, the overall scores are securely stored in the blockchain using Proof of Work (PoW) consensus algorithm. The implementation of this work is carried out using python 3.11.0 tool and performance of the proposed model are validated and compared with adequate performance metrics. The results are promising when compared to the conventional models.",Yes,"본 논문은 DRL과 블록체인 통합을 통한 새로운 E-Learning 답안 평가 모델인 EvauleBlock을 제안하고, 이를 구현 및 성능 검증한 연구 내용을 포함하고 있다. 따라서 독창적인 알고리즘 설계와 실험 결과를 제시하는 연구 논문에 해당한다."
Air Quality Monitoring through LoRa Technologies: A Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317300,"The impact that air quality has on a human being is significant. Millions of deaths are associated with air pollution and health problems. Several standards and policies to regulate and improve air quality have been established by both the World Health Organization and the Environmental Protection Agency. Currently, several authors proposed cost-effective devices for measuring air quality in real-time. The proliferation of low-cost options to measure air quality and acquire sensor data. Moreover, it is possible to develop novel techniques and use artificial intelligence to predict poor air quality scenarios. This paper presents a literature review on IoT architectures for air quality monitoring using LoRa communication technology. On the one hand, the objective of this study is to present a literature review to support future research initiates on the application of air quality monitoring system using LoRa. On the other hand, the main contribution of the paper is to present an overview on sensors, power source, communication, data storage, processing, and visualization technologies used in the proposed methods available in the literature.",No,"본 논문은 LoRa 기술을 이용한 공기질 모니터링에 관한 기존 연구들을 종합하여 문헌 리뷰를 제공하는 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 정리하고 분석하는 데 중점을 둔 리뷰 논문입니다."
Artificial Intelligence Based on Recommendation System for Startup Matchmaking Platform,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10118708,"Artificial intelligence platform-based software is currently required and recommended for education and have innovation used to develop, implement, and evaluate the process of an activity. An artificial intelligence-based Startup Matchmaking platform with a recommendation feature in the system can make it easier to bring together startups and partners as needed through a matchmaking process. In general, Startups that are being built must look for partners one by one to work, which will take a long time. To solve this problem, we need to replace the manual method to get answers to all kinds of theory questions and common questions related to startups. The main goal of this research is to create an intelligent platform that can help the industry find information through matchmaking startups. To develop this study, we use the Study Literature Review (SLR) method based on the needs where startups that are growing fast. In this research, the system is implemented entirely with integration with Communicate. Novelty in this study is the use of Artificial Intelligence in Matchmaking startups.",Yes,"논문 초록에서 인공지능 기반 스타트업 매칭 플랫폼을 개발하고 구현한 연구임을 명확히 밝히고 있으며, 기존 수동 방식의 문제점을 해결하기 위한 새로운 시스템을 제안하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문으로 판단된다."
The Trajectory of Artificial Intelligence Research in Higher Education: A Bibliometric Analysis and Visualisation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519368,"The rapid development of artificial intelligence (AI) and its applications is gaining global attention and promises to revolutionise every aspect of human life, including education. AI will transform higher education (HE) institutions through improved adaptation and competitiveness. The application of AI in HE is an emerging research area with limited review. Our paper seeks to provide a comprehensive bibliometric analysis and visualisation of research on AI application in HE in the past two decades. We evaluated 283 articles published by researchers from 59 countries in the Scopus database over the past two decades based on explicit inclusion and exclusion criteria. The study applied various bibliometric indicators and word analysis to examine emergent trends. VOSviewer was used for visualisations to map a knowledge base by uncovering keywords used within AI in HE. The results show the number of AI articles published per year, their geographic distribution, analysis by subject area and keywords, and research trends. Research in AI is interdisciplinary, dominated by computer science and engineering fields. AI research in HE is growing, the first 15 years contributed 22%, and the last five years yielded 78%. Countries with a high investment in research dominated AI research, with China and the United States leading. There is very little research from developing countries. Our paper highlights current and future research directions in AI in education, and its limitations.",No,"본 논문은 인공지능 연구 동향을 분석하고 시각화하는 문헌계량 분석 연구로, 기존 연구들을 종합하여 트렌드를 파악하는 리뷰 성격의 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
A Systematic Literature Review on Security of Vehicular Ad-Hoc Network (VANET) Based on VEINS Framework,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10122499,"Innovative framework on Vehicles in Network Simulation (VEINS) for Vehicular Ad-hoc Network (VANET) that use security aspect is mainly limited and dispersed. In order to offer valuable visions for technical settings and researchers, the study looked into the trends and gaps that were currently present. As a result, this systematic literature review was carried out to develop a comprehensive taxonomy of the research landscape. A thorough study was done for papers about (a) VANET, (b) VEINS, and (C) security aspects. This research used three databases, namely IEEE Xplore, ScienceDirect, and Scopus. These databases included in-depth research focused on VANET based on the VEINS framework. Then, on the basis of the security aspect, filtering was accomplished. The first class includes threats and vulnerabilities that evaluate the effects of threats and vulnerabilities on VANETs by using the VEINS framework and suggest ways to mitigate or lessen their effects. The second category includes articles on the solution technology that uses blockchain, machine learning, and Software-Defined Networking (SDN) techniques in VEINS-based VANET applications. The third class comprises the requirements that satisfy privacy, authentication, trust management, reliability, and revocation of the VANET security-based VEINS framework. Finally, this paper reviews the architecture and bidirectional coupling of the VEINS framework.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 분류하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 갭을 분석하는 데 중점을 두고 있습니다."
Leveraging Artificial Intelligence for Knowledge Management A Systematic Literature Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701138,"This research examines the growth of Artificial Intelligence (AI) and Knowledge Management (KM) capabilities, evaluating how AI can enhance KM by managing information, data, and knowledge within organizations. Utilizing a systematic literature review method with PRISMA guidelines, the study reviewed 72 articles from Scopus (1994-2024). Findings indicate that AI implementation in KM spans various fields, predominantly within technology. AI integration is identified as a catalyst for innovation, efficiency, and organizational learning. The study highlights the increasing application of AI in KM and the evolving nature of this research area. Insights are provided into the potential for innovation and future development of AI in KM, emphasizing the necessity for organizational members to leverage AI to enhance performance.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 분석한 것으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 통찰을 제공하는 리뷰 논문에 해당합니다."
Discerning and Leveraging Gastric Cancer Using Deep Analytics Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829130,"Gastric disease (GC) is one of the most prevalent malignant developments with poor prognostic outcomes. Endoscopic evaluation is primarily used for early recognition, while pathological confirmation and CT scanning are used for further treatment. The shortage of pathologists presents an opportunity for AI systems to ease the workload and increase diagnostic accuracy. Most gastric cancers show genetic instability, either microsatellite or chromosomal, which occurs early in carcinogenesis. New classifications based on histologic features, genotypes, and molecular profiles improve understanding, diagnosis, prevention, and treatment of each subtype. This paper introduces “Gastronet,” a deep learning system integrating three algorithms—Perform Multiple Tasks Net, Mix Net, and Overall Net—to enhance gastric cancer diagnosis. Gastronet aims to provide precise predictions with minimal additional confirmation, surpassing current diagnostic models. The roadmap includes a comprehensive literature review to identify gaps in current diagnostic methods and the potential for AI, followed by the development of Perform Multiple Tasks Net, Mix Net, and Overall Net to handle specific diagnostic tasks and data types. Data collection and preprocessing ensure a quality dataset for training and validation. Each network is trained and validated separately, then integrated and fine-tuned within the Gastronet framework. The system’s performance is compared with current models, demonstrating superior accuracy. Implementation involves developing a user-friendly interface for clinical use and integrating it into workflows. Continuous improvement is ensured by updating the system with new data and advancements in gastric cancer research. By following this roadmap, the paper aims to develop a reliable, accurate, and efficient system for early detection and diagnosis of gastric cancer, improving patient outcomes and supporting pathologists.",Yes,"논문 초록에서 제안된 ""Gastronet""이라는 딥러닝 시스템 개발과 성능 평가, 데이터 수집 및 전처리, 모델 통합 및 임상 적용 방안 등이 구체적으로 기술되어 있어 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 이는 단순 리뷰나 개념 제안이 아닌 실제 연구 개발과 검증을 포함한 논문임을 의미합니다."
Empowering Rural Minds: Leveraging Predictive Analytics to Bridge Urban-Rural Educational Disparities Through Financial Assistance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425432,"Access to quality education is a critical factor in individual empowerment, economic development, and societal progress. However, students from rural backgrounds often face significant obstacles when it comes to pursuing technical education, primarily due to financial barriers. Financial barriers act as a deterrent, restricting the participation of rural students in technical education programmes. This paper presents an innovative approach to address this issue by leveraging the power of artificial intelligence (AI) and predictive analytics to provide equitable financial assistance and scholarships for technical education. By adopting a comprehensive strategy, this paper addresses the biases, transparency shortcomings, and contextual factors that have been prevalent in previous algorithm methods. The strategy employed in this study involves a multi-faceted approach, integrating various methodologies to achieve optimal outcomes. The key steps encompassed in the methodology include algorithmic fairness, model training and selection, transparency and interpretability, human-in-the-loop review, contextualised assessment, continuous learning, and feedback loops. Adversarial Debiasing is utilised to mitigate biases and ensure a fair decision-making process. To enhance transparency and interpretability, rule-based systems and LIME (Local Interpretable Model-Agnostic Explanations) are employed. Human-in-the-loop review is an integral part of the methodology, ensuring that AI-driven decisions are validated and refined by human experts. The expected outcome of this research is an AI-enabled financial assistance system that democratises technical education by making it more accessible to students from rural backgrounds. The implementation of this model can have a transformative impact on education equality, empowering rural minds to pursue their academic dreams.",Yes,"논문 초록에서 제시된 방법론과 기술적 접근(Adversarial Debiasing, LIME, human-in-the-loop 등)은 독창적인 연구 내용과 구체적인 알고리즘 개발 및 적용을 포함하고 있어, 직접 기여하는 연구 논문으로 판단된다. 또한, 기존 문제점들을 해결하기 위한 새로운 전략과 모델을 제안하고 있어 연구 논문에 해당한다."
A Survey paper on Understanding the Rise of AI-driven Cyber Crime and Strategies for Proactive Digital Defenders,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10607393,"As artificial intelligence (AI) continues to evolve, so too does its integration into cyber criminal activities, presenting a formidable challenge to digital security. These findings investigate the escalating nexus between AI and cybercrimes, highlighting the emergent dangers posed by AI-driven malicious activities. The study delves into the various ways in which AI technologies are leveraged by cyber criminals to orchestrate sophisticated attacks, incorporating, data breaches, malware propagation, phishing, and social engineering tactics. Furthermore, the methodologies propose proactive research strategies aimed at mitigating the threats posed by AI-facilitated cybercrimes through the lens of digital forensic techniques. By analyzing current trends in AI-driven cyber offenses and their repercussions on digital security frameworks, this research endeavors to elucidate the imperative for novel approaches in digital forensics. Such proactive strategies encompass the establishment of AI-powered forensic tools, the enhancement of detection and attribution methodologies, and the augmentation of cyber resilience through predictive analytics and preemptive measures. Through a comprehensive review of existing literature, case studies, and empirical data, paper analysis seeks to offer insights into the changing landscape of AI-facilitated cybercrimes and the critical importance of digital forensics in countering these threats. By Gaining a more comprehensive grasp of the combined effects or interactions between AI technologies and cyber criminality, this research endeavors to inform stakeholders in the realms of cybersecurity, law enforcement, and policy-making, thereby contributing to the progress or development in proactive measures aimed at safeguarding digital ecosystems against emerging threats.",No,"본 논문은 AI 기반 사이버 범죄와 대응 전략에 대한 종합적인 문헌 조사 및 현황 분석을 다루는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 통합하는 리뷰 성격이 강합니다."
Advancements in Automated Tumor Detection Screening: A Comprehensive Review and Comparative Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10593110,"This paper offers a thorough examination and comparative assessment of recent advancements in automated tumor detection methodologies. Covering a range of aspects related to tumor detection, such as image capture, preprocessing, feature extraction, classification methods, and performance assessment measures, the review delves into the application of multiple medical imaging modalities, including computed tomography (CT), magnetic resonance imaging (MRI), and histopathological photography. Additionally, it looks at how deep learning and machine learning algorithms— specifically, support vector machines (SVMs), convolutional neural networks (CNNs), and ensemble learning techniques—achieve accurate tumor categorization. Moreover, the paper addresses challenges and limitations inherent in automated tumor detection systems, including data imbalance, interpretability of machine learning models, and integration with clinical workflows. By conducting a comparative analysis of existing automated tumor detection approaches, this paper aims to provide researchers and clinicians with valuable insights into current state-of-the-art techniques, their strengths, and limitations. Furthermore, it identifies promising avenues for future research to advance the field of automated tumor detection screening and ultimately improve patient outcomes in oncology.",No,"본 논문은 기존 연구들을 종합적으로 검토하고 비교 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
How Digital Games are Engaging Our Children Toward STEM Careers?,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892917,"Mathematics is a crucial skill in modern society, enhancing problem-solving abilities across various fields, from medicine to philosophy. Its concepts underpin major information systems, including machine learning and mobile applications. Nonetheless, many countries experience significant gaps in students' understanding of fundamental mathematical concepts, leading to widespread math anxiety, particularly concerning fractions. Moreover, numerous studies indicate that digital games positively influence learning outcomes by providing an engaging teaching method. To understand how existing digital games have been used to teach fractions, we conducted a Systematic Literature Review (SLR) that explores digital games that support teaching fractions and examines how they have been integrated into teaching strategies, including how their pedagogical effectiveness is assessed. Our findings underscore the need for more tools compatible with Learning Management Systems (LMS) and improved accessibility to the games/tools mentioned in the literature. Additionally, there is an urgent need to develop metrics for evaluating the effectiveness of game-based learning tools for fractions.",No,"논문은 디지털 게임을 활용한 분수 교육에 관한 기존 연구들을 체계적으로 검토한 문헌 리뷰(SLR)로, 직접적인 실험이나 새로운 연구 결과를 제시하지 않고 기존 연구들을 종합하여 분석한 내용에 초점이 맞춰져 있습니다. 따라서 독창적인 연구 기여보다는 기존 연구의 종합 및 평가에 해당합니다."
Advancement in Graph Neural Networks for EEG Signal Analysis and Application: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10916656,"Electroencephalography (EEG) can non-invasively measure neuronal events and reflect brain activity at different locations on the scalp. Early studies for EEG signal processing have focused more on extracting EEG temporal features and considered the topology of EEG channels less due to the limitation of rich spatial information. Graph neural networks (GNNs), as a new kind of deep learning method, can use EEG signals as graph vertices, capturing the hidden topological connections between signals. GNNs have made great progress in EEG studies due to the advantage. In this overview, we review the very new and fundamental models of GNNs and their modifications, such as graph regularized neural networks, graph convolutional neural networks, spatial-temporal graph neural networks, graph attention networks, and their variants in EEG signal analysis fields. The applications of GNNs are summarized in the domains of emotion detection, epilepsy seizure detection, stroke rehabilitation, Alzheimer’s disease diagnosis, motor imagery detection, neurological disease diagnosis, major depressive disorder, and driving fatigue detection. We employed a Systematic Literature Review (SLR) approach to select 79 papers for a comprehensive review. The current state is analyzed and forecasts are provided based on the available difficulties. We conclude by suggesting potential directions for future research in this rapidly developing topic.",No,"본 논문은 EEG 신호 분석을 위한 그래프 신경망(GNN)의 발전 현황을 종합적으로 정리한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 체계적으로 분석하고 요약하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 평가에 해당합니다."
Leveraging Industry 4.0 Technologies for Reverse Logistics Financial Performance: a Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10878230,"Despite the growing interest in Industry 4.0 (I4.0) technologies, their implementations across various fields, and their potential role in improving reverse logistics (RL) operations; the financial advantages of I4.0 technologies remain unexplored and lacking in application. This study deployed a systematic literature review (SLR) to identify and examine the key I4.0 technologies applicable in RL operations, and their implications on financial outcomes, to propose a novel taxonomy of reverse logistics financial performance (RLFP), and to suggest directions for further investigations. A state-of-the-art SLR of relevant peer-reviewed RLFP published between 2000 and 2023 is conducted. The qualified studies are evaluated qualitatively using template analysis through dynamic capability theory (DCT). Results indicated that I4.0 technologies positively influence RLFP, with artificial intelligence (AI), big data, advanced robotics, and internet-of-things (IoT) demonstrating dominant effects on financial results. However, findings revealed limited research efforts on cyber-physical systems (CPS), cloud computing, and cognitive computing as I4.0 technologies, indicating a substantial gap in the literature. Results suggested that operational efficiency, flexibility, risk management, and real-time visibility are the key financial performance metrics for I4.0 technology adoption. Findings further emphasized that the application of I4.0 improves competitive advantage, revenue generation opportunities, customer satisfaction, and cash flow improvement, and lowers operational expenses, which drives RLFP.",No,"본 논문은 체계적 문헌고찰(Systematic Literature Review)을 수행하여 기존 연구들을 종합하고 분석하는 데 중점을 두고 있으며, 직접적인 독창적 실험이나 새로운 연구 데이터를 제시하지 않는다. 따라서 새로운 연구 결과를 직접 기여하는 연구 논문으로 보기 어렵다."
Anomaly Based Intrusion Detection System: A Deep Learning Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323740,"In recent years, computer networks have seen a considerable proliferation in terms of performance and total traffic volume. At the same time, cyber attacks have been on the rise ever since, which led to the emergence of Intrusion Detection Systems (IDSs) to deal with them. Conversely, artificial intelligence has been a popular technique that can be applied to a variety of purposes including detection of cyber attacks. However, most related work that leveraged artificial intelligence classifiers to address this problem used outdated datasets. In this paper, we implemented an anomaly-based intrusion detection system using deep learning algorithms with the goal of achieving higher performance while using a newer dataset. That is why we used the NSL-KDD dataset, which constitutes an improvement over the widely used KDD Cup 99 dataset, as it addresses some of its imperfections such as duplicated records and obsolete attack types. Then, we developed three deep learning classifiers that are Recurrent Neural Networks (RNNs), Multi-Layer Perceptron (MLP), and a hybrid Convolutional Neural Network-Long Short Term Memory (CNN-LSTM) model. Also, we compared the effectiveness of our proposed model with Machine Learning classifiers such as Support Machine Vector (SVM), K-Nearest Neighbor (KNN), and Gradient Boosting (GB). Finally, we validate our findings with a performance evaluation of our model, which showed encouraging results.",Yes,"본 논문은 최신 데이터셋(NSL-KDD)을 사용하여 딥러닝 기반 이상 탐지 시스템을 구현하고, 여러 딥러닝 및 머신러닝 모델을 개발 및 비교 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Short-Term Temperature and Rainfall Prediction at Local and Global Spatial Scale: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404767,"Uncertainty in weather dynamics makes it important to build accurate weather prediction systems because it can save lives by better preparing people for an upcoming incidence. The aim of present research study is to present a comprehensive review of recent scientific works for short term temperature and rainfall prediction on both local and global spatial scale. The literature shows that some meteorological factors like Atmospheric pressure, precipitation, dew point temperature, solar radiation, vapor pressure, cloud cover, snowfall, humidity, wind velocity and wind direction are potential measures to predict future temperature and rainfall. We focused on recent applications of machine learning and deep learning models like Deep Echo State Network, recurrent neural network, convolutional recurrent neural network, and graph convolutional network, Autoencoders, Multi layer Perceptron and Long short-term memory. Applications of multimodal learning, reservoir computing and multitask learning have shown noticeable enhancement in the prediction accuracy of other state of art the models. Fine capability of CNN to extract suitable patterns from numeric weather data is also reported. The time interval of data recording also affects the prediction accuracy greatly. More frequently recorded input data worked better than less frequently recorded data. The use of electromagnetic sensors instead of satellite and radar setups is reliable as well as cost effective for collecting data for prediction. Evaluation indices related to hit rate of rainfall and no rainfall, caching rate, overlooking rate and Swing-and-miss rate can be considered as statistical measures along with other statistical metrics in case of rainfall prediction.",No,"초록에서 본 논문은 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 방법론을 정리하는 데 중점을 둔 논문으로 판단된다."
MitrApp: An Intelligent Recommendation System For Counselling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9312107,"In modern era there has been a phenomenal increase of suicidal syndromes with teens and student population being susceptible the most. Since most of the teens and youth are active in social media, the framework serves more to predict the suicidal tendency of the user. The proposed App captures the face of the user to identify his emotional state and immediately play or encourage him to go through audio or video content that serves as an immediate counselling session with no human intervention. Facial emotion recognition (FER) is an important topic in the fields of computer vision and artificial intelligence owing to its significant academic and commercial potential. This review focuses on studies that exclusively use facial images, because visual expressions are one of the main information channels in interpersonal communication. this paper provides a brief review of researches in the field of FER conducted over the past decades. Deep-learning-based FER approaches using deep networks enabling “end-to-end” learning are then presented. This review primarly focuses on two datasets namely, KDEF(Karolinska Directed Emotional Faces) dataset and Kaggle FER2013 dataset. The proposed App captures the face of the user to identify his emotional state and immediately play or encourage him to go through audio or video content that serves as an immediate counselling session with no human intervention. This paper takes suggests instant counter measures such as recommending some anti-suicidal videos and some online expert support. Since people having suicidal tendency will post/ likes the photos/ images related to the suicide thoughts detection of suicide related objects like suicide rope will help in predicting suicidal behaviour more accurately. Since a large section of community(about 28%) are attempting suicides by means of suicidal rope, a model is trained on the most frequently used suicide rope images.",Yes,"논문은 얼굴 감정 인식과 자살 경향 예측을 위한 딥러닝 모델을 제안하고, 이를 기반으로 즉각적인 상담 콘텐츠를 추천하는 시스템을 개발하는 연구 내용을 포함하고 있다. 또한, 자살 관련 이미지 데이터셋을 활용한 모델 학습 등 독창적인 연구 기여가 명확히 드러나 있어 연구 논문에 해당한다."
Tackling the Emerging Threat of Image-Based Steganographic Malware with Advanced Machine Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10810762,"The increasing threat of malware concealed within images through steganography underscores the need for advanced detection methods. Traditional signature-based techniques often fall short as malware evolves rapidly, employing increasingly sophisticated hiding techniques. This paper investigates the application of machine learning (ML) for detecting steganographic malware in images, capitalizing on ML’s capability to discern complex patterns and adapt to emerging threats. The study explores various ML-driven approaches for image- based malware detection, including subspace-based methods for image pattern classification, ML classifiers targeting malicious JPEGs, and Attention Capsule Networks for detecting scams. Additionally, the paper reviews advancements in enhancing the robustness of AI-based malware detection through adversarial machine learning and evaluates hybrid deep learning methods for more effective malware classification. Future research directions are identified, including the development of advanced cross-validation techniques to boost model performance and generalizability, hyperparameter-tuning methods for improved optimization, and dynamic detection strategies incorporating behavioral analysis. Real-time detection with online learning is highlighted for its potential in continuous adaptation to new threats. These prospective advancements aim to address challenges related to data, optimization, real-time performance, and model interpretability, ultimately advancing the capabilities of image-based malware detection.",Yes,"논문 초록에서 다양한 머신러닝 기법을 적용하여 이미지 기반 스테가노그래피 악성코드 탐지 방법을 연구하고 있으며, 새로운 접근법과 향후 연구 방향도 제시하고 있어 독창적인 연구 내용을 포함한 연구 논문으로 판단됩니다."
Applied Machine Learning in Geophysics Taxonomy Review Bibliometrics and Trends in Generative AI,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10737556,"This article presents a methodology to identify key studies using machine learning (ML) in geophysics. We created a comprehensive database of fundamental articles for a systematic review. The main goal is to classify the applications and methods of ML in geophysics through a modified PRISMA approach. The study results offer current insights into the trends and developments of ML applications in geophysics. This article presents a systematic method for identifying and analyzing significant studies that apply ML in geophysics. We created a detailed database of essential articles relevant to this field. Using a modified PRISMA guideline, we conducted a thorough review to evaluate and categorize the literature. The main goal of this review is to provide a clear classification of ML applications and methods in geophysics. We documented how ML techniques are used for various geophysical problems, including data analysis, pattern recognition, and predictive modeling. This classification helps clarify the range of ML applications in geophysics and highlights the specific methods and tools used. Our findings offer an updated view of current trends and developments in ML applications within geophysics. By analyzing the progress of ML and their effectiveness in geophysical applications, this study reveals emerging trends and suggests future research directions.",No,"이 논문은 기계학습이 지구물리학에 적용된 연구들을 체계적으로 분류하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 평가하는 데 중점을 둔 연구로 판단됩니다."
Leveraging Machine Learning for Consumer Purchase Behaviour Analysis: Influence of Online Reviews on E-Commerce Decisions in Electronics and Electrical Markets,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10941434,"People mostly use e-commerce platforms to search for, evaluate, and eventually buy things. Machine learning (ML) is a rapidly developing field that appears to be a desirable substitute for statistical techniques across a range of sectors. This study suggests an understanding of the potential uses of ML in eCommerce and digital marketing. This paper gives a deliberate assessment of the utilization of AI in online business to foresee buyer buy choices, recognize commitment factors, and further develop UIs. The examination researches the capability of AI calculations to foresee purchaser inclinations, accelerate the checkout technique, and produce a more redone shopping experience. As web-based business propels, the concentrate additionally analyzes the difficulties and moral pickles related with artificial intelligence-controlled customization. To give a complete comprehension of the more extensive ramifications of man-made intelligence in forming shopper conduct, points including information protection, predisposition in calculations, and the scarce difference among personalization and nosiness are explored. Our review plans to more readily comprehend what shoppers explore internet business sites and how this means for their choices to purchase. We use AI approaches since we comprehend that buyer conduct is diverse and dynamic. These strategies permit us to deal with convoluted information structures and reveal stowed away examples in the information, which upgrades how we might interpret the components driving purchaser conduct.",Yes,"논문 초록에서 머신러닝 기법을 활용하여 소비자 구매 행동을 예측하고, 구매 결정에 영향을 미치는 요인을 분석하는 연구를 수행한 점이 명확히 드러납니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 나타냅니다."
AI Empathetic Chatbot with Real-Time Emotion Detection Using Multimodal Fusion and BO-CNN Optimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893614,"This paper presents a novel AI-driven empathetic chatbot system that leverages multimodal fusion to decode and respond to user emotions across speech, text, and visual cues in real-time. Building upon advanced Speech Emotion Recognition (SER) and Face Emotion Recognition, and integrating BERT-based Natural Language Understanding (NLU), the system is designed to enhance human-like interaction and emotional responsiveness in conversational AI. A comprehensive review of literature highlights limitations in existing approaches, such as single-modal reliance and dataset constraints, which our proposed model addresses by fusing multiple data sources using the EmbraceNet framework. This fusion of facial and speech features is further optimized with Botox Optimization-based Convolutional Neural Networks (BO-CNN), enhancing accuracy in facial emotion recognition. Extensive testing validates the model's superior performance across precision, sensitivity, and specificity metrics, demonstrating its potential for application in various human-interactive systems.",Yes,"본 논문은 멀티모달 융합과 BO-CNN 최적화를 활용한 새로운 AI 공감 챗봇 시스템을 제안하며, 기존 연구의 한계를 극복하는 독창적인 모델을 개발하고 성능을 검증하는 실험 결과를 포함하고 있다. 이는 직접 기여하는 연구 내용이 포함된 연구 논문임을 나타낸다."
Heart Disease Prediction Using Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122958,"Heart plays significant role in living organisms. Diagnosis and prediction of heart related diseases requires more precision, perfection and correctness because a little mistake can cause fatigue problem or death of the person, there are numerous death cases related to heart and their counting is increasing exponentially day by day. To deal with the problem there is essential need of prediction system for awareness about diseases. Machine learning is the branch of Artificial Intelligence(AI), it provides prestigious support in predicting any kind of event which take training from natural events. In this paper, we calculate accuracy of machine learning algorithms for predicting heart disease, for this algorithms are k-nearest neighbor, decision tree, linear regression and support vector machine(SVM) by using UCI repository dataset for training and testing. For implementation of Python programming Anaconda(jupytor) notebook is best tool, which have many type of library, header file, that make the work more accurate and precise.",No,"초록에서 제시된 내용은 기존 머신러닝 알고리즘을 사용하여 심장병 예측 정확도를 계산한 것으로 보이며, 새로운 알고리즘 개발이나 독창적인 연구 기여에 대한 언급이 없습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 논문으로 판단하기 어렵습니다."
A Systematic Literature Review: Diagnosing Pneumonia Through X-Ray Images Using Deep Learning Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295364,"Pneumonia is an infectious disease affecting human lungs, it majorly affects the elderly and may be life-threatening in some cases. The World Health Organization or WHO reported that in India, one in three deaths is caused due to pneumonia. Early diagnosis of Pneumonia will save many human lives. Due to the success of deep learning algorithms in analyzing images, there are so many algorithms for pneumonia detection now. This paper aims to analyze and evaluate the accuracy of the deep learning algorithm and its application on detecting or diagnosing pneumonia from chest x-ray images. This paper used a systematic literature review approach on several papers found on Google Scholar using a keyword-based search mechanism related to pneumonia detection and deep learning. The authors compare each method from the papers and sort it with accuracy as the parameter. The method with the highest accuracy is the AlexNet model with the accuracy of 99.62%. Automation of pneumonia detection using a deep learning model is possible, however a doctor’s supervision is still necessary to prevent a case of misdiagnose.",No,"이 논문은 직접적인 실험이나 새로운 알고리즘 개발이 아닌, 기존 연구들을 체계적으로 검토하고 비교하는 문헌 리뷰 논문이다. 따라서 독창적인 연구 결과나 기여를 포함하지 않아 연구 논문에 해당하지 않는다."
Plenary Talk II: Measuring Student Engagement in Early Engineering Coursework,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334628,"This talk describes recent efforts for quantifying students’ engagement in early engineering coursework, through designing, implementing, and testing a system to measure the students’ emotional, behavioral, and cognitive engagement states. Engineering programs suffer from a high rate of attrition in the freshman year, primarily due to poor engagement of students with their classes. The project plans to develop a sensor-driven, computational approach to measure emotional and behavioral components of student engagement. This information will be used to identify teaching strategies that increase engagement, with the goal of enhancing student success and retention in STEM education pathways. The project features a multi-disciplinary collaboration between faculty and undergraduate researchers in engineering, the physical sciences, psychological sciences, and education. The project involves students in first- and second-year engineering STEM subjects and the experienced faculty who teach these courses. Findings from the project could be a valuable step toward an early warning system to detect student disengagement and anxiety in STEM and non-STEM courses. Project goals include: (i) establishment of a robust network of non-obtrusive and non-invasive sensors in mid-size classes to enable real-time extraction of facial and vital signs, which will be integrated and displayed on instructors’ dashboards; (ii) identification of robust descriptors for modeling the emotional and behavioral components of engagement using data collected by the sensor networks; (iii) pilot testing of the system’s effectiveness in gathering meaningful data for subsequent work on emotional, behavioral, and cognitive metrics of engagement. The fundamental research question to be addressed relates to improving student learning by the automated capture of non-verbal cues of engagement: How can we use students’ expressions of engagement, based on non-verbal signs such as facial expressions, body and eye movements, physiological reactions, posture, to enhance learning? Findings from the project will constitute a foundation for multi-disciplinary research to incorporate novel machine learning and artificial intelligence-based models for measuring engagement in STEM classes. This project has been funded by the National Science Foundation (NSF). The talk will describe our latest discoveries in this long-term and multidisciplinary project.",Yes,"초록에서 센서 기반 시스템 설계, 구현, 테스트와 학생 참여도 측정에 관한 구체적인 연구 활동과 실험적 검증이 언급되어 있어 독창적인 연구 내용이 포함된 연구 논문임을 알 수 있습니다. 또한, 다학제적 협력과 NSF의 연구비 지원을 받는 장기 프로젝트로서 새로운 방법론 개발에 기여하는 연구임이 명확합니다."
Fraud identification architecture using data mining and machine learning in a private transport company that operates by applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140992,"With the digital transformation over the years and the recent expansion of the use of different applications, it is possible to notice a significant change in several businesses. The diversification of electronic payments has contributed to companies suffering more from fraud. The purpose of this article is to detail a fraud detection architecture based on the identification of patterns of behavior and was applied in the racing bases of the usage of an application transport company. The study considered the construction of an artifact capable of minimizing the problem using unsupervised and supervised algorithms and machine learning techniques. The research was carried out using the DSR - Design Science Research method and considered the stages of construction of a possible conceptual structure with the systematic review of the literature, studies of fraud practices and machine learning techniques used for the detection. The architecture was implemented and allowed to validate the model capable of identifying suspected fraud in a more accurate way.",Yes,"본 논문은 DSR(Design Science Research) 방법론을 사용하여 사기 탐지 아키텍처를 설계하고 구현한 연구로, 독창적인 모델과 알고리즘 적용을 통해 실제 문제 해결에 기여하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Quantum Machine Learning Revolution in Healthcare: A Systematic Review of Emerging Perspectives and Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398184,"Quantum computing (QC) stands apart from traditional computing systems by employing revolutionary techniques for processing information. It leverages the power of quantum bits (qubits) and harnesses the unique properties exhibited by subatomic particles, such as superposition, entanglement, and interference. These quantum phenomena enable quantum computers to operate on an entirely different level, exponentially surpassing the computational capabilities of classical computers. By manipulating qubits and capitalising on their quantum states, QC holds the promise of solving complex problems that are currently intractable in the case of traditional computers. The potential impact of QC extends beyond its computational power and reaches into various critical sectors, including healthcare. Scientists and engineers are working diligently to overcome various challenges and limitations associated with QC technology. These include issues related to qubit stability, error correction, scalability, and noise reduction. In such a scenario, our proposed work provides a concise summary of the most recent state of the art based on articles published between 2018 and 2023 in the healthcare domain. Additionally, the approach follows the necessary guidelines for conducting a systematic literature review. This includes utilising research questions and evaluating the quality of the articles using specific metrics. Initially, a total of 2,038 records were acquired from multiple databases, with 468 duplicate records and 1,053 records unrelated to healthcare subsequently excluded. A further 258, 68, and 39 records were eliminated based on title, abstract, and full-text criteria, respectively. Ultimately, the remaining 49 articles were subject to evaluation, thus providing a brief overview of the recent literature and contributing to existing knowledge and comprehension of Quantum Machine Learning (QML) algorithms and their applications in the healthcare sector. This analysis establishes a foundational framework for forthcoming research and development at the intersection of QC and machine learning, ultimately paving the way for innovative approaches to addressing complex challenges within the healthcare domain.",No,"본 논문은 2018년부터 2023년까지 발표된 기존 연구들을 체계적으로 검토한 문헌 리뷰(Systematic Review)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 현황을 요약하는 데 중점을 두고 있습니다."
An AI Based Automatic Translator for Ancient Hieroglyphic Language—From Scanned Images to English Text,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10103702,"Recent advancements in the fields of Machine Learning and Deep Learning made a huge transformation in other fields that are not related to Computer Science. In this work, a new framework is proposed to tackle the problem of translating the old Egyptian Hieroglyphic writings to English language through deploying both Image Processing and Natural Language Processing techniques combined with AI approaches. Our primary goal is to design an application that completely revolutionizes a tourist’s experience while navigating Egyptian Historical sites. This work utilize different AI techniques to automatically convert the scanned photos of hieroglyphic language to understandable and readable English language, through two main sub-tasks: The automatic detection and recognizing of the scanned glyphs images and the translation of them into English language. Different data sources of this low-resource language were explored and augmented to train and test our models. Results of different models and algorithms are assessed and analyzed to evaluate our work. State-of-the-art results are achieved compared to literature in both automatic glyphs recognition, and glyphs-to-English translation.",Yes,"논문은 고대 상형문자 이미지를 영어 텍스트로 자동 번역하는 새로운 AI 기반 프레임워크를 제안하며, 이미지 처리와 자연어 처리 기법을 결합한 독창적인 연구 내용을 포함하고 있다. 또한 다양한 데이터 소스를 활용해 모델을 학습 및 평가하고, 기존 연구 대비 최첨단 성과를 달성한 점에서 직접 기여하는 연구 논문으로 판단된다."
A Deep Learning Based Approach In The Prediction Of Tinnitus Disease For Large Population Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307000,"Tinnitus is a frequent sensory disorder that puts a lot of strain on the patient. Usually, tinnitus results from disturbances occurring to the sensory systems, such as the peripheral seldom central, the somatosensory system, the head and neck, or a mix of the two. This can be found in people with high stress, anxiety, depression, and hearing disorders. Although there is progress in the medical domain using artificial intelligence (AI), research related to tinnitus using AI is limited. This work aims to bridge the gap using deep-learning techniques for evaluating the patient record by examining various parameters. The proposed research also aims to target the same to understand the severity and possible recommendations for tinnitus disease. Our findings forecast how patients will react to tinnitus treatments. From the patients' electroencephalography (EEG) data, predictive EEG variables are extracted, and later feature selection approaches are used to determine the prominent features. The patient's EEG features are supplemented by AI algorithms for training and forecasting treatment outcomes. Higher accuracy levels of the proposed model using AI help the practitioners suggest the proper diagnosis for the patients and also check the patient's recovery over a period of time.",Yes,"본 논문은 딥러닝 기법을 활용하여 이명(tinnitus) 환자의 EEG 데이터를 분석하고 치료 반응을 예측하는 모델을 제안하는 연구로, 직접적인 데이터 분석과 모델 개발을 포함한 독창적인 연구 내용을 담고 있다. 따라서 연구 논문에 해당한다."
Daphnet Freezing Recognition with Gait Data by Using Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163510,"The aim of this study was to test the success of the data set obtained by a wearable health assistant developed for the symptom of freezing (FOG) in gait of Parkinson’s patients and to increase the success of the system. The system was tested with different machine learning methods to measure the success of the wearable health assistant system. For all patients (ten patients), the highest success value was obtained and the mean sensitivity and specificity values of the system were calculated and compared with the results obtained in the literature review. In the literature, mean sensitivity and specificity were 73.1% and 81.6%, respectively; In this study, mean sensitivity and specificity were 91.9% and 71.14%, respectively. In order to better analyze the success of the system, two patients with successful and unsuccessful results were selected for the data set in line with the results obtained in the literature review. The success of the system was tested by using different machine learning methods on the data sets of two patients. Finally, the successes obtained by feature extraction methods were tried to be increased. Among the different machine learning methods on the data sets used for patient 8 and patient 3, the most successful method was obtained by combining the models (ensemble). The highest achievement value obtained by attribute extraction methods was obtained when PCA was applied. However, the success value obtained with raw data could not be increased. All results are tabulated and presented.",Yes,"본 논문은 파킨슨 환자의 보행 데이터에서 동결 현상을 인식하기 위해 웨어러블 기기와 다양한 머신러닝 알고리즘을 적용하여 성능을 평가하고 개선하는 독창적인 연구를 수행하였다. 데이터 수집, 모델 적용, 성능 비교 및 특성 추출 기법 실험 등 직접적인 연구 기여가 포함되어 있어 연구 논문에 해당한다."
Quantifying Repetition in Symbolic Music using Lempel-Ziv Compression,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316147,"Repetition serves as a fundamental element in music by creating patterns, building structure, and emphasizing melodies, yet remains largely elusive to generative AI models. In this paper, we explore the question of quantifying musical repetition by using principles of information theory, arguing that a composition’s repetitiveness is related to the compressibility of its musical data. Thus, we introduce a metric of repetition termed the LZ ratio, based on Lempel-Ziv compression. By analyzing numerous selected piano pieces, we evaluate the effectiveness of our metric in measuring different types of musical repetition. We find that our metric does well in quantifying certain types of repetition. Furthermore, we observe expected changes in our metric when we remove repetitions from pieces. Our work presents an encouraging direction to objectively quantifying the repetitiveness of music for use in generative multimedia machine learning systems.",Yes,"본 논문은 음악 반복성을 정량화하기 위한 새로운 지표인 LZ 비율을 제안하고, 이를 피아노 곡에 적용하여 효과를 평가하는 독창적인 연구 내용을 포함하고 있다. 정보 이론과 Lempel-Ziv 압축 원리를 활용한 새로운 방법론을 제시함으로써 직접적인 연구 기여가 있다고 판단된다."
The Use of Digital AI-based Tools for Prevention of Workload Injuries - An Intervention Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857129,"Work-related injuries, particularly musculoskeletal disorders (MSDs), incur significant costs for companies in terms of sick leave and reduced productivity. Maintaining correct ergonomic posture is crucial to prevent these injuries and mitigate the impact of psychosocial factors. Digital technology plays a vital role in creating efficient and flexible work environments that cater to individual needs. Rather than relying solely on specialists, workers can utilize digital applications to prevent workload and strain injuries. This study investigates the effectiveness of a digital AI-based intervention program aimed at preventing work-related injuries and improving the physical work environment by addressing musculoskeletal disorders caused by incorrect postures. Through interviews with tool users in an industry setting, a web-based prototype application was tested to enhance workplace safety and improve physical health. The application employs digital AI tools to provide real-time feedback to workers. The interviews specifically assess how users evaluate and effectively utilize the tool to enhance working postures and the overall work environment. The study seeks to evaluate the efficacy of the digital AI-based intervention program and gather insights on users’ perceptions and utilization of the application. This research has the potential to contribute to a safer and healthier workplace by harnessing the power of technology. The study seeks to evaluate the efficacy of the digital AI-based intervention program and gather insights on users’ perceptions and utilization of the application.",Yes,본 논문은 디지털 AI 기반 개입 프로그램의 효과를 평가하기 위해 산업 현장에서 웹 기반 프로토타입 애플리케이션을 테스트하고 사용자 인터뷰를 수행하는 등 직접적인 연구 방법을 사용하여 독창적인 연구 결과를 제시하고 있다. 따라서 연구 논문에 해당한다.
Critical review of machine learning approaches to apply big data analytics in DDoS forensics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8441286,"Distributed Denial of Service (DDoS) attacks are becoming more frequent and easier to execute. The sharp increase in network traffic presents challenges to conduct DDoS forensics. Despite different tools being developed, few take into account of the increase in network traffic. This research aims to recommend the best learning model for DDoS forensics. To this extend, the paper reviewed different literature to understand the challenges and opportunities of employing big data in DDoS forensics. Multiple simulations were carried out to compare the performance of different models. Two data mining tools WEKA and H2O were used to implement both supervised and unsupervised learning models. The training and testing of the models made use of intrusion dataset from oN-Line System - Knowledge Discovery & Data mining (NSL-KDD). The models are then evaluated according to their efficiency and accuracy. Overall, result shows that supervised learning algorithms perform better than unsupervised learning algorithms. It was found that Naïve Bayes, Gradient Boosting Machine and Distributed Random Forest are the most suitable model for DDoS detection because of its accuracy and time taken to train. Both Gradient Boosting Machine and Distributed Random Forest were further investigated to determine the parameters that can yield better accuracy. Future research can be extended by installing different DDoS detection models in an actual environment and compare their performances in actual attacks.",Yes,"본 논문은 기존 문헌을 검토한 후 여러 머신러닝 모델을 직접 시뮬레이션하고 평가하여 DDoS 포렌식에 적합한 모델을 추천하는 독창적인 연구를 수행하였다. 또한, 특정 알고리즘의 성능을 비교하고 파라미터 최적화까지 진행한 점에서 직접적인 연구 기여가 포함되어 있다."
Predicting Helpfulness of Crowd-Sourced Reviews: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9024814,"With the recent advancement of Web 2.0 and the popularity of social media platforms, the volume of User Generated Content (UGC) is rising explosively. Online reviews are rapidly growing and a popular source of UGC, which help customers in evaluating the quality of product and making purchase decisions. However, distilling the required information from the massively increasing volume of reviews becomes difficult for customers. Therefore, it becomes an important issue to identify helpful review accurately. The review helpfulness prediction has attracted growing attention of researchers that proposed various solutions using statistical and Machine Learning (ML) techniques. This paper aims to review the existing literature on review helpfulness prediction, to identify data sources, ML techniques and potential challenges. The review helpfulness prediction was equally taken as both regression and classification task by previous studies. However, the definition of helpfulness for each task varies significantly. Most of the studies used online reviews from Amazon to predict helpfulness. The comparison of state-of-the-art techniques and challenges will give a quick overview to researchers about the existing state of research on review helpfulness prediction.",No,"본 논문은 기존 연구들을 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 있다. 따라서 연구 논문보다는 문헌 리뷰에 해당한다."
Machine Learning Methods for Evaluating Public Crisis: Meta-Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10216730,"This study examines machine learning methods used in crisis management. Analyzing detected patterns from a crisis involves the collection and evaluation of historical or near-realtime datasets through automated means. This paper utilized the meta-review method to analyze scientific literature that utilized machine learning techniques to evaluate human actions during crises. Selected studies were condensed into themes and emerging trends using a systematic literature evaluation of published works accessed from three scholarly databases. Results show that data from social media was prominent in the evaluated articles with 27% usage, followed by disaster management, health (COVID) and crisis informatics, amongst many other themes. Additionally, the supervised machine learning method, with an application of 69% across the board, was predominant. The classification technique stood out among other machine learning tasks with 41% usage. The algorithms that played major roles were the Support Vector Machine, Neural Networks, Naive Bayes, and Random Forest, with 23%, 16%, 15%, and 12% contributions, respectively.",No,"본 논문은 메타리뷰(meta-review) 방법을 사용하여 기존 연구들을 체계적으로 분석한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합 및 평가에 초점을 맞추고 있습니다."
Using a generative AI chatbot for SRL measurement and intervention,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10604531,"This paper introduces a recently started research thesis proposal for a doctoral consortium. In recent years, identifying students at risk of dropping out or failing has become a main issue, mainly in the first year of university studies. To attain this problem, this piece of research focuses on supporting the development of students’ Self-Regulated Learning (SRL) skills. It proposes the development and evaluation of a diagnostic and supporting tool providing help to resolve students’ weaknesses in the learning process. The proposal is intended to exploit the capabilities of nowadays conversational agents with generative artificial intelligence (AI) technologies to approach the measurement and development of SRL skills in a more effective and efficient way. The paper includes a review of related works on existing solutions to measure and promote the development of SRL skills, identifying key problems related to the lack of student involvement and participation. The design-based scientific research (DSR) methodology for tool development is chosen and approximate task planning is premeditated.",No,"본 논문은 박사 논문 제안서로서 연구 계획과 관련 문헌 검토, 방법론 선정에 초점을 맞추고 있으며, 아직 직접적인 연구 결과나 독창적인 실험 데이터가 포함되어 있지 않습니다. 따라서 현재 시점에서는 독창적인 연구 내용을 포함한 완성된 연구 논문으로 보기 어렵습니다."
Impact of Application Choice on iTracker Model Performance Evaluation: A Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590349,"Eye tracking is underpinned by the eye-mind hypothesis, which posits that individuals tend to direct their gaze toward the information they are currently cognitively engaged with. This aspect has garnered significant attention within deep learning research due to its versatile applications and research. The utilization of Artificial Intelligence (AI) in conjunction with webcams for tracking eye movements has gained increasing popularity in recent years and is anticipated to continue flourishing. This literature review aims to assess the current state and depth of the research in this domain and its various applications. We examine the different methods that employ eye tracking across a diverse range of applications, and by re-exploring existing research, we are able to filter out any overlooked potential or setbacks associated with the use of AI-webcam based systems, with the ultimate goal of maximizing their potential contributions to humanity. In our study, we analyze and evaluate a pioneering eye tracking algorithm, iTracker trained on a crowd-sourced data-set, GazeCapture within the context of the applications it may be used for. While we acknowledge inherent limitations, this widely adopted model forms a robust foundation for future refinements. Furthermore, our investigation encompasses diverse applications with varying accuracy requirements, making it invaluable for individuals seeking to integrate eye tracking into their projects.",No,"본 논문은 기존 연구들을 종합하고 평가하는 문헌 리뷰(literature review)로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. iTracker 모델의 성능 평가와 다양한 응용 분야를 분석하는 관점에서 작성되었으나, 새로운 연구 방법론이나 실험 결과를 제시하지 않았습니다."
A GA-Based CNN Model for Brain Tumor Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919461,"Detection and classification of tumor types generally cover problem-specific algorithm developments. The problems of detecting tumors with the analysis of standard brain images obtained with different medical imaging tools and frequently used in the literature are always desired, developed, and discussed. This study focuses on identifying tumors, extracting different characteristics, and associating them with cancer types. The standard approach of convolutional neural networks (CNN) was used primarily for the identification of tumors. Then, the genetic algorithm (GA) approach was designed and used for hyperparameter optimization in CNN to increase the performance in all datasets. Thus, a CNN+GA hybrid model was proposed and analyzed with different tests. In this process, the results were examined in detail and the standard CNN algorithm and some machine learning methods suggested in the literature were analyzed comparatively. In addition, the data set called Gazi Brains 2020 Dataset, which was obtained within the scope of the Turkish Brain Project, is also used to test the proposed system. Here, literature reviews of the previous studies in which different machine/deep learning approaches are used together with optimization algorithms are presented. The different comparison scores obtained according to the experimental studies were presented in the tables and the outputs were evaluated in terms of significance. The results have shown that the proposed hybrid models are successful in achieving better accuracies not only with different datasets available in the literature but also DL/ML models trained with Gazi Brain 2020 Dataset. It should be concluded that the proposed method might be also used for other deep/machine learning models and applications.",Yes,"본 논문은 뇌종양 분류를 위해 CNN과 유전 알고리즘을 결합한 하이브리드 모델을 제안하고, 이를 다양한 데이터셋에 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 방법들과의 비교 분석과 실험 결과를 통해 새로운 기여를 명확히 제시하고 있어 연구 논문에 해당한다."
Opportunities and challenges in the application of Artificial Intelligence-based technologies in the healthcare Industry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059767,"This paper researches the underlying period of AI-based advancement applications and their impact on the clinical thought business. Notwithstanding a nitty gritty conversation of the parts, this examination zeroed in on a couple of certifiable examples of AI use in medical services settings. The findings demonstrate that critical crisis facilities are now utilizing AI-enabled technologies to expand clinical personnel in continuous discovering and therapy procedures for a wide range of illnesses. Furthermore, Artificial intelligence (AI) systems impacting the viability of nursing and the regulatory operations of centers. While clinical care providers enthusiastically welcome AI, its applications present both a hopeful (new opportunities) and a pessimistic (new risks)(challenges to endure). We examine the subtleties of those odds and difficulties in order to provide a realistic assessment of the usefulness of Artificial Knowledge applications in therapeutic advantages.. Speedy AI advancements and related technologies will assist caring providers to make another motivating force for their patients and work on the adequacy of their functional measures. Regardless, to reap the benefits of what advancements have to give, AI will necessitate viable masterminding and systems to change the complete thinking organisation and activities.",No,"본 논문은 AI 기술의 의료 산업 적용에 대한 기회와 도전 과제를 논의하는 리뷰 성격의 글로 보이며, 직접적인 실험이나 독창적인 연구 결과를 제시하지 않습니다. 따라서 새로운 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Practical Radio Frequency Learning for Future Wireless Communication Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020807,"In future wireless systems, intelligent capabilities are of utmost importance. To efficiently utilize resources, communication systems require knowledge of the prevalent situation in a frequency band through learning. To learn appropriately, it is critical for practitioners to select the right parameters in building learning models, use the appropriate algorithms and performance evaluation methods. In this paper, we evaluate the performance of some deep learning models compared to other machine learning methods, explore the different scenarios in which deep learning can be used for radio frequency (RF) monitoring, and evaluate performance in the various scenarios. Our work looks at the best practices and procedures for developing intelligent RF Learning. Specifically, we analysed over-the-air RF dataset collected from a USRP-based testbed to identify the number of interfering devices as a case study. From the obtained results, we discuss how Signal-to-Noise Ratio (SNR) selection for training affects the model performance as it relates to practical implementation of Deep Learning in communications systems.",Yes,"본 논문은 USRP 기반 테스트베드에서 수집한 실제 RF 데이터를 활용하여 딥러닝 모델의 성능을 평가하고, 신호대잡음비(SNR) 선택이 모델 성능에 미치는 영향을 분석하는 등 구체적인 실험과 분석을 포함하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문에 해당한다."
"Using Knowledge Graphs to Unlock Practical Collection, Integration, and Audit of AI Accountability Information",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815594,"To enhance trustworthiness of AI systems, a number of solutions have been proposed to document how such systems are built and used. A key facet of realizing trust in AI is how to make such systems accountable - a challenging task, not least due to the lack of an agreed definition of accountability and differing perspectives on what information should be recorded and how it should be used (e.g., to inform audit). Information originates across the life cycle stages of an AI system and from a variety of sources (individuals, organizations, systems), raising numerous challenges around collection, management, and audit. In our previous work, we argued that semantic Knowledge Graphs (KGs) are ideally suited to address those challenges and we presented an approach utilizing KGs to aid in the tasks of modelling, recording, viewing, and auditing accountability information related to the design stage of AI system development. Moreover, as KGs store data in a structured format understandable by both humans and machines, we argued that this approach provides new opportunities for building intelligent applications that facilitate and automate such tasks. In this paper, we expand our earlier work by reporting additional detailed requirements for knowledge representation and capture in the context of AI accountability; these extend the scope of our work beyond the design stage, to also include system implementation. Furthermore, we present the RAInS ontology which has been extended to satisfy these requirements. We evaluate our approach against three popular baseline frameworks, namely, Datasheets, Model Cards, and FactSheets, by comparing the range of information that can be captured by our KGs against these three frameworks. We demonstrate that our approach subsumes and extends the capabilities of the baseline frameworks and discuss how KGs can be used to integrate and enhance accountability information collection processes.",Yes,"논문은 AI 책임성 정보의 수집, 통합, 감사에 관한 새로운 지식 그래프 기반 접근법과 RAInS 온톨로지를 제안하고 이를 기존 프레임워크와 비교 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Yoga Pose Recognition Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497433,"Yoga pose detection holds significant importance in various aspects of the yoga practice and its integration with technology. The importance of yoga lies in its ability to promote physical health, mental well-being, stress reduction, improved focus, emotional balance, resilience, spiritual growth, and a holistic approach to life. With the increasing popularity of yoga, there is a growing need for technological advancements to support practitioners and instructors in monitoring and refining their practice. The paper begins by outlining the significance of automated yoga pose detection, highlighting the potential benefits it offers in providing real-time feedback, enhancing self-correction, and optimizing performance. It explores the existing literature on computer vision and machine learning techniques applied to human pose estimation and their applicability to yoga pose detection. Based on a thorough review of state-of-the-art methodologies, the research paper proposes a yoga pose detection that combines multiple modalities, including RGB images, depth maps, and skeletal joint data. The proposed system leverages deep learning algorithms, such as convolutional neural networks (CNNs) and long short-term memory (LSTM), to precisely recognize and continuously monitor yoga poses. Moreover, the paper discusses the challenges associated with pose variation, occlusion, and complex body movements within yoga practice. It explores strategies for data augmentation, model optimization, and performance evaluation to ensure robustness and accuracy of the proposed detection system. The practical implications of the research are discussed, emphasizing the potential for widespread adoption of yoga pose detection systems in various settings, including yoga studios, fitness centers, and home practice environments. The paper concludes by outlining future research directions and the potential for integrating the proposed system with emerging technologies, such as augmented reality (AR) and virtual reality (VR), to enhance the yoga experience and facilitate remote instruction. Overall, this research paper contributes to the advancement of automated yoga pose analysis, offering a comprehensive framework that can revolutionize the way yoga is practiced, taught, and evaluated, ultimately promoting accessibility, precision, and effectiveness in the pursuit of physical and mental well-being.",Yes,"논문 초록에서 제안된 시스템은 딥러닝 알고리즘을 활용한 요가 자세 인식이라는 독창적인 연구 내용을 포함하고 있으며, 기존 연구를 바탕으로 새로운 방법론과 성능 평가를 수행한 점이 명확히 드러나 있습니다. 따라서 본 논문은 직접 기여하는 연구 논문에 해당합니다."
Applying Hidden Markov Model for Dynamic Game Balancing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291666,"In Artificial Intelligence (AI) field, Machine Learning (ML) techniques present an interesting approach for games, where it allows some sort of adaptation along the game session. This adaptation can make games more attractive, avoiding that Non-Player-Characters (NPC) present too easy or hard patterns during the game. In both cases, the player may be frustrated due to undesired experience. Although ML techniques are appealing to be used in games, some games characteristics are hard to model. Besides, there are techniques that require a wide variety of observations, which implies two hard barriers for game application: the first is the power processing to compute a huge amount of data in games, considering the real-time characteristic of this kind of application. The second threat is related to the vast majority of games' attributes that must be described in the model. This work proposes a novel approach using ML technique based on Hidden Markov Model (HMM) for game balancing process. HMM is a powerful technique which can be used to learn patterns based on a strong co-relational between an observation and an unknown variable (the hidden part). Our proposed approach learns the player's pattern based on temporal frame observation by co-relating his/her actions (movements) with game events (NPC destruction). The temporal frame observation approach allows the game to learn about player's pattern even if a different person plays it. After the learning process, the following step is to use the knowledge pattern to adapt the game according to the current player, which normally involves making the game harder for a certain period of time. During this time, another pattern may arise, subjected to be learned. In order to validate the presented approach, a Space Invaders clone has been built, allowing to observe that 54 % of participants had more fun while playing it with ML activated in relation to a base version that did not take into account dynamic difficult balancing.",Yes,"논문은 Hidden Markov Model을 활용한 게임 밸런싱을 위한 새로운 머신러닝 기법을 제안하고, 이를 검증하기 위해 Space Invaders 클론을 제작하여 실험을 수행한 연구 내용을 포함하고 있다. 이는 독창적인 연구 기여와 실험적 검증을 포함한 연구 논문에 해당한다."
A Comprehensive Survey of Techniques for Lung Cancer Diagnosis and Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10805723,"Despite significant advancements, lung cancer remains a formidable global health challenge, necessitating effective diagnostic and prognostic methodologies. This survey paper examines current literature to identify and evaluate methods and models utilized for lung cancer diagnosis and prognosis, aiming to enhance early detection strategies for improved patient outcomes. The research addresses critical questions: What are the current methods and models employed for lung cancer diagnosis and prognosis, their strengths, limitations, and potential for future improvement? It investigates specific applications of deep learning architectures such as CNN, GoogleNet, VGG-16, U-Net, and machine learning algorithms like XGBoost, SVM, KNN, ANN, Random Forest, and hybrid models in lung cancer detection. Methodologically, a systematic review across key databases using relevant keywords was conducted to synthesize findings. The study highlights the strengths and limitations of existing approaches and identifies gaps in model interpretability, real-world validation, and integration of diverse techniques. Conclusions underscore the transformative potential of advanced computational methods in enhancing lung cancer diagnosis and prognosis, proposing avenues for future research to address current challenges and improve patient care.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 평가하는 서베이(조사) 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
Intrusion Detection using Machine Learning Techniques: An exhaustive review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10112831,"With the enormous growth of computers and data networks, as well as the vast number of relevant applications, cyber security has recently received a great deal of attention. The Internet has become more vulnerable to planned and extended cybercrimes. Consequently, the development of a powerful intrusion detection system to eliminate various cyberattacks in a network has become critical. Even though traditional security measures exist to detect and prevent cyberattacks, these measures are ineffective because cybercriminals are intelligent enough to circumvent all of them. Many cybersecurity applications employ machine learning (ML) techniques. This paper reviews the literature on intrusion detection using machine-learning techniques. This paper gives a brief overview of various machine learning methods and security datasets that are commonly used. Additionally, various metrics for evaluating the classification model's performance are discussed. This review paper looks at different machine learning methods for intrusion detection and the challenges that we can face during the application of machine learning.",No,"논문 초록에서 해당 논문은 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있으며, 독창적인 연구 결과나 새로운 실험, 모델 제안 등의 직접적인 연구 기여 내용이 포함되어 있지 않다. 따라서 연구 논문보다는 문헌 리뷰 논문에 해당한다."
A Hybrid Recommendation System: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10716623,"Over the years, recommendation engines (REs) have steadily increased in popularity, providing significant advantages by matching available items to users’ specific interests. As more users, products, and rating information are introduced into the system, the relationship between users and recommended products changes, leading to a phenomenon known as concept drift (CD), which degrades the system’s accuracy. Deep learning (DL), a subset of machine learning methodologies, involves learning over multiple layers of information processing phases and aids in addressing the CD problem. Additionally, federated recommender systems (FedRecSys) provide privacy for user data within a decentralised framework. This paper proposes a novel approach that hybridises DL methods with federated learning to detect and adapt to the concept drift issue in E-commerce-based REs. It also focuses on the benefits of collaborative RE and provides a systematic literature review of hybrid federated DL models to solve the challenges posed by concept drift for accurate recommendations. The results can be evaluated using performance measures such as Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Squared Log Error (MSLE), among others.",No,"초록에서 본 논문은 기존 연구들을 체계적으로 검토하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 중점을 둔 논문으로 판단된다."
Reliance on Artificial Intelligence Tools May Displace Research Skills Acquisition Within Engineering Doctoral Programmes: Examples and Implications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837618,"The escalation in capabilities of Large Language Models has triggered urgent discussions about their implications for tertiary education, particularly regarding how they might facilitate academic misconduct in graded engineering coursework. However, graduate research education — where a student works closely with a supervisor over years to develop both implicit and explicit research skills — has received comparatively less attention in this discussion. This paper seeks to develop this discourse by presenting targeted case studies that explore the opportunities and threats posed by artificial intelligence to engineering doctoral education. For instance, using a specimen exercise from a PhD-level research skills module, we demonstrate how artificial intelligence tools can now deeply penetrate research workflows in technical computing and scripting. We likewise investigate the capabilities of chatbot tools to assist engineering PhD candidates with the broader research skills central to their training and development. These include writing and proofreading theses and research papers, producing data visualizations, simulating peer review processes, and preparing scientific diagrams. By evaluating the capabilities and limitations of extant artificial intelligence in these areas, we can discuss both the potential benefits and ethical concerns of doctoral students engaging with such assistance.",No,"본 논문은 인공지능 도구가 공학 박사 과정에서 연구 기술 습득에 미치는 영향과 관련된 사례 연구 및 논의를 제공하고 있으나, 직접적인 실험 결과나 새로운 연구 방법론, 독창적인 연구 데이터를 제시하지는 않는다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵다."
"Solar Photovoltaic Systems: A Review of Risks, Fault Detection, and Mitigation Strategies",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10927051,"Solar Photovoltaic Systems have been widely adopted and integrated into several facets in the built environment, owing to the clean energy generated from it. However, just like every other technological innovation, risks and failure mode are inherent. This review examines the risks associated with Solar Photovoltaic (PV) systems, covering Building-Applied PV (BAPV), Building-Integrated PV (BIPV), off-grid, and on-grid applications. This paper presents a thorough review of the several potential risks, failure detection methods, risk assessment methods and mitigation strategies associated with solar PV systems. The study identifies several failure modes, including encapsulant degradation, inverter failures, potential-induced degradation (PID), and overheating, which significantly impact system performance and reliability. The review categorizes risks into technical, environmental and operational areas and highlights the influence of factors such as temperature, humidity and dust accumulation on energy output. Detection mechanisms are explored, with a focus on data-driven approaches, particularly machine learning and deep learning techniques, which have emerged as the most researched methods for fault detection. Additionally, the review examines risk assessment methodologies, including failure mode and effects analysis (FMEA), fault tree analysis (FTA), and fuzzy logic, and emphasizes the importance of a multi-faceted approach to risk management of solar PV systems. Remedial strategies such as preventative maintenance, investment in high-quality components and design optimization are discussed as essential measures to improve the reliability of solar PV systems. This comprehensive review highlights the need for ongoing research to address the unique risks associated with solar PV systems and to develop standardized protocols for effective risk management that ultimately contribute to the sustainability and efficiency of solar energy systems. This study serves as a valuable resource for stakeholders, including researchers, engineers, and policymakers, by offering insights that can guide future interventions and enhance the overall efficiency of solar PV systems.",No,"본 논문은 태양광 시스템의 위험, 고장 탐지 및 완화 전략에 대한 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리와 분석에 중점을 두고 있습니다."
Detection Systems for Distributed Denial-of-Service (DDoS) Attack Based on Time Series: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548796,"The Distributed Denial-of-Service (DDoS) attacks are one of the most critical threats to the stability and security of the Internet. With the increasing number of devices connected to the Internet, the frequency and severity of DDoS attacks are also increasing. To mitigate the impact of DDoS attacks, intelligent detection systems are becoming increasingly important. This paper reviews the recent literature on intelligent techniques, including machine learning (ML), Deep Learning (DL), and artificial intelligence (AI), for detecting DDoS attacks. We will provide an overview of the existing research in the field and analyse the trends in using time series data analysis for DDoS attack detection. A taxonomy and conceptual framework for DDoS mitigation are presented. This study highlights the use of several intelligent techniques for detecting DDoS attacks and evaluates the performance utilizing real datasets and also discusses future research directions in this field.",No,"본 논문은 DDoS 공격 탐지에 관한 기존 연구들을 종합적으로 검토하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
"A New Benchmarking for Diabetic Retinopathy Using Machine Learning, Deep Learning and Image Processing Techniques",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752482,"An elaborated insight into the detection of DR in person eye with the use of different methodologies like image processing, machine learning, and deep learning is discussed. A widely recognized fact is that the level of sugar in blood must be maintained correctly, and its plays an important role in every organ. If it varies abruptly, eventually it will leads to many disease. It is proven that if a person is affected by Diabetics, he has many possibilities of getting affected by Diabetic Retinopathy. As the eyes are in close relation to the brain, any such severe disease affects the retinal connectivity of the eye, it will finally end up with the damage in the brain nervous system, causes the total blindness of the eye, so that the earlier detection of such problems is more needed. The only portion of the human body where blood circulation can be clearly detected is the retina. DR is a disease of the human eye that causeharm to the retina of the eye which can eventually result in total blindness. There are several methods for segmenting the blood vessels in the retina, just the once the retinal nerve fibres are segmented, it is possible to determine whether the eyes are affected by diabetic retinopathy or not. Even the Diabetic Retinopathy detection depends on the Retinal Nerve Fibre Layer network's coverage area. If the entire area of the nerve fibre is less than total area of the nerve network, the eyes are strickenby DR, anywhere the total area of the nerve network is greater, the eyes are not affected by diabetic retinopathy and therefore are usual. The retinal blood vessels can be impaired by many systemic diseases, which make retinal image analysis a possible diagnostic method, since it enables simple and non-invasive assessment of vascular changes. To prevent total loss of sight, timelyrevealing of DR is needed. For the diagnosis of diabetic retinopathy, physical measures such as visual acuity tests, pupil dilation and optical accuracy tomography are used. In terms of time, however it is expensive and could impact patients. This article provides a brief overview of how machine learning, deep learning, and image processing techniques can be used to detect DR in human eyes. As well as they will gain the knowledge about the disease in the human eye. To put it more simply, the paper is written as an introduction to diabetic retinopathy and the different methodologies tried to cure this disease. Until now, various research analysts have whittling away at this title's theme. To begin that, a large number of research papers were gathered from different sources, reviewed in depth, and a brief overview of eye disease issues was produced and presented here in a precise manner. In this context, latest works done by different writers, all across the world is being discussed in this context so that any researcher working in the field of ophthalmology can identify their own new research challenge using this review article as a guide.",No,"초록에서 이 논문은 기존 연구들을 종합하여 당뇨병성 망막병증 진단에 사용된 기계학습, 딥러닝, 영상처리 기법들을 개관하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 독창적인 연구 결과나 실험적 기여를 포함한 연구 논문이 아니라 기존 연구를 정리한 개요서에 해당합니다."
Glaucoma Detection using Convolution Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083864,"The World Health Organization (WHO) ranks glaucoma as the secondary main cause of vision loss and irreversible visual impairment. Due to its effects on the optic nerve head, glaucoma is sometimes called the “silent burglary of sight” by medical professionals. Finding and classifying glaucoma early on will help patients get the treatment they need from eye surgeons, which in turn will enhance their daily lives. Blinding glaucoma strikes without warning. It is extremely difficult to diagnose glaucoma in its early stages, and advanced cases of the disease cannot be treated at this time. Multiple automated eye disease detection systems were analyzed thoroughly for the study. A comprehensive literature review was conducted on the topics of which was before, extraction of features, extraction of selection, machine learning (ML) algorithm is proposed, and testing/training datasets. Machine learning, and especially deep learning techniques, have become the standard for analyzing medical images and data. Unfortunately, little has been accomplished in automating the prediction of glaucoma, despite its critical importance. Automated eye disease diagnosis is still in its infancy, but it has advanced to the point where most ML methods can correctly identify 85 percent of patients. Optical coherence tomography is a useful tool for the early detection of glaucoma (OCT). The use of machine learning strategies trained on eye unit of measure and field of vision data for the detection of glaucoma is also discussed.",No,초록 내용은 기존 연구들을 종합적으로 검토하고 자동화된 녹내장 진단 시스템의 현황을 설명하는 문헌 리뷰 성격이 강합니다. 독창적인 연구 방법론이나 실험 결과에 대한 직접적인 기술이 없어 연구 논문으로 보기 어렵습니다.
Will Energy-Hungry AI Create a Baseload Power Demand Boom?,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630483,"The rapid expansion of generative artificial intelligence (AI) technologies is projected to significantly affect electricity use in the global data center sector. Earlier research has proposed using data centers for load-balancing the future power grid to allow higher integration of variable renewables. In this paper, we review the expected future electricity consumption of AI and evaluate the behavior of AI data centers in clean energy systems. Our work found that the levelized cost of computing (LCOC) favors higher load factors and shows a relatively low sensitivity to electricity price levels. Under our baseline cost assumptions, a baseload electricity price of $125/MWh benefits load factors higher than 64 %, depending on the market price conditions and variations. Nevertheless, high-tier data centers with higher operational costs and capital expenditures favor even higher load factors to optimize their LCOC. These findings show that a boom in AI energy use could drive significant baseload power demand in future power systems.",Yes,"본 논문은 AI의 전력 소비와 데이터 센터의 부하 특성을 분석하고, 비용 모델을 통해 미래 전력 시스템에서의 AI 에너지 수요를 예측하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 검토하는 데 그치지 않고, 새로운 분석과 평가를 수행한 연구 논문으로 판단된다."
Remote Sensing Image Scene Classification: Benchmark and State of the Art,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891544,"Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed “NWPU-RESISC45,” which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.",Yes,"이 논문은 기존 연구들을 종합적으로 리뷰하는 동시에, 대규모 데이터셋인 NWPU-RESISC45를 새로 구축하여 공개하고 이를 활용한 여러 방법들을 평가하는 독창적인 연구 내용을 포함하고 있습니다. 따라서 직접적인 연구 기여가 있는 논문으로 판단됩니다."
A Pupil Segmentation Framework with Masked Image Modeling Enhanced Swin-Transformer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995137,"Detecting pupil from the image is critical in human-machine interaction and biomedical computing applications, which is supposed to be an actual image segmentation problem. Recently developed deep learning models provide a variety of novel approaches to the pupil segmentation task. However, dataset preparation and annotation acquirement to build pupil image datasets are labor-intensive and time-consuming. The shortage of labeled samples restricted the improvement of deep learning models. In this work, we use a mask image modeling mechanism to learn the latent representation from limited data samples, which significantly helps train deep models. Further, we propose a novel pupil segmentation model based on the recently proposed Swin-Transformer to validate the improvement validity of the mask mechanism. The proposed computational framework achieves better performance on the pupil segmentation tasks based on the LPW dataset through comparison experiments with other related deep learning models. The proposed framework is a promising solution for pupil segmentation and detection in small-sample learning applications.",Yes,"본 논문은 제한된 데이터 샘플에서 잠재 표현을 학습하는 마스크 이미지 모델링 기법을 도입하고, Swin-Transformer 기반의 새로운 동공 분할 모델을 제안하여 성능 향상을 검증하는 독창적인 연구 내용을 포함하고 있다. 이는 직접적인 연구 기여로 볼 수 있다."
Classification of Error Related Potentials using Convolutional Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8776901,"A class of Electroencephalogram (EEG) generated when a person is present with a stimuli are Error-related potentials (ErrP). Classification of ErrP revolutionized the domain of Brain-Computer Interface (BCI), however, due to its poor classification accuracy, it is difficult to use it in practical applications. The use of deep learning techniques such as Convolutional neural networks (ConvNets) has been growing because of its end-to-end learning capabilities and classification performance in other fields such as computer vision, speech processing and text synthesis. Very little work has been published towards classification of EEG or its classes using deep learning approaches because of its unexplored benefits in BCI applications. In this paper we propose a novel deep ConvNet architecture to accurately classify Error-related potentials. Using the very recent advances from the field of machine learning, such as batch normalization and dropout layers, we designed a new deep ConvNet architecture. We evaluate and compare the performance of two proposed deep ConvNets by finding its classification accuracy for EEG recordings from same and cross sessions as well as cross subjects for different trials of individual subjects. We see that the net proposed later with newer layers performs very well by improving the mean performance by 4%.",Yes,"논문은 Error-related potentials(ErrP) 분류를 위한 새로운 딥러닝 기반의 컨볼루션 신경망 아키텍처를 제안하고, 이를 EEG 데이터에 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Analyzing Issues and Challenges in Standard LMS Functions and Evaluation Methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933028,"Educational facilities that support efforts to improve the quality of education in the digital era are by utilizing the Learning Management System (LMS), as a learning medium. LMS provides great opportunities to improve accessibility, efficiency, and effectiveness of the learning process, especially in higher education. However, the implementation of LMS also presents challenges, such as infrastructure readiness, the competence of educators and students in operating technology, and how LMS can truly support the achievement of optimal learning outcomes. Therefore, this study aims to see the problems and challenges of using LMS on student performance through a literature review, in order to provide deeper insight into the effectiveness and constraints of LMS implementation in the context of modern education use systematic literature review. Mapping Measurement of functions in LMS is also included in this study, thus helping developers in evaluating the functions of LMS so that they are more optimal in their application in the future.",No,"본 논문은 LMS의 문제점과 도전과제를 문헌 리뷰를 통해 분석하는 연구로, 직접적인 실험이나 새로운 연구 결과를 제시하는 독창적인 연구 내용이 포함되어 있지 않습니다. 따라서 기존 연구를 종합하고 평가하는 리뷰 논문에 해당합니다."
System Integration of Xilinx DPU and HDMI for Real-Time Inference in PYNQ Environment With Image Enhancement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10558393,"Use of edge computing in application of Computer Vision (CV) is an active field of research. Today, most CV applications make use of Convolutional Neural Networks (CNNs) to inference on and interpret video data. These edge devices are responsible for several CV related tasks, such as gathering, processing and enhancing, inferencing on, and displaying video data. Due to ease of reconfiguration, computation on FPGA fabric is used to achieve such complex computation tasks. Xilinx provides the PYNQ environment as a user-friendly interface that facilitates in Hardware/Software system integration. However, to the best of authors’ knowledge there is no end-to-end framework available for the PYNQ environment that allows Hardware/Software system integration and deployment of CNNs for real-time input feed from High Definition Multimedia Interface (HDMI) input to HDMI output, along with insertion of customized hardware IPs. In this work we propose an integration of reaL-time image Enancement IP with AI inferencing engine in the Pynq environment (LEAP), that integrates HDMI, AI acceleration, image enhancement in the PYNQ environment for Xilinx’s Microprocessor on Chip (MPSoC) platform. We evaluate our methodology with two well known CNN models, Resnet50 and YOLOv3. To validate our proposed methodology, LEAP, a simple image enhancement algorithm, histogram equalization, is designed and integrated in the FPGA fabric along with Xilinx’s Deep Processing Unit (DPU). Our results show successful implementation of end-to-end integration using completely open source information.",Yes,본 논문은 PYNQ 환경에서 HDMI 입력부터 출력까지 CNN 기반 실시간 추론과 이미지 향상 IP를 통합하는 새로운 시스템 통합 방법론을 제안하고 구현한 연구이다. 또한 Resnet50과 YOLOv3 모델을 활용해 제안한 방법을 평가하고 FPGA에 맞춤형 하드웨어 IP를 설계하여 실험 결과를 제시하므로 독창적인 연구 내용이 포함된 연구 논문으로 판단된다.
Artificial Intelligence - Based Measurement Systems for Automotive: a Comprehensive Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9855154,"The development of ever intelligent systems in various application fields is nowadays a hot research topic. Artificial Intelligence (AI) techniques become a key enabler of the transition between classical static, hard-coded algorithms and innovative, flexible ones. Actually, the automotive sector can undoubtedly benefit from the usage of the aforementioned techniques, aiming at building a novel smart automotive industry. Indeed, the application of AI spreads all round the automotive sector, ranging from on–board measuring systems to customer satisfaction analysis and demand prediction. This paper aims to review the possible applications of Artificial Intelligence techniques to the automotive sector, with a special focus on innovative measurement systems and metrology. Indeed the focus will be, between others, on Advanced Driver Assistance Systems (ADAS), in-vehicle IoT systems and intelligent industrial measuring systems, thus allowing to both increase road safety and design accurate predictive maintenance, additive manufacturing systems and, in substance, to build the smart automotive factory of the future.",No,"본 논문은 인공지능 기반 자동차 측정 시스템에 대한 포괄적인 리뷰를 제공하는 논문으로, 기존 연구들을 종합하여 정리한 내용에 초점을 맞추고 있습니다. 따라서 직접적인 독창적인 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵습니다."
Improving Cyberbullying Detection Accuracy with Advanced Machine Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10817024,"Cyberbullying poses a significant threat to individuals well-being in online environments, necessitating effective detection and prevention measures. In this study, Attack detection across different online platforms involves conducting a comprehensive literature review to survey existing research efforts and highlight the diverse feature the different scope of machine learning procedures employed in this domain. Specifically, exploring utilization of Naive Bayes, Logistic Regression, Support Vector Machine (SVM), Neural Network (NN), Gradient Boosting Machine (GBM), Random Forest (RF), Decision Tree (DT) for cyber bullying detection. Each approach is evaluated based on its performance in identifying instances of cyberbullying behavior, leveraging textual, multimedia, and social network features. The Findings reveal the limitations and strength of various machine learning methods and offer perceptions into their applicability and effectiveness in addressing the complex challenges of cyberbullying detection. Through this analysis, this paper mainly focuses to the development of robust as well as scalable solutions for mitigating the adverse effects of cyberbullying in online communities.",No,초록 내용은 기존 연구들을 종합적으로 검토하고 다양한 머신러닝 기법들의 성능을 비교하는 문헌 리뷰에 가깝습니다. 직접적인 실험 결과나 새로운 모델 개발에 대한 구체적인 독창적 연구 기여가 명확히 드러나지 않습니다.
Classification of Brain Disorder Diseases Using Machine Learning Based Data Mining Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10560792,"Brain disorders include a wide range of illnesses, including mental illnesses and neurological disorders, for which an early and precise diagnosis is essential to successful treatment. This review aims to give an in-depth analysis of current developments and problems in this important field by examining the evolution of machine learning (ML) with respect to data mining usage in the categorization of brain illnesses. The study starts by clarifying the need of prompt and accurate diagnosis in brain illnesses, highlighting the possibility of utilizing machine learning and data mining tools to supplement conventional diagnostic approaches. A thorough analysis of the literature demonstrates the wide range of approaches used for classifying brain disorders, from sophisticated deep computational systems to traditional machine learning methods. The exploration clarifies the complexities of model construction by covering important topics including feature selection, dataset qualities, and assessment measures. Numerous research show how well machine learning approaches may be used to differentiate between various brain illnesses, showing encouraging outcomes in regard to specificity, sensitivity, and accuracy. The report also outlines the difficulties those working in this subject encounter, such as the requirement for vast and diverse datasets, accessibility, and variety of data. Potential alternatives are also covered, including the creation of model-based interpretation for improved practicality and the synthesis of multimodal information. This study is an invaluable tool for scholars, employees, and health care providers working at the nexus of data mining, machine learning, and brain illness categorization. Through a comprehensive analysis of the present status of the sector and the identification of potential areas for advancement, this study adds to the continuing efforts to use technologies in the identification and therapy of brain illnesses.",No,초록에서 본 논문은 기존 연구들을 종합적으로 분석하고 현황을 정리하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하는 연구 논문에 해당하지 않습니다.
Analysis of a Single-Grade Workforce Using Stochastic Methods with Correlated Egress Clusters and Two Factors Defining Breakdown Points,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912160,"This research investigates the challenge of optimizing recruitment timing in a single-grade workforce system that experiences reductions in personnel due to either employee departures or breaks. Employee exits, occurring in random clusters, are modeled using a compound process, with each cluster’s workforce loss represented by exchangeable, exponentially correlated random variables. Break-related exits are classified by the intensity of attrition and modeled as independent and identically distributed hyper exponential random variables. Threshold of breakdown is split into two components: exits and breaks. The paper proposes two stochastic models: Model 1, where all exits are treated uniformly, and Model 2, which differentiates exits into voluntary and involuntary categories. The study applies the univariate CUM recruitment policy to calculate the mean and variance of recruitment time. Using Matlab, numerical illustrations are presented to illustrate the effectiveness of the proposed models. The research also explores how different nodal parameters influence key system characteristics, yielding insights into the dynamics of recruitment timing. A major contribution of this work lies in the development of two comprehensive stochastic models that account for both voluntary and involuntary exits, providing a more nuanced understanding of workforce attrition. The use of statistical analysis and Matlab simulations strengthens the findings, offering practical applications for optimizing workforce management strategies in dynamic environments. This study provides a foundation for future exploration into more sophisticated recruitment policies, potentially incorporating AI/ML and deep learning techniques. 2020 Mathematics Subject Classification: 60H30, 90B70, 91D35.",Yes,"논문은 단일 등급 인력 시스템에서 인력 감소를 확률적 모델로 분석하고, 두 가지 유형의 퇴사와 고장 임계점을 고려한 새로운 확률 모델을 제안하는 등 독창적인 연구 내용을 포함하고 있다. 또한 Matlab 시뮬레이션과 통계 분석을 통해 모델의 유효성을 검증하여 실질적인 기여를 하고 있다."
An approach to analyse and Forecast Social media data using Machine Learning and Data Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132895,"Twitter is one of the most used social network platforms with more than 321 million active users, sending a daily average of 500 million Tweets. Twitter basically reaches a broad audience, connects many users, and also provides a useful point of view on an ongoing topic based on their interests. This research paper includes works of Machine Learning, NLP, and sentiment analysis on twitter data which is related to Goods and Service Tax (GST) collected from twitter by web scraping technology. Basically here we have found out how many people are against or for about the GST.Here we calculated the sentiments of positive, negative and neutral according to the twitter user. Here Data visualization also used to visualize the data can include cleaning checking transforming and finding the pattern. Machine learning and NLP are used on the data-driven model from the prediction purpose. Machine learning techniques applied to corresponding twitter dataset that contain reviews of GST users to find out the relevant information and inference. Machine learning used to extract the sentiments of the user and find future trends based on current uses.",Yes,본 논문은 트위터 데이터를 수집하고 머신러닝과 자연어처리 기법을 적용하여 감성 분석 및 미래 트렌드 예측을 수행하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다.
Measuring Machine Learning Robustness in front of Static and Dynamic Adversaries*,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098084,"Adversarial machine learning brought a new way of understanding the reliability of different learning systems. Knowing that the learning confidence depends significantly on small changes, such as noise, created a mind change in the artificial intelligence community, who started to consider the boundaries and limitations of machine learning methods. However, if we can measure these limitations, we can improve the strength of our machine learning models and their robustness. Following this motivation, this work introduces different measures of robustness for machine learning models based on false negatives. These measures can be evaluated for either static or dynamic scenarios, where an adversary performs intelligent actions to evade the system. To evaluate the metrics I have applied 11 classifiers to different benchmark datasets and created an adversary that performs an evolutionary search process aiming to reduce the classification accuracy. The results show that the most robust models are related to K-Nearest Neighbours, Logistic regression, and neural networks, although none of the systems is robust enough when the target is to reach a single misclassification.",Yes,"논문은 머신러닝 모델의 강인성을 측정하기 위한 새로운 지표를 제안하고, 이를 평가하기 위해 11개의 분류기를 적용하고 진화적 탐색 기반 적대자를 설계하는 등 독창적인 연구 방법과 실험을 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Toward Holistic Energy Management by Electricity Load and Price Forecasting: A Comprehensive Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10320357,"Electricity load and price data pose formidable challenges for forecasting due to their intricate characteristics, marked by high volatility and non-linearity. Machine learning (ML) and deep learning (DL) models have emerged as valuable tools for effectively predicting data exhibiting high volatility, frequent fluctuations, mean-reversion tendencies, and non-stationary behavior. Therefore, this review article is dedicated to providing a comprehensive exploration of the application of machine learning and deep learning techniques in the context of electricity load and price prediction. In contrast to existing literature, our study distinguishes itself in several key ways. We systematically examine ML and DL approaches employed for the prediction of electricity load and price, offering a meticulous analysis of their methodologies and performance. Furthermore, we furnish readers with a detailed compendium of the datasets utilized by these forecasting methods, elucidating the sources and specific characteristics underpinning these datasets. Then, we rigorously conduct a performance comparison across various performance metrics, facilitating a comprehensive assessment of the efficacy of different predictive models. Notably, this comparison is carried out using the same datasets that underlie the diverse methodologies reviewed within this study, ensuring a fair and consistent evaluation. Moreover, we provide an in-depth examination of the diverse performance measures and statistical tools employed in the studies considered, providing valuable insights into the analytical frameworks used to gauge forecasting accuracy and model robustness. Lastly, we devote significant attention to the identification and analysis of prevailing challenges within the realm of electricity load and price prediction. Additionally, we delve into prospective directions for future research, thereby contributing to the advancement of this critical field.",No,"본 논문은 전기 부하 및 가격 예측에 관한 머신러닝과 딥러닝 기법을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 체계적으로 분석하고 비교하는 데 중점을 두고 있다. 따라서 연구 논문보다는 종설 논문에 해당한다."
Persona-Based Conversational AI: State of the Art and Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031078,"Conversational AI has become an increasingly prominent and practical application of machine learning. How-ever, existing conversational AI techniques still suffer from var-ious limitations. One such limitation is a lack of well-developed methods for incorporating auxiliary information that could help a model understand conversational context better. In this paper, we explore how persona-based information could help improve the quality of response generation in conversations. First, we provide a literature review focusing on the current state-of-the-art methods that utilize persona information. We evaluate two strong baseline methods, the Ranking Profile Memory Network and the Poly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates the importance of incorporating persona information into conversational systems. Additionally, our study highlights several limitations with current state-of-the-art meth-ods and outlines challenges and future research directions for advancing personalized conversational AI technology.",No,"본 논문은 기존 연구들을 리뷰하고 두 가지 기법을 평가하는 분석 연구에 초점을 맞추고 있으며, 독창적인 연구 방법론이나 새로운 실험 결과를 제시하지 않는다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Towards Training Reproducible Deep Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794033,"Reproducibility is an increasing concern in Artificial Intelligence (AI), particularly in the area of Deep Learning (DL). Being able to reproduce DL models is crucial for AI-based systems, as it is closely tied to various tasks like training, testing, debugging, and auditing. However, DL models are challenging to be reproduced due to issues like randomness in the software (e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There are various practices to mitigate some of the aforementioned issues. However, many of them are either too intrusive or can only work for a specific usage context. In this paper, we propose a systematic approach to training reproducible DL models. Our approach includes three main parts: (1) a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains, (2) a unified framework which leverages a record-and-replay technique to mitigate software-related randomness and a profile-and-patch technique to control hardware-related non-determinism, and (3) a reproducibility guideline which explains the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Case study results show our approach can successfully reproduce six open source and one commercial DL models.",Yes,"논문은 딥러닝 모델의 재현성을 높이기 위한 체계적인 접근법과 프레임워크를 제안하며, 구체적인 평가 기준과 기법을 포함한 독창적인 연구 내용을 담고 있다. 또한, 실제 사례 연구를 통해 제안한 방법의 효과를 검증하고 있어 연구 논문에 해당한다."
A comparative analysis of deep learning algorithms in eye gaze estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041480,"Eye tracking is the procedure of keeping track of one’s point of gaze. Researchers have proposed a range of algorithms and methods to automatically track the gaze direction and position, which can be beneficial in a number of applications. Eye tracking is growing in popularity due to its ability to make a range of activities simpler, especially for the elderly or people with special needs. This research intends to investigate and review eye tracking methodologies by focusing on modern approaches such as machine learning and deep learning for gaze detection. This paper provides a quick overview of gaze tracking based upon appearance of eye images and model based approaches. Different mapping functions are mostly used for calculating gaze points. The findings of this study indicates that when compared with machine learning algorithms for estimating gaze points, deep learning techniques produce more accurate results in gaze detection from images captured using camera. Finally, some potential applications of gaze tracking are discussed.",No,"초록에서 본 논문은 기존의 딥러닝 및 머신러닝 알고리즘을 비교 분석하고 리뷰하는 내용으로 보이며, 직접적인 독창적인 연구 결과나 새로운 알고리즘 제안에 대한 언급이 없습니다. 따라서 기존 연구를 종합하고 평가하는 리뷰 논문에 더 가깝다고 판단됩니다."
From Bricks to Bytes: The AI Transformation of Cybersecurity Firewalls,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911850,"Due to the frequent occurrence of network threats, robust intrusion detection systems (IDS) are essential for safeguarding digital data. Traditional rule-based IDS often struggle to address emerging cyber threats, underscoring the necessity for adaptable solutions. This study presents a Next Generation Firewall (NGFW), a firewall variant that incorporates machine learning (ML) and deep learning (DL) to enhance IDS capabilities. By leveraging machine learning techniques such as Support Vector Machines (SVM), Random Forests, and Deep Neural Networks (DNN), along with deep learning models like Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), the NGFW greatly enhances its threat detection accuracy and effectiveness. We conduct a thorough review of current literature to evaluate the efficacy of ML and DL in IDS. Our methodology involves systematically collecting and preparing data, selecting relevant features, and assessing model performance through metrics like accuracy, precision, and recall. Results indicate that CNNs are particularly effective in identifying malicious activities while maintaining a low false-positive rate. Further real-world testing of the NGFW enhances the reliability of the classification report, which demonstrates impressive accuracy. Each class achieved precision, recall, and F1-scores exceeding 97%, resulting in an overall accuracy of 97.29%. This highlights the potential for further research into intelligent cyber defense solutions, showcasing how ML and DL can contribute to the development of more resilient and adaptive security systems.",Yes,"본 논문은 머신러닝과 딥러닝 기법을 활용한 차세대 방화벽(NGFW)의 설계 및 성능 평가를 포함하여 직접적인 연구 방법론과 실험 결과를 제시하고 있다. 이는 기존 연구를 검토하는 데 그치지 않고, 독창적인 모델 개발과 실험적 검증을 수행한 연구 논문임을 보여준다."
Multilingual Sentiment Analysis for Under-Resourced Languages: A Systematic Review of the Landscape,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961195,"Sentiment analysis automatically evaluates people’s opinions of products or services. It is an emerging research area with promising advancements in high-resource languages such as Indo-European languages (e.g. English). However, the same cannot be said for languages with limited resources. In this study, we evaluate multilingual sentiment analysis techniques for under-resourced languages and the use of high-resourced languages to develop resources for low-resource languages. The ultimate goal is to identify appropriate strategies for future investigations. We report over 35 studies with different languages demonstrating an interest in developing models for under-resourced languages in a multilingual context. Furthermore, we illustrate the drawbacks of each strategy used for sentiment analysis. Our focus is to critically compare methods, employed datasets and identify research gaps. This study contributes to theoretical literature reviews with complete coverage of multilingual sentiment analysis studies from 2008 to date. Furthermore, we demonstrate how sentiment analysis studies have grown tremendously. Finally, because most studies propose methods based on deep learning approaches, we offer a deep learning framework for multilingual sentiment analysis that does not rely on the machine translation system. According to the meta-analysis protocol of this literature review, we found that, in general, just over 60% of the studies have used deep learning frameworks, which significantly improved the sentiment analysis performance. Therefore, deep learning methods are recommended for the development of multilingual sentiment analysis for under-resourced languages.",No,"본 논문은 다국어 감성 분석 분야의 기존 연구들을 체계적으로 검토하고 비교하는 문헌 리뷰(시스템 리뷰) 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 방법론을 종합하는 데 초점이 맞춰져 있습니다."
Applications of AI in higher education: a review of the literature engineering education from developing countries,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409934,"Publications in the field of Artificial Intelligence (AI) are growing remarkably in the last decade. This can be explained by the fact that AI applications have become widespread in scientific and industrial fields. Education is one of the most relevant domains where AI applications can improve quality and equity in access, according to UNESCO [1] recommendations. Following this global trend, higher education institutions in developing countries, as in advanced countries, have taken measures and approaches to implement AI solutions. Despites the lack of clear national strategies in many of them, several interesting use cases are identified through our literature review performed between 2012 and 2022. We aim with our work to explore applications of AI in the field of engineering education and in the context of Higher Education institutions in developing countries. Our approach shows prioritised AI applications in such countries and how those applications are exploited to prepare graduates to meet industrial needs by classifying findings and organizing them in a concepts map.",No,"본 논문은 2012년부터 2022년까지의 문헌을 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분류하고 정리하는 데 중점을 두고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 종합적 분석에 해당한다."
An Analysis on Deep Learning Approach Performance in Classifying Big Data Set,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970980,"Big data sets are mainly derived from social media as well as stock market exchange. It is commonly described according to its main characteristics the 3Vs, which refers to Volume, Velocity and Variety. Big data sets often contributed to difficulties faced by the back end groups such as data analyst, system developer, programmer, and network analyst due to its complexity issue. To overcome this issue, many researchers and professionals have proposed and initiated various solutions, for instance; algorithm, software, hardware and framework related to big data. One beneficial and popularly known approach in dealing with big data is deep learning. It is an extension of neural network that is able to analyze huge data sets without assistance from any parameterization methods. To make use of this advantage, this paper aimed to evaluate the capability of deep learning in analyzing big data sets. Several data sets were selected and support vector machine (SVM) was chosen as a benchmark method for the experimental work. The results obtained revealed that deep learning has outperformed SVM in classifying big data set. As a conclusion, deep learning can be categorized as one of the best machine learning approaches to be used in decision analysis process. It can also be used as an alternative approach to other traditional approaches such as Naive Bayes or SVM which require more data processing phases.",Yes,"논문은 딥러닝과 SVM을 이용한 빅데이터 분류 성능을 직접 평가하는 실험 연구를 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한, 실험 결과를 통해 딥러닝의 우수성을 입증하며 기존 방법과의 비교 분석을 수행하고 있다."
Artificial Intelligence and Internet of Things: A Boon for the Crime Prevention,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466124,"This paper examines the application of Internet of Things (IoT) and artificial intelligence (AI) technology to the prevention of crimes against states. In order to avoid crimes against states, this paper will look at how supervision, cyber security, and data analysis help in preventing using AI and IoT technology. The article also finds the advantages of combining AI and IoT technology, as well as the difficulties and moral dilemmas associated with doing so. Furthermore, the application of artificial intelligence (AI) and the Internet of Things (IoT) in the prevention of crimes against states is reviewed, with implications for future research. The study underscores the potential of artificial intelligence (AI) and the Internet of Things (IoT) to augment safety measures and depiction real crimes. However, it also underscores the significance of dealing with the obstacles and ethical dilemmas related to their use.",No,"초록 내용은 AI와 IoT 기술의 범용적 적용과 장단점, 윤리적 문제를 검토하는 리뷰 성격에 가깝고, 구체적인 독창적 연구 방법론이나 실험 결과, 데이터 분석 등의 직접적인 연구 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 연구 논문보다는 개념적 고찰 또는 리뷰 논문으로 판단됩니다."
Towards Measuring Happiness in Saudi Arabia based on Tweets: A research proposal,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8442024,"Social media platforms such as Twitter, Facebook and Blogs are growing enormously in terms of the number of users. Due to the rapid growth of the data produced from social media users, these platforms are becoming one of the most valuable data sources. Social media became outlets for people to express their thoughts, opinions, and emotions in a real-time manner. Therefore, Social media data have been utilized in literature for many sentiment analysis research. Although there have been many types of research on the sentimental analysis in English, the amount of Arabic-based sentiment analysis studies and tools are still limited. In this paper, we reviewed selected literature related to sentiment analysis in order to present a staged road map of Arabic tweets sentiment analysis in favor of measuring the happiness levels in Saudi Arabia cities. We reported the methodology to numerically represent the happiness of each city by determining the sentiment of 2000 geo-tagged tweets in Saudi Arabia using machine learning techniques. Expected results would not only rank the happiness levels for the cities in Saudi Arabia but certainly identify activities or circumstances that contribute to citizens' happiness. These expected results later can be used by institutions of related interest such as General Entertainment Authority to help planning new activities and attractions.",No,"본 논문은 연구 제안서(research proposal)로, 기존 문헌 검토와 연구 계획을 제시하고 있으나 실제 데이터 분석 결과나 독창적인 연구 수행 내용은 포함되어 있지 않습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
The Role of Artificial Intelligence in Measuring Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610828,"The development of artificial intelligence (AI) and AI systems has opened up new perspectives for measurements due to the ability to process a number of measurement information streams using networks, recognize images in them, predict their changes and correlations with others, self-learn during operation, as well as make decisions. The level of capabilities to implement these functions in AI-provided measuring systems produced by different manufacturers can be different. Decisions made by the AI, if a manufacturer and user do not sufficiently consider the specifics of a particular system, can result in dangerous consequences. The experience of working in a Committee on Standardization that considers dozens of standard drafts related to AI systems for various applications has given grounds for presenting an analysis of trends in the development of such measuring systems and substantiating recommendations aimed at improving their efficiency.",No,"초록에서 제시된 내용은 AI 측정 시스템의 발전 동향 분석과 표준화 위원회 경험을 바탕으로 한 권고사항 제시에 초점이 맞춰져 있습니다. 직접적인 실험, 데이터 분석, 또는 독창적인 연구 결과를 제시하는 연구 논문이라기보다는 리뷰 또는 정책 제안에 가까운 내용으로 보입니다."
AI Innovations in rPPG Systems for Driver Monitoring: Comprehensive Systematic Review and Future Prospects,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10856104,"Advanced technologies, notably camera-based systems using remote photoplethysmography (rPPG), are increasingly used in automotive safety to non-invasively monitor driver well-being and fatigue by measuring physiological metrics like heart and respiration rates. This review examines recent advancements in machine learning algorithms and signal processing for rPPG in driver monitoring. A literature search up to April 2, 2024, across major databases, identified 344 studies; 29 were analyzed in depth, focusing on: 1) rPPG signal extraction and heart rate estimation, where deep learning improved accuracy; 2) fatigue detection, showing benefits of multimodal data fusion; 3) mental state monitoring, with machine learning classifying cognitive load and distraction; and 4) emotional state monitoring and dataset development, indicating a trend toward holistic driver assessment. While deep learning has improved rPPG signal extraction, challenges remain in consistent physiological metric detection under dynamic conditions. There is also a lack of diverse population representation, especially female drivers, in datasets. The review underscores the potential of AI-enhanced camera systems to improve road safety, emphasizing the need for diverse, multimodal data integration for comprehensive monitoring.",No,"초록에서 해당 논문은 기존 연구들을 종합적으로 검토하는 체계적 문헌고찰(review)임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험 기여를 포함하고 있지 않다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Application of Entropy for Automated Detection of Neurological Disorders With Electroencephalogram Signals: A Review of the Last Decade (2012–2022),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179861,"An automated Neurological Disorder detection system can be considered as a cost-effective and resource efficient tool for medical and healthcare applications. In automated Neurological Disorder detection, electroencephalograms are commonly used, but their low signal intensity and nonlinear features are difficult to analyze visually. A promising approach for processing of electroencephalogram signals is the concept of entropy, a nonlinear signal processing method to measure the chaos in the signal. The aim of this study was to find out the effective entropy measures and the machine learning approaches that produced promising output. Using Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines as our method, we have identified 84 studies published between 2012 and 2022 that has investigated epilepsy, Parkinson’s disease, autism, Attention Deficit Hyperactive disorder, schizophrenia, Alzheimer’s disease, depression, and alcohol use disorder with machine learning approaches considering entropy measures. We show that Support Vector Machines was the most commonly used machine learning model, with consistent performance in most of the studies whereas sample entropy was the most commonly used entropy measure, followed by the approximate entropy. For epilepsy detection, the most used entropy feature was the log energy entropy, whereas the multi-scale entropy was commonly used for Alzheimer’s Disease, approximate and sample entropy used for Parkinson’s Disease, multi scale and Shannon entropy applied for autism, approximate and Shannon entropy used for attention deficit hyperactive disorder, sample entropy used for depression, approximate and spectral entropy adopted for schizophrenia, and the approximate and sample entropy employed for alcohol use disorder. According to the majority of the studies, there is growing concern about the increase in neuro patients and the heavy resource burden that is associated with their prevalence and diagnosis. Based on these studies, we conclude that Computer-Aided Design systems would be economically advantageous in detecting Neurological Disorders. To incorporate Computer-Aided Design system into the mainstream health care system, future research could focus on multi-modal approaches to the disorder and its interpretation and explanation. We believe this is the first review that has combined the electroencephalograms, entropy, and automated detection possibility of the 8 distinct neurological disorders. The study is limited to the papers that used accuracy as their performance evaluation metric. The findings and synthesis of previous studies provides a clear pathway that identifies the entropy approach as a practical solution for automated detection of neurological disorder using electroencephalograms with potential applications in other kinds of signal analysis.",No,"이 논문은 지난 10년간의 연구를 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 평가에 초점을 맞추고 있다."
Towards Intelligent Architecting of Aerospace System-of-Systems: Part II,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172585,"System-of-Systems (SoS) are composed of large scale independent and complex heterogeneous systems which collaborate to create capabilities not achievable by a single system, for example air transportation system, satellite constellations, and space exploration architectures. To support architecting of aerospace SoS, in this work we present a methodology to accurately predict different aspects of performance for design/operation and SoS architecting, expanding previous work on intelligent architecting of aerospace SoS, by adding rigorous Uncertainty Quantification via Bayesian Neural Networks. A Bayesian Neural Network is a neural network with a-priori distribution on its weights. In addition to solving the overfit problem, which is common to traditional deep neural networks, Bayesian Neural Networks provide automated model pruning (or reduction of feature design space), that addresses a well-known dimensionality curse in the SoS domain. We enable SoS design/operation by using modeling and simulation, quantifying the uncertainty inherently present in SoS, and utilizing Artificial Intelligence and optimization techniques to design and operate the system so that its expected performance or behavior when the unexpected occurs (for example, a failure) still satisfies user requirements. Much of the research effort in the field of SoS has focused on the analysis of these complex entities, while there are still gaps in developing tools for automated synthesis and engineering of SoS that consider all the various aspects in this problem domain. In this expansion of the use of Artificial Intelligence towards automated design, these techniques are used not only to discover and employ features of interest in a complex design space, but also to assess how uncertainty can affect performance. This capability supports the automated design of robust architectures, that can effectively meet the user needs even in presence of uncertainty. The SoS design and evaluation methodology presented in this paper and demonstrated on a synthetic modular satellites problem starts from modeling and simulation, and design of experiments to explore the design space. The following step is deep learning, to develop a model which relates SoS architectural features with performance metrics. Uncertainty Quantification techniques are then applied to assess the performance metrics for different architectures. Once the most critical features that affect the SoS performance are identified, stochastic optimization of the SoS on a reduced design space can be performed to determine Pareto optimal features. The final step is determining if any additional design/operation measures need to be explored to further maximize the SoS performance.",Yes,"논문은 Bayesian Neural Networks를 활용한 불확실성 정량화 및 최적화 기법을 포함하여 항공우주 시스템의 설계 및 운영에 대한 새로운 방법론을 제안하고 있으며, 이는 독창적인 연구 내용과 직접적인 기여를 포함하고 있음을 보여준다. 따라서 연구 논문에 해당한다."
The Applications of Internet of Things in Architectural Heritage Preservation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10438242,"Internet of Things (IoT) has been widely applied in architectural heritage preservation. To explore and summarize the new applications, a systematic review of the research in the last decade is made. In this study, we conducted comparative analyses of diverse technological approaches, algorithms, and platforms, while critically evaluating the limitations and challenges associated with IoT implementation in the context of architectural heritage protection. In this paper, the IoT applications in heritage conservation are introduced, the IoT devices for monitoring are listed, the distinctive features of IoT monitoring data are elucidated, and the data processing workflows and methodologies rooted in artificial intelligence (AI) are also expounded upon. This literature search endeavor serves to offer practitioners guidance in selecting appropriate technological solutions tailored to their specific requirements, thereby optimizing the efficacy of their practical applications.",No,본 논문은 지난 10년간의 연구를 체계적으로 검토하고 기존 연구들을 비교 분석하는 문헌 리뷰 연구로 보입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 종합과 평가에 중점을 두고 있습니다.
Performance Analysis of Spiking Neural Network using Temporal Spike-based Backpropagation on Field Programmable Gate Array (FPGA) platform,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864534,"This paper explores the object classification performance of spiking neural networks (SNNs) using the temporal spike-based backpropagation technique on the Field Programmable Gate Array (FPGA) platform. The FPGA board is specially designed to host the spiking neural networks for artificial intelligence tasks such as object classification, object detection, and segmentation. The publicly available classification datasets such as MNIST, CIFAR10 were employed to examine the performance of the SNNs on the FPGA platform. Similarly, the latest temporal spike-based backpropagation technique was chosen to investigate the neuromorphic ability of the low-cost FPGA board in processing SNNs for object classification tasks. The main purpose of this research proceeding is to facilitate the neuromorphic research community with the information regarding (i). the exploitation of the low-cost FPGA design for neuromorphic image processing and artificial intelligence (AI) tasks; (ii). cross-validating temporal spike-based backpropagation trained SNNs on FPGA alongside PC; (iii). assessing the performance stability and industrial choices of low-cost FPGAs for object classification tasks and related issues. The evaluation metrics such as classification accuracy, mean average precision, and processing time were utilized to assess the performance of the SNN model on FPGA alongside PC. This study will be used as an informative report for the researchers working towards perfecting the neuromorphic hardware for processing SNNs in imminent studies.",Yes,"본 논문은 FPGA 플랫폼에서 시간 기반 스파이크 역전파 기법을 사용한 스파이킹 신경망의 성능을 직접 평가하고, 다양한 데이터셋을 활용한 실험 결과를 제시하고 있다. 이는 독창적인 연구 방법과 실험을 포함한 연구 논문에 해당한다."
Deep Learning Approaches for Fashion Knowledge Extraction From Social Media: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661397,"Fashion knowledge encourages people to properly dress and faces not only physiological necessity of users, but also the requirement of social practices and activities. It usually includes three jointly related aspects of: occasion, person and clothing. Nowadays, social media platforms allow users to interact with each other online to share opinions and information. The use of social media sites such as Instagram has already spread to almost every fashion brand and been evaluated as business take-off tools. With the heightened use of social media as a means of marketing communication for fashion brands, it has become necessary to empirically analyse and extract fashion knowledge from them. Thus, social brands are investing on them. In this way, they can understand the consumer’s preferences. This change is also having a significant impact on social media data analysis. To solve this issue, the Deep learning (DL) methods are proven to be effective solutions due to their automatic learning capability. However, little systematic work currently exists on how researchers have applied DL for analysing fashion knowledge from social media data. Hence, this contribution outlines DL-based techniques for social media data related to fashion domain. In this study, a review of the dataset within the fashion world and the DL methods applied on, it is presented to help out new researchers interested in this subject. In particular, five different tasks will be considered: Object Detection, that includes Clothes Landmark Detection, Clothes Parsing and Product Retrieval, Fashion Classification, Clothes Generation, Automatic Fashion Knowledge Extraction and Clothes Recommendation. Therefore, the purpose of this paper is to underline the multiple applications within the fashion world using deep learning techniques. However, this review does not cover all the methods used: in fact, only Deep Learning methods have been analyzed. This choice was made since, given the huge amount of fashion social media data that has been collected, Deep Learning methods achieve the best performance both in terms of accuracy and time. Limitations point towards unexplored areas for future investigations, serving as useful guidelines for future research directions.",No,"본 논문은 딥러닝을 활용한 패션 지식 추출에 관한 기존 연구들을 체계적으로 정리한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 방법론을 요약하는 데 초점이 맞춰져 있습니다."
Detection of Solid Waste Deposits in Urban Areas using Artificial Intelligence and Image Processing: a Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794347,"One of the main problems that governments around the world face is the prevalent presence of solid wastes in their countries. Scattered solid wastes in rural and urban areas cause serious problems to people and the environment. In the last years, different solutions for solid waste management have been developed using technology and artificial intelligence. Computer vision is one of these areas in constant development, with improvements in techniques and algorithms to detect and classify objects in images and videos. By doing a literature review in different databases, we found 17 studies from IEEE, ACM, Science Direct, Springer and IOPScience that address the use of artificial intelligence techniques to detect and classify solid waste deposits using computer images. We analyzed information about the object detection techniques and the dataset used for algorithm training in these studies. We also depicted the metrics used to evaluate the performance, accuracy, and precision to detect garbage on images. Deep learning is the main technique used for image processing. YOLO, Deep CNN and Faster R-CNN are the principal techniques used for classification and detection of solid waste due to their speed and accuracy. These results may be very useful to induce and to guide the development of tools to detect solid waste in our country.",No,"본 논문은 인공지능과 이미지 처리 기법을 활용한 고체 폐기물 탐지에 관한 기존 연구들을 종합적으로 검토한 문헌 리뷰이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하지 않고, 기존 연구들을 분석하고 요약하는 데 중점을 두고 있다."
Machine Learning and Deep Learning Methods for Intrusion Detection Systems in IoMT: A survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738218,"The integration of healthcare-related sensors and devices into IoT has resulted in the evolution of the IoMT (Internet of Medical Things). IoMT that can be viewed as an improvement and investment in order to meet patients' needs more efficiently and effectively. It is progressively replacing traditional healthcare systems, particularly after the worldwide impact of COVID. IoMT devices have enabled real time monitoring in the healthcare field, allowing physicians to provide superior care while also keeping patients safe. As IoMT applications have evolved, the variety and volume of security threats and attacks including routing attacks and DoS (Denial of Service), for these systems have increased, necessitating specific efforts to study intrusion detection systems (IDSs) for IoMT systems. However, IDSs are generally too resource intensive to be managed by small IoMT devices, due to their limited processing resources and energy. In this regard, machine learning and deep learning approaches are the most suitable detection and control techniques for IoMT device-generated attacks. The purpose of this research is to present various methods for detecting attacks in the IoMT system. Furthermore, we review, compare, and analyze different machine learning (ML) and deep learning (DL) based mechanisms proposed to prevent and detect IoMT network attacks, emphasizing the proposed methods, performances, and limitations. Based on a comprehensive analysis of current defensive security measures, this work identifies potential open research related challenges and orientations for the actual design of those systems for IoMT networks, that may guide further research in this field.",No,본 논문은 IoMT 시스템의 침입 탐지에 사용되는 머신러닝 및 딥러닝 기법들을 종합적으로 검토하고 비교하는 서베이 논문입니다. 직접적인 실험이나 새로운 알고리즘 제안 등 독창적인 연구 결과를 포함하지 않고 기존 연구들을 분석하는 데 중점을 두고 있습니다.
An Approach to Evaluating Subjective Answers using BERT model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865706,"The state of art model for language translation, conversion from hand written to digital text, transcription are succeeded in wide range of fields using Natural Language Processing, Artificial Intelligence and Machine Learning (AIML) applications. In present, evaluation of subjective answers are not exercised systematically and graded using computer system. In this work, a mathematical method is proposed for evaluating subjective answers using Bidirectional Encoder Representation Transformers for word embedding and convert the sentence into vector space using pooling method for representing similar sentences. The proposed method evaluates the subjective answers having semantic meaning of answers based on topic Engineering and Medical related questions and answers dataset. It achieves to understand the similarity of different answers which are same semantically. The BERT model is used with machine learning methods to transform the sentence into vector space. The vector space is used to calculate percentage of similarity. The similarity of the sentences with percentage is observed and evaluated.",Yes,"논문은 BERT 모델과 머신러닝 기법을 활용하여 주관식 답변을 평가하는 새로운 수학적 방법을 제안하고 있으며, 이는 독창적인 연구 내용과 직접적인 기여를 포함하고 있다. 따라서 연구 논문에 해당한다."
Performance Evaluation of Machine Learning Algorithms for Prediction of Cardiac Failure,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368606,"Over the past few years, the world is experiencing a drastic rise in deaths caused by Heart Failures, and it is a prevalent and perhaps dangerous medical condition, which has an impact on millions of people globally. The outcomes of patients can be greatly enhanced by early detection and accurate cardiac failure prediction which could reduce healthcare costs. In recent times, artificial intelligence and machine learning algorithms have emerged as powerful tools for predicting heart failure. This paper performs a comparative study and investigation of leading machine learning algorithms for heart failure prediction. This research study presents an explanation, implementation, and comparison on the basis of various evaluation parameters like accuracy, precision, recall, Fl score, etc. The study is performed for multiple regression, KNN, support vector machines, random forest, Naive Bayes, and decision tree models. This study also reviews the different types of factors effecting heart failure prediction, such as chest pain, cholesterol, exercise angina and more. Finally, this study evaluates how well various machine learning algorithms perform on tasks that predict cardiac failure and point up the difficulties and restrictions in this area. The findings of this study demonstrate that Naive Bayes method gives highest accuracy 85.87 % and it is a useful tool in medical practice and are capable of accurately predicting cardiac failure.",Yes,"논문은 여러 머신러닝 알고리즘을 심층적으로 구현하고 평가하여 심장부전 예측 성능을 비교하는 독창적인 연구를 수행하고 있다. 또한, 다양한 평가 지표를 사용해 알고리즘의 성능을 분석한 점에서 직접적인 연구 기여가 포함되어 있다."
Identifying AI-Written Text in Academia: A Machine Learning-Based Framework,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10850944,"The rise of AI-generated text poses new challenges to academic integrity, requiring robust mechanisms to identify and differentiate human-written content from machine-generated material in scholarly publications. This paper presents a machine learning-based framework for detecting AI-written text in academia, combining multiple techniques including stylometry, semantic analysis, and citation pattern recognition. The framework utilizes supervised classification models trained on features such as lexical diversity, syntactic complexity, and redundancy patterns to distinguish AI-generated content from genuine research writing. Additionally, unsupervised anomaly detection techniques are employed to flag unusual stylistic deviations. The framework also integrates traditional plagiarism detection tools and enlists human expert review for validating suspicious sections. By addressing both technical and ethical considerations, this approach aims to preserve the authenticity of academic work while adapting to the evolving landscape of AI-driven content generation.",Yes,"논문은 AI가 작성한 텍스트를 식별하기 위한 기계 학습 기반의 새로운 프레임워크를 제안하고 있으며, 다양한 기법을 결합한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Smartwatch-based Eating Detection and Cutlery Classification using a Deep Residual Network with Squeeze-and-Excitation Module,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851333,"Several machine learning and deep learning algorithms have been developed to tackle the human behavior detection issue, emphasizing everyday tasks. Nevertheless, an intriguing and challenging human activity recognition (HAR) subject involves more complicated human behaviors, including eating-related actions. This study provided a smartwatch-based eating identification system based on hand-movement data. In the framework, we introduced a deep residual network named the ResNet-SE model that enhanced detection capability by using the benefits of shortcut connections and squeeze-and-excitation units. In addition, the effectiveness of standard deep learning models (CNN and LSTM) is evaluated and compared to that of the proposed model. In addition, we investigate the use of wristwatch sensor data for categorizing six kinds of food cutlery. To validate the correctness of the model, the accuracy, F1-score, and confusion matrices of the HAR metrics are applied to the EatingDetectionIJS dataset for evaluating the proposed frame-work. Experimentation findings demonstrate that the ResNet-SE model surpasses existing deep residual models, achieving the greatest F1-score of 91.81% for eating identification and 91.43% for cutlery categorization.",Yes,"본 논문은 스마트워치 기반의 손 움직임 데이터를 활용하여 식사 행동 인식과 식기 분류를 위한 새로운 딥러닝 모델(ResNet-SE)을 제안하고, 기존 모델과 비교 평가하는 실험 결과를 포함하고 있다. 이는 독창적인 연구 방법과 실험적 기여를 포함한 연구 논문에 해당한다."
Application of Computer Vision Based on Deep Learning in Automatic Recognition of Pathological Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796071,"This paper explores the application potential of deep learning-based computer vision technology in automating pathological image recognition, addressing significant challenges in the field. Deep learning, a powerful machine learning method, extracts high-level abstractions from complex, high-dimensional image data through layers of nonlinear processing units. The study reviews fundamental principles of computer vision and deep learning, analyzing various deep learning models like CNN sand RNN s, and their adaptations for tasks including pathological image classification, target detection, cell segmentation, and tissue structure analysis. Emphasis is placed on an innovative architecture combining attention mechanisms and multi-scale feature fusion to enhance accuracy in detecting subtle pathological changes. Experiments conducted on a public dataset validate the method's efficacy in identifying different cancer types and distinguishing between benign and malignant lesions. Results demonstrate that deep learning models outperform traditional methods, improving diagnostic accuracy while reducing dependency on expert pathologists. Challenges such as high data annotation costs, model interpretability issues, and computational demands are discussed alongside proposed solutions and future research directions. In conclusion, this study underscores the pivotal role of deep learning-based computer vision in advancing automation and intelligence in pathology, offering new technical foundations for enhancing disease diagnosis efficiency and accuracy.",Yes,"논문은 딥러닝 기반 컴퓨터 비전 기술을 활용한 병리 이미지 자동 인식에 대한 새로운 아키텍처를 제안하고, 공개 데이터셋을 이용한 실험을 통해 그 효능을 검증하는 등 독창적인 연구 내용을 포함하고 있다. 또한 기존 방법 대비 성능 향상과 문제점 해결 방안을 논의하여 직접적인 연구 기여를 하고 있음을 보여준다."
A Review on Heartbeat Classification for Arrhythmia Detection Using ECG signal Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10063143,"The electrocardiogram (ECG) provides essential characteristics of the human heart's multiple cardiac conditions. The classification of arrhythmias provides a major part in the diagnosis of cardiac disease. Any deviation from the normal sequence of electrical impulses is considered an arrhythmia. Traditional methods of signal processing, machine learning and its sub-branches, such as deep learning, are popular techniques for ECG signal analysis and classification and, above all, for the development of early detection and treatment applications for cardiac conditions and arrhythmias. This article presents a detailed literature survey on ECG signal analysis. This paper aims to analyze the most recent studies on data utilized, features, and machine learning approaches that can address the time computational challenge and be implemented in wearable technology. The study methodology began with a search for relevant papers, followed by a study of the data provided. The second stage was to explore the evaluated ECG characteristics and the machine learning method used to identify arrhythmia. According to the analysis, a significant number of studies selected the MIT-BIH database, even though it needs a substantial ratio of pre-processing effort. We address a detailed existing research work review on the data of real-time signal collection, pre-recorded diagnostic ECG data, analysis and denoising of ECG signals, identification of ECG spectrographic states based upon function technologies, and classification of ECG signals, as well as comparative discussions between the studies analyzed.",No,초록에서 해당 논문은 ECG 신호 분석과 부정맥 분류에 관한 기존 연구들을 종합적으로 검토하는 문헌 조사(review)임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않은 리뷰 논문으로 판단됩니다.
An AI-Powered Network Threat Detection System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775989,"The work develops a network threat detection system, AI@NTDS, that uses the behavioral features of attackers and intelligent techniques. The proposed AI@NTDS system combines data analysis, feature extraction, and feature evaluation to construct a detection model, which supports a more straightforward strategy by which the operating system or its operators can defend against network attacks. The Linux system interaction information of SSH (Secure Shell) and Telnet are obtained from the Cowrie Honeypot and labeled according to Enterprise Tactics of MITRE ATT&CK to ensure dataset credibility. The proposed AI@NTDS system has three levels, depending on the attacker’s attacks and the user’s risk of damage. Fifty-two features are used to detect the network threat level. The features contain message-based features for all kinds of Linux operating instructions, host-based features for all types of information in the network connection process, and geography-based features are related to the attacker’s location. AI-based algorithms LightGBM, Random Forest and the K-NN algorithm are used to verify the identification of the custom features. Finally, the detection model that is trained using the best combination of features is used to predict the test dataset. The accuracy of the proposed AI@NTDS system reaches 99%, 95.66%, and 94.08% with the LightGBM, Random Forest, and K-NN algorithms, respectively. The mutual dependencies of features and network threats are evaluated. Results of a performance analysis reveal that the proposed AI@NTDS system has an accuracy of 99.20% and an F1-score of 99.80%. It is superior to existing detection mechanisms, which it outperforms by 4% and 1% in accuracy and F1-score, respectively.",Yes,"본 논문은 AI 기반 네트워크 위협 탐지 시스템을 개발하고, 데이터 수집, 특징 추출, 모델 학습 및 성능 평가를 포함한 독창적인 연구 과정을 상세히 기술하고 있습니다. 또한, 제안된 시스템의 성능을 기존 방법과 비교하여 우수성을 입증하는 실험 결과를 제시하고 있어 연구 논문에 해당합니다."
Benchmarking of Machine Learning for Anomaly Based Intrusion Detection Systems in the CICIDS2017 Dataset,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345704,"An intrusion detection system (IDS) is an important protection instrument for detecting complex network attacks. Various machine learning (ML) or deep learning (DL) algorithms have been proposed for implementing anomaly-based IDS (AIDS). Our review of the AIDS literature identifies some issues in related work, including the randomness of the selected algorithms, parameters, and testing criteria, the application of old datasets, or shallow analyses and validation of the results. This paper comprehensively reviews previous studies on AIDS by using a set of criteria with different datasets and types of attacks to set benchmarking outcomes that can reveal the suitable AIDS algorithms, parameters, and testing criteria. Specifically, this paper applies 10 popular supervised and unsupervised ML algorithms for identifying effective and efficient ML-AIDS of networks and computers. These supervised ML algorithms include the artificial neural network (ANN), decision tree (DT), k-nearest neighbor (k-NN), naive Bayes (NB), random forest (RF), support vector machine (SVM), and convolutional neural network (CNN) algorithms, whereas the unsupervised ML algorithms include the expectation-maximization (EM), k-means, and self-organizing maps (SOM) algorithms. Several models of these algorithms are introduced, and the turning and training parameters of each algorithm are examined to achieve an optimal classifier evaluation. Unlike previous studies, this study evaluates the performance of AIDS by measuring the true positive and negative rates, accuracy, precision, recall, and F-Score of 31 ML-AIDS models. The training and testing time for ML-AIDS models are also considered in measuring their performance efficiency given that time complexity is an important factor in AIDSs. The ML-AIDS models are tested by using a recent and highly unbalanced multiclass CICIDS2017 dataset that involves real-world network attacks. In general, the k-NN-AIDS, DT-AIDS, and NB-AIDS models obtain the best results and show a greater capability in detecting web attacks compared with other models that demonstrate irregular and inferior results.",Yes,"본 논문은 10가지 머신러닝 알고리즘을 적용하여 CICIDS2017 데이터셋을 기반으로 다양한 모델을 학습시키고 평가하는 실험적 연구를 수행하였으며, 성능 지표와 시간 복잡도까지 분석하여 최적의 분류기를 도출하는 독창적인 연구 내용을 포함하고 있다. 따라서 단순 리뷰가 아닌 직접적인 연구 기여가 있는 논문으로 판단된다."
A New Pedagogical Approach of An Experimental Study Aimed to Calculate the Leakage Reactance of a Synchronous Machine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9590015,"The paper is proposing a learning strategy focused to convert a classical laboratory activity into an immersive learning experience which could be easily used even in a distance learning approach. The subject is debated in two steps: a pedagogical one and a technical one. The scope of technical approach is to propose to the students an experimental situation and use the results to calculate the leakage reactance of a synchronous machine. Regarding the pedagogical approach, after a short review of the literature, the flipped learning was proposed to be implemented as a pedagogical method to update the classical lab topics. In accordance with learning efficiency pyramid, the technical content of the basic lab was improved with videos, models and simulations and organized for an online learning as a tool of distance learning. All these new materials were made in an original manner and used in a learning scenario. The research is actual and have a positive impact in solving the problem of passing to a digital education.",No,"논문은 기존 실험실 활동을 원격 학습에 적합한 몰입형 학습 경험으로 전환하는 교수법과 학습 전략에 초점을 맞추고 있으며, 직접적인 실험 결과나 새로운 기술적 발견보다는 교육 방법론의 개선에 중점을 두고 있다. 따라서 독창적인 연구 내용보다는 교육적 접근법 제안에 가까워 연구 논문으로 보기 어렵다."
"Role, Methodology, and Measurement of Cognitive Load in Computer Science and Information Systems Research",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786995,"Cognitive load (CL), defined as the mental effort required to process information, plays a pivotal role in user performance and experience in various domains, particularly within computer science (CS) and information systems (IS). As technology grows increasingly interactive, understanding and measuring CL is crucial for designing adaptive, user-centered systems. This study investigates trends in CL measurement techniques in CS and IS research from 2017 to 2024, focusing on emerging tools, methods, and their applications. A systematic literature review (SLR) was conducted to provide a comprehensive overview of CL’s role in CS and IS, the methods used to detect it, and how it is analyzed across different tasks and environments. The motivation behind this research stems from the growing need to optimize user experiences and system efficiency through better CL management. The findings highlight a shift toward multimodal CL measurement, integrating subjective, behavioral, performance-based, and physiological data, often analyzed with machine learning in domains like human-computer interaction, education, and immersive technologies. This research highlights the importance of accurate CL measurement and suggests future directions for enhancing adaptive system design through the integration of CL metrics. Building upon these findings, future research should focus on advancing CL measurement through survey item sequencing, multimodal data integration, and device-task comparisons, while also exploring the use of AI for robust CL detection. Future research should explore survey design, multimodal data integration, device-task comparisons, and AI-based CL detection. Building on these insights, this study proposes developing non-intrusive, adaptive e-learning interfaces to optimize user engagement and personalization within LMS environments.",Yes,"본 논문은 인지 부하(Cognitive Load)의 측정 방법과 역할에 대해 체계적 문헌 고찰을 수행하고, 최신 동향과 향후 연구 방향을 제시하는 독창적인 연구 내용을 포함하고 있다. 이는 단순 리뷰를 넘어 새로운 통합적 접근과 적용 방안을 제안하는 연구 논문으로 판단된다."
Case Studies on the Use of Sentiment Analysis to Assess the Effectiveness and Safety of Health Technologies: A Scoping Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417165,"A health technology assessment (HTA) is commonly defined as a multidisciplinary approach used to evaluate medical, social, economic, and ethical issues related to the use of a health technology in a systematic, transparent, unbiased, robust manner. To help inform HTA recommendations, the surveillance of social media platforms can provide important insights to the clinical community and to decision makers on the effectiveness and safety of the use of health technologies on a patient. A scoping review of the published literature was performed to gain some insight on the accuracy and automation of sentiment analysis (SA) used to assess public opinion on the use of health technologies. A literature search of major databases was conducted. The main search concepts were SA, social media, and patient perspective. Among the 1,776 unique citations identified, 12 studies that described the use of SA methods to evaluate public opinion on or experiences with the use of health technologies as posted on social media platforms were included. The SA methods used were either lexicon- or machine learning-based. Two studies focused on medical devices, three examined HPV vaccination, and the remaining studies targeted drug therapies. Due to the limitations and inherent differences among SA tools, the outcomes of these applications should be considered exploratory. The results of our study can initiate discussions on how the automation of algorithms to interpret public opinion of health technologies should be further developed to optimize the use of data available on social media.",No,"본 논문은 기존 문헌을 대상으로 한 스코핑 리뷰로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 기존 연구들을 종합하여 분석한 내용입니다. 따라서 새로운 연구 기여보다는 연구 동향을 정리하는 개관적 성격이 강합니다."
Exploring the Impact of Synthetic Data on Human Gesture Recognition Tasks Using GANs,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10621531,"In the evolving domain of Human Activity Recognition (HAR) using Internet of Things (IoT) devices, there is an emerging interest in employing Deep Generative Models (DGMs) to address data scarcity, enhance data quality, and improve classification metrics scores. Among these types of models, Generative Adversarial Networks (GANs) have arisen as a powerful tool for generating synthetic data that mimic real-world scenarios with high fidelity. However, Human Gesture Recognition (HGR), a subset of HAR, particularly in healthcare applications, using time series data such as allergic gestures, remains highly unexplored.In this paper, we examine and evaluate the performance of two GANs in the generation of synthetic gesture motion data that compose a part of an open-source benchmark dataset. The data is related to the disease identification domain and healthcare, specifically to allergic rhinitis. We also focus on these AI models’ performance in terms of fidelity, diversity, and privacy. Furthermore, we examine the scenario if the synthetic data can substitute real data, in training scenarios and how well models trained on synthetic data can be generalized for the allergic rhinitis gestures. In our work, these gestures are related to 6-axes accelerometer and gyroscope data, serving as multi-variate time series instances, and retrieved from smart wearable devices. To the best of our knowledge, this study is the first to explore the feasibility of synthesizing motion gestures for allergic rhinitis from wearable IoT device data using Generative Adversarial Networks (GANs) and testing their impact on the generalization of gesture recognition systems. It is worth noting that, even if our method has been applied to a specific category of gestures, it is designed to be generalized and can be deployed also to other motion data in the HGR domain.",Yes,"본 논문은 GANs를 활용하여 알레르기 비염 관련 제스처 모션 데이터를 합성하고, 이를 통해 제스처 인식 시스템의 성능과 일반화 가능성을 평가하는 독창적인 연구를 수행하고 있다. 또한, 합성 데이터의 충실도, 다양성, 프라이버시 측면을 분석하며, 실제 데이터 대체 가능성까지 탐구하는 등 직접적인 연구 기여가 포함되어 있다."
Fake Review Detection using Neural Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497283,"The rise of internet purchasing and the prevalence of e-commerce platforms have given rise to an unprecedented influx of online reviews. These reviews wield substantial influence over consumer decision-making, serving as a barometer for product and service quality. Yet, as the importance of internet reviews has soared, so too has the proliferation of deceptive and counterfeit reviews. These fake reviews, deliberately crafted to mislead potential customers and manipulate ratings, pose a profound challenge to the integrity of online commerce. Consequently, the imperative to establish a robust and dependable system for detecting false reviews cannot be overstated. In the pursuit of this critical objective, our research paper introduces a pioneering fake review detection system, distinguished by its ability to identify and mitigate various manifestations of deceptive reviews. These encompass short text attacks, overlapping text attacks, substantial duplicate review campaigns, and reviews that are incongruent with the product or service they purport to evaluate. Notably, our system achieves an impressive average accuracy of 95%, a milestone realized through the development of a neural network crafted from the ground up. The efficacy of our neural network underscores the potential of cutting-edge machine learning technology in combating the proliferation of fake reviews. Beyond its academic significance, our research holds profound practical implications. By fostering trust in online reviews, this system bolsters consumer confidence, supports businesses in maintaining their reputations, and contributes to the integrity of the e-commerce ecosystem. As online commerce continues its inexorable ascent, our research illuminates a path forward in the perpetual battle against deceptive reviews. Future research endeavors may explore opportunities for further refinement and enhancement of our neural network-based detection system, as well as the continued evolution of strategies for countering emerging threats in the ever-evolving landscape of fake review generation and manipulation.",Yes,"논문 초록에서 독창적인 신경망 모델을 개발하여 다양한 유형의 가짜 리뷰를 탐지하는 시스템을 제안하고 있으며, 95%의 정확도를 달성한 연구 결과를 명시하고 있다. 이는 직접 기여하는 연구 내용이 포함된 연구 논문임을 나타낸다."
Detecting Conventional and Adversarial Attacks Using Deep Learning Techniques: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323872,"Significant progress has been made towards developing Deep Learning (DL) in Artificial Intelligence (AI) models that can make independent decisions. However, this progress has also highlighted the emergence of malicious entities that aim to manipulate the outcomes generated by these models. Due to increasing complexity, this is a concerning issue in various fields, such as medical image classification, autonomous vehicle systems, malware detection, and criminal justice. Recent research advancements have highlighted the vulnerability of these classifiers to both conventional and adversarial assaults, which may skew their results in both the training and testing stages. The Systematic Literature Review (SLR) aims to analyse traditional and adversarial attacks comprehensively. It evaluates 45 published works from 2017 to 2023 to better understand adversarial attacks, including their impact, causes, and standard mitigation approaches.",No,"논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합적 분석에 초점이 맞춰져 있습니다."
A Strategy for AI-Supplemented Teaching and Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578748,"This paper presents a strategy designed to evaluate AI-supplemented teaching and learning in a course belonging to a master program in Computer Science at the University of South-Eastern Norway. The strategy was closely related to the delivery format adopted in this program, where students take only one course at a time. Each course lasts for 6 weeks, comprising an initial “reading week”, an “intensive lectures” week, a 3-week “project assignment”, and one “assessment week”. The university supported the cost of OpenAI Plus account subscriptions offered to each student while the course was running, and specific activities were proposed to the class exploring the different ways in which AI tutoring could be used during each one of the 4 phases included in the course work plan. The strategy can be adapted to other program delivery formats by redistributing the proposed activities in accordance with the planned sequence of learning activities. It is also independent of which generative AI tool is selected, although OpenAI Plus accounts allow access to specific features that offer relevant pedagogical benefits, e.g., a simple process to create private language models that are easily customizable to each course subject.",No,"본 논문은 AI 보조 교수법 전략을 제안하고 설명하는 내용으로, 구체적인 실험 결과나 새로운 연구 발견을 제시하지 않습니다. 따라서 독창적인 연구 내용보다는 교육 방법론에 대한 설명에 초점이 맞춰져 있습니다."
The Analysis of Communication Strategy of Disabled Sports Information Based on Deep Learning and the Internet of Things,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479469,"The ever-growing landscape of Internet of Things (IoT) technology and the evolution of deep learning algorithms have ushered in transformative changes in the communication strategy for disseminating information on disabled sports. This specialized information resource aims to provide relevant support and services related to sports activities for disabled individuals. This study investigates the communication strategy of disabled sports information driven by deep learning within the framework of the IoT and assesses the practical application performance of the proposed model. To achieve this objective, an appropriate deep learning model for the dissemination of sports information for the disabled is selected through a thorough literature review. Subsequently, an experimental framework is proposed for comprehensive performance verification, evaluating the model’s performance in reasoning time and user satisfaction through comparative experiments. By constructing deep learning models, extensive data on disabled sports activities are analyzed, enabling the identification and prediction of key factors in information dissemination. The results indicate that the proposed sports information dissemination model outperforms similar models across various performance metrics, particularly in real-time performance and user experience. Comparative analysis with attention-based deep neural networks and traditional machine learning algorithms reveals that the proposed model achieves an accuracy rate as high as 0.85, significantly surpassing the 0.78 and 0.82 accuracies of these models, respectively. Moreover, the proposed model demonstrates the shortest inference time (15ms), surpassing both aforementioned models. This study validates the relative advantages of the proposed model through comparison with similar studies, offering a novel solution for the dissemination of sports information for the disabled.",Yes,"논문 초록에서 제안된 딥러닝 모델의 설계, 실험적 성능 검증, 그리고 기존 모델과의 비교 분석을 통해 독창적인 연구 내용과 직접적인 기여가 포함되어 있음을 확인할 수 있습니다. 따라서 본 논문은 연구 논문에 해당합니다."
Similarity Measures in Medical Image Registration A Review Article,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9685453,"Image registration is one of the most important problems in medical image analysis. It refers to the process of geometric alignment between two images based on correspondence. A crucial step in medical image registration process is to determine a similarity measure. There are various similarity measure techniques in this field that are applied in different registration applications. Selecting an effective similarity measure is a challenging problem, and this choice affects the accuracy of the registration results. According to past research, the similarity measures have extended from traditional to deep learning based methods. Our goal in this paper is to provide a literature review on various similarity measure techniques in medical image registration, classify them, and introduce main challenges. Thus Similarity measure techniques based on various registration approaches have been classified into two main classes and several subclasses namely distance based, correlation based, and information based in traditional methods; and statistical based, learning based, and similarity measure based loss function in learning based methods. Based on this classification, methods are introduced and each category is evaluated based on accuracy, speed, robustness, and complexity. Finally, recognizing and evaluating the different similarity criteria will help to select the appropriate similarity measure according to the intended application.",No,"이 논문은 의료 영상 정합에서 사용되는 유사도 측정 기법들을 문헌 리뷰하는 논문으로, 기존 연구들을 분류하고 평가하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 방법론 제시는 포함되어 있지 않아 연구 논문으로 보기 어렵습니다."
Machine Learning Applied to survival prediction of elderly cancer patients: Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140861,"Machine Learning (ML) is being successfully used in many science areas, medicine being no exception. On the other hand, cancer is a heterogeneous disease consisting of various subtypes and possible treatments that generate a lot of data (Big Data). This study sets out to identify, evaluate and interpret published research that examines how predicting the outcome of treating cancer can benefit from making use of ML. To achieve this, a systematic review of the literature was conducted. This review resulted in finding 1,855 studies, 32 of which were identified as primary studies. They were then classified according to research area and the aspect of ML they focus on. The results show gaps in current research as no studies were identified on ML using Comprehensive Geriatric Assessment (CGA), a fundamental tool that is used to improve caring for elderly patients with cancer.",No,"본 논문은 머신러닝을 이용한 생존 예측 연구들을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 평가하고 해석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 문헌 분석에 해당합니다."
A Data-Driven Framework for Identifying Misuse of Multidrug-Resistant Organisms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10490968,"Multidrug-resistant organisms (MDROs) pose a significant threat to public health due to their ability to cause various diseases. The emergence and evolution of superbugs have been accelerated by existing drug conservation policies, hindering the development of novel antibiotics. As a result, researchers are focusing on understanding disease causation pathways and developing advanced classes of antibiotics. Despite challenges related to transparency, regulations, ethics, and technical limitations, technology such as MDRO Dual Use Quickscan shows promise in combating MDRO-based threats. This work presents a web-based tools design that incorporates a data-driven framework and utilizes artificial intelligence (AI) to detect potential cases of dual-use research involving MDROs. A comprehensive literature review identified 15 questions relevant for evaluating MDRO dual-use research using the Quickscan tool. The Quickscan can be developed as a valuable tool for assessing the potential dual-use characteristics of MDRO research. It is recommended that expert opinions guide risk management in biologics and address potential perilous scenarios arising from dual-use activities. Artificial intelligence models, natural language processing techniques, and binary classification algorithms are suggested as viable options for detecting and assessing dual-use traits associated with MDROs. However, human supervision and assessment remain critical for interpreting outcomes and making informed decisions.",No,"논문 초록은 MDRO 관련 이중용도 연구를 평가하기 위한 도구 설계와 데이터 기반 프레임워크 제안에 초점을 맞추고 있으나, 직접적인 실험 결과나 독창적인 연구 데이터 분석 내용은 포함되어 있지 않습니다. 따라서 독창적인 연구 기여보다는 도구 개발 및 개념적 제안에 가까운 것으로 판단됩니다."
A Survey on Prediction of Suicidal Ideation Using Machine and Ensemble Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684830,"Suicide is a major health issue nowadays and has become one of the highest reason for deaths. There are many negative emotions like anxiety, depression, stress that can lead to suicide. By identifying the individuals having suicidal ideation beforehand, the risk of them completing suicide can be reduced. Social media is increasingly becoming a powerful platform where people around the world are sharing emotions and thoughts. Moreover, this platform in some way is working as a catalyst for invoking and inciting the suicidal ideation. The objective of this proposal is to use social media as a tool that can aid in preventing the same. Data is collected from Twitter, a social networking site using some features that are related to suicidal ideation. The tweets are preprocessed as per the semantics of the identified features and then it is converted into probabilistic values so that it will be suitably used by machine learning and ensemble learning algorithms. Different machine learning algorithms like Bernoulli Naïve Bayes, Multinomial Naïve Bayes, Decision Tree, Logistic Regression, Support Vector Machine were applied on the data to predict and identify trends of suicidal ideation. Further the proposed work is evaluated with some ensemble approaches like Random Forest, AdaBoost, Voting Ensemble to see the improvement.",Yes,본 논문은 소셜 미디어 데이터를 수집하고 전처리하여 여러 머신러닝 및 앙상블 학습 알고리즘을 적용해 자살 생각 예측 모델을 개발하는 연구를 수행하고 있다. 이는 기존 연구를 정리하는 서베이 논문이 아니라 직접적인 데이터 분석과 모델 평가를 포함한 독창적인 연구 내용이므로 연구 논문에 해당한다.
An Overview of Deep Neural Networks for Few-Shot Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807331,"Recent advancements in deep learning have led to significant breakthroughs across various fields. However, these methods often require extensive labeled data for optimal performance, posing challenges and high costs in practical applications. Addressing this issue, Few-Shot Learning (FSL) is introduced. FSL aims to learn effectively from limited labeled samples and generalize well during testing. This paper provides a comprehensive survey of FSL, reviewing prominent deep learning based approaches of FSL. We define FSL through literature review in machine learning and specify the “N-way K-shot” paradigm to distinguish it from related learning challenges. Next, we classify FSL methods by analyzing the Vapnik-Chervonenkis dimension of neural networks. It underscores the necessity for models with abundant labeled examples and finite hypothesis space to generalize well to new and unseen instances. We categorize FSL methods into three types based on strategies to increase labeled samples or reduce hypothesis space: data augmentation, model-based methods, and algorithm-optimized approaches. Using this taxonomy, we review various methods and evaluate their strengths and weaknesses. We also present a comparison of these techniques as summarized in this paper, using benchmark datasets. Moreover, we delve into specific sub-tasks within FSL, such as applications in computer vision and robotics. Lastly, we examine the limitations, unique challenges, and future directions of FSL, aiming to offer a thorough understanding of this rapidly evolving field.",No,"본 논문은 Few-Shot Learning 분야의 기존 연구들을 종합적으로 정리하고 분류하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Uses And Trends In Hematopoietic Cell Transplantation In The United States of America:A Machine Learning Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9708028,"Artificial Intelligence(AI) based algorithm and various machine learning algorithm are used in healthcare successfully to enhance human life. The Hematopoietic Cell Transplantation (HCT)data set are analyzed for finding out the clinical outcomes and its types. Hematopoietic cell transplantation is a well therapeutic treatment for a variety of different diseases. The number of HCT operations conducted in the United States from 2014 to 2018, as well as the trends and results of HCT as reported to the CIMBTR, are evaluated and documented in this research. The analysis observes an increase in total transplants carried out in the United States of America. An increase of 13% was found from the year 2014 to 2018. The analysis is of transplants done are categorized based on donor type, age stem cell source, indications and states in the US. The most frequent indication was found to be Multiple myeloma (34% of all HCT). Also, the trends and increase in the transplant rates over the years 2014 to 2018 are also reported. This paper also provides a comparison analysis, development over the years and perspective on the transplant activity in the United States based on donors, indications and cell source. Various studies have been performed to predict HCT outcomes like survival, relapse using ML techniques like Regression, Bayesian learning ensemble learning and Support Vector Machine. The aim of the paper is to review the existing machine learning application and to provide various research direction for developing various Data Analytics tools.",No,"본 논문은 미국 내 조혈모세포 이식 현황과 추세를 머신러닝 기법을 활용해 분석하고 기존 연구들을 리뷰하는 내용으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당합니다."
Driver drowsiness detection using behavioral measures and machine learning techniques: A review of state-of-art techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261140,"This paper presents a literature review of driver drowsiness detection based on behavioral measures using machine learning techniques. Faces contain information that can be used to interpret levels of drowsiness. There are many facial features that can be extracted from the face to infer the level of drowsiness. These include eye blinks, head movements and yawning. However, the development of a drowsiness detection system that yields reliable and accurate results is a challenging task as it requires accurate and robust algorithms. A wide range of techniques has been examined to detect driver drowsiness in the past. The recent rise of deep learning requires that these algorithms be revisited to evaluate their accuracy in detection of drowsiness. As a result, this paper reviews machine learning techniques which include support vector machines, convolutional neural networks and hidden Markov models in the context of drowsiness detection. Furthermore, a meta-analysis is conducted on 25 papers that use machine learning techniques for drowsiness detection. The analysis reveals that support vector machine technique is the most commonly used technique to detect drowsiness, but convolutional neural networks performed better than the other two techniques. Finally, this paper lists publicly available datasets that can be used as benchmarks for drowsiness detection.",No,"이 논문은 기존 연구들을 종합하여 리뷰하고 메타분석을 수행한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Beyond fear go viral: A machine learning study on infodemic detection during covid-19 pandemic,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729077,"With the restrictions in our daily life activities under the current situation of the covid-19 pandemic worldwide, billions of people rely on social media platforms to share and obtaining covid-19 related news information. This made social media platforms easily be used as a source of myths and disinformation, which can cause severe public risks. It is thus of vital importance to constraint the spread of misinformation to the public. Although many works have shown promising results on the misinformation detection problem, only a few studies focus on the infodemic detection during the covid-19 pandemic, especially in the low resource language like Thai. Therefore, in this paper, we conduct extensive experiments on the real-world social network datasets to detect misinformation about covid-19 targeting both English and Thai languages. In particular, we perform an exploratory data analysis to get the statistic and characteristics of real and fake content. Also, we evaluate a series of three feature extraction, seven traditional machine learning, and eleven deep learning methods in detecting the fabricated content on social media platforms. The experimental results demonstrate that the transformer-based model significantly outperforms other deep learning and traditional machine learning methods in all metrics, including accuracy and F-measure.",Yes,"본 논문은 코로나19 팬데믹 기간 동안 영어와 태국어로 된 소셜 미디어 데이터셋을 대상으로 허위 정보 탐지를 위한 다양한 머신러닝 및 딥러닝 기법을 실험하고 평가하는 독창적인 연구를 수행하였다. 또한, 변환기 기반 모델의 성능을 비교 분석하는 등 직접적인 연구 기여가 포함되어 있다."
AI in Sustainability:Assessing its Functions and Environmental Impact,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925068,"Artificial Intelligence (AI) has fundamentally transformed various industries, including healthcare, transportation, agriculture, energy, and media. However, while AI’s impact is widely recognized, its sustainability remains a critical concern due to significant computational and environmental costs, as well as ethical issues such as biased systems and misinformation. This paper explores the dual dimensions of AI’s impact on sustainability: its potential to promote sustainability across sectors and the sustainability of AI systems themselves. Using a comprehensive review of existing literature, we address both the benefits and challenges associated with AI in achieving sustainable goals and the need for a more integrated approach to understanding its environmental, social, and economic impacts. Our analysis of the “AI Global Index” dataset, which evaluates AI capabilities across 62 countries, highlights key trends and disparities in AI development. The findings underscore the need for a holistic approach to AI sustainability that encompasses its full range of effects and interactions with various dimensions of sustainability.",No,본 논문은 기존 문헌과 데이터를 종합적으로 검토하여 AI의 지속 가능성에 대한 전반적인 이해를 제공하는 리뷰 논문으로 보입니다. 독창적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구를 분석하고 통합하는 데 중점을 두고 있어 직접 기여하는 연구 논문으로 판단하기 어렵습니다.
Assessment and Optimization of 1D CNN Model for Human Activity Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964520,"Artificial Intelligence (AI) solves complex tasks like human activity and speech recognition. Accuracy-driven AI models introduced new challenges related to their applicability in resource-scarce systems. In Human Activity Recognition (HAR), state-of-the-art presents proposals using complex multi-layer LSTM networks. The literature states that LSTM networks are suitable for treating temporal-series data, a key feature for HAR. Most works in the literature seek the best possible accuracy, with few evaluating the overall computational cost to run the inference phase. In HAR, low-power IoT devices such as wearable sensors are widely used as data-gathering devices, but little effort is made to deploy AI technology in these devices. Most studies suggest an approach using edge devices or cloud computing architectures, where the end-device task is to gather and send data to the edge/cloud device. Most voice assistants, such as Amazon's Alexa and Google, use this architecture. In real-life applications, mainly in the healthcare industry, relying only on edge/cloud devices is not acceptable since these devices are not always available or reachable. The objective of this work is to evaluate the accuracy of convolutional networks with a simpler architecture, using 1D convolution, for HAR. The motivation for using networks with simpler network architectures is the possibility of embedding them in power- and memory-constrained devices.",Yes,"본 논문은 1D CNN 모델을 활용하여 인간 활동 인식(HAR) 문제에 대해 정확도 평가 및 최적화를 수행하는 연구로, 기존 연구와 달리 단순한 네트워크 아키텍처를 제안하고 이를 임베디드 장치에 적용 가능하도록 하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
A review of artificial intelligence based building energy prediction with a focus on ensemble prediction models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408504,"Building energy usage prediction plays an important role in building energy management and conservation. Building energy prediction contributes significantly in global energy saving as it can help us to evaluate the building energy efficiency; to conduct building commissioning; and detect and diagnose building system faults. AI based methods are popular owing to its ease of use and high level of accuracy. This paper proposes a detailed review of AI based building energy prediction methods particularly, multiple linear regression, Artificial Neural Networks, and Support Vector Regression. In addition to the previously listed methods, this paper will focus on ensemble prediction models used for building energy prediction. Ensemble models improve the prediction accuracy by integrating several prediction models. The principles, applications, advantages, and limitations of these AI based methods are elaborated in this paper. Additionally, future directions of the research on AI based building energy prediction methods are discussed.",No,"본 논문은 인공지능 기반 건물 에너지 예측 방법에 대한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들의 원리, 장단점, 향후 연구 방향을 논의하는 데 중점을 두고 있습니다."
Applying Recurrent Neural Networks to Time-Series Analysis in Big Data for Decision Support,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10859601,"The burgeoning field of big data has necessitated advanced analytical techniques capable of extracting valuable insights from complex time-series datasets. This research explores the application of Recurrent Neural Networks (RNN s), specifically Long Short-Term Memory (LSTM) models, in the context of time-series analysis for enhancing decision support systems. The study begins with a comprehensive literature review, examining the evolution of time-series analysis and the integration of deep learning methodologies. A theoretical framework is established, detailing the mathematical principles of time-series and the architecture of LSTM networks, followed by a methodological approach to model training and validation. Preliminary results are presented through insightful visualizations, showcasing the predictive capabilities of the RNN models against traditional forecasting methods. The paper delves into an advanced data analysis phase, assessing model performance across various metrics and datasets. The discussion section synthesizes the findings, evaluating the effectiveness of RNN s in decision support and contemplating future research avenues. The paper concludes by highlighting the transformative potential of deep learning in business analytics and strategic decision-making.",Yes,"논문은 RNN과 LSTM 모델을 활용한 시계열 분석에 대한 이론적 프레임워크 수립, 모델 훈련 및 검증, 그리고 성능 평가를 포함한 구체적인 연구 과정을 다루고 있어 독창적인 연구 내용을 포함하고 있다. 또한, 기존 방법과의 비교를 통해 예측 능력을 검증하는 등 직접적인 연구 기여가 명확하다."
A Novel Approach to Detect Face Fraud Detection Using Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493594,"The main aim of this research is to identify and prevent fraudulent activities which can be achieved through AI related to facial recognition systems. Nowadays the usage of facial recognition systems is very high, and in the same way the scams by fraudsters are also increased in this research by using AI bots instead of humans. The main motive of this research is to identify the misuse of facial recognition technology. The proposed method using CNN (Convolutional Neural Network) protects the individual privacy of people and their data. It detects whether the character in the image is an AI made or real human. This helps to ensure that only authorized people can use their information. Some of the sectors which use facial recognition systems are security, law enforcement, financial services, education, government services, retail. if unauthorized people access the above-mentioned sectors, the result will be imperiling. This method takes an image as an input and then python is used to process the image, importing a CV library to do this job. Next, we use deep learning models in python to identify whether the character in the image is AI generated or real human. The Computational Intelligence and Photography Lab at Yonsei University assembled a publicly available dataset for this work. Images of both real and fake human faces can be found in the Yonsei University Computational Intelligence and Photography Lab's database. The performance of the proposed system is measured using accuracy, precision and sensitivity. Experimental results shows that CNN based face recognition system outperforms.",Yes,"본 논문은 CNN 기반의 새로운 얼굴 사기 탐지 방법을 제안하고, 이를 위해 공개 데이터셋을 사용하여 실험을 수행하며 성능 평가 결과를 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단된다."
"Human Activity Recognition in Enhancing Healthcare for Aging Populations: Challenges, Innovations, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10869423,"As the global population ages, healthcare and social systems face escalating demands. Effective solutions for promoting healthy aging increasingly rely on deep learning and Internet of Things technologies to provide personalized, preventive, and proactive care. Central to these efforts is human activity recognition, which enables the continuous monitoring and assessment of daily activities—a vital indicator of well-being and independence in older adults. This paper highlights key concepts and methodologies in applying deep learning and internet of things in human activity recognition, emphasizing its role in understanding and enhancing the quality of life for the elderly through accurate, activity-based health insights. However, most current studies continue to emphasize achieving high accuracy in machine learning models, despite already reaching substantial levels of precision. This paper argues for the need to shift focus towards developing adaptable models capable of recognizing new and varied activities. Additionally, it highlights the importance of recommendation systems in healthcare, which should extend beyond mere activity recognition to provide actionable advice and preventive measures for the elderly. This study also addresses the gap in the literature by providing a comprehensive review of existing approaches and identifying key research directions that should be prioritized in the field. By signaling the lack of comprehensive studies offering in-depth insights like this paper, it underscores the necessity of moving beyond current achievements towards more dynamic and context-aware solutions in human activity recognition technologies.",No,초록에서 이 논문은 기존 연구들을 종합적으로 검토하고 향후 연구 방향을 제시하는 리뷰 논문임을 명확히 하고 있습니다. 독창적인 실험 결과나 새로운 연구 방법론에 대한 직접적인 기여가 포함되어 있지 않으므로 연구 논문으로 보기 어렵습니다.
A Study of Cantonese Covid-19 Fake News Detection on Social Media,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671722,"With the prevalence of social media, fake news has become one of the greatest challenges in journalism, which has weakened public trust in news outlets and authorities. During the COVID-19 epidemic, the widely circulated pandemic-related fake news on social media misleads or threatens the public. Recent works have investigated fake news detection on social platforms in English and Mandarin, though Cantonese fake news has been understudied. To pave the way for Cantonese COVID-19 fake news detection, we first presented an annotated COVID-19 related Cantonese fake news dataset collected from a popular local discussion forum in Hong Kong. Then, we explored the dataset by applying topic modeling to identify the topics that contain the most significant amount of fake news. Moreover, we evaluated both traditional machine learning algorithms and deep learning algorithms for Cantonese fake news detection. Our empirical results show that deep learning based methods perform slightly better than traditional machine learning methods on TF-IDF features.",Yes,"본 논문은 광둥어 COVID-19 가짜 뉴스 탐지를 위한 데이터셋 구축, 주제 모델링 분석, 그리고 전통적 및 딥러닝 기반 알고리즘 평가를 포함한 독창적인 연구를 수행하고 있다. 이는 직접적인 연구 기여와 실험적 결과를 포함하는 연구 논문에 해당한다."
A Comprehensive Investigation on Leveraging Generative AI and Large Language Models in the Healthcare Domain,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778845,"The launch and adoption of Generative Pre-Trained Transformers (GPT) based AI has been very rapid compared to any other technological adoption seen in the past. In these times of the rapid evolution and advancement of Large Language Models (LLMs) and Generative Artificial Intelligence (GenAI), measuring and reviewing the impact it has made on the industry, especially the healthcare domain, becomes very important. Since the healthcare sector is one of the early adopters of novel technologies in the industry, the penetration of LLMs and GenAI has been predominant in the sector. In this paper, we are understanding and analysing the impacts of this novel technology in the healthcare sector, categorised under different domains such as Education, Medical Information Management, Telemedicine and Telehealth, Health Record Management, Health Data Research, Mental Health Care and Personal Health Planning. Additionally, the use of LLM and GenAI in the healthcare domain raises some critical ethical concerns as well. This paper tries to investigate the potential applications and ethical challenges of GenAI and Large Language Models (LLMs) within the healthcare domain.",No,초록에서 본 논문은 기존 기술인 LLM과 Generative AI가 의료 분야에 미친 영향과 윤리적 문제를 종합적으로 조사하고 분석하는 리뷰 성격의 논문으로 보입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 판단하기 어렵습니다.
Intellectual Property Protection in AI-driven Innovations: A Comparative Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10742731,"The quick amalgamation of artificial intelligence (AI) into various industries has driven innovation to unprecedented peaks. This comparative paper addresses the critical intersection of AI and intellectual property (IP) protection, focusing on the imperative to safeguard AI-driven innovations. Employing a comparative analysis framework, this study evaluates the effectiveness of existing IP mechanisms across selected jurisdictions. The methodology involves a comprehensive examination of patent and trademark systems, utilizing defined metrics for comparative evaluation. The literature review traces the historical evolution of AI in innovations and synthesizes existing knowledge on the interaction between AI and IP. The comparative analysis includes an in-depth examination of patent protection in the context of AI-generated content and trademark protection for AI innovations. This research aims to unveil the strengths and weaknesses of IP protection mechanisms in the selected jurisdictions by employing a robust comparative framework. The findings and discussion section presents a nuanced investigation of the implications of IP protection on AI-driven innovation, offering insights into potential challenges and opportunities. The paper concludes with actionable recommendations for policy reforms and considerations for international collaboration, striking a balance between robust IP protection and promoting a conducive environment for AI innovation.",Yes,"논문은 AI와 지적재산권 보호의 교차점에 대한 비교 분석을 수행하며, 기존 IP 메커니즘의 효과성을 평가하는 독창적인 연구 방법론과 결과를 제시하고 있다. 이는 단순한 문헌 리뷰가 아닌 직접적인 연구 기여를 포함한 연구 논문임을 나타낸다."
Automating the Evaluation of Education Apps With App Store Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340366,"With the vast number of apps and the complexity of their features, it is becoming challenging for teachers to select a suitable learning app for their courses. Several evaluation frameworks have been proposed in the literature to assist teachers with this selection. The iPAC framework is a well-established mobile learning framework highlighting the learners' experience of personalization, authenticity, and collaboration (iPAC). In this article, we introduce an approach to automate the identification and comparison of iPAC relevant apps. We experiment with natural language processing and machine learning techniques, using data from the app description and app reviews publicly available in app stores. We further empirically validate the keyword base of the iPAC framework based on the app users' language in app reviews. Our approach automatically identifies iPAC relevant apps with promising results (F1 score ~ 72%) and evaluates them similarly as domain experts (Spearman's rank correlation 0.54). We discuss how our findings can be useful for teachers, students, and app vendors.",Yes,"본 논문은 자연어 처리와 머신러닝 기법을 활용하여 교육 앱을 자동으로 평가하는 새로운 방법을 제안하고, 이를 실험적으로 검증한 연구 내용을 포함하고 있다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 판단된다."
Exploring gaps in California Proposition 54 (2016),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629129,"California Proposition 54, 2016 (Prop 54) attempted to address gaps in government transparency at the state level by requiring legislative proceedings to be published on the internet, but failed to consider the traditional barriers that limit citizen participation in policy. This study aims to understand where Prop 54 falls short in regards to improving government transparency, evaluating the traditional barriers that prevent citizens from participating in government, identifying the improvements that should be made to increase the impact of Prop 54, and proposing ways artificial intelligence can help with these improvements. Prop 54 makes an overwhelming amount of information to be made available online, an amount that is not searchable by people. The data includes thousands of recordings of legislative sessions which are over an hour on average, but lack transcripts, summaries, or even a descriptive title. In addition, a barrier which prevents everyday people from participating in policy that remains is that most citizens lack the ability to comprehend sophisticated legal terminology presented in the released data. The research method used in this paper is a literature review, and this paper proposes addressing these challenges by implementing artificial intelligence-based solutions with natural language processing and computer vision. These tools can be used to create high-quality, searchable transcripts and generate simplified summaries of legislative proceedings, addressing both of the previously mentioned problems.",No,"이 논문은 문헌 리뷰를 기반으로 기존 문제점을 분석하고 인공지능을 활용한 해결책을 제안하는 내용으로, 직접적인 실험이나 새로운 데이터 수집, 독창적인 연구 결과를 포함하지 않습니다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Trustworthiness Assurance Assessment for High-Risk AI-Based Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430152,"This work proposes methodologies for ensuring the trustworthiness of high-risk artificial intelligence (AI) systems (AIS) to achieve compliance with the European Union’s (EU) AI Act. High-risk classified AIS must fulfill seven requirements to be considered trustworthy and human-centric, and subsequently be considered for deployment. These requirements are equally important, mutually supportive, and should be implemented and evaluated throughout the AI lifecycle. The assurance of trustworthiness is influenced by ethical considerations, amongst others. Hence, the operational design domain (ODD) and behavior competency (BC) concepts from the automated driving domain are utilized in risk assessment strategies to quantify different types of residual risks. The methodology presented is guided by the consistent application of the ODD and its related BC concept throughout the entire AI lifecycle, focusing on the trustworthiness assurance framework and its associated process as the main pillars for AIS certification. The achievement of the overall objective of trustworthy and human-centric AIS is divided into seven interconnected sub-goals: the formulation of use restrictions, the trustworthiness assurance/argument itself, the identification of dysfunctional cases, the utilization of scenario databases and datasets, the application of metrics for evaluation, the implementation of the proposed concept across the AI lifecycle, and sufficient consideration of human factors. The role of standards in the assurance process is discussed, considering any existing gaps and areas for improvement. The work concludes with a summary of the developed approach, highlighting key takeaways and action points. Finally, a roadmap to ensure trustworthy and human-centric behavior of future AIS is outlined.",Yes,"본 논문은 고위험 AI 시스템의 신뢰성 보증을 위한 구체적인 방법론과 평가 프레임워크를 제안하며, AI 생애주기 전반에 걸친 적용 방안을 다루고 있다. 이는 기존 연구를 종합하는 수준을 넘어 직접적인 연구 기여와 독창적인 접근을 포함하고 있어 연구 논문에 해당한다."
Large Language Models in Education: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589960,"Large Language Models (LLMs) refer to a type of generative artificial intelligence model that produces responses to natural language input. The purpose of this study is to analyze the current application status of LLMs in the field of education through a systematic review of the literature. Data were sourced from three databases: Web of Science, ERIC, and Google Scholar. The study includes 94 documents, analyzed from both qualitative and quantitative perspectives. The results show that large language models have great potential in the field of education, specifically in generating medical content, serving as an English learning assistant, assisting academic research, and evaluating the quality of tests, etc. However, there are still potential dangers such as hindering the development of critical thinking, creating academic integrity crises, and ethical and moral challenges. These findings showed the current application status of LLMs in education, laying the groundwork to inspire future research.",No,"이 논문은 체계적 문헌고찰(Systematic Review)로, 기존 연구들을 종합하여 분석한 내용이다. 따라서 직접적인 독창적 연구 결과나 실험 데이터를 제시하는 연구 논문에 해당하지 않는다."
A Comprehensive Review on Deep learning Techniques for Crop Yield Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170160,"deep learning is a contrivance for yield prediction in agriculture domain. Various deep learning techniques have been used on crop yield prediction system research. In this paper, we perform comprehensive review to extract the most widely used algorithms, most relevant features, evaluation matrices used for result validation, challenges , limitation and future research scope of the crop yield prediction system. We investigated 25 papers of yield prediction using deep learning and according to our analysis Crop yield prediction involves massive dataset consisting of Metrological parameters data, Satellite image data, Soil parameters data, other observational parameters so for that more rapidly and precise predictions can be made by using the deep learning algorithm such as Deep Neural Networks, Long-Short Term Memory, Convolution Neural Networks, which can be evaluated using Root mean square error (RMSE). Also, given research scope, future development, challenges in crop yield predictions system using deep learning.",No,"본 논문은 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
A Review on Dynamic Security Assessment of Power System Based on Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10453861,"The massive integration of renewable energy increases the uncertainty of the system operating condition, leading to the frequent operating status of the power system at the stability boundary. Multiple dynamic security risks of power systems are prominent. The dynamic security assessment (DSA) based on machine learning is an emerging method to evaluate various dynamic security risks (e.g., power angle, frequency and voltage). It can directly obtain the mapping relationship between input features and output, which has the advantage of high assessment efficiency. With the development of artificial intelligence (AI) technology, various cutting-edge machine learning methods have been explored and applied to the DSA. This paper divides the DSA into two categories: the stability-domain DSA based on dynamic features and the security-domain DSA based on steady-state features, and reviews the current research status from the following four aspects: feature selection, model training, model updating, and online assessment. Moreover, five major challenges are summarized.",No,"본 논문은 동적 보안 평가에 관한 머신러닝 기반 연구 동향을 정리한 리뷰 논문으로, 기존 연구들을 종합하고 주요 이슈를 요약하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Automating Code Generation for MDE using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172526,"The overall aim of our research is to improve the techniques for synthesizing code generators in the Model-Driven Engineering (MDE) context. Code generation is one of the main elements of Model-Driven Engineering, involving transformation from specification models to produce executable code. A code generator is designed to reduce the manual program construction work used to implement a software system, but building a code generator itself still currently needs much manual effort. Meanwhile, existing code generators are typically not flexible to adjust for changing development requirements and are hard to reuse for different target languages. Therefore, we aim to provide techniques to improve the process of building code generators, and let them be more reusable. Currently, we researched the related new and traditional approaches for generating code and projects using AI for program translation, code completion or program generation. Based on this research we decided to focus on a symbolic machine learning method related to the programming-by-example concept to build code generators. We use this “Code Generation By Example” (CGBE) concept with tree-to-tree structure mappings as the information format. CGBE has good performance in terms of training dataset size and time when applied to learning a UML-to-Java code generator, but further work is needed to extend it to generate different programming languages and to evaluate these cases, and to handle the optimisation of generated code.",Yes,"논문 초록에서 기존 코드 생성기 문제점을 해결하기 위해 기계 학습 기반의 새로운 코드 생성 방법(CGBE)을 제안하고, UML에서 Java로의 코드 생성기 학습 성능을 평가하는 등 직접적인 연구 기여와 실험 결과를 포함하고 있음을 확인할 수 있습니다. 이는 독창적인 연구 내용이 포함된 연구 논문임을 의미합니다."
How Business Intelligence Enables E-commerce: Breaking the Traditional E-commerce Mode and Driving the Transformation of Digital Economy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406894,"To pursue the sustainable development of the economy, resources and society, we should adhere to the guidance of technology leading and promote the digital transformation with the help of the opportunity of the industrial Internet. As an important field of digital transformation, e-commerce has great potential in various industries, such as service advantages and marketing methods. At present, large e-commerce platforms (Alibaba, JD, etc.) have small achievements, but there are few studies on the integration of e-commerce AI changing the e-commerce model, especially the lack of systematic review of digital marketing and digital transformation. This paper mainly discusses the application of AI in e-commerce platforms to form a diversified new e-commerce model to guide the transformation of traditional e-commerce. Based on this, 156 articles were selected from the Web of Science (WoS) database to map co-word clustering and analyze their annual trends, topics, publication locations, etc. Using scientific metrology to demonstrate the possibility and practicability of the research content in related fields will help Business managers to master how intelligent technology can enable digital e-commerce, achieve digital transformation and sustainable development, and take reasonable measures accordingly.",No,"본 논문은 AI와 비즈니스 인텔리전스가 전자상거래에 미치는 영향을 체계적으로 검토하고, 156편의 문헌을 분석하는 리뷰 논문으로 보입니다. 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구를 종합하여 디지털 전환의 가능성을 논의하는 데 중점을 두고 있습니다."
Techvar: Classification of Similarity in Software Detection Model using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761003,"In Industry 4.0, Deep Learning techniques have become an important research tool in many area namely healthcare, automobiles, video analysis, audio analytics, software systems etc. In recent years, many research are performed in software analysis using modern technologies. CrosLSim, SimMax, and atrpos models are used to review the software similarity detection techniques for different software systems. The existing models for similarity detection are not efficient to be used in major software projects. In this paper, the Techvar-DNN system which performs enhanced Probability, maintainability, testability, and reusability has been proposed. When compared with other methods namely Random Forest and Support vector machines, the proposed system provides increased recall, a smaller function size, and more efficient computing. Moreover, the proposed model results show better F-measure, precision and recall to improve the software similarity detection in a more efficient manner.",Yes,"논문 초록에서 기존 소프트웨어 유사도 검출 모델의 한계를 지적하고, 새로운 딥러닝 기반 Techvar-DNN 시스템을 제안하여 성능을 비교 분석한 내용을 포함하고 있다. 이는 독창적인 연구 기여와 실험 결과를 제시하는 연구 논문임을 나타낸다."
Strategic Innovations in Defense Systems: A Comprehensive Analysis of Emerging Technologies and Future Trends,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581013,"This research paper mainly aims to discuss a comprehensive review of recent advancements in defense technologies with a focus on innovations that enhance military capabilities. This paper covers a wide range of areas, including artificial intelligence, cyber warfare, unmanned systems, advanced materials, and space-based technologies. The goal is to analyze the impact of these technological developments on defense strategies and their potential applications in modern warfare. This research paper offers an in-depth exploration of recent strides in defense technologies, emphasizing their transformative impact on military capabilities. Focused on cuttingedge developments across artificial intelligence, cyber warfare, unmanned systems, advanced materials, and space-based technologies, the study evaluates their implications for modern warfare strategies. Through a critical analysis of these advancements, the paper aims to contribute to the understanding of evolving defense landscapes and the potential integration of emerging technologies in defense applications. The comprehensive review encompasses not only the technical aspects but also considers the ethical, legal, and strategic dimensions associated with the deployment of advanced defense technologies. Ultimately, this paper seeks to provide a nuanced perspective on the current state of defense technology and its trajectory, guiding future research, policy considerations, and operational implementations in the defense sector.",No,"초록에서 해당 논문은 주로 최근 방위 기술의 발전에 대한 종합적인 리뷰와 분석을 제공하는 것으로 보이며, 독창적인 연구 결과나 새로운 실험적 기여에 대한 언급이 없습니다. 따라서 직접 기여하는 연구 논문보다는 리뷰 논문에 더 가깝다고 판단됩니다."
A Review on Machine Learning based Security Approaches in Intrusion Detection System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763261,"The rapid growth and advances in communication and Internet technologies result in efficient business transactions, communications as well as collaborations. However, this global system of interconnected networks increases the visibility and transparency of resources and data. As a result, threats actors utilize the vulnerabilities of networks to perform malicious activities and pose challenges for security. An Intrusion Detection System (IDS) is a software/tool to detect intrusions in network to protect resources by inspecting the traffic. Threat actors evolve with new techniques and technologies to release new threats for information technology resources. Threats are dynamic in nature and type. A traditional signature and experts rules-based techniques are no longer sufficient to detect ever-evolving threats. Machine Learning (ML) algorithms play a vital role in network cyber security. The two classifications of ML algorithms are shallow modes and deep learning (DL) models. These techniques increase the accuracy rate and reduce false rates on intrusion detection. ML techniques boost the performance of the IDS significantly. In this work, we perform a literature review of existing studies on ML-based IDSs. As a result, we present our findings and potential future research directions in this work. From the literature review, we revealed deep learning models outperform shallow ML models according to the evaluation measures such as execution time, complexity, accuracy and error rate. Moreover, we highlighted various research challenges in intrusion detection and also proposed potential solutions to resolve those challenges.",No,"초록에서 해당 논문은 기존 연구들을 종합하여 정리한 문헌 리뷰임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 논문에 해당한다."
A Referenced Framework on New Challenges and Cutting-Edge Research Trends for Big-Data Processing Using Machine Learning Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10087686,"Machine Learning and Big- Data Analytics is each growing computing fields. Big data is, in reality, gaining attraction in all regions of Science and engineering. While those big-data units have the clean capacity, successfully comprehending them will call for new thinking methods and inventive learning methodologies to cope with the severe issues. On the other hand, traditional machine learning strategies may not be able to fulfil the needs of real-time data processing for big -datasets because of the big-data age approaches. As a result, machine learning will reinvent itself for the big-data era. Where improvements in any discipline Big- Data analytics or machine Learning complement the opposite. This article offers a literature evaluation of the most recent advancement in ML (machine learning) approaches for large amounts of data processing. We focused on evaluating and analyzing the challenging situation and potential solutions associated with handling huge data sets with machine learning. This review article initially presents an overview of big-data and big-data analytics, followed by reviewing traditional machine-learning approaches as well as examining new aspects of machine-learning techniques for big-data processing. After that identifying the critical challenges of big-data processing correlated with the properties of big data 5Vs-volume, velocity, variety, veracity, and value, finally proposed a reference framework for big-data processing along with new emerging tools and technologies like Hadoop, distributed computing, and parallel computing along with the identification of open issues and new research trends associated with the processing of large amounts of data sets (Big-Data).",No,"본 논문은 최신 머신러닝 기법과 빅데이터 처리에 관한 기존 연구들을 종합적으로 검토하고, 관련 도전과제와 연구 동향을 제시하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵다."
The Digital Twin City in Enhancing Flood Evacuation Systems: Future Opportunities and Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876129,"In recent decades, the increasing frequency and severity of natural disasters have necessitated the development of more effective disaster management systems. This paper explores the potential of digital twin technology to enhance flood evacuation systems. A digital twin-city model can simulate urban environments, predict flood inundation, and identify optimal evacuation routes in real-time. The study conducts a systematic literature review to assess current advancements, evaluating the integration of 3D city models, artificial intelligence, and Internet of things sensors in flood evacuation planning. Results indicate that digital twin technology significantly improves predictive capabilities and evacuation efficiency, although data integration, model accuracy, and computational demands remain. Future research should focus on enhancing data integration, refining simulation models, improving predictive analytics, developing advanced visualization tools, and validating models across diverse environments to ensure reliability and effectiveness.",No,"본 논문은 디지털 트윈 기술의 가능성과 현재 연구 동향을 체계적으로 검토하는 문헌 리뷰에 초점을 맞추고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
An Evaluation Framework for Machine Learning and Data Science-Based Financial Strategies: A Case Study-Driven Decision Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10815600,"Big data and computational technologies are increasingly important worldwide in asset and investment management. Many investment management firms are adopting these data science (DS) methods and technologies to improve performance across all investment processes. A good question is whether we can make better decisions in developing quantitative strategies. Therefore, the main objective of this research was to develop a multicriteria assessment framework and scoring decision support system to evaluate quantitative investment strategies that apply machine learning (ML) and DS techniques in their research and development. Subject matter experts will assess all framework perspectives from a systematic literature review to approve their reliability. The perspectives consist of economic and financial foundations, data perspective, features perspective, modeling perspective, and performance perspective. The research methodology applied was the hierarchical decision model (HDM) to provide a 360° view of the quantitative investment strategy and improve and generalize the concept to other asset classes and regions. This study accomplished a rigorous integration of an extensive literature review connecting DS, ML, and investment decision-making in developing quantitative investment strategies. As a result, the major contribution of this study is the comprehensive examination, which included identifying and quantifying perspectives and criteria. The results, while limited indicated significant gaps in strategies examined and therefore generated critical knowledge to improve ML/DS-driven investment strategies, which are valuable for financial companies and policymakers.",Yes,"본 논문은 머신러닝과 데이터 과학 기법을 적용한 정량적 투자 전략을 평가하기 위한 다중기준 평가 프레임워크와 의사결정 지원 시스템을 개발하는 독창적인 연구 내용을 포함하고 있다. 또한, 체계적인 문헌 검토와 전문가 평가를 통해 새로운 평가 모델을 제안하고 있어 연구 논문에 해당한다."
Comparison of Naive Bayes and SVM Algorithm based on Sentiment Analysis Using Review Dataset,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9117512,"Now a day's sentiment analysis is the most used research topic. The sentiment analysis result is based on different investigation for example politics, terrorism, economy, international affairs, movies, fashion, justice, humanity. Social media are the main resource for collecting people's opinion and their sentiment about a different trending topic. People use many abusing words in social media to express their emotion. Using sentiment analysis, we will build a platform where one can easily identify the opinions are either positive or negative or neutral. This research paper will contain supervised learning which is under the machine learning approach. We run an experiment on different queries from humanity to terrorism and find out an interesting result. First of all, we have preprocessed the dataset to convert unstructured airline review into structured review form. After that, we convert structured review into a numerical value. We have to preprocess the data before using it. Stop word removal, @ removal, Hashtag removal, POS tagging, calculating sentiment score have done in preprocessing part. Then an algorithm has been applied to classify the opinion as either it is positive or negative. In this research paper, we will briefly discuss supervised machine learning. Support vector machine as well as Naïve Bayes algorithm and compares their overall accuracy, precession, recall value. The result shows that in the case of airline reviews Support vector machine gave way better result than Naïve Bayes algorithm.",Yes,"이 논문은 감성 분석을 위해 Naive Bayes와 SVM 알고리즘을 적용하고, 데이터 전처리 및 성능 비교 실험을 수행한 독창적인 연구 내용을 포함하고 있습니다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단됩니다."
Leveraging Machine Learning for Breast Cancer Prognosis: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10867085,"Breast cancer, a complex ailment, necessitates early diagnosis and prognosis for effective treatment. This systematic review examines the application of machine learning (ML) for breast cancer prognosis, emphasizing precise risk assessment in clinical decision-making. Author evaluates existing ML methods, highlighting their strengths, limitations, and potential clinical integration. Following PRISMA guidelines, author conducted a literature search in databases like Scopus, IEEE Xplore, and PubMed, focusing on studies from 2018–2023. Among the studies screened, 25 articles met the eligibility criteria for inclusion in this review. The findings from the author's review highlight that ensemble models are the most popular and accurate strategy for breast cancer prognosis. The two primary metrics used to assess model performance in the reviewed studies are accuracy and the C-index. The study also identified the datasets utilized for breast cancer prognosis and the role of explainable ML algorithms in guiding cancer treatment decisions. Finally, some challenges observed in the review are also investigated to provide valuable insights for researchers and clinicians in this dynamic field.",No,"이 논문은 머신러닝을 활용한 유방암 예후에 관한 기존 연구들을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합과 평가에 중점을 두고 있습니다."
WoS Driven Bibliometric Analysis on Genetic Disease Prediction Using Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10441673,"The term “artificial intelligence” (AI) is used to describe the practice of simulating human intelligence in a machine. Robotics, medical diagnostics, medical statistics, and even human biology are just a few of the applications of artificial intelligence. This article aims to provide a bibliometric analysis of artificial intelligence's potential applications in predicting genetic diseases from 2018 to 2023. The primary objective of this research is to evaluate the available data on the efficacy of using AI for the prevention of hereditary illnesses. The study's authors acknowledged and affirmed the issue's significance and the topic's potential to aid future academics in achieving strategic alignment between information and technology. There have been a total of 127 articles on this subject, all of which have been analyzed using the Web of Science bibliometric technique. Articles, conference papers, and book chapters all contribute to this bibliometric study, which is used to gauge the worldwide influence of publications in a certain field. This article reviews the best practices for doing and using bibliometric research using the R software package. In the realm of genetic disease prediction, visualization makes it simple to view and comprehend various approaches.",No,"본 논문은 인공지능을 이용한 유전병 예측 분야의 연구 동향을 분석하는 서지계량학적 분석 연구로, 기존 문헌을 종합하고 평가하는 데 중점을 두고 있다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵다."
Exploring the Role of Generative Artificial Intelligence in the Energy Sector: A Comprehensive Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10795598,"Generative Artificial Intelligence (GenAI) enhances productivity by creating data, forecasting, optimizing, and understanding human language. In the energy sector, it is projected to have a $240 billion global economic impact, though research remains limited. This paper reviews GenAI's benefits, challenges, and research gaps in the energy sector, also focusing on climate change efforts. A PRISMA-SCR-based literature review from January 2022 to May 2024 was conducted using IEEE Xplore, ScienceDirect, ACM Digital Library, and Google Scholar. GenAI tools extracted data, verified by researchers. Analysis of 33 papers shows GenAI excels in knowledge integration and prediction. It generates synthetic electricity demand data, manages grids, forecasts energy demand, and optimizes renewable energy systems. Key challenges include hallucinations, data biases, privacy concerns, misuse, and system errors. Solutions involve improving training data, system fine-tuning, human oversight, and security measures. Research gaps include synthetic data realism, model evaluation standards, and integrating GenAI with blockchain and IoT.",No,"본 논문은 기존 연구들을 종합적으로 검토하는 문헌 리뷰로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 현황과 연구 갭을 분석하는 데 중점을 두고 있습니다."
Attention-Based Gated Recurrent Unit for Gesture Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240992,"Gesture recognition becomes a thriving research area in modern human motion recognition systems. The intensification of demands on efficient interactive human-machine-interface systems, commercial objectives, and many other factors contributes to fuel this revival dynamics. Understanding human gestures becomes essential for prevention and health monitoring applications. In particular, analyzing hand gestures is of paramount importance in personalized healthcare-related applications to help practitioners providing more qualitative assessments of subject's pathologies, such as Parkinson's diseases. This work proposes a novel deep neural network approach to forecast future gestures from a given sequence of hand motion using a wearable capacitance sensor of an innovative gesture recognition hardware system. To do this, we use an attention-based recurrent neural network to capture the temporal features of hand motion to unveil the underlying pattern between the gesture and these sequences. While the attention layers capture patterns from the weights of the short term, the gated recurrent unit (GRU) neural network layer learns the inherent interdependency of long-term hand gesture temporal sequences. The efficiency of the proposed model is evaluated with respect to cutting-edge work in the field using several metrics. Note to Practitioners-In this article, the problem of human hand gesture recognition is analyzed using deep learning techniques. The proposed model uses input historical motion sequences collected from a wearable capacitance sensor to predict hand gestures. The model leverages the intrinsic correlation of motion sequences and extracts the salient part of the sequences by taking into consideration their temporal, complex, and nonlinear features. The approach studies the effect of different lengths of historical motion sequences in prediction outcomes. This allows for avoiding using cumbersome data collection, heavy data treatment, and high computational cost. The model performance is trained and assessed on real-world data by performing comparisons with alternative approaches, including well-known classifiers. The model yields very encouraging results and demonstrates that the proposed approach is quite competitive as it can reproduce typical activity trends for important channels. The present findings could help in the development of intelligent wearable devices for predicting hand gestures using a limited number of channels. This work could also help practitioners to provide a more qualitative appraisal of patients suffering from different pathologies such as Parkinson's diseases to personalized healthcare-related applications and to develop wearable gesture recognition devices on a large scale.",Yes,논문은 새로운 딥러닝 기반 모델(Attention-Based GRU)을 제안하고 이를 실제 웨어러블 센서 데이터에 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 방법들과 비교 분석을 수행하여 모델의 효율성을 입증하고 있어 연구 논문에 해당한다.
Explainable Text Classification for Legal Document Review in Construction Delay Disputes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386240,"The costs involved in manually reviewing documents in legal civil litigations have grown dramatically as more and more information is stored electronically. As a result, the document review process can require an extraordinary dedication of resources. In construction litigations, quickly finding supporting documentation in a delay dispute is critical to the success of a matter. Identifying relevant delay-related communications and supporting documentation has historically been expensive and time consuming. Using machine learning technologies, respondents can be more comprehensive in their assessment of the data requiring review to respond to the claim in time. Explainable machine learning is an active machine learning research area, and in an explainable machine learning system, predictions generated from a machine learning model are explainable and human understandable. In delay dispute ‘document review’ scenarios, a document can be identified as delay-related, as long as one or more of the text snippets in a document are deemed delay-related. In these scenarios, if these delay-related snippets can be located, then attorneys could easily evaluate the model’s decision. The authors of this paper propose an approach for accurately identifying rationales and an approach for boosting document classification accuracy using delay-related snippets and their applications in construction delay disputes. The authors conducted experiments using data from a few real world delay dispute matters and the results from these experiments show that the proposed approaches have the potential to significantly advance the application of text classification in document review in construction delay dispute matters.",Yes,"본 논문은 건설 지연 분쟁 문서 검토를 위한 설명 가능한 텍스트 분류 기법을 제안하고, 실제 사례 데이터를 활용한 실험을 통해 그 효과를 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Few-shot Time-Series Forecasting with Application for Vehicular Traffic Flow,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874010,"Few-shot machine learning attempts to predict outputs given only a very small number of training examples. The key idea behind most few-shot learning approaches is to pre-train the model with a large number of instances from a different but related class of data, classes for which a large number of instances are available for training. Few-shot learning has been most successfully demonstrated for classification problems using Siamese deep learning neural networks. Few-shot learning is less extensively applied to time-series forecasting. Few-shot forecasting is the task of predicting future values of a time-series even when only a small set of historic time-series is available. Few-shot forecasting has applications in domains where a long history of data is not available. This work describes deep neural network architectures for few-shot forecasting. All the architectures use a Siamese twin network approach to learn a difference function between pairs of time-series, rather than directly forecasting based on historical data as seen in traditional forecasting models. The networks are built using Long short-term memory units (LSTM). During forecasting, a model is able to forecast time-series types that were never seen in the training data by using the few available instances of the new time-series type as reference inputs. The proposed architectures are evaluated on Vehicular traffic data collected in California from the Caltrans Performance Measurement System (PeMS). The models were trained with traffic flow data collected at specific locations and then are evaluated by predicting traffic at different locations at different time horizons (0 to 12 hours). The Mean Absolute Error (MAE) was used as the evaluation metric and also as the loss function for training. The proposed architectures show lower prediction error than a baseline nearest neighbor forecast model. The prediction error increases at longer time horizons.",Yes,"본 논문은 소수의 학습 예제로 시계열 예측을 수행하는 새로운 딥러닝 아키텍처를 제안하고, 이를 차량 교통 흐름 데이터에 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Review and Conceptual Design of FPGA-based Application for Data-Driven Power Electronic Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9503033,"AI-based data-driven methods are an emerging research direction in the field of power electronics. However, because of the absence of large datasets, the development of these solutions have some barriers to overcome. To properly train machine learning algorithms and neural networks a large amount of training data is necessary. This dataset can be a union of simulation and measured data. Generating simulation data with computer simulations can be slow process and gathering real data is not cost-effective. Real-Time simulators based on FPGAs can be powerful tools to accelerate simulation, and create datasets for AI applications in a cost-effective and accurate way. In this paper the possible FPGA-based solutions, which can be applicable for the problems, have been reviewed. Their applicability have been discussed, moreover a simplified FPGA-based concept have been designed and embedded into two possible AI-based application area.",No,"본 논문은 FPGA 기반 솔루션에 대한 리뷰와 개념 설계에 초점을 맞추고 있으며, 직접적인 실험 결과나 독창적인 연구 데이터 생성에 대한 내용은 포함되어 있지 않습니다. 따라서 기존 연구를 정리하고 개념을 제안하는 리뷰 논문으로 판단됩니다."
A Smart System for Selection of Optimal Product Images in E-Commerce,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622259,"In e-commerce, content quality of the product catalog plays a key role in delivering a satisfactory experience to the customers. In particular, visual content such as product images influences customers' engagement and purchase decisions. With the rapid growth of e-commerce and the advent of artificial intelligence, traditional content management systems are giving way to automated scalable systems. In this paper, we present a machine learning driven visual content management system for extremely large e-commerce catalogs. For a given product, the system aggregates images from various suppliers, understands and analyzes them to produce a superior image set with optimal image count and quality, and arranges them in an order tailored to the demands of the customers. The system makes use of an array of technologies, ranging from deep learning to traditional computer vision, at different stages of analysis. In this paper, we outline how the system works and discuss the unique challenges related to applying machine learning techniques to real-world data from e-commerce domain. We emphasize how we tune state-of-the-art image classification techniques to develop solutions custom made for a massive, diverse, and constantly evolving product catalog. We also provide the details of how we measure the system's impact on various customer engagement metrics.",Yes,"논문 초록에서 제시된 시스템은 머신러닝과 딥러닝 기법을 활용하여 대규모 전자상거래 카탈로그의 제품 이미지를 최적화하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 실제 데이터에 적용한 기술적 도전과 해결 방안을 다루며 시스템의 효과를 측정한 점에서 연구 논문으로 판단됩니다."
A Review on Improving the Accuracy of Effort Estimation in Software Development with Agile Method,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10761956,"This study addresses the critical challenge of effort estimation in Agile software development, particularly within the Scrum framework. Accurate effort estimation is essential for effective project planning, resource allocation, and meeting the high expectations of customers regarding quality, budget, and delivery time. Despite the widespread adoption of Agile methodologies, accurately predicting the effort required for various tasks remains complex due to the dynamic nature of software projects and the frequent changes in requirements. The research aims to explore and evaluate diverse effort estimation techniques used in Agile projects over the past six years, focusing on their effectiveness and the challenges encountered. The study employs a comprehensive literature review framework, incorporating PICOC protocols to identify relevant studies. The selected techniques are classified based on their methodology, application context, and reported outcomes. Key techniques reviewed include conventional methods, machine learning approaches, and fuzzy logic models. Findings indicate that machine learning and ontology-based multiagent systems show promise in enhancing estimation accuracy, while pair estimation techniques improve collaborative task estimation and management. The study highlights the importance of integrating appropriate machine learning models, dataset types, and validation procedures to advance effort estimation practices. The paper concludes by identifying research gaps and suggesting opportunities for further improvement in effort estimation methods within Agile environments.",No,"본 논문은 다양한 노력 추정 기법을 문헌 리뷰를 통해 종합적으로 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 분석하고 정리하는 데 중점을 두고 있습니다."
Predictive Maintenance Application in Healthcare,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893973,"Effective maintenance management of medical technology influences healthcare service quality delivery and the profitability of healthcare facilities. The objective of this study is to review recent research focused on medical equipment predictive maintenance. Predictive maintenance is a term that describes the use of sensors and forecasting techniques to anticipate when a device or machine would fail, so that preemptive measures can be undertaken to lessen the effects of an impending failure and subsequently improve service quality. In healthcare, this mostly pertains to medical devices and can be an invaluable tool that would save medical institutions money and time, and more importantly, mitigate adverse effects to patients that require these devices. Predictive maintenance is a means of providing some control in an otherwise chaotic and arbitrary industry; doctors, nurses, and administrators often do not see the value in repairing or replacing a device before it fails, especially if it were not crucial in their day-to-day or emergency operation. In this paper, we provide a comprehensive review of work that pertains to predictive maintenance in healthcare published between 2014 and 2019. The works reviewed were published in scientific journals and conferences, and for each one, we were interested in (1) what predictive maintenance techniques were used, such as Internet of Things (IoT) or deep learning (DL), (2) what devices/equipment were used to obtain results and (3) whether or not they were medical devices/healthcare-related, if not, how could this application apply to healthcare and (4) whether or not predictive maintenance is proven to be more effective than simply waiting for the lifespan of the device to come to its end. The reviewed papers all showed great potential in the techniques used for predictive maintenance and showed promising results for a variety of medical devices. The significance of this work lies in helping healthcare professionals realize the potential benefit that predictive maintenance can bring to the table, and how it fits with the cost-sensitive nature of the industry.",No,"이 논문은 2014년부터 2019년까지 발표된 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여를 포함한 연구 논문으로 보기 어렵습니다."
Intelligent Security Monitoring: Machine Learning-based Intrusion Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894147,"The rapid growth in the field of Information Technology has definitely caused a steep rise in the rate and sophistication of cyber-attacks; therefore, cybersecurity is an issue of major concern. Recently, Machine Learning and Deep Learning methodologies have turned out to be really potent tools for network intrusion detection. This paper presents a detailed review of various ML and DL models used in Intrusion Detection Systems (IDS). The models like Naïve Bayes, Artificial Neural Networks, Support Vector Machines, Decision Tree, K-Nearest Neighbors, and methods based on Reinforcement Learning are covered. In this paper, the MRF model and examine the model with other ML and DL models are focused. In this analysis, the authors use the KDD dataset, a very popular standard in intrusion detection research. This dataset has different attributes divided into four main classes. Such as Content, Host, Basic, and Traffic. All these are vital classes in deciding the false alarm rate and detection rate of IDS. The empirical evidence that the MRF model outperforms traditional models in both measures of accuracy and reliability are provided. How contribution analysis for each class of attributes towards DR and FAR can be utilized in optimizing the MRF model for better performance of intrusion detection systems are shown. The results obtained in this research work have been pointing toward the importance of choosing suitable data attributes and models so that maximum DR can be achieved with a minimum FAR, which would increase the effectiveness of IDS. The proposed Modified Random Forest-based Intrusion Detection System uses advanced feature selection and preprocessing to improve detection accuracy and reduce false alarms. The real-time adaptability is done by dynamic selection of parameters in order to prevent evolving cyber threats.",Yes,"본 논문은 기존의 머신러닝 및 딥러닝 모델들과 비교하여 Modified Random Forest(MRF) 모델을 제안하고, KDD 데이터셋을 활용한 실험을 통해 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 또한, 각 속성 클래스가 탐지율과 오경보율에 미치는 영향을 분석하여 모델 최적화 방안을 제시하는 등 직접적인 연구 기여가 명확하다."
AI Blockchain Platform for Trusting News,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8884985,"An interdisciplinary effort is needed for solving the fake news crisis, because the solutions depend not only on AI, but also on social mechanisms. In this paper, we propose an AI blockchain platform to build a strong collaboration among AI blockchain researchers and news media to advance the research fighting against fake news. This platform will provide journalists with blockchain crowd-sourced and AI validated factual data on emerging news. This platform will gather blockchain traced data and AI tools that can provide pointers to the original data sources, news propagation path, AI analyzed experts to consult on a given topic. This will provide journalists with cheaper and reliable sources of information in the Internet social media age. So that factual-sourced reporting can outpace the spread of fake news on social media which will encourage factual news sources as a way to value and promote truth for society. The technical contributions of this paper are (1) mechanism building the factual news database, (2) mechanism generating the news blockchain supply chain graph, and (3) AI blockchain based crowd sourcing fake news ranking mechanisms (4) AI blockchain platform for trusting news ecosystem. (5) reviewing the state of fake news research from the technology and social aspects, and providing list of key research issues and technical challenges.",Yes,"논문은 AI와 블록체인 기술을 활용한 새로운 플랫폼을 제안하며, 구체적인 기술적 기여(사실 뉴스 데이터베이스 구축, 뉴스 블록체인 공급망 그래프 생성, 가짜 뉴스 순위 매김 메커니즘 등)를 포함하고 있다. 이는 독창적인 연구 내용과 기술적 발전을 담고 있어 연구 논문에 해당한다."
Ship Trajectory Prediction Model for Space-Based Maritime Surveillance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10642713,"This research proposes an anomaly detection workflow intended primarily for integration into satellite-based tip and cue services for maritime surveillance. The workflow is centred around a ship trajectory prediction model which is designed to respond to anomalous events such as AIS ""shut-off"" events. This is important for accurately predicting the trajectories of potentially suspicious vessels that are moving, and subsequently scheduling the tasking of satellite acquisitions to monitor these vessels. The research implements a ship trajectory prediction model based on AIS data using Recurrent Neural Networks (RNNs). The efficacy of the model is evaluated based on the mean great circle distance between predicted and actual vessel trajectories, demonstrating satisfactory performance for typical motion patterns while acknowledging certain limitations in prediction accuracy. Overall, this study represents a significant step forward in the integration of imaging satellites and AIS data for maritime surveillance, offering a promising approach for anomaly detection and improving the efficiency of satellite-based monitoring systems. A GitHub repository containing the source code and related materials for this work is made available.",Yes,"본 논문은 AIS 데이터를 활용한 선박 궤적 예측 모델을 구현하고, 이를 기반으로 이상 탐지 워크플로우를 제안하는 독창적인 연구 내용을 포함하고 있다. 또한 모델의 성능 평가를 통해 연구 결과를 검증하였으며, 소스 코드도 공개하여 연구 기여를 명확히 하고 있다."
Unlocking the Potential of Information Modeling for Root Cause Analysis in a Production Environment: A Comprehensive State-of-the-Art Review Using the Kitchenham Methodology,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539963,"Data from production environments is now available in unprecedented volumes, making the problem-solving of incidents through root cause analysis straightforward. However, the root cause analysis process remains time-consuming. This study employs the Kitchenham standard systematic literature review methodology to explore how information models and deep learning can streamline this process. By conducting a comprehensive search across four major databases, we evaluate the current technological advancements and their application in root cause analysis. The aim of this study is to assesses the impact of information models for root cause analysis in a production environment. Our findings reveal that integrating knowledge graphs, association rule mining, and deep learning algorithms significantly improves the speed and depth of root cause analysis compared to traditional methods. Specifically, the use of neural networks in recent literature shows substantial advancements in analyzing complex datasets, facilitating large-scale data integration, and enabling automated learning capabilities. Comparing our findings with other recent studies highlights the advantages of using information modeling and deep learning technologies in root cause analysis. This comparison underscores the superior accuracy and efficiency of these advanced methodologies over traditional manual interpretation methods. The effective implementation of these technologies requires a robust foundation of clean, standardized data, giving rise to the concept of “Production IT.” Furthermore, it is crucial for this data to be openly available to facilitate academic research, thereby enabling the development of new methods for more efficient and effective root cause analysis.",No,"본 논문은 Kitchenham 방법론을 사용한 체계적 문헌고찰(Systematic Literature Review) 연구로, 기존 연구들을 종합하여 현황을 평가하고 비교하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵다."
Interpretable Machine Learning for Predicting Customer Churn in Retail Banking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125859,"Customer churn is one of the biggest problems any brokerage institution has. This is evidenced by the rapid establishment of intelligent systems to predict customer churn, retain current clients, and win new ones in various domains. Unfortunately, there is an extreme scarcity of real-world datasets for training and establishing retail banking intelligent systems. Moreover, the Machine Learning (ML) models supporting such existing systems are all black box. The trends in electronics and informatics like Explainable Artificial Intelligence (XAI) have however provided a better approach to ML model accountability. This study leverages an Interpretable Machine Learning model to transparently predict the likelihood and cause of customer churn in retail banking. A real-world database (Berka) from a Czech bank was used for feature extraction using deep clustering. A dataset of features from the Berka database and a dataset from Kaggle were used to aid customer attrition prediction. Synthetic Minority Over Sampling Techniques (SMOTE) were then used to handle dataset imbalance before training, validating and testing with four tree-based and four standard machine learning approaches. The outstanding performance was achieved with random forest, a tree-based algorithm, on both datasets, with 99% accuracy, 98.5% recall, and 98.5% fl-score on the Berka dataset. It also scored 85 % accuracy, 77.5 % recall, and 77 % fl-score on the Kaggle dataset. Finally, Model-Agnostic Explanations (LIME) and SHapley Additive explanations (SHAP) are used for ML model accountability. This work can be reliably used to establish trustworthy intelligent systems in the financial sector and related domains.",Yes,"본 논문은 실제 은행 데이터와 Kaggle 데이터를 활용하여 고객 이탈 예측을 위한 해석 가능한 머신러닝 모델을 개발하고, 다양한 기법을 적용해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 또한, 모델 설명 기법(LIME, SHAP)을 사용해 결과의 해석 가능성을 높인 점에서 직접적인 연구 기여가 명확하다."
Integration Design Analysis of Mobile Protection Against Spyware Attacks with Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10838162,"This paper explores the critical role of artificial intelligence (AI) in enhancing mobile protection against spyware attacks through Systems Integration and Architecture Security. Focusing on integrating AI-powered threat intelligence with traditional security measures, the study delves into the effectiveness of multi-layered defense mechanisms in combating cyber threats. Through an extensive literature review, the paper explains the transformative impact of AI-driven solutions in identifying and neutralizing advanced spyware risks. The results emphasize the crucial significance of AI's role in enhancing cybersecurity resilience and paving the way for proactive measures to mitigate emerging threats in today's dynamic technological landscape. our findings highlight the significant role of AI in advancing systems integration and architecture, providing essential insights for fortifying cybersecurity resilience and paving the way for proactive measures to mitigate emerging threats. As we look ahead, the collective relationship between AI, systems integration, and architecture holds immense promise for shaping the future of mobile security and safeguarding sensitive data and user privacy.",No,"초록에서 본 논문은 AI와 모바일 보안 통합에 관한 문헌 리뷰와 개념적 논의를 중심으로 하고 있으며, 직접적인 실험, 데이터 분석, 또는 독창적인 연구 결과를 제시하지 않고 있다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵다."
Development of Machine Learning-Based Assessment System for Laparoscopic Surgical Skills Using Motion-Capture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417615,"Laparoscopic surgery is a widely used surgical technique, on the other hand, its high degree of difficulty makes it difficult for beginners to learn the technique efficiently. In addition, recent working hour restrictions and shortages of surgeons have resulted in insufficient training time, and establishing the efficiently training methods is becoming urgent needs. Therefore, to promote the skill proficiency of novice surgeons, machine learning-based assessment system for laparo-scopic surgical skills was developed. A measurement system with a simple configuration was introduced so that trainees can easily use it alone. Alternatively, the indices related to the opening ratio and the rotation angle of surgical instruments, which were measured in the authors' previous study were no longer available. Therefore, comparative experiments were conducted to verify the effect of the lack of indices related to these data on the accuracy of skill evaluation. Based on the measurement data of 104 wet lab trainings measured in the previous study, a machine learning model that evaluates surgeon's skill at 3 levels based on the number of surgical experiences, and global operative assessment of laparoscopic skills (GOALS) which is a type of surgical skill evaluation index were established. By using the explainable AI method, this system can present the skill evaluation result including its basis to the trainee. Since the developed system can be easily operated by a GUI-based program, trainees can confirm the quantitative evaluation result on-site immediately after the training.",Yes,"본 논문은 복강경 수술 기술 평가를 위한 기계학습 기반 시스템을 개발하고, 이전 연구 데이터를 활용하여 새로운 평가 모델을 구축한 독창적인 연구 내용을 포함하고 있다. 또한, 설명 가능한 AI 기법을 적용하여 평가 결과의 근거를 제시하는 등 직접적인 연구 기여가 명확하다."
A Review of Machine Learning Approaches to Power System Security and Stability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121208,"Increasing use of renewable energy sources, liberalized energy markets and most importantly, the integrations of various monitoring, measuring and communication infrastructures into modern power system network offer the opportunity to build a resilient and efficient grid. However, it also brings about various threats of instabilities and security concerns in form of cyberattack, voltage instability, power quality (PQ) disturbance among others to the complex network. The need for efficient methodologies for quicker identification and detection of these problems have always been a priority to energy stakeholders over the years. In recent times, machine learning techniques (MLTs) have proven to be effective in numerous applications including power system studies. In the literature, various MLTs such as artificial neural networks (ANN), Decision Tree (DT), support vector machines (SVM) have been proposed, resulting in effective decision making and control actions in the secured and stable operations of the power system. Given this growing trend, this paper presents a comprehensive review on the most recent studies whereby MLTs were developed for power system security and stability especially in cyberattack detections, PQ disturbance studies and dynamic security assessment studies. The aim is to highlight the methodologies, achievements and more importantly the limitations in the classifier(s) design, dataset and test systems employed in the reviewed publications. A brief review of reinforcement learning (RL) and deep reinforcement learning (DRL) approaches to transient stability assessment is also presented. Finally, we highlighted some challenges and directions for future studies.",No,초록에서 해당 논문은 기존 연구들을 종합적으로 정리하고 분석하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험을 제시하는 연구 논문이 아닙니다.
Arabic Poetry Meter Categorization Using Machine Learning Based on Customized Feature Extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615302,"Text mining applications became important in various intelligent tasks. Text documents are the most materials that record many important procedures in various worldwide organizations and different people cultures. Text poetry is an important type of people culture and education domains media. Arabic text poems classification is a few experimented fields, however, it has an important presence and special influence. Both new and ancient Arabic poetry has the same unique approach for rhythmical harmony measure, which can be used for identifying Arabic poems types. Deep learning as a machine learning method has many distinctive achievements in many areas, as well as, text classification tasks. In this paper, Arabic poetry text is categorized. A customized feature selection is proposed, which is fused with a clustering technique for enhancing models efficiency. Deep learning has experimented alongside two popular machine learning techniques; support vector machine and decision tree. The proposed feature extraction method has achieved high accuracy with all three techniques. The results are better than many related works.",Yes,"본 논문은 아랍어 시의 운율 분류를 위해 맞춤형 특징 추출과 클러스터링 기법을 제안하고, 이를 딥러닝 및 머신러닝 기법과 결합하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법이 기존 연구보다 우수한 결과를 보였다고 명시하여 직접적인 연구 기여가 있음을 알 수 있다."
Empirical Analysis of Machine Learning Models towards Adaptive Network Intrusion Detection Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716526,"The exponential growth of Internet access has resulted in a huge influx of network attacks, some of which are lethal or have devastating results. As a consequence, the plethora of new threats revealed makes it harder to provide network security in the direction of identifying breaches. Furthermore, the intruders with intent launching various physical attacks within the network also cannot be overlooked. An Intrusion Detection System (IDS) is a tool which inspects internet traffic for the purposes of ensuring its confidentiality, integrity, and availability as well as defending the network from potential intrusions. Despite the researchers' best efforts, IDS still poses potential challenges and it has improved accuracy and reducing false alarm rates, and recognized new intrusions. Machine learning (ML) and deep learning based IDSs have recently been adopted as viable solutions for swiftly identifying suspicious breaches. This review paper tries to present a deep brief on various IDS concepts, evaluation metrics, and dataset selection measures. This paper is carried out by carefully considering the various ML algorithms applied in the IDS domain and also tries to figure out various results obtained by using various ML algorithms towards in field of NIDS. This paper addresses the obstacles and future potential in the field of IDSs by assessing existing illustrative research. The outcomes of several studies were examined and contrasted, providing a clear path and roadmap for future research. The extensive review of recently developed IDS models and a detailed comparative analysis show the novelty of the work. The proposed paper gives an outline of different endeavors to foster an effective IDS using single, hybrid, and ensemble ML classifiers, as well as the results of these efforts using various datasets.",No,초록에서 해당 논문은 기존 연구들을 종합적으로 검토하고 비교 분석하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험 기여보다는 기존 연구의 평가와 정리를 주된 내용으로 하고 있습니다.
Advancing E-Commerce Authenticity: A Novel Fusion Approach Based on Deep Learning and Aspect Features for Detecting False Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614578,"In the contemporary digital marketplace, the proliferation of online consumer reviews has a pivotal influence on purchasing decisions. Concurrently, the prevalence of spurious reviews poses a substantial risk to the integrity of e-commerce, misleading consumers, and detrimentally impacting businesses. This paper delineates a pioneering methodology for the identification of counterfeit reviews, which is based on the combination of deep learning attributes and aspect-based analytical features. The main contribution of this research is (1) proposing an aspect fusion network based on the hierarchical attention mechanism to address the problem of multiple aspects of representing review content. The aspect fusion network can help select important aspect words and fuse aspect dictionaries with word-level attention weights. (2) We build a cardinality fusion model so that the heuristic can mitigate the negative impact of random weights and intervals on the auxiliary model. The methodology integrates advanced deep learning paradigms with aspect-based sentiment analysis to detect fraudulent reviews. Specifically, the approach encompasses a dual-method strategy: initially utilizing a Convolutional Neural Network (CNN) for the extraction of profound characteristics from review texts, followed by employing aspect-based sentiment analysis tools, including Part-of-Speech (PoS) tagging and GloVe embedding, for the distillation of aspectual features. Subsequently, these split sets of features are synergized and applied in the training of various classifier layers. Eextensive experiments have been conducted on six public review datasets contrasting the previous work on authenticity and aspect analysis. The effectiveness and performance of the proposed authenticity fusion model have been verified by the detailed analyses. The proposed model outperforms the competitors with remarkable improvement on both review authenticity and aspect analysis. This innovative approach was rigorously evaluated using a dataset of Amazon reviews that encompassed both authentic and counterfeit reviews. The empirical results demonstrate that our proposed method attains a remarkable accuracy rate of 97.73%, substantially surpassing existing state-of-the-art methodologies. The study posits that the strategic fusion of deep learning attributes and aspect-based features significantly enhances the efficacy of counterfeit review detection systems, presenting a formidable tool in the arsenal against e-commerce fraud.",Yes,"본 논문은 딥러닝과 측면 기반 특징을 융합한 새로운 모델을 제안하고, 이를 다양한 공개 데이터셋에 적용하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 방법 대비 우수한 성능을 입증하는 실험 결과를 제시하여 직접적인 연구 기여가 명확하다."
Bias Elimination Network and Style Transfer for Facial Expression Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10674212,"Humans often evaluate the emotion through reading their facial expressions and then generate corresponding responses. Thus, in the world where the AI Chatbots are popularly investigated, the ability for robots to recognize people’s facial expressions becomes a critical issue to solve. In this work, we first interpret the necessity to eliminate image bias within those datasets, which may be the stumbling block to those previous works. Then we introduce an innovative way to overcome the challenges produced by biased images and then solve the task of Facial Expression Recognition, where the Style Transfer technique is used to enhance our data for both training and inference phases. In our experiments, we show that after applying our method to the JAFFE dataset, the recognition accuracy significantly outperforms the same model trained on unenhanced ones. Our method to eliminate data bias should be generalizable to all face-related tasks and even applicable to other field of machine learning, and we hope the performance of these tasks can take a big step forward.",Yes,"논문은 편향된 이미지 문제를 해결하기 위한 새로운 방법과 스타일 전송 기법을 제안하여 얼굴 표정 인식 성능을 향상시키는 독창적인 연구 내용을 포함하고 있습니다. 또한, 제안된 방법을 실제 데이터셋에 적용하여 성능 향상을 입증한 실험 결과를 제시하고 있어 연구 논문에 해당합니다."
A Comprehensive Study of the IoT Cybersecurity in Smart Cities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303356,"Smart cities exploit emerging technologies such as Big Data, the Internet of Things (IoT), Cloud Computing, and Artificial Intelligence (AI) to enhance public services management. The use of IoT allows detecting and reporting specific parameters related to different domains of the city, such as health, waste management, agriculture, transportation, and energy. LoRa technologies, for instance, are used to develop IoT solutions for several smart city domains thanks to its available features, but sometimes people (i.e., citizens, information technology administrators, or city managers) might think that these available features involve cybersecurity risks. This study explores the cybersecurity aspects that define an assessment model of cybersecurity maturity of IoT solutions to develop smart city applications. In that sense, we perform a systematic literature review based on a top-down approach of cybersecurity incident response in IoT ecosystems. Besides, we propose and validate a model based on risk levels to evaluate the IoT cybersecurity maturity in a smart city.",Yes,"논문은 IoT 사이버보안 성숙도 평가 모델을 제안하고 검증하는 독창적인 연구 내용을 포함하고 있으며, 체계적인 문헌 검토와 모델 개발을 통해 스마트 시티 응용을 위한 구체적인 기여를 하고 있다. 따라서 연구 논문에 해당한다."
Improving Industry 4.0 to Human-Centric Industry 5.0 in Light of the Protection of Human Rights,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569569,"In this paper, we scrutinize the human-centricity pillar of Industry 5.0—a factor improving Industry 4.0—from the viewpoint of the protection of human rights, social and economic rights ensured by several international organizations and the European Union with the AI Act in the focal point. According to criticisms against Industry 4.0, the inherent social dimension, the well-being of workers, the need for social inclusion and the adoption of technologies that do not substitute, but rather complement human capabilities (whenever possible) are missing key elements. Therefore, the Industry 5.0 initiative adopted by the European Commission in 2021 is not to replace Industry 4.0 but complements it with three substantial requirements as follows: resilience, sustainability, and human-centricity. In this regard, the human-centricity of Industry 5.0 and the human-centric approach to the responsible and trustworthy AI overlap each other, since the human-centric approach has already been adopted for digital technologies, including artificial intelligence and is (or will be) a part of the EU legislation considering the AI Act—the world's first comprehensive AI law—and other related regulatory tools. The AI Act's human-centric approach is based on the Charter of Fundamental Rights of the European Union and the values of the EU. An essential element of the document is the categorization of AI systems, which addresses the technology's inherent risk to society. On this basis, the AI Act distinguishes unacceptable (e.g., social scoring); high (e.g., recruitment, evaluating candidates, monitoring of workers) and minimal or no risk AI systems (e.g., spam filters). Therefore, the risk assessment of AI systems is cardinal to categorizing the technology before its operation. As society's inclusion is prominently observed through fundamental rights, another critical step is the impact assessment of fundamental rights in high-risk AI systems. This act is crucial to assess the relationship between technology and fundamental rights and examine its impact (whether it is acceptable or not) on human and social rights. Several international organizations and the EU have highlighted in their working documents the importance of protecting fundamental rights regarding AI (e.g., the right to equality and non-discrimination, which might be infringed by using the technology during employment decisions or recruitment). In conclusion, the analysis of the topic is pivotal as the use of AI systems is already part of our daily lives at home and the workplace.",No,"본 논문은 Industry 5.0과 AI 법안에 관한 기존 정책과 법률, 인권 보호 관점에서의 분석 및 고찰을 중심으로 하고 있으며, 직접적인 실험, 데이터 수집, 또는 새로운 기술 개발과 같은 독창적인 연구 결과를 제시하지 않습니다. 따라서 연구 논문이라기보다는 정책 및 법률 분석에 가까운 문헌 리뷰 또는 개념적 논의에 해당합니다."
Dental Positioning Medical Assistance System for BW Radiograph Based on YOLOV4,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317168,"In modern medicine, X-rays play a crucial role in diagnosis and maintaining patient records. In dentistry, bite-wing radiographs are commonly used to evaluate dental conditions. However, manually positioning teeth during diagnosis can be time-consuming and labor-intensive. To address this, this study utilizes artificial intelligence (AI) to assist dentists, speeding up the diagnosis process and providing rapid reference samples. We collaborated with five experienced dentists from Chang Gung Memorial Hospital, who provided bite-wing radiographs and corresponding tooth position data for training the model. Using the You Only Look Once (YOLO) V4 model as the foundation, this research applied image enhancement techniques inspired by existing literature, including bilateral filters and adaptive Gaussian binarization. These techniques reduced noise and improved tooth contour clarity, resulting in improved model performance, reduced processing time, and approximately 7% higher accuracy compared to existing techniques. Additionally, this research proposed image enhancement method increased the model recognition speed by 60.19% compared to using the original images. The study obtained ethical approval from the Research Institution Review Board (IRB) under application number 202002030B0. Integrating AI into tooth positioning enhances diagnostic efficiency and advances AI in dentistry, contributing to more effective and efficient dental care.",Yes,"본 논문은 YOLOV4 모델을 활용하여 치아 위치 인식 성능을 향상시키기 위한 이미지 처리 기법을 제안하고, 실제 치과 데이터로 모델을 학습 및 평가한 독창적인 연구 내용을 포함하고 있다. 또한, 성능 개선 수치와 윤리심의 승인도 명시되어 있어 연구 논문에 해당한다."
A Deep Learning Approach for Short Term Prediction of Industrial Plant Working Status,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564391,"Predictive Maintenance has gained more and more research and commercial interests, being a pivotal topic for improving the efficiency of many production industrial plants to minimize downtimes, as well as to reduce operational costs for interventions. Solutions reviewed in literature are increasingly based on machine learning and deep learning methods for prediction of fault proneness with respect to normal working conditions. Many state-of-the art solutions are not actually applied in real scenarios, and have restrictions to be executed in real-time in the production environment. In this paper, a framework for predictive maintenance is presented. It has been built upon a deep learning model based on Long-Short Term Memory Neural Networks, LSTM and Convolutional LSTM. The proposed model provides a one-hour prediction of the plant status and indications on the areas in which the intervention should be performed by using explainable LSTM technique. The solution has been validated against real data of ALTAIR chemical plant, demonstrating an high accuracy with the capability of being executed in real-time in a production operative scenario. The paper also introduced business intelligence tools on maintenance data and the architectural infrastructure for the integration of predictive maintenance approach.",Yes,"본 논문은 LSTM 및 Convolutional LSTM 기반의 딥러닝 모델을 제안하고, 실제 화학 공장 데이터를 사용하여 모델을 검증하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 실시간 예측 및 설명 가능한 기법을 적용하여 산업 현장에 직접 기여하는 연구임을 보여준다."
"ABCD technology- AI, Blockchain, Cloud computing and Data security in Islamic banking sector",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9939683,"The banking system gives the groundwork of current business. A well-working financial system is essential to an advanced economy, and banks fill significant roles for society. They should subsequently be secure. Artificial intelligence (AI) is steeply affecting innovation and skill. The financial industry is without a doubt to be the one to take on Artificial Intelligence. The Islamic banking sector is adopting this technology to evaluate vulnerabilities, perceive and prevent payment fraud, further develop against anti-money laundering (AML) processes, and play out extra administrative know-your-client (KYC) checks. Blockchain technology is centre, basic innovation with promising application possibilities in the banking industry. This could enable associations to settle and screen transactions more effortlessly than conventional methods like those of SWIFT. The emergence of cloud computing has modified the part that the Internet plays in a range of associations and organizations. Banks are slowly attempting to embrace cloud technologies to meet their varied prerequisites and to produce a versatile and light financial environment competent of rapidly response to arising business needs. For banks, physical and digital security along-with clients own data are vital to guaranteeing that banks can keep the cash and personal information they are entrusted with and the people who consistently are available in the establishment safe. Although these technologies have rapidly evolved and has the potential to revolutionise the banking industry. Some of these are fairly new concept and are therefore users should be aware of considerable risks and challenges linked with it. Thus, a systematic literature review was conducted to examine the challenges associated with these technologies, along-with the suggested solutions to mitigate them or lessen them.",No,"논문 초록에서 제시된 내용은 AI, 블록체인, 클라우드 컴퓨팅 및 데이터 보안 기술의 이슬람 은행 부문 적용에 관한 기존 문헌을 체계적으로 검토한 문헌 리뷰임을 나타냅니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들의 종합과 분석에 초점이 맞춰져 있어 연구 논문으로 보기 어렵습니다."
Pedagogy and usability in interactive algorithm visualizations: Designing and evaluating CIspace,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147100,"Interactive algorithm visualizations (AVs) are powerful tools for teaching and learning concepts that are difficult to describe with static media alone. However, while countless AVs exist, their widespread adoption by the academic community has not occurred due to usability problems and mixed results of pedagogical effectiveness reported in the AV and education literature. This paper presents our experiences designing and evaluating CIspace, a set of interactive AVs for demonstrating fundamental Artificial Intelligence algorithms. In particular, we first review related work on AVs and theories of learning. Then, from this literature, we extract and compile a taxonomy of goals for designing interactive AVs that address key pedagogical and usability limitations of existing AVs. We advocate that differentiating between goals and design features that implement these goals will help designers of AVs make more informed choices, especially considering the abundance of often conflicting and inconsistent design recommendations in the AV literature. We also describe and present the results of a range of evaluations that we have conducted on CIspace that include semi-formal usability studies, usability surveys from actual students using CIspace as a course resource, and formal user studies designed to assess the pedagogical effectiveness of CIspace in terms of both knowledge gain and user preference. Our main results show that (i) studying with our interactive AVs is at least as effective at increasing student knowledge as studying with carefully designed paper-based materials; (ii) students like using our interactive AVs more than studying with the paper-based materials; (iii) students use both our interactive AVs and paper-based materials in practice although they are divided when forced to choose between them; (iv) students find our interactive AVs generally easy to use and useful. From these results, we conclude that while interactive AVs may not be universally preferred by students, it is beneficial to offer a variety of learning media to students to accommodate individual learning preferences. We hope that our experiences will be informative for other developers of interactive AVs, and encourage educators to exploit these potentially powerful resources in classrooms and other learning environments.",Yes,"본 논문은 CIspace라는 인터랙티브 알고리즘 시각화 도구를 설계하고 평가한 연구 결과를 직접 제시하고 있으며, 사용성 연구와 교육적 효과 평가를 포함한 실험적 데이터를 바탕으로 독창적인 연구 기여를 하고 있다. 따라서 연구 논문에 해당한다."
The Need for Artificial Intelligence for Energy-Efficiency Management: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831359,The research presented in this paper examines the potential of artificial intelligence (AI) to improve energy-efficiency management in South Africa. This paper reviews and compares conventional methods with AI-based techniques used in energy-efficiency management systems. Various AI techniques have been investigated in this domain such as the artificial neural network and support vector machine. AI techniques offer the advantage of being more accurate when calculating non-linear parameters and thus are more efficient. Improved energy efficiency could allow more communities to gain access to electricity and improve their general living conditions.,No,"본 논문은 인공지능을 활용한 에너지 효율 관리 기법들을 검토하고 비교하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 종합하여 분석하는 성격이 강합니다."
Intelligent Framework to Detection of Fake Review Using Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894076,"Online reviews are becoming a more important resource for consumers as they make purchasing decisions. Recent years have seen a rise in the importance of the study fields of social media, hotel reviews, and e-commerce websites. People's decisions are influenced by online reviews. The bogus review has an impact on both consumers and commercial organizations in day-to-day existence. Inflated ratings and fake reviews paint a false picture of a product or service and are intended to mislead consumers away from or towards that product or service. In this research paper, machine learning algorithms are used for detecting fake reviews. The prevalence of fake reviews on online platforms has become a major concern, making it difficult for customers to trust the reviews they read. In this study, we used a dataset of Yelp Chicago restaurants' online reviews and applied various techniques of machine learning to identify phony reviews. The algorithms used included Naive Bayes, Random Forest, Linear SVM, and NN. The performance was evaluated for the models based on F1-score, recall, precision, and accuracy. Our results show that fake reviews can be detected effectively with high accuracy with the help of machine learning algorithms. This study provides a useful framework for identifying fake reviews on online platforms and can help improve the credibility and trustworthiness of online reviews.",Yes,"본 논문은 Yelp 시카고 레스토랑 리뷰 데이터셋을 활용하여 여러 머신러닝 알고리즘을 적용하고, 성능 평가를 통해 가짜 리뷰 탐지 모델을 개발한 연구 내용을 포함하고 있다. 이는 독창적인 연구 방법과 실험 결과를 제시하는 연구 논문에 해당한다."
Decentral and Incentivized Federated Learning Frameworks: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9997105,"The advent of federated learning (FL) has sparked a new paradigm of parallel and confidential decentralized machine learning (ML) with the potential of utilizing the computational power of a vast number of Internet of Things (IoT), mobile, and edge devices without data leaving the respective device, thus ensuring privacy by design. Yet, simple FL frameworks (FLFs) naively assume an honest central server and altruistic client participation. In order to scale this new paradigm beyond small groups of already entrusted entities toward mass adoption, FLFs must be: 1) truly decentralized and 2) incentivized to participants. This systematic literature review is the first to analyze FLFs that holistically apply both, the blockchain technology to decentralize the process and reward mechanisms to incentivize participation. 422 publications were retrieved by querying 12 major scientific databases. After a systematic filtering process, 40 articles remained for an in-depth examination following our five research questions. To ensure the correctness of our findings, we verified the examination results with the respective authors. Although having the potential to direct the future of distributed and secure artificial intelligence, none of the analyzed FLFs is production ready. The approaches vary heavily in terms of use cases, system design, solved issues, and thoroughness. We provide a systematic approach to classify and quantify differences between FLFs, expose limitations of current works and derive future directions for research in this novel domain.",No,"이 논문은 기존 연구들을 체계적으로 검토하고 분류하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합과 분석에 초점을 맞추고 있습니다."
Study of Learning Techniques for Effort Estimation in Object-Oriented Software Development,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946865,"Software effort estimation (SEE) is helpful for project managers to decide the cost and effort needed to complete the project. The techniques used for estimating effort in conventional software development are not very useful for estimating effort in object-oriented projects because of their varying nature. The machine learning (ML) approaches are achieving greater recognition in SEE research because they can demonstrate the complex relationship between software effort and other attributes. So, there is a need for a systematic literature review (SLR) that can discuss the applicability of ML techniques for SEE in object-oriented projects, which is not available in the literature. This research aims to provide a specific review and analysis of various ML-based SEE works in the object-oriented software development (OOSD) paradigm based on different perspectives: type of learning technique used, type of performance measure used, performance level achieved, the dataset used, etc. Purposefully, we have chosen appropriate articles after applying selection and quality evaluation criteria. After investigation, we found that different ML techniques have been applied in these works, and their performance is better than the classical models. Hence, more efforts are needed to encourage the application of ML techniques for SEE in the OOSD paradigm. Also, most of the works have used small-sized datasets for effort estimation in OOSD, due to which their model's performance cannot be generalized. So, the researchers should collect more large-sized datasets working in line with the software organizations.",No,"본 논문은 머신러닝 기반 소프트웨어 노력 추정 연구에 대한 체계적 문헌 리뷰(SLR)를 수행한 것으로, 기존 연구들을 분석하고 요약하는 데 중점을 두고 있다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이라기보다는 기존 연구를 종합한 리뷰 논문에 해당한다."
Adversarial Examples Identification in an End-to-End System With Image Transformation and Filters,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022936,"Deep learning has been receiving great attention in recent years because of its impressive performance in many tasks. However, the widespread adoption of deep learning also becomes a major security risk for those systems as recent researches have pointed out the vulnerabilities of deep learning models. And one of the security issues related to deep learning models is adversarial examples that are an instance with very small, intentional feature perturbations that cause a machine learning model to make a wrong prediction. There have been many proposed defensive methods to combat or detect adversarial examples but still not perfect, powerful and still need a lot of fine-tuning in the process of installing security systems. In this work, we introduce a completely automated method of identifying adversarial examples by using image transformation and filter techniques in an end-to-end system. By exploring the adversarial features that are sensitive to geometry and frequency, we integrate the geometric transformation and denoising based on the frequency domain for identifying adversarial examples. Our proposed detection system is evaluated on popular data sets such as ImageNet or MNIST and gives accurate results up to 99.9% with many optimizations.",Yes,"논문 초록에서 제안된 방법은 적대적 예제를 식별하기 위한 새로운 자동화된 시스템을 소개하며, 이미지 변환과 필터 기법을 활용한 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 시스템의 성능을 다양한 데이터셋에서 평가한 결과를 제시하여 직접적인 연구 기여가 있음을 보여준다."
Optimizing Industrial Energy Management: Advanced Simulators Enhanced With Machine Learning For Improved Forecast Accuracy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796531,"The aim is to face the problem of uncertainty of forecasting. It evaluated the integration of Artificial Intelligence (AI) into a simulator to improve its accuracy in the energy prediction, applied to industrial field. Firstly, a literature review on the main applications of AI into energy consumption forecasting was carried out. Then a case study has been taken and Random Forest has been applied to improve forecasting. The AI model improved accuracy of the prediction, being able to consider real-time data of weather and consumption. Therefore, AI has been proved to be successfully implementable for energy forecasting, in synergy with simulation.",Yes,"논문은 인공지능을 시뮬레이터에 통합하여 산업 에너지 예측의 정확도를 향상시키는 구체적인 연구를 수행하였으며, 실제 사례 연구와 Random Forest 모델 적용을 통해 예측 성능 개선을 입증하였다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하고 있음을 보여준다."
Prediction of Diabetes Patient Stage Using Ontology Based Machine Learning System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8878831,"Nowadays technology has improved the worldwide and has become vital part of our life. It aid for doctors to analyze and diagnose the medical problems and diseases. With help artificial intelligence in medicine science become high demand now. This work focuses on clinical decision support system which aid medical people to diagnose of disease. In this paper first present related work in various aspects of clinical decision support systems to provide diagnosis solutions to medical related problems. In this paper a proposed method to identify patient with diabetes disease risk level is indentified. In this work diabetes patient risk level is been detected by using ontology and machine learning technique. Ontology holds disease symptoms, causes and treatments. In machine learning, nave base algorithm is used to make decision on patient record also it defines possibilities of risk level. The proposed algorithm will be evaluated against the following metrics namely confusion matrix, precision level, mean and this proposed work is found to have better prediction level when compared with existing work.",Yes,"논문 초록에서 제안된 방법이 당뇨병 환자의 위험 수준을 예측하기 위해 온톨로지와 머신러닝 기법을 결합한 독창적인 시스템을 개발하고 있음을 알 수 있습니다. 또한, 제안된 알고리즘의 성능 평가를 통해 기존 연구와 비교한 결과를 제시하고 있어 직접적인 연구 기여가 포함된 논문으로 판단됩니다."
Strategic Decision Support Systems for Enhancing Competitive Advantage in Small and Medium Enterprises,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10779545,"This study explores the role of Decision Support Systems (DSS) in overcoming the challenges faced by SMEs, such as limited access to finance, technology, and skilled labor. Grounded in the Resource-Based View (RBV) and Porter's Five Forces model, the study addresses the problem of maintaining competitiveness in dynamic markets. The primary objectives are to assess how DSS can enhance decision-making, operational efficiency, and strategic planning in SMEs. Utilizing a systematic literature review methodology, the study synthesizes findings from various peer-reviewed sources. Key findings indicate that DSS significantly improve SMEs' strategic capabilities by facilitating data-driven decisions, enhancing agility, and optimizing resource management. The study recommends that SMEs prioritize cloud-based DSS solutions, improve data management, and invest in user-friendly interfaces. A structured computing methodology for DSS implementation is presented to ensure effective application of DSS tailored to the specific needs of SMEs, enabling real-time data-driven decisions and resource optimization. Future research should explore the integration of emerging technologies such as artificial intelligence and machine learning into DSS to further enhance their effectiveness for SMEs. Despite its contributions, the study acknowledges limitations such as reliance on qualitative data and suggests future research directions to expand and validate the findings.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 통해 기존 연구들을 종합하고 분석한 연구로, 직접적인 독창적 실험이나 데이터 수집을 통한 새로운 연구 결과를 제시하지 않는다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Review on HR digitalization and artificial intelligence contributing to smart cities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137482,"This paper aims to identify the research trends on digitalisation contributing to smart cities and the role of technology in economic and social development. Review Methodology is taken up for knowledge development in the area of digital Human Resources Management in line with AI and technology-enabled smart cities. The quality assessment of the review sample is validated through a mixed methods appraisal tool (MMAT). The review conceptualises “Smart Cities” as mainly supply-side and sector-driven, giving the private sector a lead role in problem identification and digital solution facilitating citizens with quick-service delivery. At the heart of digitalisation around smart cities is sustainableefficient- livable urban living. The key terms by peak frequency using Voyant Tools link to “HR Digitalisation” and “Smart City” includes the Internet of Things (IoT), big data analytics, artificial intelligence (AI), advanced energy storage technologies, civic technology, crewless aerial vehicles (drones) and Blockchain as an emerging technology with a substantial presence in smart cities.",No,"본 논문은 연구 동향을 식별하고 기존 연구를 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 중점을 두고 있습니다."
A Review on Sugarcane Leaf Defect Detection Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841681,"Sugarcane is a critical crop, and its productivity is significantly influenced by leaf defects caused by various diseases. Accurate and timely detection of these defects is essential for effective crop management. This review paper explores the state-of-the-art approaches for detecting sugarcane leaf defects, focusing on machine learning (ML), deep learning (DL), and transfer learning techniques. The motivation behind this review is to address the limitations of existing methods and propose future improvements. The objective is to provide a comprehensive overview of the current methodologies, evaluate their performance, and highlight areas for future research. This review aims to contribute to the development of more accurate and robust detection systems that can be effectively implemented in real-world agricultural settings.",No,"본 논문은 기존 연구들을 종합하여 설문하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 현황과 한계를 정리하는 데 중점을 두고 있습니다."
Machine Learning Algorithms for Smart Healthcare: Breast Cancer Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912353,"Background: Breast cancer continues to be a significant concern for women worldwide, emphasizing the crucial role of early detection in improving treatment outcomes. The machine learning techniques play an important role in early detection of breast cancer. Detection of breast cancer with traditional techniques is time-consuming and does not provide accurate results. So to enhance the accuracy of detection and to decrease time taken, an automatic breast cancer technique is required. This work introduced innovative machine-learning algorithms for the detection of breast cancer. Machine learning algorithms used in this paper are Neural Networks (NN), Logistic Regression (LR), Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), and K-Nearest Neighbor (KNN) with a widespread set of related parameters. The dataset taken for the work contains many patient demographics, clinical details, and histopathological information to train and validate machine learning algorithms. For the evaluation of the work accuracy and loss are calculated. The Logistic Regression outperformed with remarkable accuracy 97.4%, over the mentioned five other algorithms. All five models are compared to verify and validate the experimental results. The proposed work requires very less human intervention and attained high accuracy in considerable time. In future, for validation and its applications in clinical settings, the author can design a hybrid technique that can handle real-time clinical data.",Yes,"본 논문은 유방암 조기 진단을 위해 여러 머신러닝 알고리즘을 적용하고, 데이터셋을 사용해 모델을 학습 및 평가한 독창적인 연구 내용을 포함하고 있다. 또한 다양한 알고리즘의 성능을 비교 분석하여 직접적인 연구 기여를 하고 있으므로 연구 논문에 해당한다."
Advancements in Skin Cancer Diagnosis: A Literature Survey and Hybrid Approach Employing SVM and DNN Models with Results Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871822,"This study combines a thorough literature review with the development of a novel framework for image analysis to present a comprehensive study in the field of skin cancer detection. Traditional machine learning, specifically Support Vector Machines (SVM), is combined with deep learning via a Deep Neural Network (DNN) in this methodology. The proposed framework starts with users loading skin cancer images via a user-friendly graphical interface, with the option of including ground truth images for reference. Preprocessing steps include grayscale conversion, Gaussian Low Pass Filter (GLPF) application, and segmentation via thresholding and region growing. When a ground truth image is provided, the segmented image is evaluated for accuracy. For further analysis, image features such as area, perimeter, color features, variance, and symmetry are extracted. The script prepares data for both SVM and DNN models during the training phase, using traditional feature extraction and deep learning on the Skin Cancer dataset. The trained SVM model and DNN are used to categorize features extracted from the input skin cancer image, with the predicted class (Benign or Malignant) displayed in the GUI. The graphical interface displays visualizations of the input image, ground truth, preprocessing results, and segmentation results, as well as quantitative metrics such as accuracy, sensitivity, and specificity for in-depth analysis. Initial experimental results show that the combined SVM and DNN approach is effective in skin cancer detection, laying the groundwork for future research and refinement in this critical domain.",Yes,논문은 기존 문헌 조사를 포함하면서도 SVM과 DNN을 결합한 새로운 프레임워크를 개발하고 실험 결과를 제시하는 등 독창적인 연구 내용을 포함하고 있습니다. 따라서 직접 기여하는 연구 논문에 해당합니다.
"Harmony in Agriculture: Crafting an Integrated Framework for Optimal Crop, Irrigation and Fertilizer Recommendations - A Comprehensive Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724530,"This comprehensive review paper synthesizes a variety of research findings in the domains of crop recommendation, fertilizer application, and water management. The aim is to offer a holistic perspective on the integrated approach to optimizing agricultural productivity. The review explores the intricate relationships among these three fundamental components, investigating synergies and trade-offs within their application. In the first section, the review navigates through studies on crop recommendation systems, examining advancements in precision agriculture, machine learning, and data analytics. It evaluates the effectiveness of various models in predicting suitable crops based on soil types, climate conditions, and historical data. Special emphasis is placed on the evolving landscape of digital agriculture and the role of emerging technologies in shaping modern crop recommendation frameworks. Moving forward, the review explores the complex dynamics of fertilizer application, encompassing studies on nutrient management strategies, soil health considerations, and the ecological impacts of different fertilization practices. Attention is given to sustainable approaches that balance high yields with environmental stewardship, considering the long-term viability of agricultural systems. The third section of the review focuses on water recommendation systems, analyzing research on efficient irrigation methods, water use efficiency, and technologies for real-time monitoring of soil moisture. It assesses evolving strategies to optimize water resources, particularly in regions facing water scarcity and climate variability. Throughout the review, critical insights are drawn from key research papers, addressing gaps in existing knowledge and identifying avenues for future investigations. By combining information from these three crucial domains, this review contributes to the development of a unified framework for sustainable and precision agriculture, enhancing our understanding of the complex interactions between crop selection, fertilizer application, and water management.",No,"논문 초록에서 명확히 ""comprehensive review paper""라고 명시되어 있으며, 기존 연구들을 종합하고 평가하는 내용임을 알 수 있습니다. 따라서 직접적인 독창적인 연구 결과나 실험을 포함한 연구 논문이 아니라 기존 연구를 정리한 리뷰 논문에 해당합니다."
Enhancing Freelancer Project Matching with a BERT-Powered Deep Learning Indonesian Chatbot,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10732347,"The rapid expansion of the freelancing sector has underscored the necessity for more efficient project matching systems. Traditional methods, which largely depend on keyword searches and manual reviews, are often slow and error-prone. This paper is developing and evaluating Indonesian chatbot powered by Bidirectional Encoder Representations from Transformers (BERT), aimed at enhancing freelancer project matching. The proposed chatbot utilizes advanced natural language processing (NLP) techniques to deliver personalized project recommendations based on users' skills and experiences. It leverages BERT’s bidirectional training approach for better context understanding and integrates Long Short-Term Memory (LSTM) networks to maintain context over extended interactions. The system’s adaptability allows for continuous learning and updating, accommodating new fields and trends. Data was collected using ChatGPT 4.0 and was processed to create a dataset focused on Information Technology, Business Management, and Graphic Design. The models, including Recurrent Neural Networks (RNN) and LSTM, demonstrated impressive performance with LSTM showing superior results in 97% accuracy and context retention in the chatbot. This research highlights the advantages of using BERT with deep learning models for accurate and efficient project matching, providing a scalable solution for the evolving freelancing landscape. Future work will explore expanding the system to other languages and domains to further enhance its applicability.",Yes,"논문은 BERT와 LSTM을 활용한 인도네시아어 챗봇 개발 및 평가라는 독창적인 연구 내용을 포함하고 있으며, 데이터 수집, 모델 적용, 성능 평가 등 직접적인 연구 기여를 명확히 제시하고 있다. 따라서 연구 논문에 해당한다."
Multilingual Receipt Image Preprocessing Optimization for OCR,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10908135,"Data extracted from receipts are necessary for various applications in many industries, and this extraction process can be significantly simplified through deep learning. However, high-quality receipt images are essential for algorithms to produce accurate models. This paper focuses on the techniques to improve the image preprocessing process and the information extraction methods applied in Vietnamese and English. The paper will provide an overview of the research question and scope of the review, followed by a literature review of related works. The paper then describes the image preprocessing techniques used in the research, including measuring image blur using the Fuzzy C-Means clustering and removing the background through thresholding techniques such as brightness and chromaticity distortion. Finally, the paper discusses the information extraction methods employed in Vietnamese and English, including using two specialized models. ViptOCR and Convolutional Neural Network.",Yes,"논문은 이미지 전처리 기법과 정보 추출 방법을 구체적으로 설명하며, Fuzzy C-Means 클러스터링과 임계값 기법을 활용한 독창적인 연구 내용을 포함하고 있다. 또한 베트남어와 영어에 특화된 모델을 적용한 실험적 접근을 다루고 있어 연구 논문에 해당한다."
Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043731,"Domain-specific hardware is becoming a promising topic in the backdrop of improvement slow down for general-purpose processors due to the foreseeable end of Moore's Law. Machine learning, especially deep neural networks (DNNs), has become the most dazzling domain witnessing successful applications in a wide spectrum of artificial intelligence (AI) tasks. The incomparable accuracy of DNNs is achieved by paying the cost of hungry memory consumption and high computational complexity, which greatly impedes their deployment in embedded systems. Therefore, the DNN compression concept was naturally proposed and widely used for memory saving and compute acceleration. In the past few years, a tremendous number of compression techniques have sprung up to pursue a satisfactory tradeoff between processing efficiency and application accuracy. Recently, this wave has spread to the design of neural network accelerators for gaining extremely high performance. However, the amount of related works is incredibly huge and the reported approaches are quite divergent. This research chaos motivates us to provide a comprehensive survey on the recent advances toward the goal of efficient compression and execution of DNNs without significantly compromising accuracy, involving both the high-level algorithms and their applications in hardware design. In this article, we review the mainstream compression approaches such as compact model, tensor decomposition, data quantization, and network sparsification. We explain their compression principles, evaluation metrics, sensitivity analysis, and joint-way use. Then, we answer the question of how to leverage these methods in the design of neural network accelerators and present the state-of-the-art hardware architectures. In the end, we discuss several existing issues such as fair comparison, testing workloads, automatic compression, influence on security, and framework/hardware-level support, and give promising topics in this field and the possible challenges as well. This article attempts to enable readers to quickly build up a big picture of neural network compression and acceleration, clearly evaluate various methods, and confidently get started in the right way.",No,"이 논문은 다양한 신경망 압축 및 하드웨어 가속 기법들을 종합적으로 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구들을 체계적으로 요약하고 분석한 리뷰 논문에 해당합니다."
A Survey of Punjabi Language Translation using OCR and ML,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10112285,"The use of Natural Language Processing (NLP) in machine translation has grown in significance as technologies as well as computers have become more prevalent in our daily lives. This essay's goal is to evaluate the work done in the field of natural language processing (NLP) to translate the Gurmukhi script, which is primarily used to represent the Punjabi language. Together with Hindi and Punjabi, the languages written throughout the Gurumukhi script has more than 1.5 billion native users, making it among the most used language worldwide. However, there is limited research available on the Gurmukhi script. The study presented in this paper focuses on using machine learning and AI techniques to translate written Gurmukhi text into Hindi and English languages. The input for the translation process will be in the form of machine-printed Gurmukhi text, while the output will be provided in both Hindi and English languages. The goal of this study is to enhance how well global languages absorb information and technology, making it easier and more accurate to translate from regional languages. In conclusion, the paper provides a comprehensive review of the work related to the translation of Gurmukhi script using NLP techniques, with a focus on the use of machine learning and AI. The findings of this study can contribute to the development of more advanced and accurate translation systems, making the exchange of information and knowledge more accessible for people who speak different languages.",No,"초록에서 본 논문은 기존 연구들을 종합적으로 검토하는 설문조사(survey) 논문으로 보이며, 직접적인 독창적 연구 결과나 새로운 실험, 모델 개발에 대한 언급이 없습니다. 따라서 연구 논문이라기보다는 기존 연구를 정리한 리뷰 논문에 해당합니다."
VMMISD: An Efficient Load Balancing Model for Virtual Machine Migrations via Fused Metaheuristics With Iterative Security Measures and Deep Learning Optimizations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459085,"As cloud computing continues to evolve, the need for efficient and secure management of virtual machine (VM) migrations has become increasingly evident. Traditional models often fall short in optimizing load balancing and energy consumption while ensuring a high level of security. In this work, we propose the load balancing and energy-efficient migration model, an innovative approach that leverages bioinspired algorithms and advanced security measures to optimize VM migrations. The initial novelty of our model is the integration of Genetic Algorithms with Ant Colony Optimization for resource scheduling operations. These algorithms were specifically chosen for their proven effectiveness in solving complex optimization problems by simulating natural processes. Additionally, our model incorporates a deep reinforcement learning-based iterative-learning contextual side chaining model to enhance security measures. This approach not only learns and adapts to new security threats over time but also utilizes contextual side-chaining to link related security events, thereby providing a robust defense mechanism against potential threats. The affinity between VMs and physical machines is quantified using K Means clustering and fuzzy logic, which ensures optimal load balancing while accounting for the uncertainty inherent in the migration process. Furthermore, we employ bidirectional long short-term memory networks with recurrent graph neural network, for accurate workload prediction and informed migration decision making process. The selection of these techniques is grounded in their proven capability to analyze historical data and predict future trends with high accuracy levels. Our proposed model demonstrates marked improvements in several key performance metrics. We achieved a 4.5% reduction in makespan, a 4.9% increase in deadline hit ratio, and a 3.9% improvement in task diversity. Furthermore, computational complexity was reduced by 8.3%, VM migration efficiency improved by 2.5%, and the delay of computation was significantly reduced by 9.5%. Importantly, the integration of the Iterative-learning Contextual Side chaining Model significantly enhanced the security and quality of service (QoS) under attack scenarios, resulting in a 10.4% improvement in response speed, a 2.5% reduction in energy consumption during block mining, a 3.9% improvement in throughput, and an 8.5% reduction in storage costs. This load balancing and energy-efficient migration model represents a significant advancement in addressing the challenges of load balancing, energy efficiency, and security in VM migrations. Through the meticulous integration of bioinspired algorithms, advanced security measures, and machine learning techniques, our model provides a comprehensive and innovative solution that markedly improves system performance, reduces energy consumption, and fortifies security, thereby paving the way for a more efficient and secure cloud computing ecosystem.",Yes,"본 논문은 유전 알고리즘, 개미 군집 최적화, 딥 강화학습 등 다양한 기법을 융합하여 가상 머신 마이그레이션의 부하 분산과 보안 문제를 해결하는 새로운 모델을 제안하고 있으며, 성능 개선 수치와 보안 향상 결과를 구체적으로 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문임을 명확히 보여준다."
Improving the DDoS Protection and A.I. Systems Using Wireshark,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469319,"Cyber-attacks like Distributed Denial of Service (DDoS) and Denial of Service (DoS) negatively affect the functioning of a computer or a server by overloading the victim with data packets. The aim of the study is to formulate a strategy to improve the protection against such attacks and also improve the performance of the hardware as well as the Artificial Intelligence (A.I.) based systems while also filling the gap of conducting a comparative analysis of network analysis tools in order to find the most optimal network analysis tool, and settings to help with the protection against cyber-attacks. The role of Wireshark network analysis software in the reduction of the harmful effects on the hardware, and identification of the origin of such attacks is evaluated. The PC performance aspects like Memory, HDD, CPU, and Disk are benchmarked, analyzed and the effectiveness of the network analysis tools is observed through empirical evaluation. Data of primary type is obtained through experimentation while data of secondary type is collected by a literature review of the recent research papers. The main research results show that Wireshark can improve the performance of the hardware at least by 5% during an attack when compared with the scenario of not using a network analysis software. Through hypothesis testing and comparative analysis, it is concluded that Wireshark is the most optimal network analysis software which is also found to be less burdensome on hardware than other network analysis tools. Cost Effective Protection Framework is created to improve the monitoring, analysis and protection capabilities of users that use A.I. systems as well as those that do not use it. The future research is possible by expanding the framework to cover more users, more A.I. systems, testing more network analysis tools, and discovering more filters to improve the protection capabilities.",Yes,"본 논문은 Wireshark를 활용한 DDoS 공격 방어 성능 향상과 하드웨어 및 AI 시스템의 성능 개선에 대한 실험적 평가와 가설 검증을 포함하고 있어 직접적인 연구 기여와 독창적인 실험 결과를 제시하고 있다. 또한, 비교 분석과 새로운 보호 프레임워크 제안 등 연구 내용이 포함되어 있어 연구 논문으로 판단된다."
Crime Prediction by Comparing Machine Learning and Deep Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10489578,"Preventing crime is important for justice and safety in cities. Using computers to predict crime trends can help make cities safer. Reliable real-time crime prediction is necessary for public safety, but there are still difficulties facing science. Crime rates are influenced by numerous intricate factors. Crime is low in comparison to many predictable events. For computer systems, determining the criminal activity rate, kinds, and hotspots based on historical patterns is both a challenge and an opportunity. The accuracy of machine learning using SVM stacking, KNN, Naïve Bayes, Random Forest, and deep learning using LSTM are compared in this study. This paper presents a comprehensive review of the literature on deep learning and machine learning approaches to crime prediction. It's an important tool for scholars studying the subject because it provides information that law enforcement organizations can use to improve how they prevent and deal with criminal activity. Moreover, the system outlined in the paper offers predictions about potential future crimes, allowing for proactive measures to be taken to prevent them.",Yes,"논문 초록에서 다양한 머신러닝 및 딥러닝 알고리즘을 사용하여 범죄 예측의 정확도를 비교하는 연구를 수행한 점이 명확히 드러나 있습니다. 또한, 기존 문헌 리뷰뿐만 아니라 실제 예측 시스템을 제안하여 미래 범죄를 예측하는 데 기여하고 있으므로 독창적인 연구 내용이 포함된 연구 논문으로 판단됩니다."
Smart Posture Monitoring and Predictive Health Classification for Bedridden Patients Using IoT and AI,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845586,"Posture monitoring has emerged as a critical area of research and application, particularly considering increasing concerns about musculoskeletal disorders and their association with prolonged sitting and poor ergonomic practices. This study investigates the importance of posture monitoring and classification using a comprehensive dataset that includes demographic, and physiological, variables such as age, height, activity level, pain indicators, and personality traits. A pressure mapping system was used to gather data and various AI algorithms were applied to classify in-bed posture using the mined features of the dataset. With different in-bed postures, the study reached high classification accuracy. The performance of these algorithms was then evaluated with machine learning algorithms on the posture dataset. A dataset for posture monitoring and classification study is introduced for accurate and reliable monitoring and classification of these modern health and well-being issues. Relations between features and posture types are identified through the analysis and preprocessing of the dataset. We implemented the classification model using machine learning algorithms and evaluated the model performance evaluation using standard metrics like accuracy, precision, recall, and F1 Score. This paper details current posture assessment methods, providing an overview as well as a background for postural health management and ergonomic solutions. This study aims to propose an implementable posture classification model for a comprehensive assessment of body posture that may help bedridden patients have better health control and a better quality of life. This work might provide more effective care strategies to minimize the risks related to immobility. To achieve, both real-time monitoring and proactive health management, a smart classification monitoring system employs machine learning and Internet of Things technology to automatically identify, classify, and provide actionable feedback on user postures.",Yes,"논문은 IoT와 AI를 활용한 새로운 자세 모니터링 및 분류 시스템을 제안하고, 실제 데이터셋을 수집하여 다양한 머신러닝 알고리즘을 적용해 성능을 평가하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 자세 분류 모델 구현과 평가를 통해 직접적인 연구 기여를 하고 있음을 보여준다."
Machine Learning based Clustering for Identifying Power Quality Events,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033785,"This paper presents an online power quality disturbance identification technique. The AI technique is based on unsupervised machine learning clustering algorithm and pattern recognition approach. The unsupervised machine learning is presented as a powerful tool for clustering application in order to recognize sag, swell and interruption for power quality applications. The K-means clustering algorithms automatically cluster similar dataset samples together and assign every training samples to its closest centroid. In addition to the training based applications, the classifier just needs training at an earlier stage and finally the classifier guarantee machine-like work all the time, conditional upon his past training. Also, the current ongoing critical theoretical contextual of AI application for society and modern power system is addressed in detail. The validated approach is very accurate, flexible to apply for any power system related problems and reasonably fast in identification online based application.",Yes,"논문 초록에서 제시된 내용은 비지도 학습 기반 클러스터링 알고리즘을 활용한 전력 품질 이상 탐지 기법을 제안하고 있으며, 구체적인 알고리즘 적용과 검증 결과를 포함하고 있다. 이는 독창적인 연구 방법과 실험적 검증을 포함한 연구 논문으로 판단된다."
SnapCode - A Snapshot Based Approach to Code Stylometry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9031980,"Artificial neural networks have seen significant advancements in recent times with the growing popularity of deep learning. Deep learning allows us to learn representations that are otherwise difficult to extract and helps in better classification tasks. Images, videos and speech processing are the major areas where deep learning is applied. Our work is related to the application of deep learning to source codes. Previous works in this domain have failed to easily capture structural and behavioral aspects of the code. Thereby relying on manual feature engineering for applications like author identification, code quality analysis, cyber-attack investigation, malware recognition and plagiarism detection. We propose a novel approach to capture these feature representations by processing snapshots of code instead of processing source code token by token. We, therefore, propose SnapCode, a snapshot-based approach to extract deep convolutional features from text which would otherwise be impossible using currently known approaches. SnapCode uses a deep convolutional neural network coupled with transfer learning to learn the structural representation of the source code. We show that simple networks fail to learn these features and deep network coupled with transfer learning gives us the best results. SnapCode can capture behavioral aspects of source code as we employ it to the task of author detection, also known as ""code stylometry"". We choose author detection to validate our approach as it requires most number of manual and complicated features. Although source code is simply text, we aim to process text data in a way similar to humans and show that we could learn meaningful representations.",Yes,"본 논문은 기존 연구의 한계를 지적하고, 코드 스냅샷을 활용한 새로운 딥러닝 기반 특징 추출 방법인 SnapCode를 제안하여 직접적인 연구 기여를 하고 있다. 또한, 제안한 방법을 저자 식별(코드 스타일로메트리) 문제에 적용하여 성능을 검증하는 실험적 결과를 포함하고 있어 독창적인 연구 내용이 포함된 논문으로 판단된다."
Information security issues in educational institutions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141014,"The information is currently constituted as an asset within the institutions, whether public or private, and with the technological advance the security of this information is put at risk, due to the set of threats that are presented on the Internet, many of the institutions have tools to prevent these risks. The present work is a systematic review of literature (SRL) on the problems that arise in information security in Higher Education also security measures used to protect it and the tools that are used to detect and battle informatic attacks. For the effect, 35 scientific articles from the last five years were investigated, analyzed and compiled, extracted from four most relevant libraries, being these: ScienceDirect, Taylor & Francis, Esmerald Group Publishing y AIS eLibrary. These scientific articles answer four questions posed within the bibliographic research that was carried out. With the results obtained, it was possible to identify the security issues, the policies and therefore the existing relationship of these factors with the organizational culture and, in that way safeguard the security of information in HEIs.",No,"본 논문은 직접적인 실험이나 새로운 연구 결과를 제시하는 것이 아니라, 기존 문헌을 체계적으로 검토하고 분석한 문헌 리뷰(Systematic Review of Literature) 연구입니다. 따라서 독창적인 연구 내용보다는 기존 연구를 종합하는 데 중점을 두고 있습니다."
A Novel Web Platform for COVID-19 diagnosis using X-Ray exams and Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533469,"Modern computer vision techniques applied to radiographic studies are presented as an alternative to assist the specialist in screening and diagnosing the respiratory syndrome (SARS-CoV-2), assisting in clinically severe cases, such as acute pneumonia, acute respiratory failure, organ failure, and death. This work proposes a screening method based on the Internet of Medical Things (IoMT) based on deep learning techniques for the classification of COVID-19 from chest X-ray (CXR) exams. The proposed system called Computer-Aided Remote medical diagnostics System (CARMEDSys) applied to the diagnosis of COVID-19 consists of three main stages: 1) segmentation of the lung region in X-ray images, 2) deep extraction of attributes from the filtered pulmonary area and 3) Prediction patient status with machine learning assistance. The performance of CARMEDSys was evaluated considering twelve different deep neural networks, via the transfer of learning. Besides, the performance of this approach is evaluated against recent studies for the classification of healthy patients, with pneumonia, or with COVID-19. The evaluation methodology considered two different sets of radiographic images, reaching Sensitivity (99.97%), F1-Score (99.43%), and Accuracy (98.89%) promising to distinguish patients with pneumonia and COVID-19 combining DenseNet201 as attribute extractor with Support Vector Machine with radial basis function, exceeding up to 12.31 % sensitivity for prediction of COVID-19 recent related works.",Yes,"논문은 COVID-19 진단을 위한 새로운 웹 플랫폼과 딥러닝 기반 분류 방법을 제안하며, 구체적인 시스템 설계와 성능 평가를 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한 다양한 신경망 모델을 적용하고 기존 연구와 비교 분석한 점에서 직접적인 연구 기여가 명확하다."
Machine Learning and Clinical Insights Analysis of BMI Dataset Predictive Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503402,"Machine learning (ML) has developed at a superlative rate, accompanying requests spanning various fields. This research investigates the experience of strength data, exceptionally the request of machine learning (ML) algorithms to a Body Mass Index (BMI) dataset. The basic aim of searching out unwinds the dossier’s many linkages and patterns, eventually chief to more thorough information of the variables deciding BMI. The study starts accompanying an initiation to the subject within reach, understood by a thorough study of appropriate work, a complex mechanics division, and an itemized reason of the reached results. However, because of advances in Machine Learning, we immediately have the talent to handle this issue in a more excellent manner. We’ve built an advance dossier-study system that can think a patient has diabetes, a suggestion of correction, admitting for early mediation. This predicting plan uses dossier analysis methods to extractable intuitions from a big number of diabetes-accompanying facts. Its basic aim is to correctly determine a patient’s risk of diabetes. We’ve working categorization plans to a degree Decision Tree, Artificial Neural Networks (ANN), Naive Bayes, and Support Vector Machine (SVM) algorithms to cultivate the model. These outcomes show the influence of the subsystems in thinking diabetes risk admit a large size of veracity. This predictive finish can create a meaningful dissimilarity in labeling at-risk things early and providing bureaucracy with essential care and counseling before the ailment progresses. In summary, our machine intelligence-located scheme offers a natural still strong solution to call the risk of diabetes in subjects. By controlling the wherewithal of dossier reasoning and categorization algorithms, we can enhance early discovery and deterrent measures for this weighty affliction, eventually reconstructing patient consequences and reducing the burden of BMI-related complications.",Yes,"논문은 BMI 데이터셋을 활용하여 머신러닝 알고리즘을 적용하고, 당뇨병 위험 예측 모델을 개발하는 독창적인 연구 내용을 포함하고 있습니다. 또한 다양한 분류 알고리즘을 실험하고 결과를 분석하여 새로운 예측 시스템을 제안하고 있으므로 연구 논문에 해당합니다."
A conceptual analysis for the measurement of stress intensity by deep learning using EEG signals,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865846,"Several Health-related issues are caused due to mental stress. In Initial stages the level of mental stress can be measured using tools which are developed by Physicians and Scientists. Many neuroimaging tools have been suggested to measure the stress developed in the workplace. Electroencephalogram (EEG) signals are a key component as they contain vast knowledge related to conditions and states of mind. Existing methods for EEG Signal analysis are discussed to measure level of stress in the paper. This review focuses the crucial variations in-between the research results and debates those fluctuations from the data analysis methods give rise to many contrary results. The fluctuations in results may be because of several factors like no proper protocol, the area of interest within the brain, stressor types, experiment period, EEG analysis, methodology used by which properties are extracted, and classifier type. Hence, the major factor relating to detection stress in mind, by choosing the major suitable properties. A convoluted come varied band of properties relating to EEG, with variation in time, working, and dynamic connections of brain, required inclusion of different methods to learn about their connections with mental stress. Subsequently, the evaluation recommends merging cortical activations with the integration to the network calculates with approaches of deep learning to enhance the correctness range of mental stress calculation.",No,초록에서 해당 논문은 기존 연구 결과들을 검토하고 EEG 신호 분석 방법들의 차이점과 문제점을 논의하는 개념적 분석 및 리뷰 논문임을 명확히 하고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여가 포함된 연구 논문으로 보기 어렵습니다.
Deep Learning for Automatic Vision-Based Recognition of Industrial Surface Defects: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113226,"Automatic vision-based inspection systems have played a key role in product quality assessment for decades through the segmentation, detection, and classification of defects. Historically, machine learning frameworks, based on hand-crafted feature extraction, selection, and validation, counted on a combined approach of parameterized image processing algorithms and explicated human knowledge. The outstanding performance of deep learning (DL) for vision systems, in automatically discovering a feature representation suitable for the corresponding task, has exponentially increased the number of scientific articles and commercial products aiming at industrial quality assessment. In such a context, this article reviews more than 220 relevant articles from the related literature published until February 2023, covering the recent consolidation and advances in the field of fully-automatic DL-based surface defects inspection systems, deployed in various industrial applications. The analyzed papers have been classified according to a bi-dimensional taxonomy, that considers both the specific defect recognition task and the employed learning paradigm. The dependency on large and high-quality labeled datasets and the different neural architectures employed to achieve an overall perception of both well-visible and subtle defects, through the supervision of fine or/and coarse data annotations have been assessed. The results of our analysis highlight a growing research interest in defect representation power enrichment, especially by transferring pre-trained layers to an optimized network and by explaining the network decisions to suggest trustworthy retention or rejection of the products being evaluated.",No,본 논문은 220편 이상의 기존 연구를 종합하여 딥러닝 기반 산업 표면 결함 인식 분야의 현황과 발전을 리뷰하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과를 제시하기보다는 기존 연구들을 분류하고 분석하는 데 중점을 두고 있습니다.
Graph Neural Networks for Intrusion Detection: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123384,"Cyberattacks represent an ever-growing threat that has become a real priority for most organizations. Attackers use sophisticated attack scenarios to deceive defense systems in order to access private data or cause harm. Machine Learning (ML) and Deep Learning (DL) have demonstrate impressive results for detecting cyberattacks due to their ability to learn generalizable patterns from flat data. However, flat data fail to capture the structural behavior of attacks, which is essential for effective detection. Contrarily, graph structures provide a more robust and abstract view of a system that is difficult for attackers to evade. Recently, Graph Neural Networks (GNNs) have become successful in learning useful representations from the semantic provided by graph-structured data. Intrusions have been detected for years using graphs such as network flow graphs or provenance graphs, and learning representations from these structures can help models understand the structural patterns of attacks, in addition to traditional features. In this survey, we focus on the applications of graph representation learning to the detection of network-based and host-based intrusions, with special attention to GNN methods. For both network and host levels, we present the graph data structures that can be leveraged and we comprehensively review the state-of-the-art papers along with the used datasets. Our analysis reveals that GNNs are particularly efficient in cybersecurity, since they can learn effective representations without requiring any external domain knowledge. We also evaluate the robustness of these techniques based on adversarial attacks. Finally, we discuss the strengths and weaknesses of GNN-based intrusion detection and identify future research directions.",No,이 논문은 그래프 신경망을 이용한 침입 탐지 분야의 기존 연구들을 종합적으로 정리한 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구들의 리뷰와 분석에 초점이 맞춰져 있습니다.
An Application of Transfer Learning Techniques in Identifying Herbal Plants in Sri Lanka,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842681,"Sri Lanka has a considerable collection of plant species that have been utilized for generations as medicinal treatments. Knowledge regarding herbal plants is restricted mainly among practitioners in traditional medicine. Available systems studied; had no proper methodology to search information regarding herbal plants, which can be identified through analyzing an image of an herbal plant given. Systematic literature review was done based on herbal plants in Sri Lanka, transfer learning and plant image recognition and two open ended interviews were conducted with traditional medicine practitioners. As main objective of the study, reorganization of Information was done building a technique to enhance capability of identifying herbal plants based on deep convolutional neural networks and image processing techniques which would ultimately assist more locals with identification. Five herbal plant types were chosen to analyze further in detail and the images of the plants were acquired from web and also images photographed via 13MP camera creating a data set validated through traditional medical practitioners. Images were preprocessed and retrained on Inception-v3, Resnet, MobileNet and Inception Resenet V2 based on transfer learning. Algorithm was fine-tuned using image processing techniques for preprocessing and prototype was tested 5 times reaching highest average accuracy of 95.5% on Resnet for the identification of 5 different plant types. Conclusively, this study enhanced the capability of searching herbal plants by reorganizing the information.",Yes,"본 논문은 전이 학습과 딥러닝 기반 이미지 인식 기법을 활용하여 스리랑카의 약초 식물을 식별하는 새로운 방법론을 제안하고, 직접 수집한 데이터셋과 실험을 통해 모델을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 연구 논문에 해당한다."
Prediction of yield and diseases in crops using vegetation indices through satellite image processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10717792,"Crop yield prediction represents a crucial tool used by farmers to optimize the management of their crop areas and maximize their income. It allows farmers, governments, and other relevant actors to make informed decisions about planting planning, production management, resource allocation and crop marketing. On the other hand, early detection of diseases in crops allows quick and precise measures to be taken to control the spread of the disease.In this context, the objective of this work was to develop crop yield and disease prediction models through computational processing of satellite images implementing vegetation indexes (VI) and multiple linear regression mathematical models.In this study, satellite images from the Landsat 8 and Sentinel-2 constellation were used to analyze agricultural production farms during the productive periods between 2019 and 2024. These images were used to calculate the Normalized Differentiated Vegetation Index (NDVI) in areas of cucumber, bean and corn crops located in the region of Comayagua, Honduras. NDVI had particularly close relationships with crop growth stages and served as an indicator of phenological stages. Finally, mathematical models were developed to predict crop yield and diseases through multiple regression analysis using field reference data. The yield prediction models for the cucumber crop reached an accuracy of 98.92%, while for bean it was 96.74% and for corn 98.87%. Soil reference yield data sets from the years 2019 to 2024 were used to compare with yields predicted by mathematical models. In addition, field reports from the period 2019 to 2024 were used to validate the effectiveness of VI for the detection of crop diseases. This prediction methodology can be automated using artificial intelligence (AI) software, allowing the execution of these tasks efficiently and automatically.The findings suggest that the NDVI can be used to predict yields and identify diseases in crop areas. Mathematical multiple regression methods applied with time series satellite images can achieve reliable models. These models being used by farmers could be used to improve logistics and decision making related to agricultural production. The utilization of satellite images combined with vegetation indices represents a powerful tool for contemporary agricultural management. It offers a comprehensive and dynamic perspective on crop conditions, enhancing the capability to adapt to fluctuations and optimize agricultural production.",Yes,"본 논문은 위성 이미지와 식생 지수를 활용하여 작물 수확량과 질병 예측 모델을 개발하는 독창적인 연구를 수행하였으며, 다중 회귀 분석을 통해 예측 정확도를 검증하는 등 직접적인 연구 기여를 포함하고 있다. 따라서 연구 논문에 해당한다."
Hybrid Horizons: Advancing Water Potability Prediction Through Hybrid Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10625242,"Ensuring access to safe drinking water is a critical global concern with significant implications for public health. This paper investigates the application of the hybrid machine learning model in assessing water potability, offering a comprehensive review of current methodologies and prospects. With water quality assessment a critical component of public health management, integrating machine learning techniques shows promising avenues for improving accuracy, efficiency, and predictive capabilities. This paper synthesizes existing literature on machine learning models in water quality analysis, highlighting various approaches, such as supervised and hybrid machine learning models utilized for water potability assessment. Furthermore, it examines using diverse data sources, including the pH level of the water, water hardness, total dissolved solids in the water, Chloramines concentration, sulfate concentration, electrical conductivity, organic carbon content, Trihalomethanes concentration, and turbidity level to enhance model performance and robustness. Our experiment results on the Water Quality and Potability dataset show that the proposed hybrid machine learning model achieved 68% classification accuracy compared to traditional supervised machine learning techniques. By critically evaluating the strengths and limitations of supervised and hybrid machine learning models, our research contributes to the ongoing discourse on leveraging technology to safeguard water quality and public health, ultimately fostering sustainable water management practices.",Yes,"논문은 기존 문헌을 종합하는 동시에 하이브리드 머신러닝 모델을 제안하고, 이를 실제 데이터셋에 적용하여 성능을 평가한 실험 결과를 포함하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문으로 판단된다."
AI Models for Defect Detection in Lean Manufacturing: A Comparative Study of Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10875971,"In this comparative study, we evaluate deep learning techniques for defect detection within lean manufacturing settings. Our methodical literature review identified key deep learning architectures—CNNs, R-CNNs, and YOLO variants—for their track record in accurate defect identification. Employing a standardized dataset, models were assessed against lean manufacturing criteria: accuracy, precision, recall, and processing speed. The analysis revealed that while DCNNs offer high precision in detecting surface defects, R-CNNs decrease false negatives, vital for production reliability. Models like FCNNs show promise for rapid defect recognition, an essential component for maintaining manufacturing throughput. Our findings suggest these techniques can significantly contribute to the goals of lean manufacturing by enhancing defect detection and optimizing quality control processes. Future research should continue to refine AI model integration and scalability within varied manufacturing environments, emphasizing adaptability and efficiency to support lean manufacturing principles. This study provides a foundational understanding of AI's potential in improving defect detection, a step towards integrating cutting-edge technology in manufacturing systems for enhanced quality assurance and waste reduction.",Yes,"논문은 딥러닝 기법을 활용한 결함 탐지 성능을 표준화된 데이터셋으로 평가하고 비교하는 독창적인 연구를 수행하고 있다. 또한, 다양한 모델의 정확도, 정밀도, 재현율, 처리 속도 등을 분석하여 제조 공정에 적용 가능한 실질적 기여를 하고 있으므로 연구 논문에 해당한다."
Applications of IoT and Artificial Intelligence in Water Quality Monitoring and Prediction: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358675,"Currently, internet of things (IoT) devices like environmental sensors are used to capture real-time data that can be viewed and interpreted via a visual format supported by a server computer. However, to facilitate modeling and forecasting, artificial intelligence (AI) techniques are effective in statistically analyzing complex non-linear systems and a large amount of historical data series within a short period. This present review article covers selected research journals published from 2014 to 2020. The findings from previous research indicate that despite the limitations of artificial neural network (ANN) tools, ANN has proved to be useful and powerful techniques that can be used in the field of hydrology. Similarly, ANN tools have the ability to evaluate historical data collected from different river stations and wastewater treatment plants with minimum errors within a short time. Therefore, based on the selected past literature used for this review we found that different types of ANN algorithm such as feed-forward backpropagation (FFBP) algorithm, gradient descent, Broyden-Fletcher-Goldfarb-Shanno (BFGS), conjugate gradient, radial basis function neural networks (RBFNN), neural network fitting (NNF), cascade forward back propagation (CFBP), ensemble ANN (EANN) and single AAN (SANN) have been employed in the prediction and monitoring of water quality parameters with satisfactory outcome. Furthermore, modeling alongside forecasting of water quality parameters would act as a big leap for government agencies and independent organisations in monitoring, decision making and regulating waste discharged into natural water bodies in order to achieve a safe and improved water quality for users.",No,본 논문은 2014년부터 2020년까지 발표된 연구들을 종합하여 IoT와 AI의 수질 모니터링 및 예측 적용에 대해 리뷰한 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들을 요약하고 분석하는 리뷰 논문에 해당한다.
The Current State and Challenges of Fairness in Federated Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10183984,"The proliferation of artificial intelligence systems and their reliance on massive datasets have led to a renewed demand on privacy of data. Both the large data processing need and its associated data privacy demand have led to the development of techniques such as Federated Learning, a distributed machine learning technique with privacy preservation built-in. Within Federated Learning, as with other machine learning based techniques, the concern and challenges of ensuring that the decisions being made are fair and equitable to all users is paramount. This paper presents an up-to-date review of the motivations, concepts, characteristics, challenges, and techniques/methods related to fairness in Federated Learning reported in the literature. It also highlights open challenges and future research directions in evaluating and enforcing fairness in Federated Learning systems.",No,초록에서 이 논문은 페더레이티드 러닝에서의 공정성에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 하고 있습니다. 따라서 직접적인 독창적 연구 결과보다는 기존 문헌의 정리와 향후 연구 방향 제시에 중점을 두고 있습니다.
The Role of Artificial Intelligence in Enhancing English Language Communication and Operational Efficiency in Logistics and Transportation Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757183,"This research analyzes the transformational influence of artificial intelligence (AI) on logistics and transportation systems, focusing on both quantitative and qualitative characteristics. By utilizing a mixed-methods approach, we polled industry experts and conducted in-depth interviews to understand the extent and types of AI adoption, perceived advantages, problems faced, and future investment intentions. Quantitative findings demonstrated considerable increases in operational efficiency, cost savings, and service quality attributable to AI technologies such as predictive analytics, machine learning, robotics, and autonomous vehicles. However, practical issues such as high initial costs, technological difficulties, and data protection concerns were also noted. Qualitative findings highlighted strategic benefits, including competitive advantages and enhanced decision-making capacity. Case studies showcased the actual uses of AI in warehouse management, fleet management, supply chain optimization, and urban mobility, demonstrating significant operational gains. This thorough review underscores AI's potential to transform logistics and transportation; however, additional research is required to evaluate technology integrations, socio-economic implications, and environmental advantages. The paper concludes with suggestions for addressing current challenges and leveraging AI to enhance efficiency, sustainability, and resilience in logistics and transportation networks.",Yes,"논문은 AI가 물류 및 운송 시스템에 미치는 영향에 대해 혼합 방법론을 사용하여 정량적, 정성적 데이터를 수집하고 분석한 독창적인 연구 내용을 포함하고 있습니다. 또한 실제 사례 연구와 전문가 인터뷰를 통해 새로운 연구 결과와 통찰을 제공하고 있어 연구 논문에 해당합니다."
Comparative Study to Detect Driver Drowsiness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404761,"Driver Drowsiness is one of the major causes of road accidents which leads to fatal and non-fatal injuries, sudden deaths and substantial monetary losses. Recently, various approaches have been identified to detect the driver drowsiness by research community due to advancement in the area of Artificial Intelligence (AI) and Machine Learning (ML). This further assists with saving the precious human life and reduces the monetary losses. Many researchers have proposed different techniques and methods to detect the driver drowsiness. The most common methods are subjective measures, vehicle-based measures, physiological measures, behavioural measures and hybrid measures. The detailed review of these measures, working of the existing systems, limitations associated with these systems are discussed in the paper. This paper also highlights the comparative analysis of the hybrid measures and its effectiveness. Hybrid measures is state of the art and it is the combination of two or more measures to detect driver drowsiness with higher accuracy. We conclude that developing a driver drowsiness detection system by using hybrid measures would be more efficient and it is highly recommended.",No,초록 내용은 기존 연구들을 종합하여 다양한 운전자 졸음 감지 방법들을 비교 분석하는 리뷰 논문에 해당합니다. 직접적인 실험이나 새로운 방법론 제안 등 독창적인 연구 기여가 포함되어 있지 않으므로 연구 논문으로 보기 어렵습니다.
AI Approaches in Education Based on Individual Learner Characteristics: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10264043,"The number of students who demand high quality education is growing continuously. Targeted, efficient education becomes increasingly important. Digital teaching formats combined with artificial intelligence offer promising opportunities and provide insights to develop seminal educational systems. In an ideal world the necessary data mining is integrated in those approaches and does not require sensors, surveillance or the close supervision of teachers. This review paper investigates the current state of research regarding actual applications of AI in educational learning concepts together with a focus on individual learner characteristics data. Within the study, 1.025 scientific papers from Scopus where screened and filtered. 67 papers were finally classified and evaluated. The review takes a close look at identified application categories such as the educational level of learners, academic subjects considered, learning environments used, types and objectives of the AI approaches, as well as a detailed examination of the underlying data. The actuality of the “AI in Education” topic is clearly visible in the growing number of publications. A substantial proportion of applications focus on university education with an accumulation in STEM subjects. Often, supervised AI approaches are used which focus on the prediction of learner performances. Data-wise, we see a lot of similarities in the approaches together with opportunities for improvement in terms of transparency and standardization.",No,"본 논문은 1,025편의 연구 논문을 스크리닝하고 67편을 분류 및 평가한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 분석하는 데 초점이 맞춰져 있습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향을 정리하는 문헌 고찰에 해당합니다."
The Empirical Analysis of Machine Learning Approaches for Enhancing the Cyber security for better Quality,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754195,"In recent years, there have been significant advances in both technologies & tactics so in area of cyber security, with (ML) machine learning at the forefront of the transformation. It is the ability to obtain security event characteristics or findings from cyber security information and then develop a matching information model that will allow a security system to become autonomous and smart. The widespread proliferation and the usage of Web and Smartphone applications has increased the size of cyber world as a consequence. When a computerized assault takes too long to complete, the internet becomes vulnerable. Security measures may be improved by recognizing and reacting to cyber-attacks, thanks to cyber security techniques. Security measures that were previously used aren't any longer appropriate because scammers have learned how to evade them. It is getting more difficult to detect formerly unknown and unpredictable security breaches, which are growing more widespread. Cyber security is becoming more dependent on machine learning (ML) techniques. Machine learning algorithms' dependability remains a major challenge, given its continual advancement. It is possible to find malicious hackers in internet that are ready to exploit ML defects that have been made public. A thorough review of machine learning techniques safeguarding cyberspace against attacks is provided in this paper, which presents a literature review on Cyber security using machine learning methods, such as vulnerability scanning, spam filtering, or threat detection on desktop networks as well as smart phone networks. Among other things, this paper provides brief descriptions of each machine-learning technique and security info, essential machine-learning technology, and evaluation parameters for a classification method.",No,초록에서 해당 논문은 기계 학습을 이용한 사이버 보안 분야의 기존 연구들을 종합적으로 검토하는 문헌 리뷰임을 명확히 밝히고 있습니다. 따라서 독창적인 연구 결과나 실험적 기여보다는 기존 연구의 정리와 요약에 초점이 맞춰져 있어 연구 논문으로 보기 어렵습니다.
Melanoma Breslow Thickness Classification Using Ensemble-Based Knowledge Distillation With Semi-Supervised Convolutional Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685145,"Melanoma is considered a global public health challenge and is responsible for more than 90% deaths related to skin cancer. Although the diagnosis of early melanoma is the main goal of dermoscopy, the discrimination between dermoscopic images of in situ and invasive melanomas can be a difficult task even for experienced dermatologists. Recent advances in artificial intelligence in the field of medical image analysis show that its application to dermoscopy with the aim of supporting and providing a second opinion to the medical expert could be of great interest. In this work, four datasets from different sources were used to train and evaluate deep learning models on in situ versus invasive melanoma classification and on Breslow thickness prediction. Supervised learning and semi-supervised learning using a multi-teacher ensemble knowledge distillation approach were considered and evaluated using a stratified 5-fold cross-validation scheme. The best models achieved AUCs of 0.8085±0.0242 and of 0.8232±0.0666 on the former and latter classification tasks, respectively. The best results were obtained using semi-supervised learning, with the best model achieving 0.8547 and 0.8768 AUC, respectively. An external test set was also evaluated, where semi-supervision achieved higher performance in all the classification tasks. The results obtained show that semi-supervised learning could improve the performance of trained models in different melanoma classification tasks compared to supervised learning. Automatic deep learning-based diagnosis systems could support medical professionals in their decision, serving as a second opinion or as a triage tool for medical centers.",Yes,"논문은 멜라노마 분류 및 Breslow 두께 예측을 위한 새로운 반지도 학습 기반 딥러닝 모델을 제안하고, 다양한 데이터셋을 활용해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Factors Impacting Adoption of Social Media Channels for Customer Service Management: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9942218,"The number of active social media users as of 2022 is estimated at 4.62 billion, accounting for 58% of the global population and registering rapid growth. This growth can be attributed to various factors such as advances in smartphone technology, cloud, Artificial Intelligence (AI), and rapid progress in network access mediums. Furthermore, social media platforms provide rich, collaborative ways for consumers to stay connected and to express their experiences, feedback, and sentiment on products and services across both public and private forums. This has a direct impact on enterprise brand valuations, and as such, it is becoming increasingly important for enterprises to actively track such public expressions.This has led to the rapid proliferation of social media management platforms and their adoption by enterprises to monitor and analyse social media trends and is a key aspect of digital transformation. This includes various use cases, for example, the ability to run marketing campaigns to boost brand awareness and to analyse consumer trends and brand perception through social media analytics.However, given the impact social media can have on enterprise brand valuations, it is now becoming increasingly important for businesses to re-evaluate their Customer Experience (CX) management strategies to include social media channels for customer service, beyond marketing and social insights. This is in addition to contemporary customer service channels such as online, contact centres and branches. This study aims to explore the key factors impacting the adoption of social media channels for customer experience management. It does so by way of a literature review conducted on the evolution of social media channels, their usage by consumers and the corresponding impact on enterprise brands, and the application of social media management tools. The study notes that social media adoption by enterprises depends on various factors such as usability, response strategies, blending with other customer service channels, technology integration and corporate governance. These factors are formulated in the form of research questions which could form the basis for future studies and be validated empirically. The paper contributes by providing practical insights to enterprises on factors to consider for adopting social media channels for customer service management.",No,"본 논문은 문헌 리뷰를 통해 소셜 미디어 채널 채택에 영향을 미치는 요인들을 탐구하고 있으며, 직접적인 실험이나 새로운 연구 결과를 제시하지 않는다. 따라서 독창적인 연구 내용보다는 기존 연구를 종합하고 향후 연구 방향을 제안하는 리뷰 논문에 해당한다."
Classifying sentiments in Nepali subjective texts,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785374,"With the advent of the online social media such as Facebook, Twitter and blogs, the way people perceive things around them has dramatically changed. One simple example could be how people today buy a mobile phone. If in the past, shopping involved moving from one store to the other, these days one cares more about the opinions expressed by people in product reviews rather. There is an increasing tendency to leave one's opinion(s) on a product, service or any entity on the web thus opening doors for an interesting yet challenging field of Sentiment Analysis. Much work has been done in the last decade on Sentiment Analysis not surprisingly for English because of the availability of a large number of resources but other languages are also gradually taking pace, some of them already in advanced stages and already developing competing resources and applications compared to English. Nepali opinionated content is also increasing rapidly in the last few years. Nepal fares as one of the countries with the highest number of Facebook subscribers. Online presence and voicing one's opinion in the Internet through different social media platforms has become a norm both for individuals or businesses. Nepali is a morphologically rich, under-resourced and a free word order language. Although some works related to the language and technology have been done in the past by institutions like Madan Puraskar Pustakalaya1 and Language Technology Kendra2, Nepali still remains an under-resourced language as it has very few tools and annotated corpus available for the language. This makes it challenging and difficult to do any linguistic computational works in the language including Sentiment Analysis. In this work, we look into applying three Machine Learning classifiers, namely Support Vector Machine, Multinomial Naive Bayes and Logistic Regression for developing a model to classify book and movie reviews written in Nepali into “Positive” and “Negative”. We evaluate and validate our model using 5-fold cross-validation techniques. Experimental results show that the Multinomial Naive Bayes classifier performs with a higher accuracy than the other two classifiers.",Yes,"이 논문은 네팔어 감성 분석을 위해 세 가지 머신러닝 분류기를 적용하고, 책과 영화 리뷰를 긍정/부정으로 분류하는 모델을 개발하는 독창적인 연구 내용을 포함하고 있다. 또한 5겹 교차 검증을 통해 모델을 평가한 실험 결과를 제시하고 있어 연구 논문에 해당한다."
Intelligent system for detecting “hidden” errors in protection settings,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398575,"Due to many developments taking place within the electricity supply industry, the network and its operation has grown ever more in recent years, which brings significant challenges for power system protection engineers. Apart from the significant efforts that are required to ensure that the protection setting process is effective, work also needs to be carried out to check the validity of the settings after initial calculation and application. However, solely relying on personnel and procedures to assess the validity of the protection relay setting files may occasionally result in a hidden error (or errors) remaining undetected until an in-service mal-operation event is experienced. This may bring significant consequences, in terms of economic costs, potential safety hazards and damage to the reputation of the utility company. This paper will present the initial research of making use of artificial intelligence technology (expert system) to help protection engineers validate the protection settings. Existing expert systems for protection settings applications will be reviewed and a new intelligent system that can open a setting file and interrogate the protection functions and settings in the file will be introduced. The advantages of this novel intelligent tool over existing protection setting expert systems will be discussed.",Yes,본 논문은 인공지능 기술을 활용한 새로운 지능형 시스템을 제안하고 기존 전문가 시스템과의 차별성을 논의하는 초기 연구를 포함하고 있어 독창적인 연구 내용을 담고 있다. 따라서 연구 논문에 해당한다고 판단된다.
Detection of Visual Signals for Pneumonia in Chest Radiographs using Weak Supervision,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020631,"Pneumonia usually manifests as areas of increased opacity in chest radiographs. Diagnosing pneumonia often requires a review of a chest radiograph by highly trained specialists. Automatic identification of regions of interest in chest radiographs could be an initial step in prioritizing radiology worklist for radiologists to review. In this paper, we show how to exploit class-activation mapping and weak supervision to automatically localize regions in chest radio-graphs exhibiting signs of pneumonia. For this research project, we evaluate a dataset of 30,000 chest radiographs collected by the Radiology Society of Northern America (RSNA). Data was annotated by six board-certified radiologists who provided bounding boxes to specify lesion location. We investigated the utility of weak supervision for detecting visual signs of pneumonia in chest radiograph (CXR). We formulate the problem as one of inference in binary pneumonia classification. Binary classification only requires a binary label indicating the presence of pneumonia as opposed to bounding boxes or detailed annotations outlining the regions of interest. We compared the regions of interest identified by our approach and a data-set of chest x-rays with detained ROI bounding box annotations prepared by the Radiological Society of North America (RSNA) in collaboration with the US National Institutes of Health, We show that our approach achieves good correlation intersection-over-union with the radiologists ground truth annotations from the (RSNA) dataset. Weakly supervised learning can improve the data curation burden by using weak labels in training for identification of pneumonia. These findings have broader implications, as annotating and labeling large datasets for medical imaging applications is a difficult task, and is often the bottleneck in efforts to apply advances in machine-learning and deep learning.",Yes,"이 논문은 흉부 방사선 사진에서 폐렴의 시각적 신호를 약한 감독 학습을 통해 자동으로 탐지하는 방법을 제안하고 평가하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 실제 데이터셋을 사용하여 성능을 검증하고 기존 방법과 비교하는 실험적 기여가 명확히 드러나 있습니다."
A Literature Survey and Analysis of Defending Cyber Attacks Targeting IoT in Critical Infrastructure,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590189,"In an era defined by the relentless advancement of technology, the critical infrastructure that sustains our daily lives has become increasingly reliant on complex digital systems like Internet of Things (IoT). While these complex digital systems have brought convenience and efficiency to critical infrastructure, they have also become prime targets for malicious parties seeking to exploit vulnerabilities, raising concerns across vital sectors like energy, healthcare, transportation, and beyond. Through systematic literature review, historical incident analysis, and examination of recent attacks, this paper aims to deepen our understanding of the evolving threat landscape, emphasizing the urgency of strengthening defenses, particularly in the IoT realm to secure the foundations of modern society. This literature survey encompasses a spectrum of defending technologies and methodologies, including Artificial Intelligence (AI)-based defenses, and anomaly-based intrusion detection systems. Analyzing variations techniques and evaluates their efficiency and complexity.",No,"논문 초록에서 명확히 ""systematic literature review""와 ""literature survey""를 수행했다고 언급하고 있어, 기존 연구들을 종합하고 분석하는 리뷰 논문임을 알 수 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 평가에 초점이 맞춰져 있습니다."
A Systematic Review on Fundus Image-Based Diabetic Retinopathy Detection and Grading: Current Status and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596287,"Diabetic Retinopathy (DR) is a prevalent outcome of diabetic mellitus. This causes lesions to form on the retina, impairing eyesight. Most likely, blindness can be avoided if the DR condition is discovered at an initial stage. Since DR is a non-reversible condition, early detection and treatment can significantly reduce the chance of visual loss. Fundus images manually detect DR, which is a laborious and error-prone procedure. In assessing and categorizing medical images, machine learning and deep learning have emerged as the most efficient methods, surpassing human performance, common image processing methods, and other computer-aided detection systems. For this article, the most recent approaches for utilizing fundus images to classify and detect DR using machine learning and deep learning methods have been researched and evaluated. The freely accessible DR Datasets consisting of fundus images have also been discussed. We reviewed several DR pipeline components, including the datasets that researchers frequently used and the preprocessing and data augmentation steps, feature extraction methods, commonly used detection and classification algorithms, and the generally used performance metrics. This paper ends with a discussion of current challenges that have to be tackled by researchers working in this field to translate the research methodology into actual clinical practice. Finally, we conclude with a discussion of the future perspectives of DR.",No,"본 논문은 당뇨망막병증 검출 및 분류에 관한 기존 연구들을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 종합적 현황과 향후 방향을 제시하는 리뷰 논문에 해당합니다."
Ethical Guidelines For Utilization of Artificial Intelligence In Healthcare: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10842807,"The key issue is to develop guidelines for Artificial Intelligence (AI) data protection that respect individual rights and further the general welfare. Where possible, AI systems have to minimize data of a personal nature to the absolute minimum, anonymize such data, and encrypt it in maintaining data security in accordance with regulations such as the General Data Protection Regulation (GDPR). AI systems need to follow the levels defined by the European Commission to achieve proper transparency and explain ability for building confidence and enabling proper ethical control. The present review article aims to bring ethical standard for AI utilization upfront. Additionally, the present article focus on uncovering ethical guidelines for maintaining standards for AI use. A thorough search approach was used to find relevant reviews of the literature for the assessment. Phases of the strategy included scanning several databases, evaluating publications, and choosing the most relevant research for review. This review examined electronic databases including PubMed, Science Direct, EMBASE, and Google Scholar. These databases were chosen to provide comprehensive coverage of relevant content. Making use of studies and reviews published between 2000 and 2024.",No,"초록에 따르면 이 논문은 기존 문헌과 연구를 종합하여 윤리적 가이드라인을 제시하는 리뷰 논문이다. 독창적인 연구 결과나 실험적 기여가 포함되어 있지 않고, 기존 연구를 평가하고 정리하는 데 중점을 두고 있다."
Using AI-based NiCATS System to Evaluate Student Comprehension in Introductory Computer Programming Courses,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9962681,"This Research to Practice Full Paper presents the use of data collected by our Non-Intrusive Classroom Attention Tracking System (NiCATS) to evaluate student comprehension. Quantifying students' cognitive processes in classrooms in a non-intrusive way is challenging. By analyzing various aspects of the eye metrics against defined regions of interest (ROI), instructors can better understand students’ cognitive processes as they acquire new knowledge. Eye-tracking studies primarily define ROIs based on commonly used metrics (source code complexity, significant fixation durations, etc.). While helpful, these metrics, when used independently, do not accurately represent their comprehension patterns. This paper contributes an alternative, multilayered approach for calculating gaze metrics against automatically defined ROIs. The work utilizes the AI-based Non-Intrusive Classroom Attention Tracking System (NiCATS - developed by the researchers), collecting raw-gaze data in real-time as information is presented on a computer screen. This paper reports the results of a study in which undergraduate students in a CS programming course were asked to identify defects seeded in Java programs. Each JAVA program included its own unique sets of ROIS defined using two different granularities: lexer-based and line-based. The ROI sets were then used to calculate relevant eye metrics in the context of each ROI layout. The results of the eye metric analysis at specific ROIs w.r.t their code review task provide insights into the cognitive processes students undergo when trying to comprehend new material. Subdividing this region into lexer-based regions, we determined “content topics” students struggled with (e.g., using complex data types) in a specific area. This feedback is valuable to the instructor as it enables the ability to identify hard-to-comprehend content topics post-hoc and gives the ability to validate student learning in the classroom. While this experiment focused on students in introductory programming courses, we intend to conduct experiments in other learning settings where students are expected to read material on a computer screen or solve actual problems. To summarize, the analysis of these eye metrics using more fine-grained ROIs (lexer-based, line-based) as an extension of complexity-based ROIs provides instructors with deeper insights into the cognitive processes used by students when compared to the current state-of-the-art techniques.",Yes,"논문은 AI 기반 NiCATS 시스템을 활용하여 학생들의 인지 과정을 분석하는 독창적인 연구를 수행하고 있으며, 실험을 통해 새로운 다층적 시선 추적 지표 계산 방법을 제안하고 있다. 이는 기존 연구와 차별화된 직접적인 연구 기여를 포함하고 있음을 보여준다."
Machine learning methods as a tool for diagnostic and prognostic research in cardiovascular disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670168,"Machine learning (ML) methods are the main tool of artificial intelligence, the use of which makes it possible to automate the processing and analysis of big data, to reveal hidden or non-obvious patterns on this basis, and to extract new knowledge. The review presents an analysis of scientific literature on the use of ML methods for diagnosing and predicting the clinical course of coronary heart disease. Provides information on reference databases, the use of which allows you to develop models and validate them (European ST-T Database, Cleveland Heart Disease database, Multi-Ethnic Study of Atherosclerosis, etc.). The advantages and disadvantages of individual ML methods (logistic regression, support vector machines, decision trees, naive Bayesian classifier, k-nearest neighbors) for the development of diagnostic and predictive algorithms are shown. The most promising ML methods include deep learning, which is implemented using multilayer artificial neural networks. It is assumed that the improvement of models based on ML methods and their introduction into clinical practice will help support medical decision-making, improve the effectiveness of treatment and optimize health care costs.",No,초록에서 해당 논문은 기존 문헌과 데이터베이스를 분석하고 ML 기법들의 장단점을 정리한 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 독창적인 연구 결과나 새로운 실험 데이터를 제시하는 연구 논문이 아닙니다.
Caption Driven Video Event Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10626468,"Video activity detection is used in a wide range of applications, such as healthcare, video surveillance, intelligent vehicle systems and more. Recently, the emphasis has shifted to using the power of neural networks to improve the accuracy and effectiveness of activity detection. This paper provides an indepth review of the latest techniques for using neural networks for video activity detection. The paper examines the development of activity detection methods, ranging from traditional statistical approaches to the recent explosion of supervised learning techniques. It focuses on transfer learning, multi instance learning and deep learning methods with a particular focus on convolutions neural network (CNN’s). CNN’s are widely used for their ability to segment humans, to classify and to recognize activity within video sequences or static images. The review covers the entire video activity detection pipeline, from prepossessing and human segmentation to feature extraction and classification. It explains the challenges associated with each step and provides innovative solutions. The paper also discusses the critical role of benchmark data sets as well as simulation tools for evaluating activity detection models. In addition, the paper discusses the challenges that are still present in the field of video activity detection and suggests directions for future research. It emphasises the importance of this field for ensuring public safety and advancing applications in healthcare and surveillance. By summarising the most recent advances and research trends, the purpose of this review is to help researchers and practitioners to develop more efficient video activity detection solutions with neural networks.",No,초록에서 해당 논문은 최신 기술에 대한 종합적인 리뷰를 제공하는 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구들을 정리하고 분석하는 리뷰 논문에 해당합니다.
ICT for Improved Maternal Healthcare in Uganda: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10569857,"This paper adapts a Systematic Literature Review (SLR) to investigate the influence of ICTs on Uganda's maternal health sector development based on the premise that ICTs have supported improvements in maternal health services such as antenatal, pregnancy, and postnatal care. The SLR analysed twenty articles and reports, which were classified using the Search, Appraisal, Synthesis, and Analysis (SALSA) framework and Mendeley referencing tool. Six ICT initiatives in maternal health were identified and evaluated, centred on their developmental contributions. Using a theoretical framework for analysis of ICT-based development initiatives, we categorized explanatory development perceptions about the ICT initiatives. Conceptual categories adopted from the framework include better lives for the poor and improved government services. The categories rationalized inferred contextual literature relating to the developmental contributions of the ICT initiatives. Our findings provide an interpretive understanding of the ICT initiative's significance towards service delivery access and improvement. However, the study was limited by insufficient literature about ICTs in Uganda's maternal health sector. Future studies aim to develop an Artificial intelligence system to support improved maternal healthcare in Uganda and a theoretical framework.",No,"본 논문은 체계적 문헌 고찰(Systematic Literature Review)을 수행하여 기존 연구들을 분석하고 평가한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 해석에 중점을 둔 논문입니다."
An Efficient Algorithm for Mapping Deep Learning Applications on the NoC Architecture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466053,"Artificial Intelligence (AI) has a very important role in the modern world. Through the use of the AI, organizations are able to make better decisions, and can improve business processes because speed and accuracy both are increased in decision making. This study is related to mapping of Artificial Intelligence Algorithms, particularly Neural Networks (NN) on cores in a Network-on-chip (NoC) platform. In this work, neurons are the tasks of a NN in real life and our goal is to divide those tasks among processing cores of the NoC. To complete this process, multiple optimization algorithms are used to map neurons of NN onto the NoC to reduce its computation time. Furthermore, to evaluate the solutions, the hidden layer complexity of the NN is varied, and Octave/Google Colaboratory based simulations are used to get these results. The result indicates improvement in terms of energy consumption, on-chip communication, and application processing time.",Yes,"논문 초록에서 인공신경망의 뉴런을 NoC 아키텍처의 코어에 매핑하는 새로운 최적화 알고리즘을 제안하고, 시뮬레이션을 통해 성능 개선을 평가한 점이 명확히 독창적인 연구 기여임을 보여준다. 따라서 본 논문은 직접 기여하는 연구 논문에 해당한다."
Review and Trends on Hand Gesture Recognition of Sign Language based on Deep Learning Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10141353,"Hand gestures are observed as an effective tool for making the interaction in the community with individuals having intellectual disabilities. It is highly essential for communicating the computers and people. Therefore, it is aimed to design an automatic hand gesture recognition approach that is utilized for repeatedly performing human-computer interaction. Sign languages are considered the natural languages utilized by hearing-impaired people that involve some expressional way of communication in routine life. It reveals the sentences, words, and letters present in the spoken language for performing the gesticulations that enable communication between them. The deaf community makes communicates with normal people using an automation system that relates the signs with the words of speech. The hand gesture recognition system is implemented independently of requiring any unique hardware rather than using the webcam. Thus, it is highly significant to make a short review of hand gesture recognition based on deep learning techniques considering the Indian sign language. Hence, this paper discusses and clarifies existing research work based on hand gesture recognition in Indian sign language with algorithmic classification. This survey also compares different performance measures, datasets utilized, and also different tools used for the implementation. Then, upcoming research and also current research gaps in hand gesture recognition in Indian sign language are analyzed. This review on state-of-the-art hand gesture recognition for Indian sign language tools has shown their potential for providing the right solution in different real-life situations. It is hoped that the contents and illustrations in this paper assist researchers in laying a good foundation to inform their studies.",No,초록에서 해당 논문은 기존 연구들을 정리하고 비교하는 리뷰 논문임을 명확히 밝히고 있습니다. 독창적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구 동향과 연구 갭을 분석하는 데 중점을 두고 있습니다.
Systematic Review on Frameworks for Intrusion Detection using Machine Learning and Deep Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10699009,"These days, information technology-driven businesses and companies are primarily concerned with cybersecurity challenges and their increasing complexity. Computer systems are still quite susceptible to several kinds of assaults, even with the introduction of multiple intrusion detection systems to combat zero-day cybersecurity threats. This sophisticated cyberattack has led to severe revenue losses in previous years due to several system breakdowns and service disruptions and irreversible harm to one’s reputation. This research paper provides an extensive, methodical analysis of cyberattacks. In addition to making thorough comparisons between the papers published by reputable venues in this field from a variety of angles. To greatly increase business sustainability, private corporations, governments, and enterprises can greatly benefit from implementing the survey paper’s results and major conclusions in their local or worldwide operations. By conducting a literature review, providing preliminary details on the various intrusion detection algorithms, and evaluating their performance while considering various parameters, this study strives at investigating the machine learning and deep learning-based strategies for intrusion detection systems.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 비교하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 알고리즘 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 문헌 조사 및 분석에 초점을 맞춘 서베이 논문에 해당합니다."
Using AWS SageMaker to Deploy ML Credit Card Fraud Detection Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493954,"Credit card fraud remains a persistent and costly challenge within the financial industry, necessitating the development of robust and efficient solutions for detection and prevention. Machine learning has emerged as a powerful tool for identifying fraudulent transactions, offering the potential to save financial institutions and consumers billions of dollars annually. This research study explores the practical application of Amazon Web Services (AWS) SageMaker, a comprehensive machine learning platform, to deploy an effective fraud detection system for credit cards. The study commences with a comprehensive review of existing literature on credit card fraud detection methods, shedding light on the limitations faced by conventional approaches. Subsequently, an innovative approach is presented, encompassing key stages such as feature engineering, data preparation, and the implementation of diverse machine learning techniques. To harness the scalability, deployment simplicity, and cost-effectiveness of AWS SageMaker, a detailed guide is provided through the entire model development and deployment process on the platform. Research findings demonstrate the efficacy of the machine learning model in accurately identifying fraudulent credit card transactions. A thorough analysis of the model's performance is presented, including metrics such as accuracy, precision, recall, F1-score, and ROC AUC. Additionally, a discussion is included on the advantages and considerations of utilizing AWS SageMaker for deploying machine learning models in real-world scenarios involving fraud detection. This research study aims to furnish financial institutions, data scientists, and researchers with valuable insights into effectively leveraging AWS SageMaker for credit card fraud detection. The conclusion emphasizes the significance of this approach in fortifying security measures and mitigating financial losses stemming from fraudulent activities.",Yes,"논문 초록에서 기존 방법의 한계를 검토하고, 특징 공학과 다양한 머신러닝 기법을 적용한 혁신적인 접근법을 제시하며, 모델 성능 평가를 통해 실질적인 연구 결과를 도출하고 있음을 확인할 수 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문임을 의미합니다."
Artificial Intelligence in Industry 4.0: A Review of Integration Challenges for Industrial Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10774364,"In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that can be leveraged by Artificial Intelligence (AI) for applications including predictive maintenance and pro-duction planning. However, despite the demonstrated potential of AI, its widespread adoption in sectors like manufacturing remains limited. Our comprehensive review of recent literature, including standards and reports, pinpoints key challenges: system integration, data-related issues, managing workforce-related concerns and ensuring trustworthy AI. A quantitative analysis highlights particular challenges and topics that are important for practitioners but still need to be sufficiently investigated by academics. The paper briefly discusses existing solutions to these challenges and proposes avenues for future research. We hope that this survey serves as a resource for practitioners evaluating the cost-benefit implications of AI in CPS and for researchers aiming to address these urgent challenges.",No,"이 논문은 Industry 4.0에서 인공지능 통합의 도전 과제를 종합적으로 검토하는 리뷰 논문으로, 직접적인 실험이나 독창적인 연구 결과를 제시하지 않습니다. 따라서 기존 문헌과 보고서를 분석하고 요약하는 데 중점을 둔 연구 논문이 아닙니다."
A Platform-Agnostic Framework for Automatically Identifying Performance Issue Reports With Heuristic Linguistic Patterns,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10504708,"Software performance is critical for system efficiency, with performance issues potentially resulting in budget overruns, project delays, and market losses. Such problems are reported to developers through issue tracking systems, which are often under-tagged, as the manual tagging process is voluntary and time-consuming. Existing automated performance issue tagging techniques, such as keyword matching and machine/deep learning models, struggle due to imbalanced datasets and a high degree of variance. This paper presents a novel hybrid classification approach, combining Heuristic Linguistic Patterns (HLPs) with machine/deep learning models to enable practitioners to automatically identify performance-related issues. The proposed approach works across three progressive levels: HLP tagging, sentence tagging, and issue tagging, with a focus on linguistic analysis of issue descriptions. The authors evaluate the approach on three different datasets collected from different projects and issue-tracking platforms to prove that the proposed framework is accurate, project- and platform-agnostic, and robust to imbalanced datasets. Furthermore, this study also examined how the two unique techniques of the framework, including the fuzzy HLP matching and the Issue HLP Matrix, contribute to the accuracy. Finally, the study explored the effectiveness and impact of two off-the-shelf feature selection techniques, Boruta and RFE, with the proposed framework. The results showed that the proposed framework has great potential for practitioners to accurately (with up to 100% precision, 66% recall, and 79% F1-score) identify performance issues, with robustness to imbalanced data and good transferability to new projects and issue tracking platforms.",Yes,"논문은 기존 방법들의 한계를 극복하기 위해 새로운 하이브리드 분류 접근법과 독창적인 언어적 패턴을 제안하고, 이를 다양한 데이터셋에서 평가하여 성능을 입증하는 연구 내용을 포함하고 있다. 따라서 직접 기여하는 독창적인 연구 결과를 담고 있는 연구 논문으로 판단된다."
Parametrized Quantum Circuits for Reinforcement Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10850927,"Research on quantum computing is still in its infancy, but it has a lot of potential uses. One topic with potential is machine learning, namely in the field of reinforcement learning. This work examines the integration of parametrized quantum circuits (PQC) into reinforcement learning (RL) algorithms, assessing the potential of quantum-enhanced models to solve classical RL tasks. It closely follows the example found on the TensorFlow website. This paper reviews applications of quantum reinforcement learning (QRL). We examine PQCs in a standard RL scenario, the CartPole-v1 environment from Gym, using TensorFlow Quantum and Cirq, to evaluate the relative performance of quantum versus conventional models. In comparison to conventional deep neural network (ONN) models, PQCs show slower convergence and higher processing needs, even if they are still able to learn the task and perform competitively. After they are fully trained, the quantum models show unique difficulties during the early training stages and reach a performance stability level like classical methods. This study sheds light on the present constraints as well as possible uses of quantum computing in reinforcement learning, particularly in situations with intricate, high-dimensional settings that prove difficult for classical computers to handle effectively. As we look to the future, we suggest that investigating hybrid quantum-classical algorithms, developing quantum hardware, and using quantum RL for increasingly difficult tasks are essential first steps. The study presents findings from both a classical reinforcement learning algorithm and a quantum integrated reinforcement learning algorithm. To provide a reliable comparison between quantum reinforcement algorithms and their classical equivalents, further work remains. This work lays the groundwork for future advances in the field by investigating the viability and use of quantum algorithms in reinforcement learning, even if it is not particularly unique. The purpose of this work is to help newcomers to this emerging field of study.",No,"본 논문은 기존 TensorFlow 예제를 따라가며 양자 강화학습과 고전적 강화학습 모델을 비교하는 리뷰 및 기초 연구에 가깝고, 독창적인 연구 결과나 새로운 알고리즘 개발보다는 현황 소개와 가능성 탐색에 중점을 두고 있습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
A Machine Learning-Based Forecasting Model with Deep Learning Capabilities for Unbalanced Time Series Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10862401,"During the last several years, deep learning capabilities have surpassed those of more conventional models on a wide variety of machine learning-related tasks. Neural networks with deep layers have indeed been effectively used to time series forecasting issues, which are crucial to the field of data mining. Their ability to instantly understand the temporal connections contained in time series has made them a viable option. Yet, deciding on a deep neural network architecture and then optimizing its parameters calls for a high level of skill. Thus, it is essential to conduct comprehensive studies of the predictive capacities of all current architectural frameworks. The completion of this work is hampered by two primary factors: (1) a complete examination of the most recent research on depth learning's application to time series forecasting, and (2) an empirical inquiry analysing the effectiveness of the most commonly used architectures. Seven distinct deep learning models are evaluated and compared for their efficiency and accuracy. We present scores and distributions based on our analysis of the suggested models trained with various architectures and hyperparameter settings. The 50,000 time series in the databases are used to solve 12 unique forecasting issues. Using this data to train approximately 38,000 models, we present the largest deep learning research to date for forecasting time series. While LSTMs were shown to be the most significantly predictive, both LSTMs & CNNs showed promise in this research. Comparable performance is attained by CNNs, but with less variance in results as well as a more based on the best available overall.",Yes,"본 논문은 다양한 딥러닝 모델을 사용하여 시계열 예측 문제에 대해 대규모 실험과 비교 분석을 수행한 독창적인 연구 내용을 포함하고 있습니다. 또한, 38,000개의 모델을 훈련시키고 성능을 평가하는 실증적 연구를 통해 기여를 하고 있으므로 연구 논문에 해당합니다."
Towards Privacy-Preserving Deep Learning based Medical Imaging Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802193,"Following the reports of breakthrough performances, machine learning based applications have become very popular in the medical field. However, with the recent increase in concerns related to data privacy, and the publication of specific regulations (e.g. GDPR), the development and, thus, exploitation of deep learning based applications in clinical decision making processes, has been rendered impossible in many cases. Herein, we describe and evaluate an approach that employs Fully Homo-morphic Encryption for allowing computations to be performed on sensitive data. Specifically, the solution exploits the MORE scheme and does not disclose patient data. The chosen encryption scheme increases the runtime only marginally and, importantly, allows for operations to be performed directly on floating point numbers, which represents a critical property for artificial neural networks. The feasibility and performance are first evaluated on a standard benchmarking application (MNIST digit classification). Next, we considered a medical imaging application, i.e. classification of coronary views in X-ray angiography. The reported results indicate that the proposed solution has great potential: (i) computational results are indistinguishable from those obtained with the unencrypted variants of the deep learning based applications, and (ii) run times increase only marginally. Finally, we also discuss in detail security concerns, and emphasize that the proposed solution may be employed in several practical applications, while still significant limitations remain to be solved in future work.",Yes,"논문은 Fully Homomorphic Encryption을 이용한 새로운 딥러닝 기반 의료 영상 분류 방법을 제안하고, MNIST 및 실제 의료 영상 데이터에 적용하여 성능과 실행 시간을 평가하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 보안 문제와 한계점에 대해 논의하며 향후 연구 방향도 제시하고 있어 연구 논문에 해당합니다."
Review of Semi-Structured Document Information Extraction Techniques Based on Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10339515,"With the advent of global digital transformation, using an intelligent method based on deep learning to extract crucial information from semi-structured documents, as represented by various types of receipts and invoices, has emerged as an imperative measure to ensure business stability, data security, and improved work efficiency. This paper provides a detailed review on deep learning-based techniques for information extraction, with systematic introduction, hierarchical analysis, method comparison, and summary with expectations for future development. The review begins with a comprehensive explication of the defining characteristics of semi-structured documents, along with a detailed introduction to the research background, application areas, and technical challenges related to information extraction from semi-structured documents. Then the review extends to an overview of two developmental stages, i.e. the shift from traditional information extraction to deep learning-based information extraction, followed by discussion about technical architecture and method classification, which elaborates on key technologies in terms of typical datasets, detection and recognition, and information reduction. Lastly, paper summarizes the prospects and development in the field. Future research will focus on strengthening algorithm universal and lightweight, as well as improving information protection capabilities and the diversity of datasets.",No,본 논문은 반구조화 문서 정보 추출 기법에 대한 심층 학습 기반의 기술들을 체계적으로 정리하고 비교하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구들을 종합하여 분석하는 내용에 초점이 맞춰져 있습니다.
Pre-Trained Neural Language Models for Automatic Mobile App User Feedback Answer Generation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680321,"Studies show that developers’ answers to the mobile app users’ feedbacks on app stores can increase the apps’ star rating. To help app developers generate answers that are related to the users’ issues, recent studies develop models to generate the answers automatically. Aims: The app response generation models use deep neural networks and require training data. Pre-Trained neural language Models (PTM) used in Natural Language Processing (NLP) take advantage of the information they learned from a large corpora in an unsupervised manner, and can reduce the amount of required training data. In this paper, we evaluate PTMs to generate replies to the mobile app user feedbacks. Method: We train a Transformer model from scratch and fine tune two PTMs to evaluate the generated responses, which are compared to RRGEN, a current app response model. We also evaluate the models with different portions of the training data. Results: The results on a large dataset evaluated by automatic metrics show that PTMs obtain lower scores than the baselines. However, our human evaluation confirm that PTMs can generate more relevant and meaningful responses to the posted feedbacks. Moreover, the performance of PTMs has less drop compared to other model when the amount of training data is reduced to 1/3. Conclusion: PTMs are useful in generating responses to app reviews and are more robust models to the amount of training data provided. However, the prediction time is 19X than RRGEN. This study can provide new avenues for research in adapting the PTMs for analyzing mobile app user feedbacks.",Yes,"본 논문은 사전학습된 신경망 언어 모델(PTM)을 활용하여 모바일 앱 사용자 피드백에 대한 자동 응답 생성 모델을 개발하고 평가하는 독창적인 연구를 수행하고 있다. 실험을 통해 기존 모델과 비교 분석하며, 데이터 양에 따른 성능 변화도 검증하는 등 직접적인 연구 기여가 포함되어 있다."
A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072101,"The Internet of Things (IoT) integrates billions of smart devices that can communicate with one another with minimal human intervention. IoT is one of the fastest developing fields in the history of computing, with an estimated 50 billion devices by the end of 2020. However, the crosscutting nature of IoT systems and the multidisciplinary components involved in the deployment of such systems have introduced new security challenges. Implementing security measures, such as encryption, authentication, access control, network and application security for IoT devices and their inherent vulnerabilities is ineffective. Therefore, existing security methods should be enhanced to effectively secure the IoT ecosystem. Machine learning and deep learning (ML/DL) have advanced considerably over the last few years, and machine intelligence has transitioned from laboratory novelty to practical machinery in several important applications. Consequently, ML/DL methods are important in transforming the security of IoT systems from merely facilitating secure communication between devices to security-based intelligence systems. The goal of this work is to provide a comprehensive survey of ML methods and recent advances in DL methods that can be used to develop enhanced security methods for IoT systems. IoT security threats that are related to inherent or newly introduced threats are presented, and various potential IoT system attack surfaces and the possible threats related to each surface are discussed. We then thoroughly review ML/DL methods for IoT security and present the opportunities, advantages and shortcomings of each method. We discuss the opportunities and challenges involved in applying ML/DL to IoT security. These opportunities and challenges can serve as potential future research directions.",No,"이 논문은 IoT 보안을 위한 머신러닝 및 딥러닝 기법에 대한 종합적인 서베이(조사) 논문으로, 기존 연구들을 정리하고 분석하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적인 연구 결과나 새로운 기법 제안이 포함되어 있지 않아 연구 논문으로 보기 어렵습니다."
Feasibility of a Deep Learning approach to estimate Shear Wave Speed using the framework of Reverberant Shear Wave Elastography: A numerical simulation study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871532,"Reverberant Shear Wave Elastography (RSWE) is an ultrasound elastography technique that offers great advantages, however, current estimators generate underestimations and time-consuming issues. As well, the involvement of Deep Learning into the medical imaging field with new tools to assess complex problems, makes it a great candidate to serve as a new approach for a RSWE estimator. This work addresses the application of a Deep Neural Network (DNN) for the estimation of Shear Wave Speed (SWS) maps from particle velocity using numerically simulated data. The architecture of the proposed network is based on a U-Net, which works with a custom loss function specifically adopted for the reconstruction task. Four DNNs were trained using four different databases: clean, noisy, acquired at variable frequency, and noisy and acquired at variable frequency data. After the training of the DNNs, the predicted SWS maps were evaluated based on different metrics related to segmentation, regression and similarity of images. The model for clean data showed better results with a Mean Absolute Error (MAE) of 0.011, Mean Square Error(MSE) of 0.001, modified Intersection over Union (mIoU) of 98.4%, Peak Signal to Noise Ratio (PSNR) of 32.925 and a Structural Similarity Index Measure (SSIM) of 0.99, for 250 (size of Testing Sets); while the other models delivered SSIM in the range of 0.87 to 0.96. It was concluded that noisy and clean data could be effectively handled by the model, while the other ones still need enhancement. Clinical Relevance— This work is focused on the application of a Deep Learning approach to accurately asses the Shear Wave Speed in numerical simulations of Reverberant Shear Wave Elastography approach. This novel estimator could be useful for future clinical experiments specially with real time applications to determine the status of living tissue such as detection of malignant or benign tumors located in breast cervix prostate or skin and in the diagnosis of other pathologies such us liver fibrosis.",Yes,"본 논문은 딥러닝 기반의 새로운 추정기 개발을 위해 수치 시뮬레이션 데이터를 사용하여 네트워크를 설계, 학습, 평가하는 독창적인 연구를 수행하였다. 또한 다양한 데이터 조건에서 모델 성능을 분석하고 결과를 제시함으로써 직접적인 연구 기여를 포함하고 있다."
Deep Gait Recognition: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714177,"Gait recognition is an appealing biometric modality which aims to identify individuals based on the way they walk. Deep learning has reshaped the research landscape in this area since 2015 through the ability to automatically learn discriminative representations. Gait recognition methods based on deep learning now dominate the state-of-the-art in the field and have fostered real-world applications. In this paper, we present a comprehensive overview of breakthroughs and recent developments in gait recognition with deep learning, and cover broad topics including datasets, test protocols, state-of-the-art solutions, challenges, and future research directions. We first review the commonly used gait datasets along with the principles designed for evaluating them. We then propose a novel taxonomy made up of four separate dimensions namely body representation, temporal representation, feature representation, and neural architecture, to help characterize and organize the research landscape and literature in this area. Following our proposed taxonomy, a comprehensive survey of gait recognition methods using deep learning is presented with discussions on their performances, characteristics, advantages, and limitations. We conclude this survey with a discussion on current challenges and mention a number of promising directions for future research in gait recognition.",No,"본 논문은 딥러닝 기반 보행 인식 분야의 기존 연구들을 종합적으로 정리하고 분류하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 연구 동향 및 현황을 소개하는 리뷰 논문에 해당합니다."
Deep learning for 5G and 6G,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158628,"Deep learning (DL) is a promising technology for enhancing the development of fifth generation (5G) and sixth generation (6G) mobile networks, as it can improve their capabilities, security, and performance. However, there are still significant challenges to be addressed in the implementation of DL techniques in these networks. To address these challenges, we conducted a systematic review of the literature on DL techniques in 5G and 6G applications following the PRISMA guidelines. The review was conducted in three stages: data collection, analysis, and reporting of primary findings. After evaluating and reviewing the databases, we found that hybrid DL and ensemble techniques show promise in optimizing 5G and 6G networks, given proper implementation. Finally, we discussed the open issues and challenges in this field. This review provides important insights into the potential of DL techniques in improving 5G and 6G networks, and it highlights the need for further research to overcome the remaining challenges. The results of this primary communication will be further developed and extended into a journal article.",No,"이 논문은 5G 및 6G 네트워크에서 딥러닝 기술의 적용에 관한 체계적인 문헌 리뷰를 수행한 것으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Recent Developments in Arabic Conversational AI: A Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723043,"Conversational AI is one of the most active research areas in AI, and it has gained more attention from academia as well as industry. Given recent advancements in several conversational AI systems in addition to the availability of several datasets, the aim of this study is to explore the landscape of Arabic text-based conversational AI systems. In this work, we provide a thorough review of recent Arabic conversational AI systems. We group them into three categories based on their functionality: (1) question-answering (QA) systems, (2) task-oriented dialogue systems (DS), and (3) chatbots. Furthermore, we describe the common datasets used in building and evaluating conversational AI systems in Arabic. Few surveys have targeted the conversational AI field for the Arabic language, and we aim to cover this gap with this study. Our contribution focuses on reviewing and analyzing the literature in the field and highlighting future research directions towards human-like conversational AI systems in Arabic.",No,"본 논문은 아랍어 대화형 AI 시스템에 관한 기존 연구들을 종합적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 분석하고 정리하는 데 중점을 두고 있습니다."
Automatic Generation of Conceptual Enterprise Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233131,"The world organizations operate in is becoming increasingly complex and uncertain, due to technical disruptions, changes in operational environments and social structures, global markets, or, as recently seen, through global diseases. Those changes require the development of smart technological and human systems, based on Conceptual Enterprise Models. Through the increasing complexity, modeling is difficult for human actors and therefore requiring machine-support, for example through Artificial Intelligence approaches, leading to Hybrid Intelligence systems and automated generation of Conceptual Models. While various approaches for the automated generation of Conceptual Enterprise Models exist, for example in requirements engineering, process mining or in the context of Digital Twins, the approaches mostly support only parts of the modeling process. Additionaly, the approaches lack a common base, such as standardized frameworks. Therefore, my further research aims at showing how conceptual modeling can be automated, developing a modeling standard for Digital Twins in enterprise context, showing its technical feasibility and evaluating the implementation in real-world use cases. The research is performed through design science research, systematic literature research, argumentative-deductive reasoning, prototyping and use cases. At this stage, a first literature review is completed, and the next research steps are in preparation.",No,"초록에 따르면 현재까지는 문헌 리뷰가 완료된 단계이며, 본격적인 연구 진행과 구현, 평가 단계는 준비 중에 있다. 따라서 아직 직접 기여하는 독창적인 연구 결과가 포함된 논문으로 보기 어렵다."
How important are Chatbots within Engineering Education? A literature-based review from 2011 to 2023,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10344203,"Over the years, the emergence of the fourth industrial revolution has brought a much-needed transition within the education sector. COVID-19 has contributed immensely on how teaching and learning is currently evolving. Most of the traditional face-to-face institutions found themselves utilizing technology to enhance teaching and learning activities. Technological tools brought amongst others, artificial intelligence that is currently dictating the direction of education system. Chatbots amongst others found its space within teaching and learning, allowing both students and academics to find education interesting. The current study evaluated the existence of Chatbots in engineering education fraternity by carrying out a literature-based review method focusing 2011 until recent. The findings of the study revealed that Chatbots were spotted as early as 2008 within the engineering education. It continued to make headways in various engineering modules such as Thermodynamics, Design Engineering, as well as in the administration of teaching and learning itself. In conclusion, the challenges and concerns associated with AI chatbots in education underscore the need for a nuanced and balanced approach. Ethical considerations, including data privacy and algorithmic bias, must be carefully navigated.",No,"본 논문은 2011년부터 2023년까지의 문헌을 기반으로 한 리뷰 연구로, 기존 연구들을 종합하여 챗봇의 중요성을 평가하고 있다. 따라서 직접적인 독창적 연구 결과나 실험 데이터를 제시하는 연구 논문이 아니라 문헌 고찰에 해당한다."
MedSeg: A Medical Image Analyzer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696156,"This research paper aims to address the critical need for efficient and accurate identification of chest diseases using chest X-rays through a combination of advanced image processing techniques and machine learning algorithms. With the growing prevalence of respiratory and cardiovascular conditions worldwide, timely and precise diagnosis is paramount for effective patient care. The study begins with a comprehensive review of existing methodologies and technologies employed in the identification of chest diseases from X-ray images. It critically evaluates the strengths and limitations of current approaches, highlighting the challenges faced in achieving high accuracy, speed, and scalability. To address these issues, the project aims to develop an AI-powered system for medical image analysis. In response to these challenges, our research proposes a novel approach that integrates Inception V3 model and imagenet. We leverage a large dataset of annotated chest X-rays to train a deep neural network capable of recognizing subtle patterns indicative of various diseases, including pneumonia, pneumothorax, lung and cardiac abnormalities. The model is optimized to provide not only accurate diagnoses but also to minimize false positives and negatives. In conclusion, this research contributes to the ongoing efforts in utilizing chest X-ray images for disease identification, presenting a robust and efficient methodology that could revolutionize the current diagnostic landscape. The findings hold promise for the development of automated systems capable of assisting healthcare professionals in the accurate and timely detection of chest diseases, ultimately contributing to enhanced patient care and management.",Yes,논문 초록에서 제안된 연구는 기존 방법들의 한계를 극복하기 위해 Inception V3 모델과 대규모 주석 데이터셋을 활용한 새로운 딥러닝 기반 의료 영상 분석 시스템을 개발하는 독창적인 연구 내용을 포함하고 있다. 이는 직접적인 연구 기여를 나타내므로 연구 논문에 해당한다.
Quantum Low Entropy based Associative Reasoning–QLEAR Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587003,"It is well known that the field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. There are many algorithms based on different theoretical backgrounds that could be used for pattern recognition in practical applications. Generally, most of the algorithms are applied in areas like classification, regression or change point detection.Recently, it has been shown that a probabilistic model based on two of the main concepts in quantum physics – a density matrix and the Born rule, can be suitable for the modeling of learning algorithms in biologically plausible artificial neural networks framework. It has been shown that the proposed probabilistic interpretation is suitable for modeling on-line learning algorithms for Independent/Principal/Minor Component Analysis, which could be realized on parallel hardware based on very simple computational units. Also, it has been shown that the quantum entropy of the system, related to that model, can be successfully used in the problems like change point or anomalies detection as well as simple classification problems. Here another application of the proposed quantum probabilistic model is going to be presented. A general paradigm called QLEAR learning (Quantum Low Entropy based Associative Reasoning) would be presented and tested in classification context. Proposed method potentially can overcome the problem that classifier performance depends greatly on the characteristics of the data to be classified. It is known that until now, there is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Here we will try to propose a classification algorithm that, actually, automatically adjusts its performance according to characteristics of the data on which it is applied. An interesting aspect is that proposed method inherently solves the problem of unbalanced classes (classes that have significantly different size). The proposed paradigm can be applied in any area in which standard classification techniques are applied. The method is going to be analyzed in the context of classification, prediction and solving some mathematical problems.We’ll analyze, mainly, the case in which data is represented by vectors. Generalization toward multiway data would be discussed only on one example. The approach is based on the idea that classification can be understood as supervised clustering, where quantum entropy, in the context of the quantum probabilistic model, will be used as a “capturer” (measure, or external index) of the “natural structure” of the data. By using quantum entropy we don’t make any assumption about linear separability of the data that are going to be classified. The basic idea is to find close neighbours to a query sample and then use relative change in the quantum entropy as a measure of similarity of the newly arrived sample with the representatives of interest. In other words, method is based on calculation of quantum entropy of the referent system and its relative change with the addition of the newly arrived sample. Referent system consists of vectors/matrices that represent individual classes and that are the most similar, in Euclidean distance sense, to the vector that is analyzed. The classification problem is analysed in the context of measuring similarities to prototype examples of categories. The proposed method could be seen as a hybrid of nearest neighbor and optimization machine learning technique which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice.",Yes,논문은 양자 확률 모델과 양자 엔트로피를 활용한 새로운 분류 알고리즘(QLEAR learning)을 제안하고 이를 다양한 문제에 적용하여 성능을 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다.
Data Mining Applications to Fault Diagnosis in Power Electronic Systems: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628027,"Early fault detection in power electronic systems (PESs) to maintain reliability is one of the most important issues that has been significantly addressed in recent years. In this article, after reviewing various works of literature based on fault detection in PESs, data mining-based techniques including artificial neural network, machine learning, and deep learning algorithms are introduced. Then, the fault detection routine in PESs is expressed by introducing signal measurement sensors and how to extract the feature from them. Finally, based on studies, the performance of various data mining methods in detecting PESs faults is evaluated. The results of evaluations show that the deep learning-based techniques given the ability of feature extraction from measured signals are significantly more effective than other methods and as an ideal tool for future applications in the power electronics industry are introduced.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 데이터 마이닝 기법들의 적용 현황을 소개하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합적 분석에 초점이 맞춰져 있습니다."
Corona Virus Detection and Classification with Radiograph images using RNN,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452950,"We are working on detecting the symptoms of Corona virus also known as Covid-19 in this project. From diagnosis to treatment, COVID-19 has posed a significant threat to journalists, researchers, physicians, and organizations all over the world. Analysts are working nonstop to gain proper knowledge in order to monitor the pandemic in their respective areas. As a result, it is critical to incorporate an intelligent system as an alternative option for preventing COVID-19-related effects or death in society. The determination of the level of risk, with the aid of various Artificial Intelligence methodologies, may be one of the answers to controlling the damage. A number of machine learning models have been developed to diagnose and evaluate the seriousness of a variety of cases. Clinical evidence from patients who have been infected with a virus has been used in the proposed study to predict the seriousness of the disease if the patient has been infected. The signs of the disease (corona virus) as well as other parameters specific to the person's health have been taken into consideration. COVID-19 is a highly infectious disease that has been declared a Public Health Emergency and a Pandemic by the World Health Organization. The virus has infected over 25 million people worldwide which has killed over 840,000 people and threatened the lives of millions more. COVID-19 is characterized by a dry cough, sore throat, and ahigh temperature. It is critical to find quick and accurate results for Covid-19 at this time in order to stop it in its early stages and avoid it from being a problem. Deep learning concepts are being used to analyze and classify symptoms from radiograph images. Chest radiographs are one of the early screening tests to assess the onset of disease since the infection seriously affects the lungs. In this proposal, we used a recurrent neural network model combined with a multilevel thresholding technique to detect Corona virus. One of the machine learning techniques for prediction is the RNN model. A Recurrent Neural Network is used to decide if the given images belong to Covid-19 during the classification process. This implementation is based on a publicly available dataset of radiograph images.",Yes,"논문은 코로나 바이러스 감염 여부를 방사선 사진 이미지를 이용해 RNN 모델로 탐지하고 분류하는 구체적인 연구 방법과 모델 적용을 제안하고 있어, 독창적인 연구 내용을 포함하고 있다. 또한 공개 데이터셋을 활용한 실험적 접근을 통해 직접적인 연구 기여를 하고 있음을 알 수 있다."
Analysis of Feature Selection Methods in Software Defect Prediction Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10359521,"Improving software quality by proactively detecting potential defects during development is a major goal of software engineering. Software defect prediction plays a central role in achieving this goal. The power of data analytics and machine learning allows us to focus our efforts where they are needed most. A key factor in the success of software fault prediction is selecting relevant features and reducing data dimensionality. Feature selection methods contribute by filtering out the most critical attributes from a plethora of potential features. These methods have the potential to significantly improve the accuracy and efficiency of fault prediction models. However, the field of feature selection in the context of software fault prediction is vast and constantly evolving, with a variety of techniques and tools available. Based on these considerations, our systematic literature review conducts a comprehensive investigation of feature selection methods used in the context of software fault prediction. The research uses a refined search strategy involving four reputable digital libraries, including IEEE Explore, Science Direct, ACM Digital Library, and Springer Link, to provide a comprehensive and exhaustive review through a rigorous analysis of 49 selected primary studies from 2014. The results highlight several important issues. First, there is a prevalence of filtering and hybrid feature selection methods. Second, single classifiers such as Naïve Bayes, Support Vector Machine, and Decision Tree, as well as ensemble classifiers such as Random Forest, Bagging, and AdaBoost are commonly used. Third, evaluation metrics such as area under the curve, accuracy, and F-measure are commonly used for performance evaluation. Finally, there is a clear preference for tools such as WEKA, MATLAB, and Python. By providing insights into current trends and practices in the field, this study offers valuable guidance to researchers and practitioners to make informed decisions to improve software fault prediction models and contribute to the overall improvement of software quality.",No,"본 논문은 소프트웨어 결함 예측에서 사용되는 특징 선택 방법에 대한 체계적인 문헌 리뷰를 수행한 연구로, 직접적인 실험이나 새로운 방법론 제안보다는 기존 연구들을 종합하고 분석하는 데 중점을 두고 있다. 따라서 독창적인 연구 내용이나 직접적인 기여를 포함한 연구 논문으로 보기 어렵다."
Utilizing Bio Metric System for Enhancing Cyber Security in Banking Sector: A Systematic Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10194299,"Biometric authentication is gaining the interest of private, public, consumer electronics and corporate security systems. For the protection of cyberspace from hackers and other harmful people, biometric security is growing more and more popular among organizations, individuals and enterprises. The word “cyber security” refers to the procedures, techniques, and tools used to safeguard data, network system, computer networks and software from potential attacks online. Online financial service delivery is referred to as “cyber banking.” As the trend of exchanging things has changed, internet banking has grown. Despite the benefits, there have been instances of security threat-related issues with Internet banking. To identify persons, biometric security verifies their physical attributes and behavioral traits. For identification verification, it is the most reliable and effective physical security method. According to biometric authentication, people can be recognized precisely based on their innate behavioral or physical traits. Numerous security measures have been implemented throughout the entire Internet banking service to address these issues. Globally, cybercrime has deep roots and poses a significant threat to the occurrence of criminal or terrorist behavior. Without being addressed by a single authority, these risks can compromise security on the inside as well as the outside. If the cybercrime goes unnoticed, both money and personal data are lost. Internet services and information infrastructure have previously been targeted in assaults. Online fraud and hacker attacks are only two examples of the daily computer-related crimes that take place. The Internet of Things (IoT) is the most reliable foundation for facilitating high-quality, comfortable human living. IoT has had a substantial impact across a range of application domains. Smart gadgets are more vulnerable to hackers because of their rapid development and trust in wireless mechanics for data transport. As a result, the rate of cybercrime is rising daily. Artificial Intelligence (AI)-based cybersecurity emerges because of technological advancement and poses a risk to public safety, personal property rights, and privacy protection for people. The study elaborates on the key features of biometrics system in conventional and Islamic banking to counter the risk of cybersecurity and provide high safety and security to the banking industry. For this systematic literature review, the most suitable and most relevant 101 articles from the reputed online libraries are selected. This analysis absorbed four research questions and pertinent keywords from the period of 2009 to 2022 (a part of 2023 was included).",No,"초록에서 본 논문은 기존 문헌을 체계적으로 분석한 문헌 리뷰(Systematic Literature Review)로 보이며, 직접적인 독창적 연구 결과나 실험, 새로운 방법론 제시는 포함되어 있지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합한 분석 논문에 해당합니다."
Artificial Intelligence for Emerging Technology in Surgery: Systematic Review and Validation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798714,"Surgery is a high-risk procedure of therapy and is associated to post trauma complications of longer hospital stay, estimated blood loss and long duration of surgeries. Reports have suggested that over 2.5% patients die during and post operation. This paper is aimed at systematic review of previous research on artificial intelligence (AI) in surgery, analyzing their results with suitable software to validate their research by obtaining same or contrary results. Six published research articles have been reviewed across three continents. These articles have been re-validated using software including SPSS and MedCalc to obtain the statistical features such as the mean, standard deviation, significant level, and standard error. From the significant values, the experiments are then classified according to the null (p < 0.05) or alternative (p>0.05) hypotheses. The results obtained from the analysis have suggested significant difference in operating time, docking time, staging time, and estimated blood loss but show no significant difference in length of hospital stay, recovery time and lymph nodes harvested between robotic assisted surgery using AI and normal conventional surgery. From the evaluations, this research suggests that AI-assisted surgery improves over the conventional surgery as safer and more efficient system of surgery with minimal or no complications.",No,"본 논문은 기존 연구들을 체계적으로 검토하고 통계 소프트웨어를 이용해 결과를 재검증하는 리뷰 논문으로, 직접적인 독창적 실험이나 새로운 연구 결과를 제시하지 않습니다. 따라서 연구 논문보다는 체계적 문헌고찰에 해당합니다."
Electrical engineering teaching and distance learning using a desktop virtual reality system,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6715005,"Higher education has evolved in the last decade with the use of information technology. This change was called distance education, a teaching method in which the student does not need to meet with the teacher on a certain day and time. The student may be either at home or at work and may have no interaction with the other parts, either the teacher or other students. It has allowed the institutions to resolve geographical gaps in order to reach the largest number of students. On the other hand, it paved the way for the “non-traditional” universities oriented for “adult work”, in a narrow range of graduation programs, compatible with the current demands from industry. It is also important to mention that distance education is becoming increasingly appropriate for non-academic studies, such as corporate training environments. This paper addresses Circuit Theory systems, more specifically laboratory practices geared towards teaching and learning. The choice was made from observing the needs in the specific context of a measures and instrumentation laboratory, mainly related with access to the means and equipment to carry out laboratory practice. The purpose of the work is the use of virtual experimentation to carry out laboratory practice and also as an alternative tool to meet the needs of access to the means and equipment of the laboratory. In the present case, the basis of the project was the construction of a 3D lab environment (Measures and Instrumentation) where the equipment and the components can be seen and manipulated. The project involves simple electrical schematics, which later can be changed in values, presenting new results, and displays a set of menus and submenus to support experiments. The virtual laboratory can accommodate new devices and scenarios, being adapted to new subjects, such as electric machines and power system analysis of the Electrical Engineering program. This work was developed to demonstrate how a desktop VR prototype, “Virtual Electric Manual” — VEMA, can be applied to an engineering unit and used to enhance security and resourcefulness in using electrical equipment. Several interactive scenes were developed to illustrate the idea using a measurements and instrumentation laboratory as virtual environment. The added value of these various features in the educational context is that they contribute to the construction of new virtual environments, able to benefit the communication between teachers and students and among themselves, thus creating new opportunities for each student to participate more actively in his/her own learning construction process. Rather than being seen as mere information files, these e-learning platforms should be perceived as a means to promote interaction and experimentation through technological resources.",Yes,"논문은 3D 가상 실험실 환경을 구축하고 이를 전기공학 교육에 적용하는 구체적인 연구 프로젝트를 다루고 있으며, 새로운 데스크탑 VR 시스템을 개발하고 실험하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Machine Learning Role in Cognitive Mental Health Analysis amid Covid-19 Crisis: A Critical Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9850873,"Coronavirus is a disease caused by SARS-CoV-2, which can cause severe respiratory problems in humans. World Health Organization declared it to be a pandemic, as per the rate of spread and scale of its transmission. The mental health of people is impacted rudely by Covid-19. The influence of the Covid19 virus on psychological health leads to depression, anxiety, posttraumatic stress, dementia, mental stress, helplessness, fear of losing, etc. Machine Learning is performing a critical role in the rapid advancement of the healthcare system in the past few years. Machine Learning techniques are employed to forecast, diagnose disease, evaluate data by studying the earlier data, and construct different patterns of it. Therefore, the purpose of this report is to address the issue of psychiatric illnesses by identifying those who are at an elevated risk of mental conditions, due to increased stress throughout the Covid19 crisis. Due to the current ongoing pandemic, the mental health crisis needs time, and proactive interference to confront and endure the anxiety. In this research, A comprehensive literature survey was undertaken to examine some machine learning predictive models for primitive prediction of a particular type of mental illness using machine learning algorithms. Several existing research papers were reviewed and after evaluation results show that among various algorithms like, Gradient Boosting Machine, Support Vector Machines, Naïve Bayes, K-Nearest Neighbors; Support Vector Machine (98.6%), Random Forest, and Random Forest (97.07%) are the most accurate algorithm for predicting mental illness amid the ongoing pandemic.",No,"본 논문은 기존 연구들을 종합적으로 검토한 문헌 조사(리뷰) 연구로, 직접적인 독창적 실험이나 새로운 데이터 분석 결과를 제시하지 않고 기존 알고리즘들의 성능을 비교 평가하는 내용에 집중하고 있습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
A Survey of Machine Learning Methods for Diagnosing Parkinson's Disease with Handwriting,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10304996,"Parkinson's disease is a nervous system disorder for which early diagnosis is important. In recent years, promising results have been obtained for diagnosing Parkinson's disease with machine learning techniques. This review examines different studies on diagnosing Parkinson's disease using machine learning models and methods. Various methods, such as machine learning, deep learning, and transfer learning, classify Parkinson's disease. These models successfully distinguish Parkinson's patients from healthy individuals by using different data types, such as handwriting data and acoustic features. In this study, studies using handwriting for diagnosing Parkinson's were evaluated with a critical approach. It also informs the literature for future research in diagnosing Parkinson's disease using machine learning.",No,초록에서 해당 논문은 기존 연구들을 검토하고 평가하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험을 제시하는 연구 논문이 아닙니다.
A Survey of EEG and Machine Learning-Based Methods for Neural Rehabilitation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10268416,"One approach to therapy and training for the restoration of damaged muscles and motor systems is rehabilitation. EEG-assisted Brain-Computer Interface (BCI) may assist in restoring or enhancing ‘lost motor abilities in the brain. Assisted by brain activity, BCI offers simple-to-use technology aids and robotic prosthetics. This systematic literature review aims to explore the latest developments in BCI and motor control for rehabilitation. Additionally, we have explored typical EEG apparatuses that are available for BCI-driven rehabilitative purposes. Furthermore, a comparison of significant studies in rehabilitation assessment using machine learning techniques has been summarized. The results of this study may influence policymakers’ decisions regarding the use of EEG equipment, particularly wireless devices, to implement BCI technology. Moreover, the literature review results offer suggestions for further study and new research areas. We plan to identify the additional characteristics of each EEG equipment and determine which one is most suited for each industry by measuring the user experience based on various devices in future research.",No,"본 논문은 EEG와 머신러닝 기반 신경 재활 방법에 대한 체계적인 문헌 리뷰로, 기존 연구들을 종합하고 비교하는 내용을 담고 있습니다. 직접적인 실험이나 새로운 연구 결과를 제시하는 독창적인 연구 논문이 아니라 기존 연구를 정리한 서베이 논문입니다."
AppAuth: Authorship Attribution for Android App Clones,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853275,"Android app clone detection has been extensively studied in our community, and a number of effective approaches and frameworks were proposed and released. However, there still remains one open challenge that has not been well addressed in previous work, i.e., the authorship attribution for the detected app clones. Although state-of-the-art approaches could accurately identify repackaged apps in one way or another, no convincing method has been proposed to identify the original app and the authentic author from the repackaged app pairs, which greatly limits the usage scenario of app clone detection techniques. For example, app market maintainers have to manually confirm the identified repackaged app pairs, while in most cases, it is challenging for them to make an accurate decision. In this paper, we propose AppAuth, a novel learning-based approach to predict the authorship of app clones. To be specific, for a given Android app clone pair (or a group of repackaged apps identified), AppAuth could accurately infer the original author of the plagiarized apps. Our approach is motivated by the traditional authorship attribution studies on binary files. AppAuth first extracts a number of coding-style-related features from the executable .apk files, and then relies on machine learning techniques to train a classification model. We have conducted extensive experiments to evaluate the effectiveness of AppAuth. The experiment results suggest that we are able to infer the authorship for Android app clones with high precision. Our work is the first one that tackles the problem systematically and we believe our efforts could positively contribute to the research community and boost the research of app repacking detection and authorship attribution studies.",Yes,"본 논문은 Android 앱 클론의 저자 식별 문제를 해결하기 위해 새로운 학습 기반 접근법(AppAuth)을 제안하고, 이를 평가하는 실험을 수행한 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구와 차별화된 직접적인 기여가 있는 연구 논문으로 판단된다."
Corpus Indexing Database Construction based on Deep Learning Algorithm,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10494006,"This paper explores a novel method for constructing a corpus index database using deep learning algorithms. The author reviews the evolution of these algorithms and their central role in natural language processing, particularly in corpus construction and information retrieval. The paper details how deep learning algorithms, in conjunction with the hash model, efficiently organize large datasets for corpus indexing. In addition, a new deep information retrieval algorithm is presented that balances efficiency and improved search results. The author illustrates the practical application of bilingual parallel corpora in machine translation and validates its feasibility. These results not only contribute to the methodology of translation research, but also provide a scientific tool for translation practice. Test results show a remarkable performance improvement of the proposed model compared to traditional methods, achieving accuracy between 98.45% and 98.86%, as shown in Table II, confirming its robustness for broader information retrieval application.",Yes,"논문 초록에서 새로운 딥러닝 기반 정보 검색 알고리즘을 제안하고, 기존 방법 대비 성능 향상을 실험적으로 검증한 점이 명확히 나타나 있어 독창적인 연구 내용을 포함한 연구 논문으로 판단됩니다. 또한, 구체적인 성능 수치와 실험 결과를 제시하여 직접적인 기여가 있음을 보여줍니다."
Non-functional Requirements for Machine Learning: Understanding Current Use and Challenges in Industry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604595,"Machine Learning (ML) is an application of Artificial Intelligence (AI) that uses big data to produce complex predictions and decision-making systems, which would be challenging to obtain otherwise. To ensure the success of ML-enabled systems, it is essential to be aware of certain qualities of ML solutions (performance, transparency, fairness), known from a Requirement Engineering (RE) perspective as non-functional requirements (NFRs). However, when systems involve ML, NFRs for traditional software may not apply in the same ways; some NFRs may become more prominent or less important; NFRs may be defined over the ML model, data, or the entire system; and NFRs for ML may be measured differently. In this work, we aim to understand the state-of-the-art and challenges of dealing with NFRs for ML in industry. We interviewed ten engineering practitioners working with NFRs and ML. We find examples of (1) the identification and measurement of NFRs for ML, (2) identification of more and less important NFRs for ML, and (3) the challenges associated with NFRs and ML in the industry. This knowledge paints a picture of how ML-related NFRs are treated in practice and helps to guide future RE for ML efforts.",Yes,"본 논문은 산업 현장에서 머신러닝 관련 비기능 요구사항(NFRs)을 이해하고 도전 과제를 탐구하기 위해 실제 엔지니어들과 인터뷰를 수행한 연구로, 독창적인 데이터 수집과 분석을 통해 새로운 지식을 제공하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문에 해당한다."
Use of an expert system in a personnel evaluation process,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6031673,"The purpose of this study was to develop an Expert System (ES) to evaluate Nuclear Power Plant (NPP) operators. The study included a review of the Artificial Intelligence (AI) literature relevant to the NPP and human resource management, knowledge acquisition, knowledge representation, knowledge encoding, and Inference engine. An expert system development tool FuzzyCLIPS6.1 was used to develop a fuzzy rule-based expert system called NPPOEX. NPP operator records obtained from a Chinese NPP were entered into NPPOEX and the system produces a crisp value for Evaluation in the range [0,1], a certainty factor and brief explanations for the result.",Yes,"본 논문은 원자력 발전소 운영자 평가를 위한 전문가 시스템을 개발하는 연구를 수행하였으며, 인공지능 기법과 퍼지 규칙 기반 시스템을 적용하여 직접적인 연구 결과물을 제시하고 있다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 판단된다."
Early Skin Cancer Detection Using CNN-ABCD Rule Based Feature Extraction Classification and K-Means Clustering algorithm through Android Mobile Application,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493828,"Our proposed system focuses on the development of a skin cancer prediction model using machine learning algorithms. Skin cancer is the most common cancer disease in almost all the country. Current estimates are more than a million cases are witnessing skin cancer in their lifetime. As per the statistics, the north region of India stands first with 100,000 skin for both males and females with 1.62lakhs and 1.21lakhs, respectively. This proposed system's aim is to create an accurate and efficient diagnostic mobile application to classify skin cancer based on a dataset of skin lesion images obtained from the Kaggle data source. Advanced machine learning techniques such as ABCD Feature rule is employed in the proposed model to achieve high predictive accuracy, and the model outcome will undergo comprehensive performance analysis, evaluating metrics such as sensitivity, specificity and accuracy. The significance is underscored by the growing global incidence of skin cancer, as early detection is crucial for improving patient outcomes. The versatile tool such as an android mobile application generated by the proposed model can be deployed in diverse clinical settings, aiding dermatologists and medical professionals in making timely and informed decisions for accurate diagnosis and effective treatment plans. The research methodology is based on qualitative analysis where various kinds of literature are being reviewed based on machine learning.",No,"논문 초록에서 제안된 시스템은 기존 데이터셋을 활용한 피부암 예측 모델 개발과 문헌 리뷰에 기반한 정성적 분석에 초점이 맞춰져 있으며, 독창적인 연구 방법론이나 새로운 실험 결과에 대한 직접적인 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
A statistical framework for labeling unlabelled data: a case study on anomaly detection in pressurization systems for high-speed railway trains,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892880,"The ability to perform predictive maintenance, as one of the main asset of Industry 4.0, is known to help improve downtime, costs, control and production quality. Modern predictive maintenance programs involve machine learning techniques, within the AI umbrella, that work in a data-driven fashion. This is true in all machinery where, through intelligent sensors, it is possible to collect data to be processed to detect faults or carry out anomaly detection activities. This paper presents a system for the detection of anomalies in the railway context and, specifically, in the pressurization systems of Italian high-speed trains. The available real-world dataset is in form of unlabeled time series of fixed length of 600 samples. Hence, it is proposed a two-stage machine learning workflow where the first stage acts in an unsupervised fashion through a statistical technique validated by field experts with the aim of building a labeled dataset. In the second stage, the faced problem is conceived as a classification task in the context of a strong class imbalance problem - very likely in predictive maintenance - where are compared two feature engineering techniques. The first one considers directly the raw signals as input of a SVM algorithm. In the second, time series are subjected to an adaptive heuristic procedure of piece-wise approximation, whose output is a sequence of $\mathbb{R}^{2}$ vectors (slopes and intercepts). In this case, the classification task is carried out in the so-called “dissimilarity space” for pattern recognition adopting different dimensions of the representation set obtained through a clustering algorithm. The dissimilarity measure consists of an ad-hoc edit distance capable of measuring the dissimilarity between 2-dimensional sequences. In this study a k-medoids clustering procedure is adopted for balancing the dataset together with further additional techniques for solving the challenging problem of unbalanced data, offering a deep comparison related to various experimental methodologies.",Yes,"본 논문은 고속열차 압력 시스템의 이상 탐지를 위한 통계적 프레임워크와 머신러닝 기반의 두 단계 워크플로우를 제안하며, 실제 데이터셋을 활용한 실험과 새로운 기법 비교를 포함하고 있어 독창적인 연구 내용을 담고 있다. 따라서 연구 논문에 해당한다."
An Emperical ALAL YSIS of Level of Quality in Sustainalbe Smart Grids,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10451195,"The increasing importance of power quality (PQ) concerns is discussed in this study in light of distributed generation, smart grid technologies, and integration of renewable energy supplies, as well as rising consumer awareness. It highlights how important PQ is in light of the widespread use of smart grid technologies and the development of delicate electrical devices. In order to improve smart grid applications, the combination of 5G networks with the Internet of Things (IoT) being investigated. These networks provide advantages in high-speed data transmission, distant sensing, and smart sensor interfacing. This paper's main goal is to provide an extensive overview of power quality disturbance detection and categorization. The study explores artificial intelligence technologies and signal processing approaches, evaluating the benefits and drawbacks of each. The power input signal kinds (synthetic, real, and noisy), pre-processing tools, feature selection strategies, artificial intelligence algorithms, and operating modes (online vs offline) are all taken into consideration when critically analyzing automatic recognition systems. In addition, the study provides ideas for future developments and serves as a useful reference for scholars who are interested in PQ analysis. After reviewing more than 300 research articles, the study summarizes the most important studies in the subject and provides in-depth comparisons using tabular presentations.",No,"본 논문은 300편 이상의 기존 연구를 종합적으로 검토하고 비교하는 리뷰 논문으로 보이며, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 분석과 정리에 중점을 둔 것으로 판단됩니다."
High Impedance Fault Location Methods: Review and Harmonic Selection-Based Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10042433,"High Impedance Faults (HIFs) are recurring events in electrical Distribution Systems (DSs) and occur by the contact between energized conductors and high impedance surfaces. HIFs may pose hazards to living beings and cause bushfires. However, the HIF protection has not been completely solved due to the small fault current and varying impedance, inhibiting traditional protection techniques from functioning correctly. In the literature, researchers have mainly focused on detection techniques. Thus, the development of HIF Location Methods (HIFLMs) is recent, and evidences for conclusive solutions are still lacking. Moreover, to this date, no existing study reviews the main challenges concerning HIFLMs in DSs. This paper proposes a systematic analysis of the common stages to design the main existing HIFLMs. The strategy is evaluating the similar characteristics that pose a common research path regarding challenges faced in real-world conditions. Additionally, this paper proposes a case study to assess the best input signals, metrics, and machine learning-based decision algorithms of a new HIFLM. The results are promising, with high identification rates, even in noisy conditions. The methodology can help to select the datasets for supervised learning-based HIFLM. Highlighting the state-of-art of current methods and support development of HIFLMs are this paper’s main contributions.",Yes,"본 논문은 기존 연구들을 체계적으로 분석하고, 새로운 하모닉 선택 기반의 고임피던스 고장 위치 추정 방법을 제안하며, 사례 연구를 통해 머신러닝 알고리즘의 성능을 평가하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 단순 리뷰가 아닌 직접적인 연구 기여가 있는 논문으로 판단된다."
Machine Learning Tools for Long-Term Type 2 Diabetes Risk Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491154,"A steady rise has been observed in the percentage of elderly people who want and are still able to contribute to society. Therefore, early retirement or exit from the labour market, due to health-related issues, poses a significant problem. Nowadays, thanks to technological advances and various data from different populations, the risk factors investigation and health issues screening are moving towards automation. In the context of this work, a worker-centric, IoT enabled unobtrusive users health, well-being and functional ability monitoring framework, empowered with AI tools, is proposed. Diabetes is a high-prevalence chronic condition with harmful consequences for the quality of life and high mortality rate for people worldwide, in both developed and developing countries. Hence, its severe impact on humans’ life, e.g., personal, social, working, can be considerably reduced if early detection is possible, but most research works in this field fail to provide a more personalized approach both in the modeling and prediction process. In this direction, our designed system concerns diabetes risk prediction in which specific components of the Knowledge Discovery in Database (KDD) process are applied, evaluated and incorporated. Specifically, dataset creation, features selection and classification, using different Supervised Machine Learning (ML) models are considered. The ensemble WeightedVotingLRRFs ML model is proposed to improve the prediction of diabetes, scoring an Area Under the ROC Curve (AUC) of 0.884. Concerning the weighted voting, the optimal weights are estimated by their corresponding Sensitivity and AUC of the ML model based on a bi-objective genetic algorithm. Also, a comparative study is presented among the Finnish Diabetes Risk Score (FINDRISC) and Leicester risk score systems and several ML models, using inductive and transductive learning. The experiments were conducted using data extracted from the English Longitudinal Study of Ageing (ELSA) database.",Yes,"본 논문은 당뇨병 위험 예측을 위한 새로운 머신러닝 모델(WeightedVotingLRRFs)을 제안하고, 이를 평가 및 비교하는 독창적인 연구 내용을 포함하고 있다. 또한, 데이터셋 생성, 특징 선택, 분류 과정 등 연구 방법론을 구체적으로 다루어 직접적인 연구 기여가 명확하다."
Evaluating Integration of Autonomous Underwater Vehicles into Port Protection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9977239,"As underwater vehicle technology, manufacturing processes, and artificial intelligence continue to advance, autonomous underwater systems are now faster, smarter, more connected, and capable of completing a great variety of mission types. Although the ocean science community uses these systems for oceanography and exploration, potential adversaries could re-task these systems to threaten port facilities and infrastructure. It is imperative that our nation and our industry partners evaluate and develop port protection systems that utilize, and defend against, autonomous underwater vehicle threats of the future. This paper reviews the problem of port protection, its importance, technical challenges, and existing and novel technology that may support the required protection functions. A literature review outlines the history of port security capabilities, both supporting and adversarial systems, including underwater vehicles and marine mammals. Assumptions, variable definitions, and methodology for evaluating port protection scenarios are presented. A port protection solution is introduced with corresponding evaluation strategies and potential technologies for implementation. We conclude by recommending that autonomous underwater vehicle technology and associated protection solutions be further developed, and research gaps be resolved, to develop a comprehensive port protection solution for the future.",Yes,"논문은 자율 수중 차량을 활용한 항만 보호 시스템에 대한 평가 방법론과 기술적 해결책을 제시하며, 가정과 변수 정의, 평가 전략 등 구체적인 연구 방법을 포함하고 있다. 이는 기존 연구를 검토하는 데 그치지 않고 독창적인 연구 내용과 기여를 포함한 연구 논문임을 나타낸다."
Development of an Interpretable Maritime Accident Prediction System Using Machine Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9759297,"Every year, maritime accidents cause severe damages not only to humans but also to maritime instruments like vessels. The authors of this work therefore propose a machine learning-based maritime accident prediction system that can be used to prevent maritime accidents from happening by predicting and interpreting the accidents. This work overcomes the limitations of the existing works that lack practicability in the sense that the ex-post analyses are conducted to suggest accident prevention strategies but maritime accidents are not analyzed holistically. Using extensive literature reviews and expert interviews, a large number of risk factors associated with maritime accidents are identified, and related data are collected and utilized in the work. Throughout variable selection, data retrieval, hot-spot identification, and the maritime accident prediction model construction process, various machine learning algorithms are exploited in order to construct an organized system. In addition, interpretations for the resulting accident predictions are given using interpretable machine learning algorithms so as to provide explainable results to users. Finally, the proposed system is evaluated using a SERVQUAL model and proves its effectiveness in real-world applications.",Yes,"본 논문은 해양 사고 예측 시스템을 개발하기 위해 다양한 머신러닝 알고리즘을 적용하고, 변수 선택부터 모델 구축, 해석 가능성 제공까지 직접적인 연구 과정을 포함하고 있다. 또한, 실제 데이터 수집과 전문가 인터뷰를 통해 독창적인 연구 내용을 제시하고 있어 연구 논문에 해당한다."
A Manual: Developing Artificial Social Intelligence (ASI) Lite-Scale for Service Robots,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545546,"The manifestation of “Artificial Social Intelligence (ASI)” stands as a cornerstone of a social robot system development that influences users’ interactions and experiences overall in the ever-evolving landscape of user-centric Human-Robot Interaction (HRI). Recognizing the pivotal need to evaluate a socially interactive system accurately, this paper presents a unidimensional-scale measurement of ASI that measures a focused dimension of users’ perceived social intelligence in a robot, minimizes participants’ fatigue to generate higher response rates, maximizes the ability to conduct user-friendly research, and enhances the ease of interpreting the results that makes it more accessible to a diverse audience. Employing a cross-disciplinary literature review, personal interviews (n = 14), and large-scale surveys (n = 2,358) consisting of its five video-based stimuli data collection process, this study adhered meticulously to numerous scale measurement procedures to develop an “ASI Lite-Scale” and validated it with multiple tests, including Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), and Exploratory Graph Analysis (EGA), assessment tests of convergent, discriminant, and nomological validity, and multi-group measurement invariance analysis to establish its robustness and ability to be generalized. This study of ASI Lite-Scale provides a structured scale development manual to help fellow researchers employ this methodology and reach a wider readership, thereby fostering the development of validated scale measurements in the field of HRI.",Yes,"본 논문은 인공 사회적 지능(ASI) 측정을 위한 새로운 척도(ASI Lite-Scale)를 개발하고, 이를 검증하기 위해 다양한 실험과 분석을 수행한 독창적인 연구 내용을 포함하고 있다. 척도 개발과 검증 과정에서 직접 수집한 데이터와 통계적 방법을 활용하여 연구에 기여하고 있으므로 연구 논문에 해당한다."
Heart Stroke Prediction using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10425509,"The system proposed in this paper specifies. An overlook that monitors stroke prediction. Prediction is done based on the condition of the patient, the ascribe, the diseases he has, and the influences of those diseases that lead to a stroke, early prediction of heart stroke risk can help in timely Intercede to minimize the risk of stroke, by making use of Machine learning algorithms, for stroke prediction. This study aims to develop a machine-learning model that can accurately predict a stroke. The data collected for the study include the clinical and demographic characteristics of the patients. Either had a stroke or are in danger of having one. On the preprocessed dataset, various machine learning models are trained, including logistic regression, decision trees, random forests, and the KNN model, Naive Bayes. Accuracy, precision, recall, and F1-score are a few examples of standard assessment metrics that are used to assess each model's performance. The models are also tested using a confusion matrix to determine how well they can distinguish between individuals at high and low risk of having a stroke. Stroke is a leading cause of death worldwide, and early identification of individuals at risk can significantly improve outcomes, and help people be cautious and take preventative measures. Machine learning algorithms have been well suited and their flexibility in predicting stroke risk by analyzing large datasets of patient information. This review provides an outlook on recent research on stroke prediction using machine learning, including the types of data used, the algorithms employed, and the performance metrics reported. Machine learning algorithms have been applied to these data sources to identify patterns and develop predictive models. F1-Score of 96% was achieved in this study using the XGBoost classifier. The applied machine learning algorithms show potential for improving stroke risk prediction, by signifying a pattern, studying the case study, and outlying factors.",Yes,논문 초록에서 환자 데이터를 수집하고 여러 머신러닝 모델을 훈련시켜 뇌졸중 예측 모델을 개발하고 성능을 평가한 구체적인 연구 과정을 기술하고 있습니다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 나타냅니다.
Literature Review On Metaheuristics Techniques In The Health Care Industry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155079,"In recent times, machine learning has provided increasingly satisfying results in the field of medicine, providing results with very high accuracy while helping to reduce costs and diagnose the disease in real time. To achieve this, it is necessary to develop different deep machine learning techniques. Some of these are metaheuristic techniques that offer practical solutions for different types of chronic diseases. These types of algorithms have received the most attention in solving optimization problems. Therefore, this paper presents a wide review of the literature for solving the problems of feature selection using metaheuristic algorithms and selecting those that have had the highest performance compared to the results given by other algorithms. In this paper, a study of 71 articles from a research database was carried out, from which metaheuristic algorithms were analyzed and evidenced on the optimization and selection of features for the prediction of chronic diseases using numerical, binary, or even imaging data. The efficiency of the algorithms is measured based on the accuracy results, error rate, F-means, or other parameters or graphical representations found in this study. This work will help researchers to improve any of the methods, hybridize them, or even build applications for predicting diseases in the future. Gaps in this field have also been identified, and future studies should be conducted.",No,"본 논문은 메타휴리스틱 기법에 관한 기존 연구들을 종합적으로 검토한 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 요약하고 분석하는 리뷰 논문에 해당합니다."
A Review on the Applications of PSO-Based Algorithm in Demand Side Management: Challenges and Opportunities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130282,"The increase in energy consumption, environmental pollution issues, and low-carbon agenda has grown the research area of demand side management (DSM). DSM programs provide feasible solutions and significantly enhance the efficiency and sustainability of electrical distribution systems. This paper classifies and discusses the broad definition of DSM based on the comprehensive literature study considering demand response and energy efficiency. The implementation of Artificial Intelligence algorithms in DSM applications has been employed in many studies to help researchers make optimal decisions and achieve predictions by analyzing the massive amount of historical data. Owing to its simplicity and consistent performance in fast convergence time, Particle Swarm Optimization (PSO) is widely used as a part of the swarm AI algorithm and has become a prominent technique in the optimization process to exploit the full benefit of the demand-side program. The variants of PSO have been developed to overcome the limitations of the original PSO and solve the high complexity and uncertainty in the DSM optimization process. The proposed PSO-based algorithm can optimize consumers’ consumption curves, reducing the peak demand and hence minimizing the electricity cost when integrated with the DR programs or EE measures. The research works of the PSO algorithm in DSM have seen an increasing trend in the past decade. Therefore, this paper reviewed the application of the PSO-based algorithm in DSM fields with some constraints and discussed the challenges from the previous work. The potential for new opportunities is identified so that PSO methods can be developed for future research.",No,"초록에서 해당 논문은 PSO 기반 알고리즘의 DSM 적용에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구의 분류, 논의, 한계점 및 향후 연구 방향 제시에 중점을 두고 있습니다."
Deep Learning-Based Classification of Rice Varieties Using Image Analysis: A Comparative Study of Neural Network Architectures,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911814,"Rice is quite rich in genetic variation that has led to thousands of varieties each with unique patterns, shapes, and colors. The work of the research uses image analysis together with machine learning approaches to classify five types of rice: Arborio, Basmati, Ipsala, Yasemin, and Karacadag. The paper contains a data set of 75,000 images and an advanced architecture for a neural network that will improve quality grading of rice and help push agricultural application computer vision technology forward. The authors describe a deep learning framework capable of classifying various types of rice by using agricultural data from different collections of rice images. The model is demonstrated through experiments on activities related to image classification. The evaluation performance measures used were accuracy, precision, and recall. The experiments gave a peak accuracy of 97.59% on the dataset in question. These results underscore the fact that deep learning models can augment both the accuracy and efficiency of skin cancer diagnosis while emphasizing the necessity to utilize data from an extensive and diverse population for the training of the models. Furthermore, the higher effectiveness of AlexNet architecture compared with LeNet architecture suggests that more complex and deeper convolutional neural network architectures would perhaps be appropriate for cancer prediction.",No,"초록에서 제시된 연구는 쌀 품종 분류를 위한 딥러닝 모델 개발과 성능 평가에 관한 내용이나, 마지막 부분에서 피부암 진단과 관련된 내용이 혼재되어 있어 연구 내용이 일관되지 않고 독창적인 연구 기여로 보기 어렵습니다. 또한, 기존 신경망 아키텍처 비교에 초점을 맞춘 기술 검토 성격이 강해 직접적인 독창적 연구 결과로 판단하기 어렵습니다."
Artificial Intelligence based assessment and development of student’s Non-cognitive skills in Professional Education through an online Learning Management System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171137,"Professional education programs like Engineering, Medical, Legal, Management basically process to transform the raw student into a polished professional of the respective domain. Hence the holistic development of students in both the cognitive and non-cognitive aspects is the aim of the processes. The educational objective is very well defined in such programs which are mapped to students outcome at a graduating level. There are various assessment tools to capture student’s development in terms of skills and knowledge which covers the cognitive aspect but these tools miss out on the non-cognitive part of total assessment and development too. Most of the professional institutes have their online Learning management system (LMS) to harness various teachinglearning-assessment activities of the education process. LMS provides a common platform for various stockholders like faculty, student, parents, and policymakers. The paper focused on artificial intelligence based on various approaches for LMS so that it can be effective and efficient for the holistic development of the students. In this paper section 1: give detail background about non-cognitive skill requirements in the professional education program as per literature review by researchers. Section 2: take through various outcomes that get affected due to Non-cognitive skill as per international studies. Section 3: discuss various techniques or instruments which are used to measure Non-cognitive skills. Section 4: it is about the role of the Artificial Intelligence system in the Learning Management aspect. Finally in section 5: researchers propose three different approaches using AI to enhance LMS for holistic assessment and development of students.",Yes,논문은 인공지능을 활용하여 온라인 학습관리시스템(LMS)에서 학생의 비인지 능력을 평가하고 개발하는 세 가지 새로운 접근법을 제안하는 등 독창적인 연구 내용을 포함하고 있다. 또한 기존 문헌 검토와 다양한 기법 분석을 바탕으로 실제 적용 가능한 AI 기반 방법론을 제시하고 있어 연구 논문에 해당한다.
Agent-Based Intelligent Decision Support Systems: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222332,"Decision-making complexity, in a distributed environment, is due to hard tasks that a system must resolve. This complexity makes researchers focus on looking for solutions to cope with distributed environment problems. Multiagent system (MAS) technology is one of several proposed solutions. This technology rose the challenge in decision-making applications. To date, no systematic review has been conducting, to the best of the authors’ knowledge, to give an overview of a multiagent-based decision-making system in various areas of science or technology. In this study, we review of 58 studies published from 2007 to 2019. The aim of this article is a critical analysis of recent approaches. We try to survey their impact on practice and research. The analysis of the extracted studies is based on three selection criteria that are defined in the paper. All included studies have analyzed from four different points of view: 1) theoretical view; 2) technical view; 3) agent view; and 4) application view. Moreover, we adopt the SWOT analysis to evaluate studied approaches. Multiagent technology is actually in the process of evolution and enhancement with the appearance of new trends in artificial intelligence, such as neural network and deep learning. The results of this review show suggestions for further research and practice.",No,"이 논문은 2007년부터 2019년까지 발표된 58개의 연구를 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과를 제시하기보다는 기존 연구들을 분석하고 평가하는 데 중점을 두고 있습니다. 따라서 새로운 실험이나 연구 결과를 포함한 원본 연구 논문으로 보기 어렵습니다."
A Comprehensive Review of Signal Processing and Machine Learning Technologies for UHF PD Detection and Diagnosis (I): Preprocessing and Localization Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422716,"Partial discharge (PD) detection and diagnosis based on the ultra-high frequency (UHF) signals is one of the most widely adopted methods to evaluate the internal insulation status of high voltage equipment. Benefit from the rapid development of computing hardware and data processing algorithms, the intelligent PD fault diagnosis method based on the UHF data has made considerable progress in the past two decades. This two-part paper aims to give a comprehensive review about the application of signal processing and machine learning technologies in UHF PD detection and diagnosis. These technologies are divided into three categories according to their respective purpose, which are the preprocessing technology, source localization technology and pattern recognition technology. As the first one of the two-part review, we focus on the preprocessing and localization approaches in this paper. Specifically, for the preprocessing topic, the methods for signal denoising, multi-source separation, and pulse segmentation are included. While for the localization topic, the time difference of arrival (TDOA) method, direction of arrival (DOA) method, received signal strength indicator (RSSI) method, and other latest methods are reviewed. For each topic, the basic ideas, recent research progresses, advantages and limitations are discussed in detail. Before the conclusion, we also make a discussion about the application effects of the above technologies and prospect some future directions accordingly. In the second paper, the pattern recognition problems based on the UHF PD data will be concentrated, especially the application of deep learning algorithms.",No,"본 논문은 UHF 부분방전 검출 및 진단을 위한 신호처리와 머신러닝 기술에 대한 포괄적인 리뷰 논문으로, 기존 연구들을 정리하고 비교하는 내용을 담고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Detecting Throat Cancer From Speech Signals Using Machine Learning: A Scoping Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10945305,"Cases of throat cancer are rising worldwide. With survival decreasing significantly at later stages, early detection is vital. Artificial intelligence (AI) and machine learning (ML) have the potential to detect throat cancer from patient speech, facilitating earlier diagnosis and reducing the burden on overstretched healthcare systems. However, no comprehensive review has explored the use of AI and ML for detecting throat cancer from speech. This review aims to fill this gap by evaluating how these technologies perform and identifying issues that need to be addressed in future research. We conducted a scoping literature review across three databases: Scopus, Web of Science, and PubMed. We included articles that classified speech using ML and specified the inclusion of throat cancer patients in their data. Articles were categorised based on whether they performed binary or multi-class classification. We found 27 articles fitting our inclusion criteria, 12 performing binary classification, 13 performing multi-class classification, and two that do both binary and multi-class classification. The most common classification method used was neural networks, and the most frequently extracted feature was mel-spectrograms. We also documented pre-processing methods and classifier performance. We compared each article against the TRIPOD-AI checklist, which showed a significant lack of open science, with only one article sharing code and only three using open-access data. Open-source code is essential for external validation and further development in this field. Our review indicates that no single method or specific feature consistently outperforms others in detecting throat cancer from speech. Future research should focus on standardising methodologies and improving the reproducibility of results.",No,"이 논문은 기존 연구들을 종합하여 AI와 ML을 이용한 후두암 음성 신호 탐지 연구 현황을 평가하는 문헌 리뷰 논문입니다. 직접적인 독창적 연구 결과나 실험을 제시하지 않고, 기존 연구들을 분석하고 요약하는 데 중점을 두고 있습니다."
Text Summarization for Tamil Online Sports News Using NLP,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736154,"Text summarization plays an important problem in natural language understanding and information retrieval. Automatic text summarization get much more attention by people presently because it is efficiently and effectively serve time in decision making process even for day to day life. Presently deep learning models get more attention than the traditional approaches. The primary objective of this research work is to propose a methodology to address the problem of summarization for Tamil sports news which can automatically create extractive summary for the news data with the use of Natural Language Processing (NLP) and a generic stochastic artificial neural network. Features such as sentence position, sentence position related to paragraph, number of named entities, term frequency and inverse document frequency and Number of numerals are employed to construct the feature matrix for each sentence and Restricted Boltzmann Machine is used to improve those features while enhancing the accuracy without loosing the main idea of the text. Experimentation is carried out using Online Tamil sports news and ROUGE tool kit is used to evaluate the recall, precision and F-measure for the summary generated by both the human experts and the system.",Yes,"본 논문은 Tamil 스포츠 뉴스의 자동 요약 문제를 해결하기 위한 새로운 방법론을 제안하고, 제한된 볼츠만 머신을 활용하여 특징을 개선하는 독창적인 연구 내용을 포함하고 있다. 또한 실험과 평가를 통해 제안한 모델의 성능을 검증하였으므로 연구 논문에 해당한다."
Bilingual Chatbot Powered by Artificial Intelligence for Academic Advice,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426517,"The role of academic advising in advancing institutions' educational goals is becoming more widely recognised. Utilizing technology tools like Canvas and Degree Works, universities have made considerable investments in assisting instructional advisers and student success measures. The demand for innovation that can support academic advisors in their duty to direct students toward academic achievement is growing, and AI tools that encourage the effective use of human capital are well-placed to fill that requirement. Functions for chatbots can be used as technology-mediated assistance to give knowledge and engaging content to help students succeed. This project set out to create, build, and assess an automated, AI-enabled advising resource (the AVA chatbot) for use in an honour’s college setting to provide academic counselling and university-related information. The researchers came to the conclusion that chatbots could assist educational universities in efficiently supporting their student populations by giving 24/7 advice, following and evaluating total assistance, and identifying data perceptions in real time. The consequences of this research can be helpful for chat application design and manufacturing as well as technology adoption in higher education.",Yes,"본 논문은 AI 기반의 이중언어 챗봇을 개발하고 평가하는 프로젝트를 수행하였으며, 이는 독창적인 연구 내용과 직접적인 기여를 포함하고 있다. 또한 챗봇의 기능과 효과를 실증적으로 분석하여 학술적 조언 지원에 대한 새로운 기술적 해결책을 제시하고 있다."
Evaluating Model Performance Through a User-Centric Explainable Framework for Probabilistic Load Forecasting Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10474692,"Load forecasting models ensure efficient, secure, and stable operation of the modern power system. Probabilistic forecasting accounts for uncertainties associated with missing features that are often overlooked by deterministic approaches. However, machine learning-based probabilistic models are complicated. This paper proposes a user-centric explainable AI framework that presents global and local interpretations aligned with the expertise and explanation needs of the targeted user. The overall influence of temporal and spatial exogenous features at the model development stage is evaluated using the Permutation Feature Importance technique. Such an explanation provides a holistic picture of the knowledge gained by the Gradient Boosting Regressor-based probabilistic load forecasting model. Further-more, the proposed framework suggests the implementation of SHapely Additive exPlanations (SHAP) at the post-deployment stage for individual forecast instances. Local explanations provided by SHAP are used to distinguish between interval forecasts with higher and lower forecast accuracy. Such distinction is applied for both the lower and upper bounds of the forecast interval. This is specifically useful for the non-AI expert end-users that need load forecasts for their strategizing their daily operations. This work is validated on the Kaggle data set on the national load demand of Panama supported with several other exogenous features such as weather-related quantities, holidays, and date-time details. Results show the efficacy of the proposed framework and its ability to provide user-friendly interpretations aligned with users' explanation goals.",Yes,"논문은 확률적 부하 예측 모델의 성능 평가를 위해 사용자 중심의 설명 가능한 AI 프레임워크를 제안하고, 이를 실제 데이터셋에 적용하여 검증한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Intrusion Detection Systems Based on Machine Learning Approaches: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124506,"The proliferation of Internet use poses certain security problems for networks. Intrusion detection (ID) in cybersecurity technology is to recognize unexpected entry to or assaults on secured networked computers. In the research, many machine learning (ML) and deep learning (DL) algorithms have been used to tackle intrusion detection systems (IDS). Nevertheless, few publications examine and explain the present state of employing ML approaches to tackle ID issues. This systematic review (SR) analyzes 11 papers published between 2016 and 2021 that focused on developing single, hybrid, and ensemble classifiers. Similar research is evaluated based on their classifier designs, the datasets they used, and their conceptual frameworks. Recent accomplishments and limits in developing IDS systems using ML are presented and analyzed. In addition, many prospective study possibilities are offered.",No,"이 논문은 머신러닝 기반 침입 탐지 시스템에 관한 기존 연구들을 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 분석에 초점이 맞춰져 있습니다."
"Comparative Analysis of Research Article Matching using SIF, RNN, Attention, and Hybrid Methods",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330854,"Search engines make it easier to conduct literature reviews. However, for niche topics, search results are often poor. Snowballing can help, but it is limited by the initial articles, especially by the authors’ access when they were written. As an alternative, research paper databases have provided recommendation features; however, these are limited to their own articles. A tool to search for similar articles without relying on a specific database would be helpful, but before that, a proper method to match similar articles must be found. This research aims to match similar articles based on title, authors, and keywords using deep learning methods, which are SIF, RNN, Attention, and Hybrid methods, and evaluate them. This study also compares the combinations of features used in matching. The attention method using only the article title as a feature yielded the best result. The attention method was also faster than the hybrid method for training and use. Using only one feature should be even faster. In addition, the title field was found to be the best feature for predicting similarity matches. The author name feature was bad on its own but could improve the results when combined with the title. The keyword feature was found to be almost as good as the title, but combining them did not result in significant improvement.",Yes,"본 논문은 SIF, RNN, Attention, Hybrid 등 다양한 딥러닝 기법을 활용하여 연구 논문 매칭 방법을 개발하고 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 여러 특징 조합의 성능을 비교 분석하는 실험적 기여가 명확히 제시되어 있어 연구 논문에 해당한다."
Deep reinforcement learning from error-related potentials via an EEG-based brain-computer interface,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621183,"Reinforcement learning (RL) from human preferences suffered from temporal and interaction environments' limitations, which rule out real-time and real-world robotic applications of deep RL. To overcome the limitations, this study introduced the electroencephalography (EEG)-measured error-related potentials (ErrPs) to train a robotic RL system based on a brain-computer interface (BCI). We decoded ErrP signals by selecting human preferences in real-time to train robotic behavior by deep RL during a binary object selection task. Twelve healthy subjects participated in the ErrP experiments, in which they were asked to select and adjust self-favored behavior after a machine's random selections. The decoded ErrP signals classified by a convolutional neural network (CNN)architecture to achieve an average classification accuracy and an area under the ROC curve of 67.49% and 0.639, respectively. By using the well-trained ErrP signals classifier to train the deep RL system, our final results for training robotic behavior through ErrP-based preferences showed an average of 15.21% improvement in efficiency while obtaining acceptable rewards in RL. Thus, the work used brain signals instead of pressing or clicking buttons as the rewards of RL, and constructed a real-time and free from interaction interference intuitive RL system.",Yes,"본 논문은 EEG 기반 뇌-컴퓨터 인터페이스를 활용하여 오류 관련 전위(ErrP)를 딥 강화학습에 적용하는 독창적인 연구를 수행하였으며, 실험을 통해 분류기 성능과 강화학습 효율 향상 결과를 제시하고 있다. 이는 직접적인 연구 기여와 실험적 검증을 포함한 연구 논문에 해당한다."
Twitter (X) Spam Detection Using Natural Language Processing by Encoder Decoder Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10866112,"Applications for social networking, such as Twitter, have grown in popularity in several fields, including politics, religion, economics, and entertainment. There is a lot of information available because of this popularity spike, some of which might be accurate while others might not be. By identifying irrelevant people and their material, spam detection is essential in resolving this problem. But up until recently, most research has concentrated on using activity detection and related technologies to collect user profile data. However, these techniques might not work as well if user profiles show temporal dependency or don't accurately represent the stuff they create. By concentrating on user profile data and content-based spam detection, this study seeks to address this problem. It presents three noteworthy additions. First, it uses cutting-edge natural language processing (NLP) techniques to create an extensive dataset with a wide variety of content-based attributes. Second, it analyzes this dataset using a hybrid machine learning model that combines deep learning and machine learning techniques. The practical value of this approach is highlighted by extensive simulations, which show that modeling both profile and content-generated data together works better than using individual techniques, with a combined spam detection accuracy of over 98 %. Finally, the paper presents a new approach based on logistic regression that is backed up by mathematical formulas. With the use of this technique, it is possible to evaluate the dataset and determine the likelihoods that legitimate users will differ from spammers. Future user categories can be predicted using mathematical results by varying the settings for each dataset. As a result, this method shows itself to be adaptable and efficient in identifying and classifying various user groups.",Yes,"논문 초록에서 새로운 데이터셋 구축, 하이브리드 머신러닝 모델 개발, 그리고 로지스틱 회귀 기반의 새로운 접근법 제시 등 독창적인 연구 기여가 명확히 드러나 있습니다. 또한, 실험을 통한 성능 검증도 포함되어 있어 연구 논문에 해당합니다."
A Novel Target Detection Method Based on Improved YOLO Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345070,"Real-time traffic flow prediction based on artificial intelligence refers to the use of large data of vehicles in intelligent transportation system to calculate the depth of artificial intelligence and then predict real-time traffic flow. However, the related work of AI is facing many challenges, such as the large number of traffic physical objects, the low accuracy of spatial location of traffic information and the diversity of vehicles, which will have a great impact on real-time traffic flow forecasting. Therefore, this paper studies the artificial intelligence algorithms of traffic physical object (TPO) and traffic information space (TIS) under the background of intelligent transportation system. A state-based filtering module is used to improve the prediction accuracy of ITS. The theoretical analysis and experimental results show that the proposed method has high execution efficiency and improves the accuracy of real-time traffic flow prediction.",No,"논문 제목은 YOLO 기반의 타겟 검출 방법에 관한 내용이지만, 초록에서는 인공지능을 활용한 실시간 교통 흐름 예측에 관한 연구를 다루고 있습니다. 또한, 초록 내용이 기존 알고리즘 개선과 실험 결과를 포함하고 있으나, 직접적인 독창적 연구 기여보다는 기존 AI 알고리즘 적용 및 분석에 초점이 맞춰져 있어 연구 논문으로 보기 어렵습니다."
Generative Adversarial Network for Synthetic Time Series Data Generation in Smart Grids,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587464,"The availability of fine grained time series data is a pre-requisite for research in smart-grids. While data for transmission systems is relatively easily obtainable, issues related to data collection, security and privacy hinder the widespread public availability/accessibility of such datasets at the distribution system level. This has prevented the larger research community from effectively applying sophisticated machine learning algorithms to significantly improve the distribution-level accuracy of predictions and increase the efficiency of grid operations. Synthetic dataset generation has proven to be a promising solution for addressing data availability issues in various domains such as computer vision, natural language processing and medicine. However, its exploration in the smart grid context remains unsatisfactory. Previous works have tried to generate synthetic datasets by modeling the underlying system dynamics: an approach which is difficult, time consuming, error prone and often times infeasible in many problems. In this work, we propose a novel data-driven approach to synthetic dataset generation by utilizing deep generative adversarial networks (GAN) to learn the conditional probability distribution of essential features in the real dataset and generate samples based on the learned distribution. To evaluate our synthetically generated dataset, we measure the maximum mean discrepancy (MMD) between real and synthetic datasets as probability distributions, and show that their sampling distance converges. To further validate our synthetic dataset, we perform common smart grid tasks such as k-means clustering and short-term prediction on both datasets. Experimental results show the efficacy of our synthetic dataset approach: the real and synthetic datasets are indistinguishable by solely examining the output of these tasks.",Yes,"논문은 스마트 그리드 분야에서 GAN을 활용한 합성 시계열 데이터 생성이라는 독창적인 연구 방법을 제안하고, 이를 평가하기 위한 실험적 검증을 수행하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Investigating adversarial attacks against Random Forest-based network attack detection systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154328,"A significant research effort in cybersecurity currently deals with Machine Learning-based attack detection. It is aimed at providing autonomous attack detection systems that require less human expert resources, and are then less expensive in time and money. Indeed, such systems are able to autonomously learn about benign and malicious traffic, and to classify further traffic samples accordingly. In such context, hackers start designing adversarial learning approaches in order to design new attacks able to evade from the Machine Learningbased detection systems. The work presented in this paper aims at exhibiting how easy it is to modify existing attacks to make them evade from the Machine Learning-based attack detectors. The Random Forest algorithm has been selected for this work as it is globally evaluated as one of the best Machine Learning algorithm for cybersecurity, and it provides informations on how a decision is made. Indeed, the analysis of the related Random Forest trees helps explaining the limits of this Machine Learning algorithm, and gives some information that could be helpful for making attack detection somewhat explainable. Several other Machine Learning algorithms as SVM, kNN ans LSTM have been selected for evaluating their ability to detect the adversarial attack presented in this paper.",Yes,"본 논문은 Random Forest 기반 네트워크 공격 탐지 시스템에 대한 적대적 공격(adversarial attacks)을 조사하고, 이를 회피하기 위한 공격 변형 방법을 제안하는 등 독창적인 연구 내용을 포함하고 있다. 또한 여러 머신러닝 알고리즘의 탐지 성능을 평가하는 실험적 기여도 포함되어 있어 연구 논문에 해당한다."
Harnessing Machine Learning for Mental Health: A Study on Classifying Depression-Related Social Media Posts,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10775133,"This study is of particular relevance in the way it identifies depression-related content on social media using a machine learning model to classify posts and comments. This dataset, encompassing around 6500 entries from various platforms including Facebook, was rigorously annotated by four proficient English-speaking undergraduate students together with the final label which is established via majority voting. Data Preprocessing, initial cleaning, normalization and TF-IDF feature creation through vectorization for the output of POS tags. The different machine learning models that were trained and tested are Logistic Regression, Random Forest, SVM (Support Vector Machine), Naive Bayes Gradient Boosting Algorithm K-NN (K nearest Neighbors) AdaBoost Decision Tree. Authors evaluated the models and measured their accuracy, precision score, recall rate (also known as sensitivity) in addition to F1-score. Gradient Boost, Random Forest, and SVM were top performers among which Gradient boosting was found to be an overall best one with almost 98.5%. They show that machine learning model can successfully predict the label of social media posts, as a way for accurately identifying depression from text data. This detailed model performance evaluation is useful in understanding what each approach does well and poorly, shedding light into whether they are / would be actually suitable for real-world applications. This study not only developed discriminative classifiers, but also included detailed analysis of their performance which should hopefully guide future work and help in practical implementations for real-time mental health monitoring. Through this work, this study aim to facilitate timely identification of depression-related posts, ultimately supporting mental health awareness and intervention efforts on social media platforms.",Yes,"논문은 머신러닝 모델을 활용해 우울증 관련 소셜미디어 게시물을 분류하는 독창적인 연구를 수행하고 있으며, 다양한 알고리즘을 적용하고 성능을 평가하는 구체적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Masked Face Recognition by Applying SSD and ResNet Model for Attendance System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807814,"The rapid development of technology in artificial intelligence (AI) includes face recognition. Some of these developments in face recognition systems can be found easily in everyday life, such as entry access, payment, or attendance recording systems. Notably, the demand for image-based face recognition methods to record attendance increases due to its effectiveness and efficiency. Compared to conventional methods like fingerprint or RFID, facial recognition offers better results. On the other hand, with the arising needs in dealing with the novel coronavirus (COVID-19) pandemic, official health protocols require mask-wearing and maintaining a minimum distance of 1 meter between individuals to prevent the spread of the virus. Along with such a situation, a face recognition system to record attendance can reduce the occurrence of direct contact and allow each individual to maintain a safe distance, including from the attendance device. This paper presents the performance of a masked-faced recognition system that implemented SSD (Single Shot Detection) and ResNet feature extraction. The face recognition system application developed using Python and related libraries show a stable level of masked face recognition accuracy. This evaluation was made at predetermined distances between the face and the camera and measured at room lighting of 200 lux with an average accuracy of 67%. The application also has a feature to send notification emails to every employee who is unable to attend work on their scheduled workdays.",Yes,"논문은 SSD와 ResNet 모델을 적용한 마스크 착용 얼굴 인식 시스템을 개발하고, 그 성능을 평가한 내용을 포함하고 있어 독창적인 연구 기여가 명확하다. 또한, 구현된 시스템의 정확도 측정과 기능적 특징(출석 알림 이메일 전송)도 기술되어 있어 연구 논문으로 판단된다."
ReLink: Complete-Link Industrial Record Linkage Over Hybrid Feature Spaces,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458710,"Record Linkage (ReL) is the task of identifying records from a pair of databases referring to the same realworld entity. This has many applications in organisations of all sizes where related data often exist in silos leading to inefficiency in data engineering and analytics applications as well as ineffectiveness of business applications (e.g., unable to personalise marketing campaigns).State-of-the-art (SOTA) machine learning and deep learning based ReL techniques use adaptive similarity measures and learn their relative contributions based on labeled data. However, we report here that they do not work with similar efficacy on industrial data owing to its fundamental differences such as magnitude of schema heterogeneity, need for leveraging structure of the data, lack of training data etc. Through our proposed system ‘ReLink’, we carefully mitigate these challenges and demonstrate that it not only significantly outperforms SOTA baselines on industrial datasets but also on majority of research benchmarks. ReLink introduces the notion of complete-linkage over attributes as well as uses hybrid feature spaces on lexical and semantic similarity measures using pre-trained models such as BERT. Going beyond empirical demonstration, we provide insights and prescriptive guidance on choice of ReL techniques in industrial settings from our observations and lessons learnt from the experience of transferring and deploying for real use-cases in a large financial services organization.",Yes,"논문은 기존 기법들의 한계를 지적하고, 이를 극복하기 위한 새로운 시스템 'ReLink'를 제안하며, 산업 데이터에 특화된 완전 연결 및 하이브리드 특징 공간을 활용한 독창적인 방법론을 제시하고 있다. 또한, 실험을 통해 성능을 입증하고 실제 대규모 금융기관에 적용한 경험을 공유하는 등 직접적인 연구 기여가 포함되어 있다."
Text Mining for the Identification and Synopsization of Trusted Ratings,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10862818,"Online reviews have become increasingly valuable in the realm of purchasing and business decision-making. They not only contribute to brand loyalty but also aid customers in understanding product experiences. Unfortunately, the rise of fraudulent reviews, where authors are rewarded for generating fake evaluations to manipulate readers' perceptions, is a growing concern. This research paper aims to address this issue by developing a product review summarizer specifically designed to generate summaries based on genuine Amazon product reviews. The summarizer utilizes text analysis techniques to assess the linguistic characteristics of sentences and determine their value. The paper provides a comprehensive overview of existing and previous studies on sentiment analysis and text description. To enhance classification accuracy, a new hybrid classification system is proposed, combining multiple classification methods using arcing classifiers. The quality of these classifiers is thoroughly evaluated. In this study, supervised machine learning algorithms are compared with SVD dimensionality reduction, deep learning, and a text mining approach for summarization purposes. The model is built using a labeled Amazon review dataset. The key contribution of this paper lies in its innovative approach of generating text summaries for Amazon reviews while effectively filtering out fraudulent content.",Yes,"논문은 아마존 리뷰 데이터셋을 활용하여 새로운 하이브리드 분류 시스템과 텍스트 요약 모델을 개발하는 등 독창적인 연구 방법과 기여를 포함하고 있다. 기존 연구를 검토하는 데 그치지 않고, 실제 문제 해결을 위한 새로운 알고리즘과 모델을 제안하고 평가한 점에서 연구 논문에 해당한다."
Six-Sigma Quality Management of Additive Manufacturing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272610,"Quality is a key determinant in deploying new processes, products, or services and influences the adoption of emerging manufacturing technologies. The advent of additive manufacturing (AM) as a manufacturing process has the potential to revolutionize a host of enterprise-related functions from production to the supply chain. The unprecedented level of design flexibility and expanded functionality offered by AM, coupled with greatly reduced lead times, can potentially pave the way for mass customization. However, widespread application of AM is currently hampered by technical challenges in process repeatability and quality management. The breakthrough effect of six sigma (6S) has been demonstrated in traditional manufacturing industries (e.g., semiconductor and automotive industries) in the context of quality planning, control, and improvement through the intensive use of data, statistics, and optimization. 6S entails a data-driven DMAIC methodology of five steps—define, measure, analyze, improve, and control. Notwithstanding the sustained successes of the 6S knowledge body in a variety of established industries ranging from manufacturing, healthcare, logistics, and beyond, there is a dearth of concentrated application of 6S quality management approaches in the context of AM. In this article, we propose to design, develop, and implement the new DMAIC methodology for the 6S quality management of AM. First, we define the specific quality challenges arising from AM layerwise fabrication and mass customization (even one-of-a-kind production). Second, we present a review of AM metrology and sensing techniques, from materials through design, process, and environment, to postbuild inspection. Third, we contextualize a framework for realizing the full potential of data from AM systems and emphasize the need for analytical methods and tools. We propose and delineate the utility of new data-driven analytical methods, including deep learning, machine learning, and network science, to characterize and model the interrelationships between engineering design, machine setting, process variability, and final build quality. Fourth, we present the methodologies of ontology analytics, design of experiments (DOE), and simulation analysis for AM system improvements. In closing, new process control approaches are discussed to optimize the action plans, once an anomaly is detected, with specific consideration of lead time and energy consumption. We posit that this work will catalyze more in-depth investigations and multidisciplinary research efforts to accelerate the application of 6S quality management in AM.",Yes,"논문은 6시그마 품질 관리 방법론을 적층 제조(AM)에 적용하기 위한 새로운 DMAIC 방법론을 설계, 개발, 구현하는 내용을 포함하고 있으며, AM의 품질 문제 해결을 위한 데이터 기반 분석 기법과 프로세스 개선 방법을 제안하는 등 독창적인 연구 기여를 담고 있다. 따라서 직접적인 연구 내용이 포함된 연구 논문으로 판단된다."
VSEST 29110 Tool: Using ChatGPT to Evaluate the Implementation of the ISO/IEC 29110 Work Products,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646341,"The global software industry is predominantly composed of micro, small, and medium-sized enterprises (MSMEs), highlighting the need for software quality management to ensure the proper functioning and quality of the software. This research focuses on the evaluation of the implementation of the ISO/IEC 29110 standard work products, which is a standard tailored by the ISO/IEC specifically for MSMEs, which improves the software development process by implementing two processes in its basic profile: Project Management (PM) and Software Implementation (SI). Despite this standard being tailored specifically for this type of enterprise, implementing ISO/IEC 29110 faces several challenges, such as a lack of knowledge and difficulties in adequately implementing the work products regarding the compliance of standard criteria, among others. To address these challenges, we introduce VSEST 29110, a web tool designed to evaluate the ISO/IEC 29110 standard implementation work products by leveraging Artificial Intelligence (AI) technologies, specifically the ChatGPT model, provide detailed feedback on compliance with standard criteria, offer suggestions for improvement based on ChatGPT analysis and streamline the implementation process for MSMEs. To achieve this, our research incorporates a systematic literature review and validation through a case study by document analysis, demonstrating VSEST 29110’s effectiveness in enhancing compliance and providing comprehensive feedback compared to auditor recommendations, which impacts 69.33% on average.",Yes,"본 논문은 ISO/IEC 29110 표준 구현 평가를 위한 웹 도구 VSEST 29110을 개발하고, 이를 ChatGPT를 활용해 평가 및 피드백을 제공하는 독창적인 연구 내용을 포함하고 있다. 또한 문헌 검토와 사례 연구를 통해 도구의 효과성을 검증한 점에서 직접 기여하는 연구 논문으로 판단된다."
Deep Learning-Based Traffic Light Detection in a Custom Embedded Hardware Platform for ADAS Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10659816,"Automotive Driver Assistance Systems (ADAS) applications are currently an intensive field of study and innovation. The development of an ADAS is a multidisciplinary task involving electronic hardware design, advanced software implementation, safety considerations and many more. Building an ADAS application implies some challenges that are addressed in this paper. Firstly, all ADAS applications run on highly specific hardware devices embedded in the car with limited computation resources. In this work a novel embedded platform, iADASys, is developed and tested. The platform integrates the elements required to implement an artificial vision based ADAS application such as high performance processor with Deep Learning (DL) computation co-processors or multi-channel high resolution video streaming hardware. Secondly, this work implements an artificial vision application for traffic light detection based on deep neural networks. The model selected in this work is SSD_Mobilenet_V1 and it was trained using Bosch Small Traffic Light (BSTL) dataset. To fulfill real time requirement, the model image input resolution was maintained low at 300×300 pixel. However, the small object size in the dataset together with low resolution lead to poor detection performance. This situation was addressed by fine tuning the model training hyperparameters related to detection scales and aspect ratios. Lastly, the model is deployed in the hardware platform and its performance is measured. Model inference is executed on a specialized mathematical co-processor obtaining the required real time response. The object detection performance is also measured, obtaining promising results.",Yes,"본 논문은 ADAS용 임베디드 하드웨어 플랫폼 개발과 딥러닝 기반 교통신호등 검출 모델의 학습 및 하이퍼파라미터 튜닝을 포함한 독창적인 연구 내용을 다루고 있습니다. 또한, 개발된 모델을 실제 하드웨어에 배포하고 성능을 평가하는 실험적 기여도 포함되어 있어 연구 논문에 해당합니다."
Online Adjustment of Two-stage Inference for Knowledge Caching,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8932779,"With the rapid development of deep learning, many deep learning based smart services have emerged. These smart services usually consist of two components, the frontend user device and the back-end cloud server. The front-end device only collects queries from user and sends them to the server, and all operations of deep learning are computed in the server. This design has drawbacks of increasing load on the server and violating personal privacy. Knowledge caching is proposed in our prior work to mitigate these issues, which processes deep learning inference for frequently used queries of users in front-end devices. In this paper, in addition to cache a deep learning model for frequently used and privacy-related queries in the front-end device, we extend our prior work to implement the system on physical device and server. In particular, we design the online adjustment system for managing the status of devices and servers. This allows us to specify the process of caching the model, updating the cached model, and operating the entire system. We evaluate the system which is composed of NVIDIA Jetson Tegra X2 as the front-end device and the back-end server with TITAN Xp to confirm feasibility. As a result of evaluation, our new system shows better accuracy than the general model in the server.",Yes,"본 논문은 기존 연구를 확장하여 실제 장치와 서버에 시스템을 구현하고, 온라인 조정 시스템을 설계하여 모델 캐싱 및 업데이트 과정을 관리하는 독창적인 연구 내용을 포함하고 있다. 또한, NVIDIA Jetson Tegra X2와 TITAN Xp를 이용한 평가를 통해 시스템의 유효성을 검증하였으므로 연구 논문에 해당한다."
Leveraging Deep Learning to Strengthen the Cyber-Resilience of Renewable Energy Supply Chains: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433003,"Deep learning shows immense potential for strengthening the cyber-resilience of renewable energy supply chains. However, research gaps in comprehensive benchmarks, real-world model evaluations, and data generation tailored to the renewable domain persist. This study explores applying state-of-the-art deep learning techniques to secure renewable supply chains, drawing insights from over 300 publications. We aim to provide an updated, rigorous analysis of deep learning applications in this field to guide future research. We systematically review literature spanning 2020–2023, retrieving relevant articles from major databases. We examine deep learning’s role in intrusion/anomaly detection, supply chain cyberattack detection frameworks, security standards, historical attack analysis, data management strategies, model architectures, and supply chain cyber datasets. Our analysis demonstrates deep learning enables renewable supply chain anomaly detection by processing massively distributed data. We highlight crucial model design factors, including accuracy, adaptation capability, communication security, and resilience to adversarial threats. Comparing 18 major historical attacks informs risk analysis. We also showcase potential deep learning architectures, evaluating their relative strengths and limitations in security applications. Moreover, our review emphasizes best practices for renewable data curation, considering quality, labeling, access efficiency, and governance. Effective deep learning integration necessitates tailored benchmarks, model tuning guidance, and renewable energy data generation. Our multi-dimensional analysis motivates focused efforts on enhancing detection explanations, securing communications, continually retraining models, and establishing standardized assessment protocols. Overall, we provide a comprehensive roadmap to progress renewable supply chain cyber-resilience leveraging deep learning’s immense potential.",No,"이 논문은 300여 편의 기존 연구를 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분석하고 정리하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 현황 및 연구 방향 제시에 초점이 맞춰져 있습니다."
A Statistical Study on Intelligent Systems for the Diagnosis of Heart Failure,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397887,"ML approaches are widely utilized in the domain of health and have a major influence on people's health worldwide. The numerous benefits of such approaches are extensively being used by various health professionals, which lead to a significant enhancement in the healthcare sector. Machine learning approaches are effectively employed in the designing and development of a concurrent health monitoring system based on ML technology. These systems further assist in the prediction of heart failure in its earlier stages so that a patient can treat successfully before his or her condition gets worse. Hence, a number of publications are there that enlightened the implementation and development of such medical diagnostic systems, which assist in the prediction of heart failure by using different approaches of machine learning. Therefore, the main intent of the conducted study is to analyze the different research work presented by various authors on the development of inference systems using ML statistically. This work also investigates other review papers which cover selected approaches and systems along with the research papers which enclose the execution of these models. Different parameters such as the aim of studies, the year in which a particular paper has been published, the evaluated gap in research gap and the final outcome of the paper are used to differentiate the considered papers from one another.",No,"초록에서 본 논문은 기존 연구들을 통계적으로 분석하고 리뷰하는 연구로, 직접적인 독창적 연구 결과나 새로운 모델 개발에 대한 내용이 포함되어 있지 않습니다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 또는 메타분석에 해당합니다."
Design of an Intelligent System for Sports Motion Recognition Based on Data Analysis Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10709292,"This study is dedicated to the design and implementation of an intelligent sports motion recognition system based on data analysis algorithms, aiming to enhance the efficiency and safety of sports training and competitions. By collecting and analyzing athletes' motion data, the system is capable of recognizing and classifying various sports movements in real-time, providing coaches and athletes with immediate feedback and suggestions for improvement. The paper begins by reviewing related research in the field of sports motion recognition, particularly the application of machine learning and deep learning methods in motion recognition. Subsequently, we introduce the methodology of the system, including the detailed process of data collection, preprocessing, and algorithm selection. The system design section elaborately describes the architecture of the intelligent system, comprising hardware components and software implementation, as well as user interface design. A series of experiments were conducted to validate the effectiveness of the proposed system, indicating its high accuracy in recognizing a variety of sports movements. Moreover, we discuss the challenges and limitations encountered during the system implementation and propose potential directions for improvement. The results of this research are anticipated to significantly contribute to the scientific level of sports training and the performance of athletes.",Yes,"본 논문은 스포츠 동작 인식을 위한 지능형 시스템을 설계하고 구현하며, 데이터 수집, 전처리, 알고리즘 선택 및 실험 검증 과정을 포함한 구체적인 연구 방법론과 실험 결과를 제시하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문에 해당한다."
Possibilistic Similarity Measures for Data Science and Machine Learning Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9028115,"Measuring similarity is of a great interest in many research areas such as in data sciences, machine learning, pattern recognition, text analysis and information retrieval to name a few. Literature has shown that possibility is an attractive notion in the context of distinguishability assessment and can lead to very efficient and computationally inexpensive learning schemes. This paper focuses on determining the similarity between two possibility distributions. A review of existing similarity measures within the possibilistic framework is presented first. Then, similarity measures are analyzed with respect to their capacity to satisfy a set of required properties that a similarity measure should own. Most of the existing possibilistic similarity measures produce undesirable outcomes since they generally depend on the application context. A new similarity measure, called InfoSpecificity, is introduced and the similarity measures are categorized into three main methods: morphic-based, amorphic-based and hybrid. Two experiments are being conducted using four benchmark databases. The aim of the experiments is to compare the efficiency of the possibilistic similarity measures when applied to real data. Empirical experiments have shown good results for the hybrid methods, particularly with the InfoSpecificity measure. In general, the hybrid methods outperform the other two categories when evaluated on small-size samples, i.e., poor-data context (or poor-informed environment) where possibility theory can be used at the greatest benefit.",Yes,"본 논문은 기존의 가능성 이론 기반 유사도 측정 방법들을 검토하고, 새로운 유사도 측정 지표인 InfoSpecificity를 제안하며, 이를 실제 데이터에 적용한 실험 결과를 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함하는 연구 논문에 해당한다."
Which Features of Postural Sway are Effective in Distinguishing Parkinson’s Disease Patients from Controls? An Experimental Investigation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669828,"Computer-assisted quantification and analysis of postural sway may support identifying individuals affected by Parkinson’s disease (PD). Balancing, and its associated postural sway, is a complex process that requires the cooperation of several sensory systems in the brain. Unsurprisingly, a neurodegenerative disease can affect such processes, manifesting itself in the postural sway of affected individuals. Different aspects of postural sway can be quantified and represented as features, which can be used to distinguish between patients and controls. Our aim, inspired by a recent systematic literature review, was to experimentally determine whether sampling frequency and visual state had a meaningful impact on the effectiveness of features in distinguishing between the two groups, and whether overall discriminability could be improved using machine learning. We extracted 102 unique features from 78 postural sway recordings and found that the effectiveness (quantified by an effect size and the average area under the receiver operating characteristic curve) with a sampling frequency of 10 Hz was superior to 20, 40, and 100 Hz, though not with high confidence (quantified through Bayesian analysis). We also concluded that effectiveness under the eyes closed condition was higher than the eyes open condition (confirmed through Bayesian analysis), though combining features from both conditions was superior. Finally, we showed that using machine learning to analyse multiple features through feature selection resulted in higher discriminability in almost all cases. The code for these experiments have been released at https://github.com/Wenbo-G/pd-sway-analysis under the MIT license. When using our code, please cite this paper.",Yes,"본 논문은 파킨슨병 환자와 대조군을 구분하기 위해 자세 흔들림(postural sway)의 다양한 특징을 실험적으로 분석하고, 샘플링 주파수와 시각 상태가 구분 성능에 미치는 영향을 평가하는 독창적인 연구를 수행하였다. 또한, 머신러닝 기법을 적용하여 구분력을 향상시키는 방법을 제안하는 등 직접적인 연구 기여가 포함되어 있다."
Automated Diagnosis System for Outpatients and Inpatients With Cardiovascular Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178348,"The identification of heart related diseases is challenging due to several contributory factors associated with patients, medical staff or medical materials used for diagnosis. Electrocardiogram (ECG) signal represents the electrical activity of the heart. It is the most common method used to diagnose patients with cardiovascular abnormalities. The evaluation commonly practiced by trained physicians can be sometimes subjective, time consuming and related to the observer status. This subjectivity can be more critical due to the double signification of the recorded ECG signals, mainly frequency and duration. In our paper, we present a comparative study of different Artificial Intelligence (AI) approaches as a very relevant tool to assist and improve the accuracy of cardiovascular diseases diagnosis. These models are trained on an online available MIT-BIH arrhythmia, normal rhythm sinus and BIDMC congestive heart failure databases and tested on our own collected data consisting of more than 72000 samples recorded in accordance with patients suffering from the same pathologies. The abnormal ECG signals are judged abnormal by comparison with normal heart beats. The work consists of testing and evaluating the performance of trained support vector machine (SVM), convolutional neural networks (CNN), quadratic discriminant, k-nearest neighbors and naïve Bayes as classifiers to correctly and efficiently classify newly unlabeled data. Further, methodology comprises continuous wavelet transform (CWT), discrete wavelet transforms (DWT), maximum overlap discrete transform (MODWT) and autoregressive modelling (AM) as feature extraction techniques. We tested the prelisted methods with principal component analysis (PCA) to evaluate the dimensionality reduction influence on the overall accuracy and runtime measures. The consistency of performance is evaluated using overall accuracy with confidence interval (CI), misclassification cost and runtime. The study resulted on an overall accuracy of 99.92% with a CI of 99.07-100% and 98.63% with a CI of 95.1%-100% using quadratic discriminant and KNN respectively, both with a certainty level of 99%. The developed approach is robust and accurate and can be used for automated diagnosis of cardiovascular diseases.",Yes,"본 논문은 다양한 인공지능 기법을 활용하여 심혈관 질환 진단의 정확성을 향상시키기 위한 독창적인 연구를 수행하였으며, 자체 수집한 데이터와 공개 데이터셋을 이용해 모델을 학습하고 평가한 실험 결과를 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
A Deep Learning Approach to Prevent Problematic Movements of Industrial Workers Based on Inertial Sensors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892409,"Nowadays, manufacturing industries still face difficulties applying traditional Work-related MusculoSkeletal Disorders (WMSDs) risk assessment methods due to the high effort required by a continuous data collection when using observational methods. An interesting solution is to adopt Inertial Measurement Units (IMUs) to automate the data collection, thus supporting occupational health professionals. In this paper, we propose a deep learning approach to predict human motion based on IMU data with the goal of preventing industrial worker problematic movements that can arise during repetitive actions. The proposed system includes an initial Madgwick filter to merge the raw inertial tri-axis sensor data into a single angle orientation time series. Then, a Machine Learning (ML) algorithm is trained with the obtained time series, allowing to build a forecasting model. The effectiveness of the developed system was validated by using an open-source dataset composed of different motions for the upper body collected in a laboratory environment, aiming to monitor the abduction/adduction angle of the arm. Firstly, distinct ML algorithms were compared for a single angle orientation time series prediction, including: three Long Short-Term Memory (LSTM) methods - a one layer, a stacked layer and a Sequence to Sequence (Seq2Seq) model; and three non deep learning methods - a Multiple Linear Regression, a Random Forest and a Support Vector Machine. The best results were provided by the Seq2Seq LSTM model, which was further evaluated for WMSD prevention by considering 11 human subject datasets and two evaluation procedures (single person and multiple person training and testing). Overall, interesting results were achieved, particularly for multiple person evaluation, where the proposed Seq2Seq LSTM has shown an excellent capability to anticipate problematic movements.",Yes,"본 논문은 IMU 데이터를 활용한 인간 동작 예측을 위해 딥러닝 모델을 제안하고, 다양한 알고리즘을 비교 평가하며 실험을 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Advances of Deep learning in Breast Cancer Modeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417961,"Deep learning (DL) has recently gained popularity in forecasting, detecting, categorizing, and diagnosing for breast cancer with promising results. Developing a review paper to assess the efficacy of DL methods in this context is essential. We've established a standardized database initially containing fundamental publications for methodical reviews. The primary objective of this review is to systematically present the current state-of-the-art, using an updated PRISMA guidelines to better review and evaluate the DL's effectiveness in breast cancer applications. The research follows three main stages: data collection, data analysis, and summarization of initial outcomes. The results highlight accuracy as the prevailing and comprehensive metric used in evaluating DL tools across varied breast cancer applications. Convolutional neural networks (CNNs) have found to have widespread utility, notably surpassing other DL methods. In contrast, collaborative teamwork and employing advanced DL techniques yield optimal performance.",No,"초록에서 이 논문은 딥러닝 방법의 유효성을 평가하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 체계적으로 정리하고 분석하는 데 중점을 두고 있다. 따라서 본 논문은 연구 논문이 아닌 리뷰 논문으로 판단된다."
Automating the Diagnosis of Cucumber Plant Diseases Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10877122,"Farmers all over the world struggle to fight plant diseases. However, the incorrect use of pesticides can waste time, effort, and money and can also be very harmful to the environment and human health. In this work, we present an empirical study that involves identifying two pathogens related to Cucumbers plant diseases: Powdery Mildew and Alternaria Cucumerina. The study aims to develop a computer-based solution based on deep learning and machine learning algorithms to detect these diseases. To achieve this, a dataset of hundreds of samples was collected from ordinary farms in Beqaa, an agricultural region 15 KM north of Amman, Jordan. Three machine learning models were created in this study using Support Vector Machines (SVM), Logistic Regression (LR), and Multi-layer perceptron algorithms. The constructed models were applied to the dataset after embedding the dataset images using the Convolutional Neural Networks (CNN) InceptionV3 pre-trained model. The model was tested and evaluated using cross-evaluation and F1-Score, CA, AUC, Precision, Recall, and Specificity, which all showed excellent results, particularly using the SVM algorithm. The constructed methods are intended to be deployed to farms in their field condition environments using a CCTV system that can capture images of plants during growth, which can help identify plant diseases at that right stage, which would save time, effort, and money and reduce the impact of wrong and overuse of pesticides on consumers and the environment.",Yes,"본 논문은 오이 식물 질병 진단을 위해 딥러닝과 머신러닝 알고리즘을 적용한 독창적인 연구를 수행하였으며, 실제 농장 데이터셋을 수집하고 여러 모델을 개발 및 평가한 실증적 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Casting Fault Detection by Deep Convolution Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009971,"The gold standard for fault identification in the casting process is the human visual inspection though that includes human error and it is time-consuming. In this work, to automate the inspection process in casting production three deep learning-based artificial intelligence systems for detecting manufacturing flaws in submersible pump impellers are created. The suggested casting fault detectors intend to create a system for fault identification in casting goods that are automated, reliable, and accurate. Here on the top view image data of submersible pump impellers, two transfer learning-based convolution neural networks and one lightly structured convolution neural network are created and verified. The performance of the suggested models is measured in terms of accuracy, sensitivity, precision, and f1-score, and then compared with the state-of-the-art works. The best performance among the proposed casting fault detection systems are 99.88% accuracy, 100% precision, 99.77% recall, and 99.88% f1-score in binary classification and this outperformed previous related work.",Yes,본 논문은 주조 결함 검출을 위해 세 가지 딥러닝 기반 인공지능 시스템을 개발하고 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 기존 방법과 비교하여 높은 정확도와 성능을 달성한 점에서 직접적인 연구 기여가 있다고 판단된다.
Optic disc determination in retinal images with deep features,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404339,"Deep learning has attracted so many researchers in the image processing and machine learning communities. So many new applications have been presented with DL day by day. Thus, in this paper, we propose a hybrid approach to detect the optic disc in retinal images by using deep Convolutional Neural Networks (CNN) and K-Nearest Neighbor (KNN) classifier. More specifically, we extract effective deep features from the fc6 layer of a pre-trained CNN model and classify them into optic disc and non-optic disc classes with KNN classifier. The AlexNet is considered for the pre-trained CNN model which extracts 4096 dimensional feature vector for each patch image. To this end, three retinal image datasets are considered for construction of the training and test sets. 500 optic disc patches and 1565 non-optic disc patches of size 280×280 are collected and then resized to 227×227 for feature extraction and construction of the KNN classifier setting. In addition, 165 retinal images are used for testing the presented hybrid approach. A series of experimental works have been carried out for showing the efficiency of the proposed approach. Accuracy, sensitivity and specificity values are calculated. According to the obtained results, 95.74% accuracy, 84.46% sensitivity and 99.08% specificity values are recorded. These results are quite encouraging for related future works.",Yes,"본 논문은 딥러닝과 KNN 분류기를 활용하여 망막 이미지에서 시신경 유두를 탐지하는 새로운 하이브리드 방법을 제안하고, 세 가지 데이터셋을 사용한 실험을 통해 성능을 평가하였다. 이는 독창적인 연구 방법과 실험 결과를 포함한 연구 논문에 해당한다."
Smart Ice Cloud Sensing (SMICES): An Overview of its Submillimeter Wave Radiometer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883671,"The Smart Ice Cloud Sensing (SMICES) is an active/passive sensor. SMICES is sponsored by NASA Earth Science Technology Office (ESTO) under Instrument Incubator Program 19 (IIP-19) awarded to Northrop Grumman Corporation (NGC) and Jet Propulsion Laboratory (JPL). The instrument is designed to measure upper tropospheric and lower stratospheric cloud ice and water vapor. SMICES uses a suite of passive radiometers that are constantly conically scanning to locate ice clouds. The ice clouds are located using an artificial intelligence controller that identifies key labels related to the ice cloud. Once an ice cloud is identified, the artificial intelligence controller activates and targets the on-board radar. While the SMICES instrument is currently being developed for an airborne demonstration, the final goal is to deploy it as a small satellite (SmallSat) instrument in low-Earth orbit (LEO). The onboard AI controller will significantly reduce DC power consumption of the satellite mission. This will enable the SMICES system to be hosted on a smaller platform with fewer solar cells and significantly drive down mission costs while maintaining the quality of scientific data. This work presents the latest development on the SMICES microwave radiometer.",No,"논문 초록은 SMICES 장비의 개요와 개발 현황, 목표에 대해 설명하고 있으나, 구체적인 실험 결과나 독창적인 연구 방법론, 데이터 분석 등 직접적인 연구 기여 내용은 포함되어 있지 않습니다. 따라서 본 논문은 연구 논문보다는 기술 개요 및 개발 보고서에 가깝다고 판단됩니다."
Customer Feedback Analysis using Customer Sentiment from Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837524,"Sentiment analysis is tapped as a crucial tool for driving insights into diverse domains. Through the rising volume of users’ content on online platforms, is now becoming a dominant tool. This paper precisely goes through the methods, applications, challenges, and latest improvements that SA is entailed with. It devotes itself to the multifarious aspects of SA, exploring different techniques of dealing with sentimental classification, opinion-spam detection, and assessing online reviews. It sheds light on the importance of the method of Aspect-Based Textual Analysis (ABSA) being used for the identification of sentiments directed to definite aspects or features of entities. This document demonstrates an extensive examination of some updated SA techniques such as lexical and then sophisticated machine learning approaches. It spells out the data collection tools, preprocessing, feature extraction, and classification models utilized in primary tasks (SA). Logistic Regression, Naïve Bayes Classifiers, Random Forest Classifier and Feedforward Neural Networks are evaluated on both Bag of Words and TF-IDF vectorized data, the main mission being to show how they are applied in this intricate process. The research finds that the marketers of nearly all industries can benefit in every step of the decision process from product analysis to consumer insight by incorporating SA into their process.",No,"초록 내용은 기존의 감성 분석 기법과 응용, 도전 과제, 최신 개선 사항을 정리하고 평가하는 리뷰 논문에 가깝습니다. 독창적인 연구 결과나 새로운 방법론 제안보다는 기존 기법들의 비교와 설명에 중점을 두고 있어 직접 기여하는 연구 논문으로 보기 어렵습니다."
Current Situation and Prospect of Intelligent Monitoring and Maintenance of Power System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800093,"With the growth of energy demand and the optimization of energy structure, the operation and maintenance of power systems are facing increasingly severe challenges. The introduction of intelligent monitoring and maintenance technology has become a key measure to improve the management level of power system and improve energy efficiency. This paper reviews the current situation and prospect of intelligent monitoring and maintenance of power system, starts from the concept and significance of intelligent monitoring and maintenance of power system, and expounds its important role in improving the safety, reliability and efficiency of power system. The wide application of intelligent monitoring technology in power system is discussed, including fault diagnosis and prediction, real- time monitoring and control, equipment health management, etc. At the same time, the development trend and challenges of intelligent monitoring and maintenance technology are analyzed, including the development trend of Internet of Things technology, the application of big data and artificial intelligence, and the integration of visualization and virtual reality technology. Through a comprehensive analysis of the current situation and prospects, this paper aims to provide useful enlightenment and guidance for the research and practice in the field of intelligent monitoring and maintenance of power systems.",No,"본 논문은 전력 시스템의 지능형 모니터링 및 유지보수의 현황과 전망을 종합적으로 리뷰하는 내용으로, 기존 연구들을 정리하고 분석하는 개관적 성격을 띠고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여가 포함된 연구 논문으로 보기 어렵습니다."
Micro Expression Recognition Using Convolution Patch in Vision Transformer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10250810,"Humans possess an intrinsic ability to hide their true emotions. Micro-expressions are subtle changes in facial muscles that are involuntary by nature and easy to hide. To address these issues, several machine and deep learning models have been proposed in the past few years. Convolution neural network (CNN) is a deep learning method that has widely been adopted in vision-related tasks due to its remarkable performance. However, CNN suffers from overfitting due to a large number of trainable parameters. Additionally, CNN cannot capture global information with respect to an input image. Furthermore, the identification of important regions for the classification of micro-expressions is a challenging task. Selfattention mechanism addresses these issues by focusing on key areas. Furthermore, specific transformers, known as vision transformers are widely explored in vision-related applications. However, existing vision transformers divide an input image into a fixed number of patches due to which local correlation of image pixels is lost. Further, a vision transformer relies on self-attention mechanism which effectively captures global dependencies but does not exploit the local spatial relationships in an image. In this work, we propose a vision transformer based on convolution patches to overcome this problem. The proposed algorithm generates c number of feature maps from input images using c filters through convolution operation. These feature maps are then applied to a transformer model as fixed-size image patches to perform classification. Thus, the proposed architecture leverages advantages of both convolutional layers and transformer, and captures both spatial information and global dependencies respectively, leading to improved performance. The performance of the proposed model is evaluated on three benchmark datasets: CASMEI, CASME-II, and SAMM and compared with state-of-the-art machine and deep learning models, which generated classification accuracy of 95.97%, 98.59%, and 100%, respectively.",Yes,"본 논문은 기존의 비전 트랜스포머와 CNN의 한계를 극복하기 위해 새로운 합성곱 패치 기반 비전 트랜스포머 모델을 제안하고, 이를 통해 미세 표정 인식 성능을 향상시킨 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 모델을 세 가지 벤치마크 데이터셋에서 평가하여 기존 방법들과 비교한 실험 결과를 제시하고 있어 연구 논문에 해당한다."
NLP Based Review Categorization: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9788183,"User reviews on social media were quickly gaining interest in the use of emotional analysis which serves as a response to government, public and private companies. Text Mining has various applications like emotional analysis, spam detection, humor detection and news sharing. The rapid growth of unorganized mountains of text data accompanied by an increase in analytics tools opens up great opportunities and challenges for mining research. Automatic labeling of text data is difficult because people tend to express ideas in complex ways. In addition, Emotional data sets are often very sensitive to the domain and difficult to perform analysis on it because emotions like feelings, attitudes and ideas are often full of sayings, onomatopoeia, synonyms, phonemes, symbols and abbreviations. This paper surveys the major contributions made by previous researchers in this area by using Classical machine learning, LSTM (Long Short-Term Memory), CNN (Convolutional Neural Network), GRU (Gated Recurrent Unit), RNN (Recurrent Neural Network), Ontology Learning, Summarization, Hybrid Models etc. Then this research work proposes a deep neural network model using Long-Short-Term Memory (LSTM) with word embedding features. The design includes sub-systems such as pre-processing, train-test splitting of the dataset, feature extraction using Word2Vec model and Deep Neural Network. The proposed model will be evaluated using several different metrics such as accuracy, precision, recall and F1-Score.",Yes,"본 논문은 기존 연구들을 조사하는 survey 부분과 더불어, LSTM 기반의 딥 뉴럴 네트워크 모델을 제안하고 이를 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 단순 리뷰가 아닌 직접적인 연구 기여가 있는 논문으로 판단된다."
Automatic Transformation of Natural to Unified Modeling Language: A Systematic Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9806783,"Context: Processing Software Requirement Specifications (SRS) manually takes a much longer time for requirement analysts in software engineering. Researchers have been working on making an automatic approach to ease this task. Most of the existing approaches require some intervention from an analyst or are challenging to use. Some automatic and semi-automatic approaches were developed based on heuristic rules or machine learning algorithms. However, there are various constraints to the existing approaches to UML generation, such as restrictions on ambiguity, length or structure, anaphora, incompleteness, atomicity of input text, requirements of domain ontology, etc. Objective: This study aims to better understand the effectiveness of existing systems and provide a conceptual framework with further improvement guidelines. Method: We performed a systematic literature review (SLR). We conducted our study selection into two phases and selected 70 papers. We conducted quantitative and qualitative analyses by manually extracting information, cross-checking, and validating our findings. Result: We described the existing approaches and revealed the issues observed in these works. We identified and clustered both the limitations and benefits of selected articles. Conclusion: This research upholds the necessity of a common dataset and evaluation framework to extend the research consistently. It also describes the significance of natural language processing obstacles researchers face. In addition, it creates a path forward for future research.",No,"이 논문은 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 새로운 방법론을 제시하지 않고 기존 연구들의 한계와 장점을 분석하는 데 초점을 맞추고 있습니다. 따라서 연구 논문이라기보다는 리뷰 논문에 해당합니다."
Convolution Neural Networks based lungs disease detection and Severity classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128188,"Classification is a crucial component of Computer Aided Diagnosis (CADx) systems. This phase comprises the extraction of features. Deep features have emerged as a new topic of study in numerous disciplines, including medical imaging. However, these works contain flaws, such as excessive classification, and do not reflect the real world. This paper provides an overview of deep learning for detecting lung illness in medical photos. In the past five years, just one review article has been published on deep learning for lung illness diagnosis. We investigate utilizing deep learning to detect and categorize Convolution neural network (CNN) numerous lung illnesses from chest X-ray images. We developed a pipeline for segmenting chest X-ray (CXR) images before classification and compared the performance of our framework to that of existing techniques. To recover lung characteristics, the Binary Spotted Hyena optimizer (BSHO) was used in this study. We demonstrated that simple models and classifiers, such as shallow CNN, can compete with complex systems. Furthermore, we validated our method using publicly available lung datasets from Shenzhen and Montgomery and compared its efficacy to that of existing methods. Despite having fewer trainable parameters, our technique outperformed the top performing models trained on the Montgomery dataset in terms of accuracy. In addition, although being computationally cheaper, our CNN-BSHO model performed nearly as well as the top solution on the Shenzhen dataset. This research employed four classifiers, including Support Vector Machine (SVM), Nave Bayes, Random Forest, and Visual Geometry Group (VGG). Using CNN-BSHO, an accuracy of 98.324% was reached.",Yes,"논문은 폐 질환 검출 및 중증도 분류를 위한 새로운 딥러닝 기반 파이프라인을 개발하고, 기존 방법들과 성능을 비교하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 공개 데이터셋을 활용한 실험 결과를 제시하여 직접적인 연구 기여를 하고 있음을 보여줍니다."
Cyber Situation Awareness with Active Learning for Intrusion Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020599,"Intrusion detection has focused primarily on detecting cyberattacks at the event-level. Since there is such a large volume of network data and attacks are minimal, machine learning approaches have focused on improving accuracy and reducing false positives, but this has frequently resulted in overfitting. In addition, the volume of intrusion detection alerts is large and creates fatigue in the human analyst who must review them. This research addresses the problems associated with event-level intrusion detection and the large volumes of intrusion alerts by applying active learning and cyber situation awareness. This paper includes the results of two experiments using the UNSWNB15 dataset. The first experiment evaluated sampling approaches for querying the oracle, as part of active learning. It then trained a Random Forest classifier using the samples and evaluated its results. The second experiment applied cyber situation awareness by aggregating the detection results of the first experiment and calculating the probability that a computer system was part of a cyberattack. This research showed that moving the perspective of event-level alerts to the probability that a computer system was part of an attack improved the accuracy of detection and reduced the volume of alerts that a human analyst would need to review.",Yes,본 논문은 UNSWNB15 데이터셋을 활용한 두 가지 실험을 통해 능동 학습과 사이버 상황 인식을 적용하여 침입 탐지 정확도를 향상시키고 경고량을 줄이는 방법을 제안하고 평가하였다. 이는 기존 연구와 차별화된 독창적인 연구 방법과 실험 결과를 포함하고 있어 연구 논문에 해당한다.
Data-Driven Traffic Simulation: A Comprehensive Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10440492,"Autonomous vehicles (AVs) have the potential to significantly revolutionize society by providing a secure and efficient mode of transportation. Recent years have witnessed notable advancements in autonomous driving perception and prediction, but the challenge of validating the performance of AVs remains largely unresolved. Data-driven microscopic traffic simulation has become an important tool for autonomous driving testing due to 1) availability of high-fidelity traffic data; 2) its advantages of enabling large-scale testing and scenario reproducibility; and 3) its potential in reactive and realistic traffic simulation. However, a comprehensive review of this topic is currently lacking. This paper aims to fill this gap by summarizing relevant studies. The primary objective of this paper is to review current research efforts and provide a futuristic perspective that will benefit future developments in the field. It introduces the general issues of data-driven traffic simulation and outlines key concepts and terms. After overviewing traffic simulation, various datasets and evaluation metrics commonly used are reviewed. The paper then offers a comprehensive evaluation of imitation learning, reinforcement learning, deep generative and deep learning methods, summarizing each and analyzing their advantages and disadvantages in detail. Moreover, it evaluates the state-of-the-art, existing challenges, and future research directions.",No,본 논문은 데이터 기반 교통 시뮬레이션 분야에 대한 기존 연구들을 종합적으로 검토하고 미래 연구 방향을 제시하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구의 요약과 분석에 중점을 두고 있다.
Automate Traditional Interviewing Process Using Natural Language Processing and Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9418115,"Nowadays, almost everything is equipped with technology. People can save time by using modern day technological applications in the most convenient way. Smart Interviewing System is one such software/tool which automates the traditional interviewing process using modern Natural Language Processing techniques and deep learning applications. The system will be mainly beneficial for interviewers and HR management employees working for different organizations who conduct technology related interviews. The system works with human voice and writing patterns. The system converts human language into system understandable text-based inputs, and these are used as inputs in the automated interviewing process. The system then checks the accuracy of the answers which candidates provided on the both oral interviews/ technical interviews and written tests. Later, the system automatically predicts scores for each answer using concepts of the deep learning. Interviewers can reduce the effort that they have to put in for selecting the most suitable candidates who are qualified enough to work with their organization. SIS is developed based on modern DL and NLP concepts using Python programming language alongside with ReactJS Framework. This system checking and evaluating candidate more accurately in every stage of the interview using advance evaluation parameters than human oriented evaluations. Above process lead system to find more human errors which critically can be affected to future of the organizations. Because of that, it can be led organizations to find best human resources comparing to the traditional interviewing process by sacrificing less time and effort.",Yes,"논문 초록에서 제안된 시스템은 자연어 처리와 딥러닝을 활용하여 전통적인 면접 과정을 자동화하는 구체적인 방법과 구현을 다루고 있어 독창적인 연구 내용이 포함되어 있다. 또한, 시스템의 개발과 평가 방법에 대한 기술이 포함되어 있어 연구 논문으로 판단된다."
Evaluation of Intelligent Resource Allocation Methods for Interference-Limited Satellite Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376128,"Satellite systems serve as a crucial tool for non-terrestrial networks, offering a solution to areas where terrestrial network coverage is limited. With the advanced capabilities of modern satellite systems that leverage frequency reuse across multiple beams, the provision of efficient 6G services becomes possible worldwide. However, achieving this efficiency requires an optimal resource allocation (RA) strategy that considers the interference-limited environment and the constraints of limited power and bandwidth. This research paper provides a comprehensive review of existing studies on RA problems in interference-limited multi-beam satellite systems, specifically exploring low-complexity methods. The paper thoroughly investigates the operational principles underlying various RA methods that use simple machine learning algorithms. Additionally, it conducts an in-depth analysis of the advantages and disadvantages associated with these methods. To validate the effectiveness of the proposed low-complexity techniques, the paper presents simulation results obtained from a multi-beam satellite system operating in a Low Earth Orbit (LEO). These results serve as evidence for the efficacy of employing simple RA methods based on linear machine learning.",Yes,"논문은 기존 연구들을 종합적으로 검토하는 동시에, 저복잡도 기계학습 기반 자원 할당 기법을 제안하고 시뮬레이션을 통해 그 효과를 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 단순 리뷰 논문이 아니라 직접적인 연구 기여가 있는 논문으로 판단된다."
Gas Discrimination & Quantification using Sensor Array with 3D Convolution Regression Dual Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660938,"Smart sensor system design requires intelligent data processing, which analyzes raw time-series sensor data to efficiently and precisely discriminate and quantify target gases. The work presented here utilizes the response of twin gas sensor arrays for gases such as ethanol, ethylene, methane, and carbon monoxide to discriminate and quantify the target gases. We propose a 3D convolution neural-based regression dual network (3D-CNRDN) for both gas quantification and discrimination. The spatiotemporal correlation of sensor array responses inspired us to design the deep neural network for the gas concentration estimation model. The sensor array set is spatially correlated, and all the twin array responses are temporally related. 3D-CNRDN uses raw time-series gas sensor array data. The data is fed to the network as the 3D pattern, which contains 2D spatial information varying with third dimension time to recognize patterns that eventually predict the concentration. The model evaluation shows that the proposed methods are an effective technique for gas quantification and identification with RMSE=0.3179 and classification accuracy 94.37%. Furthermore, the proposed method outperforms and provides higher discrimination accuracy and lower RMSE than other machine learning and deep learning methods.",Yes,본 논문은 3D 컨볼루션 신경망 기반의 새로운 회귀 이중 네트워크 모델을 제안하여 가스 센서 배열 데이터를 이용한 가스의 정량화 및 분류 문제를 해결하는 독창적인 연구 내용을 포함하고 있습니다. 또한 제안된 모델의 성능 평가와 기존 방법과의 비교를 통해 연구 기여를 명확히 제시하고 있어 연구 논문에 해당합니다.
Advancements in Food Recognition: A Comprehensive Review of Deep Learning-Based Automated Food Item Identification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10576416,"Food recognition is a vital component of various applications, including dietary monitoring, nutritional analysis, and restaurant menu optimization. In recent years, deep learning techniques have shown remarkable promise in automating the process of food item identification from images, significantly improving accuracy and efficiency. This review paper provides a comprehensive overview of the advancements in food recognition using deep learning, offering insights into the evolution of techniques and datasets employed in this domain. The paper begins by discussing the motivations behind automated food recognition, highlighting its significance in addressing health-related concerns and improving food-related services. It then delves into the fundamental concepts of deep learning and its relevance to image-based food recognition, providing readers with a solid foundation for understanding the subsequent sections. A major focus of this review is the analysis of state-of-the-art deep learning architectures and methodologies employed in food recognition. Furthermore, the review presents an extensive survey of publicly available food image datasets, evaluating their suitability for training and benchmarking deep learning models. It concludes by highlighting key research gaps and suggesting potential directions for future advancements in this rapidly evolving field.",No,"본 논문은 딥러닝 기반 음식 인식 기술에 대한 종합적인 리뷰를 제공하는 논문으로, 기존 연구들을 정리하고 분석하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적인 연구 결과나 새로운 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Fairness Meets Cross-Domain Learning: A Benchmark of Models and Metrics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487929,"Deep learning-based recognition systems are deployed at scale for real-world applications that inevitably involve our social life. Although of great support when making complex decisions, they might capture spurious data correlations and leverage sensitive attributes (e.g., age, gender, ethnicity). How to factor out this information while maintaining high performance is a problem with several open questions, many of which are shared with those of the domain adaptation and generalization literature which aims at avoiding visual domain biases. In this work, we propose an in-depth study of the relationship between cross-domain learning (CD) and model fairness, by experimentally evaluating 14 CD approaches together with 3 state-of-the-art fairness algorithms on 5 datasets of faces and medical images spanning several demographic groups. We consider attribute classification and landmark detection tasks: the latter is introduced here for the first time in the fairness literature, showing how keypoint localization may be affected by sensitive attribute biases. To assess the analyzed methods, we adopt widely used evaluation metrics while also presenting their limits with a detailed review. Moreover, we propose a new Harmonic Fairness (HF) score that can ease unfairness mitigation model comparisons. Overall, our work shows how CD approaches can outperform state-of-the-art fairness algorithms and defines a framework with dataset and metrics as well as a code suite to pave the way for a more systematic analysis of fairness problems in computer vision (Code available at: https://github.com/iurada/fairness_crossdomain).",Yes,"본 논문은 14개의 교차 도메인 학습 방법과 3개의 최신 공정성 알고리즘을 실험적으로 평가하고, 새로운 Harmonic Fairness 점수를 제안하는 등 독창적인 연구 기여를 포함하고 있다. 또한, 얼굴 및 의료 이미지 데이터셋을 활용한 공정성 문제에 대한 체계적인 분석과 새로운 평가 지표를 제시하여 연구 논문으로 판단된다."
A Hybrid Approach for Usability Evaluation of Learning Management Systems Using Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893160,"This research full paper describes a hybrid approach based on data, questionnaire answers, and machine learning algorithms to predict usability scores in Learning Management Systems (LMSs) to improve student learning and satisfaction. Students need to achieve their learning goals by interacting with LMSs. To attain these goals, usability evaluation ensures effectiveness (task completion), efficiency (time measurement), and satisfaction (positive attitude). Usability evaluation usually follows questionnaires, user testing of the LMS, and expert reviews. Although these methods are widely used due to several benefits, they face challenges related to trying these software systems multiple times until the system satisfies student needs, human subjectivity perception, and lack of software system adaptability. We propose this hybrid approach to face these challenges, promote student engagement with the system, and create a better design in the LMS courses. The aim is to identify features extracted from the LMS to predict usability scores with machine learning techniques. We evaluated this strategy through a case study with data collected from undergraduate students at a public university in the United States. The students' tasks were answering a quiz, posting in a forum, and uploading an assignment. These activities in the LMS allow the extraction of ten features into the machine learning algorithms. These attributes are time quiz, time forum, time assignment, grade quiz, word count message post, file size, file type, clicks module quiz, clicks module forum, and clicks module assignment. The four targets are from scores of the System Usability Scale and UseLearn questionnaires. Random Forest produces the best performance of average mean square error and root mean square error among machine learning algorithms. The results are promising, though there are alternatives for improvements. Our proposed approach contributes to the engineering and computing education field by providing a predictive tool for usability scores to improve the student learning experience and the components of the LMS.",Yes,"이 논문은 학습 관리 시스템(LMS)의 사용성 점수를 예측하기 위해 데이터와 머신러닝 알고리즘을 활용한 하이브리드 접근법을 제안하고, 실제 학생 데이터를 수집하여 기법을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
The Application of Data Analytics to Career Choice Prediction: A Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10051101,"Finding the right career is one of the most critical yet challenging decisions individuals must make through different academic and professional stages. This review aims to tackle the modern career guidance approaches by adapting data analytics techniques and methods. The most relevant papers were synthesized and presented in terms of overall purpose, methodology, the dataset used, and data analytics applications. This review offers a reference for career guidance practitioners in different educational settings; it presents an overview of the latest data analytics applications within the domain of career guidance. It was recognized that artificial intelligence and machine learning techniques are successfully utilized to aid individuals by offering them career choice prediction and recommendations. The main findings of this review also indicate that machine learning algorithms such as Naive Bayes and Random Forest are the most accurate predictors of career choices. However, future research should be extended to measure how successfully individuals continue within career pathways recommended by data analytics applications.",No,"본 논문은 데이터 분석을 활용한 진로 선택 예측에 관한 기존 연구들을 종합하여 리뷰한 문헌 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하지 않고, 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다."
SMOTE-MRS: A Novel SMOTE-Multiresolution Sampling Technique for Imbalanced Distribution to Improve Prediction of Anemia,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721227,"Anemia is a widespread worldwide health problem that has a substantial effect on groups who are particularly susceptible. The objective of this work is to improve the diagnosis of anemia by creating a hybrid machine learning model called SMOTE-MRS. This model combines SMOTE, K-Means Clustering, and Random Over Sampling techniques. The model aims to enhance diagnosis accuracy and ultimately improve healthcare outcomes, specifically in Indonesia, by resolving the imbalance in the dataset. The SMOTE-MRS model mitigates the issue of imbalanced datasets by combining the techniques of SMOTE, K-Means clustering, and random oversampling. K-Means clustering first divides the dataset into K groups. SMOTE then produces artificial instances of the underrepresented class inside every cluster. Random oversampling is a technique that replicates instances of the minority class to make the dataset balanced and enhance the training of machine learning models. The research evaluated the performance of Random Forest (RF), Naïve Bayes (NB), and Support Vector Machine (SVM) models in predicting anemia, using the SMOTE-MRS technique. The SMOTE-MRS algorithm exhibited outstanding performance, achieving scores of 0.973 for Accuracy, 0.990 for Recall, 0.968 for Precision, 0.979 for F1-Score, and 0.994 for AUC. These findings demonstrate its exceptional capacity to effectively handle class imbalance and enhance prediction accuracy. The SMOTE-MRS algorithm has outstanding performance on a wide range of datasets, such as those related to anemia, diabetes, breast cancer, and kidney failure datasets. This showcases its robustness and versatility in many predictive modeling scenarios. The findings demonstrate that SMOTE-MRS outperforms standard approaches such as SMOTE, SMOTE-ENC, and ROS in effectively addressing class imbalance. The research affirms the better performance of SMOTE-MRS in addressing class imbalance for the prediction of anemia. SMOTE-MRS surpasses traditional approaches, attaining exceptional results in all measures. Future studies should prioritize the optimization of SMOTE-MRS to minimize small performance declines, conduct a comparative analysis with other approaches, and validate its adaptability across various domains. Furthermore, it is advisable to include SMOTE-MRS in deep learning models and real-time applications.",Yes,"본 논문은 SMOTE-MRS라는 새로운 하이브리드 머신러닝 모델을 제안하고, 이를 통해 빈도 불균형 문제를 해결하여 빈혈 예측 정확도를 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한 다양한 데이터셋에 적용하여 성능을 평가한 실험 결과를 제시하고 있어 연구 논문에 해당한다."
Detection of Cyber-attacks using Deep Convolution in Honeyed Framework,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150916,"A network-related environment called a “honeyed framework” served to defend official network resources against harm. This framework creates a scenario that motivates the intrusive person to engage in resource-stealing activity. To recognise an unauthorised assault, this framework applied the Attack-detection-procedure. Here, we attempt to identify DoS attacks using the suggested Honeyed framework system. In order to safeguard your network from assaults, NIDS (Network Intrusion Detection System) is one of the first security solutions to make it easier to identify intrusions. In this work, we offer a system that reveals an assault while validating the defense against it. The new cyber security benchmark IoT dataset is used in this white paper to assess the most recent machine learning techniques. This work’s major goal is to develop an architecture that can foresee and stop DDOS attacks, malware, and botnet attacks using these Honeyed designs. Deep Convolution Reinforcement Neural Networks are used for network surveillance and to categories network users from potential threats (DCRNN). A two-step technique of network understanding is used to enhance the functionality of the suggested solution. DSAE (Deep Sparse Auto Encoder) is used for feature engineering challenges at the initial step of the processing process, data pre-processing. The Deep Convolution Reinforcement Neural Network learning strategy is used in the second step to facilitate categorization. The honeyed firewall and web server are then implemented, following the deployment of the honeyed framework. The DCRNN deployment is finished, and users can now be monitored and analyzed as well as data on network users collected. In this study, data from a loT environment was used to test the effectiveness of the published technique. This data included the heterogeneous datasets ""IoT-23,"" ""NetML-2020,"" and ""LITNET-2020."" With contemporary methods for network discovery, the statistical relevance of this strategy is evaluated.",Yes,"논문은 새로운 Honeyed framework와 Deep Convolution Reinforcement Neural Networks를 활용한 사이버 공격 탐지 시스템을 제안하고, 이를 IoT 환경 데이터셋을 통해 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Explaining Cyber Risks in Transportaion 5.0: A Data Driven Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10848812,"The primary objective of the research is to explore the cybersecurity risks associated with AI-enabled intelligent transportation systems (Transportation 5.0) within the context of developing future smart cities. In Transpiration 5.0, the critical infrastructure increasingly relies on automated systems, which enhances the threat of ransomware attacks, specifically targeting vital cyber-physical systems (CPS), smart grids, and intelligent transportation systems (ITS). The paper explores transportation ransomware attack data to incorporate advanced visualization and extract relevant data attributes for cyber risk evaluation. The study uses the NIST risk management framework and ISACA risk quantification to develop a cyber risk evaluation method. With the findings of the literature review, the research highlights ransomware as a significant threat to intelligent transportation systems (ITS), given the negative impact of malware on IoT, IIoT, and network-connected devices. While substantial research exists on malware detection techniques, performance, accuracy, and cloud-based strategies, it’s crucial for organizations to comprehend the risks of ransomware attacks to implement effective security controls and enable cyber excellence. The research executes exploratory data analysis on the given dataset to find out the correlation between different attributes such as severity, data loss, affected systems, and ransom amount for evolving a risk method to evaluate cyber risk. The focus is to simplify the risk evaluation process by fitting the right data attributes into a standard risk management framework and risk formula for broader reusability. The method was applied to different transportation modes to calculate the risk scores respectively and generate insights accordingly. The overall analysis supports that effective risk predictions along with consistent monitoring can help to control ransomware attack incidents and improve cyber resiliency. This paper proposes a data-driven approach for cyber risk assessment in transportation, applicable to other cyber-attacks. However, it highlights the need of synthetic data integration for a generic solution.",Yes,"본 논문은 AI 기반 지능형 교통 시스템의 사이버 보안 위험을 데이터 분석과 위험 평가 프레임워크를 활용해 직접적으로 연구하고 있으며, 새로운 데이터 기반 사이버 위험 평가 방법을 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 연구 논문에 해당한다."
Evaluation of Topic Models and Information Retrieval Methods in Support of Lessons Learned and Knowledge Management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10608962,"A lesson learned is the application of previous experiences to improve decision-making and organizational efficiency. The lack of learning from past organizational mistakes and successes has contributed to extended schedules, increased program costs, and excessive program rework. Applying the lessons learned process is critical in meeting program objectives, mitigating risks, and improving organizational effectiveness by avoiding costly errors. This type of learning requires communication between the source of the knowledge and the receiver(s) of the lesson to magnify the benefits of a lesson learned. A significant factor in gathering, maintaining, sharing, and reviewing lessons learned is for an organization to implement a knowledge management system or a lessons learned database. This system/database allows the team members access to all previous lessons learned, which, as a result, allows the organization to learn from past mistakes/successes by applying that knowledge in the present and future. There is limited research on an automatic lessons learned/knowledge management database that removes the need for users to perform manual searching and provides highly relevant results to the users. One significant step in constructing an automatic lessons learned/knowledge management database is the implementation of topic models. Topic models are machine learning algorithms produced to identify the foundational semantic structures of corpora using Bayesian hierarchical modeling. This research evaluates topic models intending to improve relevancy in lessons learned query searches in knowledge management/lessons learned databases. Widely utilized topic models such as the Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI) are investigated in this paper in addition to the Term Frequency – Inverse Document Frequency (TF-IDF), Hierarchical Dirichlet Process (HDP), and Random Projections (RP) topic models. The TF-IDF topic model outperformed all topic models evaluated in this work.",Yes,"논문 초록에서 다양한 토픽 모델을 평가하고 TF-IDF 모델이 가장 우수하다는 결과를 제시하는 등, 직접적인 실험과 분석을 통해 독창적인 연구 기여를 하고 있음을 알 수 있습니다. 이는 기존 연구를 검토하는 리뷰 논문이 아니라 새로운 연구 결과를 포함한 연구 논문임을 의미합니다."
Heart Disease Diagnosis Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434227,"Cardiovascular disease is a leading cause of death in the general population, and late detection significantly impacts the patient survival rates. Factors like age, sex, cholesterol level, sugar level, and heart rate influence heart problems, making expert evaluation challenging due to the high number of variables. This manuscript proposes a novel approach using deep learning methods and feature augmentation techniques for evaluating patient’s risk of cardiovascular disease. The results show a significant improvement, achieving a precision of 90%, outperforming other state-of-the-art methods by 4.4%. Given the complexity and severity of cardiovascular diseases, an efficient and accurate heart attack prediction system becomes crucial. In this context, the paper utilizes Recurrent Neural Networks (RNN) as a powerful classification algorithm in the domain of deep learning to predict the likelihood of heart-related diseases in patients. The proposed model combines deep learning and data mining to provide accurate results with minimal errors, addressing the limitations of existing medical data mining and machine learning techniques. This work serves as a direction and precedent for the development of a new generation heart attack prediction platform, potentially aiding in the early detection and treatment of heart diseases, especially silent heart attack, which often goes undiagnosed and poses significant challenges to healthcare professionals.",Yes,"논문 초록에서 제안된 심장 질환 진단을 위한 딥러닝 기반의 새로운 접근법과 모델 개발, 그리고 기존 방법 대비 성능 향상 결과를 명확히 기술하고 있어 독창적인 연구 내용이 포함된 연구 논문으로 판단된다. 또한, 구체적인 알고리즘(RNN) 적용과 성능 평가가 이루어져 직접적인 연구 기여가 확인된다."
Generating Healthcare Time Series Data for Improving Diagnostic Accuracy of Deep Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9421374,"Data scarcity and class imbalance are common occurrences in healthcare datasets and have an adverse effect on classification performance of machine learning models. Artificial data generation in various applications can be used to handle these challenges. This article proposes a guided evolutionary synthesizer (GES), a tool derived from principles of genetic algorithm, and designed to generate artificial healthcare time series data for improving classification performance of machine learning models. We conducted a series of promising and confirmatory preliminary experiments performance using traditional machine learning and nonresidual convolutional neural network models to evaluate the effectiveness of GES synthetic on data classification. Motivated by the preliminary results, we conducted eight detailed experiments using residual neural network (ResNet), which demonstrated the flexibility of GES and the effectiveness of GES synthetic data in improving the classification performance of deep neural networks. These experiments use GES generated electrocardiogram (ECG) and electroencephalogram (EEG) datasets. Our findings show that models trained with GES synthetic data performed better than models trained with regular perturbed data, had better diagnostic performance for both EEG and ECG datasets, achieved better performance with lower training volume for EEG dataset, eased learning biases seen in the literature for ECG normal sinus and premature ventricular complex rhythms, and had better classification outcomes than comparable related work models trained with similar ECG classes.",Yes,"논문은 유전 알고리즘 기반의 인공 데이터 생성 도구(GES)를 제안하고, 이를 통해 심전도(ECG)와 뇌파(EEG) 시계열 데이터의 분류 성능을 향상시키는 실험을 수행하였다. 이는 기존 연구와 차별화된 독창적인 방법론과 실험 결과를 포함한 연구 논문으로 판단된다."
Smart Contract Vulnerability Detection Using Deep Learning Algorithms on EVM bytecode,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577852,"In the quickly changing world of blockchain technology, it is critical to guarantee the security of the self-executing contracts, written in programming languages like Solidity called smart contracts. Not all security vulnerabilities in smart contracts will be found by human code reviews and security audits using traditional methods. Deep learning networks have become a promising answer to this problem. In this paper, we present the architecture of two models—using convolutional and recurrent neural networks—that are intended to effectively discover five vulnerabilities in smart contracts. To train and validate the models, we used a dataset that includes 106474 audited smart contracts taken from the public Ethereum blockchain. Instead of the source code that is typically used by most deep learning-based solutions, the models receive input in the form of Ethereum Virtual Machine (EVM) bytecode. Across all five vulnerabilities, the Recurrent Neural Network model has an average micro F1-score of 0.93, whereas the Convolutional Neural Network achieves an average micro F1-score of 0.89. Through comparative research with various deep learning systems and static analysis tools, we have determined that EVM bytecode may be leveraged as a feature to detect vulnerabilities in smart contracts.",Yes,"이 논문은 스마트 계약의 취약점을 탐지하기 위해 두 가지 딥러닝 모델을 설계하고, 대규모 데이터셋을 사용해 모델을 학습 및 검증하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 방법과 비교 분석을 수행하여 EVM 바이트코드를 활용한 취약점 탐지 가능성을 제시하고 있어 연구 논문에 해당한다."
5G Networks Security Mitigation Model: An ANN-ISM Hybrid Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841375,"The advent of Fifth-Generation (5G) networks has introduced significant security challenges due to increased complexity and diverse use cases. Conventional threat models may fall short of addressing these emerging threats effectively. This paper presents a new security mitigation model using artificial neural network (ANN) with interpretive structure modeling (ISM) to improve the 5G network security system. The main goal of this study is to develop a 5G network security mitigation model (5GN-SMM) that leverages the predictive capabilities of ANN and the analysis of ISM to identify and mitigate security threats by providing practices in 5G networks. This model aims to improve the accuracy and effectiveness of security measures by integrating advanced computational practices with systematic modeling. Initially, a systematic evaluation of existing 5G network security threats was conducted to identify gaps and incorporate best practices into the proposed model. In the second phase, an empirical survey was conducted to identify and validate the systematic literature review (SLR) findings. In the third phase, we employed a hybrid approach integrating ANN for real-time threat detection and risk assessment and utilizing ISM to analyze the relationships between security threats and vulnerabilities, creating a structured framework for understanding their interdependencies. A case study was conducted in the last stage to test and evaluate 5GN-SMM. The given article illustrates that the proposed hybrid model of ANN-ISM shows a better understanding and management of the security threats than the conventional techniques. The component of the ANN then comes up with the potential of the security breach with improved accuracy, and the ISM framework helps in understanding the relationship and the priorities of the threats. We identified 15 security threats and 144 practices in 5G networks through SLR and empirical surveys. The identified security threats were then analyzed and categorized into 15 process areas and five levels of 5GN-SMM. The proposed model includes state-of-the-art machine learning with traditional information security paradigms to offer an integrated solution to the emerging complex security issues related to 5G. This approach enhances the capacity to detect threats and contributes to good policy enforcement and other risk-related activities to enhance safer 5G networks.",Yes,"본 논문은 5G 네트워크 보안 위협을 식별하고 완화하기 위한 ANN과 ISM을 결합한 새로운 보안 완화 모델을 제안하고, 실증적 조사와 사례 연구를 통해 모델을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Big Data and the Application of IoT Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579030,"As IoT devices become more common, security must be a priority. Many devices lack built-in protection due to design oversights and resource constraints. These limitations make them vulnerable to hacking and unable to support sophisticated security software or automatic updates. Attackers exploit these weaknesses to access and manipulate customer settings despite some devices using SSL/TLS protocols. With over 25 trillion IoT devices, achieving complete security is nearly impossible. Intrusion detection and prevention systems struggle to keep up with new attack methods, leaving devices vulnerable to various attacks like Mirai Botnet, Denial of Service, Synflood, and Man in the Middle. Furthermore, intelligent home automation systems, medical Internet devices, and building automation equipment handle sensitive user information that must be appropriately monitored. As a result, proposing an intrusion detection system for Internet of Things devices is vital to limiting the threat surface and protecting consumer data. Many literature reviews have proposed a solution using Deep Learning (DL) models. However, we found that Deep Learning requires high processing power, which is unsuitable for constraint devices. This work proposes a framework using machine learning classifiers to detect various attacks. We evaluated the classifiers against a variety of attack data. The accuracy of each classifier was recorded as follows: LR (Logistic Regression) has 97%, SVC (Support Vector Machine) has 98%, Extreme Gradient Boosting Random Forest (XGBRF) has 99%, K-Nearest Neighbors (KNN) has 99%, AdaBoost (AB) is 99% accurate, Random Forest (RF) has 99%, Decision Tree (DT) has 99%, and Extreme Gradient Boosting (XGB) has 99%.",Yes,"본 논문은 IoT 기기 보안을 위한 침입 탐지 시스템 프레임워크를 제안하고, 다양한 머신러닝 분류기들의 공격 탐지 정확도를 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
A Review on Deep Learning Applications in Prognostics and Health Management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8889720,"Deep learning has attracted intense interest in Prognostics and Health Management (PHM), because of its enormous representing power, automated feature learning capability and best-in-class performance in solving complex problems. This paper surveys recent advancements in PHM methodologies using deep learning with the aim of identifying research gaps and suggesting further improvements. After a brief introduction to several deep learning models, we review and analyze applications of fault detection, diagnosis and prognosis using deep learning. The survey validates the universal applicability of deep learning to various types of input in PHM, including vibration, imagery, time-series and structured data. It also reveals that deep learning provides a one-fits-all framework for the primary PHM subfields: fault detection uses either reconstruction error or stacks a binary classifier on top of the network to detect anomalies; fault diagnosis typically adds a soft-max layer to perform multi-class classification; prognosis adds a continuous regression layer to predict remaining useful life. The general framework suggests the possibility of transfer learning across PHM applications. The survey reveals some common properties and identifies the research gaps in each PHM subfield. It concludes by summarizing some major challenges and potential opportunities in the domain.",No,"본 논문은 딥러닝을 활용한 예지보전 분야의 기존 연구들을 종합적으로 검토하고 연구 동향과 공백을 제시하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험 기여를 포함하지 않고, 기존 연구들을 분석하는 데 중점을 두고 있습니다."
Bibliometric Analysis of Deep Learning Approaches for Early Detection of Cardiovascular Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837161,"Cardiovascular diseases (CVD) are a major global health challenge, demanding advanced methods for early detection and intervention. This survey paper offers a comprehensive review of state-of-the-art deep learning approaches for early prediction of CVD, focusing particularly on artificial intelligence methods, including gradient boosting techniques. The main goal is to ensure that the developed predictive models are reliable, accurate, and comprehensive by leveraging diverse datasets that integrate objective, exploratory, and subjective elements. This research also provides a bibliometric survey to analyze the trends, key contributors, and influential publications in the field of CVD prediction using deep learning. It evaluates various deep learning techniques, highlighting their strengths and limitations in predicting cardiovascular risks. Furthermore, it discusses the significance of feature selection, data preprocessing, and model optimization in enhancing predictive performance. The findings of this survey aim to provide physicians with advanced, sophisticated, and nuanced tools for diagnosing patients at risk of cardiovascular illness and its complications, thereby enabling early intervention with specialized preventive therapy. This paper also outlines future research directions and the potential for integrating multimodal data sources to further improve the accuracy and utility of predictive models in clinical practice.",No,"본 논문은 딥러닝을 이용한 심혈관 질환 조기 탐지 연구 동향과 주요 기여자들을 분석하는 서베이 및 서지계량 분석 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Wearable Ear EEG Device for Emotion Recognition in Human-Robot Interaction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10918263,"Wearable electroencephalography (EEG) devices are emerging as crucial tools in human-robot interaction (HRI), enabling intuitive and effective communication between humans and robots. These devices non-invasively measure brain activity, providing real-time insights into a user’s mental state, intentions, and cognitive load. This paper explores the advancements and applications of wearable ear EEG technology in HRI, with a focus on detecting human intent, monitoring emotional and cognitive states, and delivering real-time feedback for adaptive robot behavior. The benefits of wearable EEG over traditional scalp EEG, such as enhanced user comfort, reduced setup time, and improved long-term wearability, are thoroughly examined. Additionally, the paper covers advanced applications, including emotion-aware adaptive interactions, neurofeedback training, and direct robot control via brain-computer interfaces (BCIs). A review of the latest advancements in device miniaturization, integration with other wearable sensors, and applications in virtual reality, gaming, and healthcare is provided. Future directions focus on expanding applications, enhancing human-AI collaboration, and improving accuracy and reliability in HRI. The studies discussed in this paper represent our state-of-the-art research across various fields and applications of wearable EEG systems. This study highlights the potential of wearable Ear-EEG devices to revolutionize human-robot interactions by enhancing customization, responsiveness, and overall efficacy.",No,"초록 내용이 주로 웨어러블 귀 EEG 기술의 발전과 응용에 대한 종합적인 리뷰 및 미래 방향 제시에 초점을 맞추고 있어, 독창적인 연구 결과나 실험적 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 논문에 더 가깝다고 판단됩니다."
Intelligent Feedback and Evaluation for the Assessment and Improvement of Student Peer Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10747900,"Research into peer evaluation shows positive effects on student achievement and attitudes. To foster authentic peer reviews, students need to know their peer reviews will be seen and graded. However, the time-consuming nature of assessing feedback makes it impractical for instructors to evaluate thousands of feedback items generated by students in massive online courses. This paper describes the development of a deep learning (DL) tool to help assess the quality of feedback written by students in a peer-review system. We report tool development progress built upon years of student meta-feedback data that consists of numeric scores and written evaluations of the feedback received. We describe: the underlying DL platform, the labeled data set with which the algorithm was trained, the supervised training process, and the performance evaluation of the resulting three-class multiclass classifier. The evaluation showed satisfactory reliability when compared to student meta-feedback evaluations. We conclude by identifying issues to be addressed and the applications for this tool in massive online courses, including but not limited to giving formative feedback to students as they evaluate peers and helping identify underperforming reviewers.",Yes,"논문은 학생 동료 평가 피드백의 질을 평가하기 위한 딥러닝 도구 개발이라는 독창적인 연구 내용을 포함하고 있으며, 알고리즘 훈련과 성능 평가 등 구체적인 연구 과정을 기술하고 있다. 이는 직접적인 연구 기여를 나타내므로 연구 논문에 해당한다."
A Closer Look at Evaluating the Bit-Flip Attack Against Deep Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897693,"Deep neural network models are massively deployed on a wide variety of hardware platforms. This results in the appearance of new attack vectors that significantly extend the standard attack surface, extensively studied by the adversarial machine learning community. One of the first attack that aims at drastically dropping the performance of a model by targeting its parameters stored in memory, is the Bit-Flip Attack (BFA). In this work, we point out several evaluation challenges related to the BFA. First, the lack of an adversary’s budget in the standard threat model is problematic, especially when dealing with physical attacks. Moreover, since the BFA presents critical variability, we discuss the influence of some training parameters and the importance of the model architecture. This work is the first to present the impact of the BFA against fully-connected architectures that present different behaviors compared to convolutional neural networks. These results highlight the importance of defining robust and sound evaluation methodologies to properly evaluate the dangers of parameter-based attacks as well as measure the real level of robustness offered by a defense.",Yes,"본 논문은 Bit-Flip Attack(BFA)에 대한 평가 방법론의 문제점을 지적하고, 다양한 신경망 구조에 대한 BFA의 영향을 실험적으로 분석하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 기존 연구와 차별화된 완전 연결 신경망에 대한 공격 결과를 제시하여 새로운 기여를 하고 있음을 알 수 있다."
"Intelligent Biofeedback Comprehension Assessment: Theory, Research, and Tools",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9843030,"The present paper describes the use of nonintrusive biofeedback sensors (e.g., ECG) and eye-tracker to study the cognitive load (CL) associated with two mental tasks: a) content reading and comprehension b) code review. The paper addresses the theoretical underpinnings of the comprehension assessment included in content reading (for understanding) and code review evaluation using biofeedback sensors and Artificial Intelligence (AI) techniques. Moreover, it demonstrates the current research directions that the authors developed in evaluating these two tasks. Finally, the paper presents the design of one of the tools being developed to use biofeedback sensors and AI to evaluate the code review quality by assessing the code reviewer's comprehension and engagement level.",Yes,"논문은 비침습적 바이오피드백 센서와 AI 기법을 활용하여 인지 부하와 이해도를 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 두 가지 정신 과제에 대한 평가 도구 설계와 연구 방향을 제시하여 직접적인 연구 기여를 하고 있음을 보여준다."
Survey of Machine Learning-Based Predictions of Photovoltaic Power Outputs,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374517,"Solar Photovoltaic (PV) energy has been deployed at exponential rates since the last decade to produce power from renewable and green resources as a contribution to the causes of climate change and global warming. PV power generation is subject to very dynamic changes due to its dependence on the environment and the geography, often fluctuating erratically. The variability of PV power generation poses electric grid system stability, reliability, and planning downturns. Hence, grid operations could operate at higher levels of performance and efficiencies through adequate grid scheduling that primarily relies on accurate prediction of photovoltaic power output. Due to the paramount importance of the topic, the research community investigated several forecasting strategies that rely on numerical methods, probabilistic methods, physical models and machine learning-based (ML-based) techniques. The present paper presents a comparative review of the literature targeting ML-based algorithms for short-term PV power output prediction. A complementary case study exposes a much-needed homogeneous comparison of the state-of-the-art ML-based forecasters, trained on a dataset from the University of Liège. Model structural enhancement for the state-of-the-art is proposed and evaluated in the light of literature findings.",Yes,"본 논문은 ML 기반의 태양광 발전 출력 예측에 관한 기존 연구들을 비교 분석하고, 데이터셋을 활용한 사례 연구와 모델 구조 개선을 제안하여 직접적인 연구 기여를 포함하고 있다. 따라서 단순 리뷰가 아닌 독창적인 연구 내용이 포함된 연구 논문으로 판단된다."
Blockchain or AI: Web Applications Security Mitigations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696861,"Given the rapid advancement of web applications and the increasing importance of web security, it is crucial to prioritize web security as a fundamental aspect of ensuring a secure online environment. Web apps are becoming more and more popular, but with that comes a wider range of security threats that can cause more serious harm. To safeguard web apps and discourage cyber-attacks, numerous research have been conducted. Some security issues that might affect web applications are provided in this article. SQL injection attacks are the most prominent attacks on web applications. These attacks could allow third parties to access private information without authorization, change or remove data, or even take down whole websites and databases. Traditional methods of detecting and neutralizing SQL injection attacks are often resource-intensive, making them impractical for devices handling large volumes of traffic. Examining and evaluating blockchain and AI-based technologies for improving web application security against “structured query language injection” (SQLI) threats is the goal of this Study. A literature review has been conducted to find the applications of blockchain and AI technologies in web application protection. Taking into consideration the review, an opinion is presented regarding which technology is superior and in what circumstances.",No,"본 논문은 블록체인과 AI 기술을 활용한 웹 애플리케이션 보안에 관한 기존 연구들을 문헌 리뷰하는 형태로, 직접적인 실험이나 새로운 방법론 제시 등 독창적인 연구 결과를 포함하고 있지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 더 가깝다고 판단됩니다."
Levenstein's Algorithm On English and Arabic: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406547,"Survey research is one of the important types of scientific research needed to address the advancement in specific research areas. In this paper, we aim to provide a critical review of name-matching algorithms related to Levenstein's algorithm used in Arabic and English languages. We have collected and reviewed 50 papers related to Levenstein's algorithm. Name matching algorithms are becoming increasingly important in areas such as text classification, proofreading, automatic correction, plagiarism detection, extracting and aggregating specific information from huge data. In addition to the retrieval and tracking topics as well as answering questions, recognizing handwriting, evaluating articles, and summarizing. Among the new application in the area of artificial intelligence is the conversation systems, which are programs that communicate directly with humans using different natural languages. The most important part of name-matching algorithms is finding similarities between words or terms. The application of natural language processing tasks in Arabic and English represents a very difficult process as they contain many characteristics. Nevertheless, several name-matching algorithms have been introduced in Arabic and English texts, the attention of this research is focused on reviewing the research studies related to Levenstein's algorithm besides listing down developed applications in different areas.",No,"본 논문은 Levenstein 알고리즘과 관련된 기존 연구들을 수집하고 비판적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 알고리즘 개발을 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석하는 리뷰 논문에 해당합니다."
Enhancing Women's Safety - A Comprehensive Review of Emerging Technologies and Automated Crime Detection Using ML/DL Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10871586,"The increase in crimes against women, with reported cases escalating from 371,503 in 2020 to over 445,260 in 2022, underscores a pressing need for improved safety measures. This paper explores the challenges women face, such as physical violence and harassment in public spaces, and evaluates Women Safety Technologies (WST) including mobile apps, wearables, GPS tracking, surveillance systems, and IoT devices. A novel “SIRIP” search strategy is introduced to better assess WST effectiveness and limitations, emphasizing the need for user-friendly enhancements. Additionally, emerging automated crime detection technologies are reviewed, featuring machine learning (ML) and deep learning (DL) algorithms—Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Support Vector Machines (SVM), and Autoencoders to detect suspicious behaviours. Issues like data privacy, accuracy, and computational load are addressed, with solutions proposed using hybrid ML-DL models. A four-layer taxonomy has been introduced, highlighting the shortcomings of the current study and identifying areas where WST can be improved for future research.",No,"본 논문은 여성 안전 기술과 자동 범죄 탐지 기술에 대한 포괄적인 리뷰를 제공하며, 기존 연구들을 종합하고 평가하는 내용에 중점을 두고 있다. 독창적인 실험 결과나 새로운 연구 데이터를 제시하기보다는 기존 기술과 알고리즘을 분석하고 분류하는 데 초점이 맞춰져 있어 직접 기여하는 연구 논문으로 보기 어렵다."
VANET Meets Deep Learning: The Effect of Packet Loss on the Object Detection Performance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746657,"The integration of machine learning and inter- vehicle communications enables various active safety measures in internet-of-vehicles. Specifically, the environmental perception is processed by the deep learning module from vehicular sensor data, and the extended perception range is achieved by exchanging traffic-related information through inter-vehicle communications. Under such condition, the intelligent vehicles can not only percept the surrounding environment from self-collected sensor data, but also expand their perception range through the information sharing mechanism of Vehicular Ad-hoc Network (VANET). However, the dynamic urban environment in VANET leads to a number of issues, such as the effect of packet loss on the real-time perception accuracy of the received sensor data. In this work, we propose a point cloud object detection module via an end-to-end deep learning system and enable wireless communications between vehicles to enhance driving safety and facilitate real-time 3D mapping construction. Besides, we build a semi- realistic traffic scenario based on the Mong Kok district in Hong Kong to analyze the network performance of data dissemination under the dynamic environment. Finally, we evaluate the impact of data loss on the deep-learning-based object detection performance. Our results indicate that data loss beyond 50% (which is a common scene based on our simulation) can lead to a rapid decline of the object detection accuracy.",Yes,"본 논문은 딥러닝 기반의 객체 탐지 모듈을 제안하고, 차량 간 무선 통신 환경에서 패킷 손실이 객체 탐지 성능에 미치는 영향을 실험적으로 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Online Scheduling: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211826,"In this article a deep search of the literature of online scheduling is conducted. This paper intends to assess the developments and solutions found for online scheduling problems. Online scheduling is a very important topic since most of the real scheduling problems have dynamic characteristics. First, it was developed a literature review about scheduling problems, dividing them in stochastic and deterministic problems as well as in online and offline problems. Then, a bibliometric analysis was performed. Finally, some case studies in the field of online scheduling were analyzed. Online Scheduling is mostly explored in industry and health areas. In some articles explored there is a rescheduling, and the sequence of task may change due to the arrival of new tasks. In other cases, the new tasks are introduced in blocks of time that do not affect the previous schedule. This last technique is limited, since, with the arrival of new tasks, the schedule is not re-evaluated. Therefore, it is thought that, in future work, within the scope of online scheduling, when new tasks or other significant changes enter the system, the system should be evaluated, allowing the necessary changes to be made to the existing schedule. The Industry 4.0 and the evolution of Internet of Things (IoT), Deep Learning and Machine Learning favours a continuous and real-time flow of information, which allows the implementation of real-time online scheduling. This is a branch that should be explored in future works.",No,"이 논문은 온라인 스케줄링에 관한 기존 연구들을 종합적으로 검토하고 분석하는 문헌 조사(survey) 논문입니다. 독창적인 연구 결과나 새로운 실험, 방법론 제시보다는 기존 연구 동향과 사례 분석에 초점을 맞추고 있어 연구 논문으로 보기 어렵습니다."
WIP: Use of a Self-Executing Notebook Based on Matlab's Live Editor for the Development of Autonomous Learning Competency in an Analog Communications Course,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893526,"This innovative practice WIP paper describes how autonomous learning competency is developed in a course called “Communications Theory 1” that covers analog communications topics within the Telecommunications Engineering Program. To this end, all course material has been redesigned as a self-executing notebook using Matlab Live Editor11https://www.mathworks.com/products/matlab/live-editor.htm1 and is delivered chapter by chapter throughout the semester. This allows students to read the course content and interact with the simulations, modifying them freely to explore the session theory in greater depth and, finally, recognize relevant limits and characteristics. Within the course platform, based on Moodle, the contents have been accompanied by exercises in the Matlab Grader model, which allow feedback to the student when answering the questions provided, validating the developed code. This work helps to strengthen their knowledge in parallel to the activities carried out in the classroom. Since the self-executing notebook provides the information necessary to understand the behavior of the different analog communication techniques seen in the course, in the final chapter, all the knowledge is integrated into an activity that combines the flipped classroom and the puzzle technique, allowing students through teamwork, fully develop the final chapter of the course with a high level of autonomy. This work will be evaluated with the support of rubrics that will allow us to know its progression in the different learning outcomes related to the specific competencies developed in the course and the advancement of generic competencies, including autonomous learning. As a work in progress, initial measurements in applying the self-executing notebook indicate high student satisfaction with the notebook's content and the associated activities. Additional assessments are then planned to refine the teaching-learning cycle and improve course outcomes. Finally, new activities are proposed to complement the training by applying AI technologies and using new laboratory resources.",No,"본 논문은 Matlab Live Editor를 활용한 자율 학습 역량 개발을 위한 교육 방법 및 도구의 적용 사례를 설명하는 작업 진행 중(WIP) 보고서로, 직접적인 독창적 연구 결과나 실험 데이터 분석보다는 교육 방법론과 도구 개발에 초점을 맞추고 있습니다. 따라서 새로운 연구 결과를 제시하는 연구 논문으로 보기 어렵습니다."
Triple Diseases Prediction System using Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493563,"With the existing deep learning models in predicting multiple diseases primarily focus on analyzing individual diseases in isolation, lacking a unified system for multi-disease prediction. This project presents an approach to predict multiple diseases using Flask API, with a specific focus on brain tumors, COVID-19 and pneumonia. The proposed work represents a significant contribution to the field of disease prediction, harnessing the power of deep learning algorithms and modern web application development. The primary focus is on disease prediction, with a particular emphasis on ensuring accuracy and accessibility for end-users. The initial phase of this research involves data collection, where relevant datasets of various diseases are gathered. These datasets serve as the foundation for training and validating the deep learning models. Two prominent deep learning algorithms, Sequential CNN and VGG16, are employed for this purpose. These algorithms are chosen for their ability to handle complex data and recognize patterns within medical images and other health-related data. The core of the research involves training the deep learning models using the collected datasets. This step is crucial in enabling the models to learn and generalize from the provided data, ultimately enhancing their predictive capabilities. The models are modified to elevate their performance and accuracy. Following the training phase, the models are rigorously tested to evaluate their predictive accuracy. This assessment is vital in gauging the real-world applicability of the models in medical diagnosis. To make these powerful disease prediction models accessible to a wider audience, a front-end web application is developed.",Yes,"논문은 뇌종양, COVID-19, 폐렴 등 여러 질병을 예측하기 위해 딥러닝 모델을 수집된 데이터로 학습시키고 성능을 향상시키는 과정을 포함하고 있어, 독창적인 연구 내용과 실험적 기여가 명확하다. 또한, 모델의 정확도 평가와 웹 애플리케이션 개발을 통해 실제 적용 가능성을 검증하는 연구임을 보여준다."
Gamified and Artificial Intelligence-Assisted Knowledge Base in a Social Platform for Self-Learning in Higher Education,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10788662,"This paper proposes a technical model for a self-learning platform in higher education that integrates gamification, artificial intelligence, an online knowledge base, and a social network. Using WordPress as the central platform, specific tools such as GamiPress, AI Engine, BuddyPress, and BetterDocs are evaluated. Unlike previous studies, this work uniquely combines these technologies to address specific challenges in higher education, such as student engagement and personalized learning. Through an exhaustive literature review and a rigorous comparison of available plugins, it is argued that WordPress is a viable option for implementing this solution. This study provides a solid foundation for future implementations and empirical evaluations, presenting evidence on the effectiveness of integrating these tools in enhancing self-directed education.",Yes,"본 논문은 기존 연구와 차별화된 기술 모델을 제안하고, 여러 도구의 평가 및 비교를 통해 새로운 통합 솔루션을 제시하는 독창적인 연구 내용을 포함하고 있다. 또한, 향후 구현과 실증 평가를 위한 근거를 제공하여 직접적인 연구 기여가 있다고 판단된다."
Exploit Detection and Mitigation Technique of Cache Side-Channel Attacks using Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10404798,"As a persistent danger to computer security, cache side-channel attacks take advantage of minute flaws in microarchitectures to access sensitive data without authorization. In-depth analysis of these attacks’ numerous forms and related Common Vulnerabilities and Exposures (CVEs) is provided in this study report. The study emphasizes cache side-channel attacks’ practical ramifications and how they affect system security. The methods for identifying these threats are examined, with an emphasis on how machine learning and deep learning approaches might be used to strengthen security precautions. The research offers useful insights via pseudo code and data structure figures that highlight the necessity of real-time monitoring and detection tactics in reducing cache side-channel vulnerabilities. Artificial intelligence is used to design proactive countermeasures. The research article calls for proactive security measures to thwart these developing risks in addition to illuminating the complicated landscape of cache side-channel attacks. The contribution to this domain goes beyond theoretical discussion and includes actual application. This paper demonstrates the critical relevance of real-time monitoring and detection tactics in the fight to defend systems from cache side-channel attacks via the presentation of pseudo code and data structure diagrams. The results of this work provide guidance for future research and highlight the crucial value of interdisciplinary cooperation in the continuous quest for safe computing systems.",Yes,"논문 초록에서 인공지능을 활용한 캐시 사이드 채널 공격 탐지 및 완화 기법을 실제로 설계하고, 의사코드와 자료구조 도식을 통해 구체적인 적용 방안을 제시하는 등 독창적인 연구 내용과 실질적 기여가 포함되어 있음을 명확히 하고 있습니다. 따라서 이 논문은 연구 논문에 해당합니다."
Deep Dive into IIoT Intrusion Detection Systems: A Review and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837200,"The Industrial Internet of Things (IIoT) has quickly become a cutting-edge technology that can transform various sectors by digitalizing and connecting them, boosting business opportunities and global economic growth. Sectors like manufacturing, logistics, transportation, energy, and aviation are increasingly adopting IIoT. However, IIoT is vulnerable to cyber threats, requiring robust security measures. The numerous sensors in IIoT systems generate vast amounts of data, attracting cybercriminals. An Intrusion Detection System (IDS), which observe network traffic to detect unusual behavior, is crucial for securing IIoT applications. Recently, machine learning and deep learning techniques have shown potential in mitigating security threats and enhancing IDS performance. This paper reviews different deep learning-based IDS methods for IIoT, including techniques, datasets, and comparative analysis, and highlights existing limitations and future research directions.",No,본 논문은 IIoT 침입 탐지 시스템에 관한 기존 연구들을 종합적으로 검토하고 미래 연구 방향을 제시하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문에 해당하지 않습니다.
"Deep Learning in Arabic Text Summarization: Approaches, Datasets, and Evaluation Metrics",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009528,"Recently, there is a massive amount of data available on the internet. Hence, it is quite difficult for the users to go through all the available online information to generate a precise summary manually. Automatic Text Summarization (ATS) systems provide a solution to this problem as they produce a shorter and manageable version of the input text while keeping the most important information. Deep learning has achieved good results in Natural Language Processing (NLP) tasks and the use of deep learning techniques specifically in Automatic Text Summarization (ATS) has increased in English language. However, there is still a shortage of studies evaluating these techniques in Arabic language. In this research work, we review several articles that address the usage of deep learning with Arabic language. Specifically, we study the available models, datasets, and evaluation metrics for extractive and abstractive Arabic text summarization. We reviewed 12 research papers and found that most of the studies employed deep learning for the abstractive summarization type.",No,"본 논문은 기존 연구들을 리뷰하고 정리하는 문헌 조사(review) 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
"Sentiment Analysis in Low-Resource Settings: A Comprehensive Review of Approaches, Languages, and Data Sources",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10526251,"The field of low-resource sentiment analysis has seen significant developments in recent years. This research review SLR evaluates the approaches and data sources utilized in low-resource sentiment analysis by deep learning. The primary aim is to discover suitable approaches for future sentiment analysis in low-resource. Our studies explore various languages, models, and data sources expressing a desire to create effective approaches. Our emphasis lies in the critical evaluation of the approaches and the datasets utilized, to identify areas where further research is needed. Our analysis study adds to the existing body of literature reviews, encompassing multilingual low-resource sentiment analysis research spanning from 2018 to 2023. The findings indicate that the transfer learning approach is the most frequently used, followed by word embedding learning and machine translation systems. Additionally, the study shows that social media is the most used platform for data collection, followed by product reviews, movies, and hotels. There has been a significant surge in the adoption of pre-trained transformers, indicating a growing interest in exploring the potential of these models for low-resource languages within the natural language processing (NLP) community. This trend is largely attributed to the novel nature of these models and their feature of being non-labour intensive. However, the scarcity of annotated datasets for such languages remains a major hurdle. finally, these research findings are relevant and informative for any researcher working in the field of low-resource multilingual sentiment analysis. The study introduces a conceptual framework for performing sentiment analysis in low-resource. The study provides a valuable resource for future researchers.",No,"본 논문은 저자들이 기존 연구들을 종합적으로 검토하고 평가하는 문헌 리뷰(SLR) 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 방법론을 정리하는 데 중점을 두고 있습니다."
An Analysis of Travelling Salesman Problem Utilizing Hill Climbing Algorithm for a Smart City Touristic Search on OpenStreetMap (OSM),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8567045,"Travelling Salesman Problem (TSP) can be applied to find the most efficient route to travel between various nodes. The goal is to make smart cities to be created by heuristic algorithms on the real maps to perform some tasks through TSP. Therefore, Hill Climbing heuristic search algorithm which is generally used for mathematical optimization problems in Artificial Intelligence (AI) field has been preferred in this study. This application takes a city from the OpenStreetMap (OSM), which is a real map as an input given to the algorithm, and calculates a path to visit all the nodes on the related route. The output was intended to be found in the shortest possible way and in the least possible time. On the market, there are some travelling, public transport and discovery applications or games on the smart maps. Also there are some publications about TSP and metaheuristic approaches in the literature but the sources are generally commercial products and for limited cities. There is no application that takes all the cities as a source and makes a travel plan for tourists. This study intended to create an open-source, location independent travel plan advisor and develop an indigenous product. Application was tested for Rome and Ankara as an instance but because a flexible working area OSM was used, application can be generated for all the routes and also various applications can be developed by researchers based on this study.",Yes,"본 논문은 Hill Climbing 알고리즘을 이용해 OpenStreetMap 데이터를 기반으로 TSP 문제를 해결하는 독창적인 연구를 수행하였으며, 특정 도시를 대상으로 알고리즘을 적용하고 결과를 도출하는 실험적 기여가 포함되어 있다. 따라서 직접적인 연구 내용과 새로운 응용 개발이 포함된 연구 논문으로 판단된다."
Deep Learning Approaches for Enhanced Lower-Limb Exoskeleton Control: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556541,"Recent advancements in robotics have pushed the development of active exoskeletons and orthoses for assistive, augmentative, and rehabilitative purposes. Deep Learning approaches, particularly in motion analysis and prediction, hold promise for enhanced control of lower limb exoskeletons, offering the potential for improved outcomes in assistive and rehabilitative interventions. This review paper explores recent advancements in deep learning approaches for controlling lower limb exoskeletons. The study encompasses papers published from 2018 to the present, focusing on various aspects of deep learning models, including recognition, prediction, and other related tasks. The paper includes 103 papers covering various tasks and parameters like the gait phase, kinematics, and kinetics of the lower limb exoskeleton. Each aspect is thoroughly examined, highlighting the parameters utilized in the respective models. Moreover, the results obtained from these approaches are evaluated and compared against classical control strategies, shedding light on their effectiveness and potential benefits. The review also addresses the limitations of current deep learning techniques in lower limb exoskeleton control and outlines potential avenues for future research and improvement. By consolidating the latest findings and advancements in this field, this review paper provides valuable insights into the application of deep learning in the control of lower limb exoskeletons, paving the way for enhanced rehabilitation and assistive technologies in the future.",No,"본 논문은 여러 연구 결과를 종합하여 최근 동향과 발전 방향을 제시하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 정리하고 평가하는 데 중점을 두고 있습니다."
A Survey on Algorithms and Software for the Frequent Itemset Hiding Problem,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786657,"Privacy preserving data mining is an increasingly popular field of research with many problems being actively researched in this domain. Specifically, the Frequent Itemset Hiding Problem (FIHP) poses significant challenges in data mining, particularly concerning the privacy-preserving extraction of frequent itemsets from sensitive datasets. This survey paper comprehensively explores the landscape of software solutions dedicated to addressing the FIHP. Through an extensive review of existing methodologies, algorithms, and implementations, this survey provides a systematic analysis of the state-of-the-art in FIHP software. The survey categorizes and evaluates various approaches, ranging from traditional Apriori-based algorithms to recent advancements leveraging graph-based techniques and machine learning. By synthesizing the existing literature and high-lighting emerging trends, this survey aims to guide researchers and practitioners in selecting suitable software solutions for tackling the FIHP in diverse data mining applications.",No,"본 논문은 FIHP 문제에 대한 기존 알고리즘과 소프트웨어를 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 알고리즘 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석하는 리뷰 논문에 해당합니다."
Dependable DNN Accelerator for Safety-Critical Systems: A Review on the Aging Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198265,"In the modern era, artificial intelligence (AI) and deep learning (DL) seamlessly integrate into various spheres of our daily lives. These cutting-edge disciplines have given rise to numerous safety-critical applications such as autonomous driving with a paramount concern on ensuring a high promise of dependability because of the high risk of human injury in the case of malfunction. Even the dependability becomes more crucial as shrinking CMOS technology feature size enhances resilience concerns due to factors like aging. In the context of DL accelerators, which heavily rely on the efficiency and speed of computations, addressing the effects of aging is of utmost significance to ensure their optimal design and performance. This paper addresses the overarching dependability issue of advanced deep neural networks (DNN) accelerators from the aging perspective. Especially, a comprehensive survey and taxonomy of techniques used to evaluate and mitigate aging effects are introduced. We cover different aging effects like permanent faults, timing errors, and lifetime issues. We review research by the layer-wise approach and categorize several resilience classes to bring out major features. The concluding part of this review highlights the questions answered and several future research directions. This study is expected to benefit researchers in different areas of DNN deployment, especially the dependability of this emergent paradigm.",No,논문 초록에서 본 연구는 기존 연구들을 종합하여 노화 관점에서 DNN 가속기의 신뢰성 문제를 리뷰하고 분류하는 내용임을 알 수 있습니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구의 체계적 정리와 분석에 초점이 맞춰져 있어 연구 논문으로 보기 어렵습니다.
ML Based Interactive Disease Prediction Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752947,"The application of Machine learning algorithms to predict diseases is one of the finest methodology to reduce heavy work load on doctors and related medical staff. Based on the World Health Organization (WHO) report, about 85% heart disease deaths are due to Heart Attacks and Heart Strokes. In India the average death rate due to cardiovascular diseases is about 272 per 10,000 population which is greater than global average of 235 per 10,000 population. From the recent survey results, which was released by the Union Ministry of Family and Health Welfare (MoFHW), the Diabetes disease positive ratio is gradually increasing in India. 11.5 percent people were tested positive for Diabetes among urban and rural Indians who are with age 45 and above. Even there is availability of wide range of treatment methods of heart stroke patients & diabetes, Heart attack with Diabetes is the major cause of death in all parts of rural and urban areas of entire India. There are several factors causing heart and diabetes problems which include Age, Gender, Blood Pressure, Glucose levels, Skin thickness and Insulin. These are easily measured in primary care facility centres. The accurate estimation and analysis of heart & diabetes disease patients reports data may help in predicting future heart problems including diabetes. Globally, the application of computerized machine learning methods to predict future problems is in trend now. The Health Monitoring Departments and Fields uses machine learning algorithms to predict and analyse in a wider way to solve problems in fraction of seconds. From the famous proverb “Prevention is Better Than Cure”, if we apply this to medico and health field we can save people from major Heart Diseases (HD's) along with Diabetes. The proposed Dual disease prediction technique is user interactive based method. The proposed method observe inputs from the end user with realistic data to predict heart and diabetes disease. In the presented work, we used Logistic regression model (LR) and Support vector machine (SVM) model for prediction of diseases. The proposed model works with 85 and 78 percent accuracy in prediction of heart and diabetes diseases respectively.",Yes,"논문 초록에서 제안된 모델이 직접적으로 심장병과 당뇨병 예측을 위한 머신러닝 알고리즘(Logistic Regression, SVM)을 사용하여 새로운 예측 기법을 제시하고 있으며, 정확도 수치를 포함한 실험 결과를 제시하고 있어 독창적인 연구 내용이 포함된 연구 논문으로 판단됩니다."
Increasing anomaly handling efficiency in large organizations using applied machine learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606717,"Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis.",Yes,논문 초록에서 직접적으로 머신러닝 프로토타입을 개발하고 이를 실제 대규모 시스템에 적용하여 이상 보고서 할당 및 결함 위치 파악 과정을 개선하는 연구를 수행했다고 명시하고 있습니다. 이는 독창적인 연구 내용과 실험적 검증을 포함하는 연구 논문임을 나타냅니다.
Use Cases for AI in Reliability and Maintainability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10935281,"This paper explores writing a use case for applying AI to support asset reliability and maintainability. The primary concentration is on establishing a system description, defining actors, and outlining objectives. Use cases when applied in systems engineering or other contexts are a valuable tool for describing with clarity the different ways actors (i.e., users) will interact with a system and what constitutes success resulting from those interactions. AI is a complex technology requiring subject matter experts in the field. Most organizations will not have sufficient capability to apply AI without some outside assistance. It is unrealistic to assume that outside assistance, though skilled in AI, will have a working knowledge of your industry. A use case brings together the skill and knowledge of AI experts with the skill and knowledge of your organization and its operations into a common framework. Writing a use case is a multistep process. System function and system interaction are logically connected. Use cases applied to complex systems will be hierarchical to mirror those connections. This results in a compilation of use cases at multiple levels and with connections to multiple actors. Each actor typically has multiple objectives, some common with other actors and some unique to individual actors. Objectives are often intertwined with multiple linkages across different actors. In most organizations that use assets to provide a product, service, or perform a mission there are groups and departments with specific focus areas that relate to each other but do not have a detailed understanding of what each other do and what it take to be successful. Exploring all the steps of writing a use case for applying AI in R&M will require more writing space and time than the constraints of this paper and conference will allow. The exploration will be confined to the first three steps: 1) system description; 2) identifying actors; and 3) defining the goals and objectives for each actor. These three steps are foundational in uncovering the information needed to outline all the ways in which AI can provide valuable capability to enhance use of R&M data to elevate an organization's performance. Reliability and Maintainability (R&M) touches all aspects of management, operations, and maintenance in organizations that use physical assets to produce products, provide services, or perform missions. Asset reliability is needed for operations to perform their function. It is also the purpose of maintenance to sustain and restore reliability efficiently so downtime is minimized. Finally, management needs asset reliability to meet the organization's commitments and realize success. R&M is a data driven undertaking. AI is a major turning point in all technical fields going beyond what is considered evolutionary progress into what most will agree is a true revolution in how data can be used in detecting, defining, and clarify problems then ultimately deriving and evaluating solutions. Making the effort to compile use cases for all the actors (any group within the organization who will interact with the AI technology or its outcome) prior to stepping into an arrangement with AI professionals to implement AI in your organization will increase probability of success. It is the simplest and oldest intention when communicating between individuals or groups to clearly state in easy-to-understand terms what is expected and what is the end goal. The use case is purposefully designed as a means for the provider to understand what the customer expects.",No,"이 논문은 AI를 신뢰성 및 유지보수 분야에 적용하기 위한 유스케이스 작성 방법론과 개념을 설명하는 데 중점을 두고 있으며, 직접적인 실험 결과나 독창적인 연구 데이터를 제시하지 않는다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵다."
Optimizing Cloud Security: A Study of Effective Cybersecurity Measures for Organizations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10823273,"With cloud computing gradually forming the bedrock of organizational systems, the protection of data in the cloud remains important. This study aims to examine potential security measures that organizations can employ to safeguard their data in cloud systems. The study concerns the literature review of existing threats including data leaks, insider threats, malware, and advanced persistent threats (APTs), and assesses the efficiency of various security solutions. The combined IAM, data encryption, and a network security strategy, integrating AI and ML in the proposed model will help build a strong defense against cloud threats. Information was gathered from interviews, questionnaires, CSA reports, cybersecurity incident repositories, as well as CSP records. The dataset was pre-processed by thoroughly removing or masking any identifiable data from the dataset. This has been done in terms of such categories as Response Time Reduction, Incident Mitigation Rate, Resource Utilization Efficiency, and User Satisfaction. These findings showed favorable changes in all measures in comparison to benchmark models. The proposed model has realized 92% of the Incident Mitigation Rate, 85% of the Response Time Reduction, 78% of the Resource Utilization Efficiency, and an 88% level of User satisfaction. These results show that the model is not only capable of solving security incidents but is also efficient in managing resources and serves the purpose of a real-life IT security professional. The same was observed when carrying out the epoch-wise and batch size-wise analysis, which corroborated the reliability and portability of the model. The strategies and solutions highlighted provide valuable recommendations for improving cloud security and protecting organizational information against emerging cyber threats.",Yes,"본 논문은 기존 문헌 검토뿐만 아니라 인터뷰, 설문조사, 데이터셋 전처리 및 모델 평가를 통해 새로운 보안 모델을 제안하고 그 성능을 수치로 입증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Analysis of Attention Span of Students using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10575321,"This research work presents an experimental work of the Analysis of Attention Span of Students using Deep Learning, a novel application employing deep learning techniques for assessing student engagement in educational settings. The system incorporates facial recognition, eye tracking, and head pose analysis to offer real-time insights into students' attentiveness during lessons.The introduction outlines the motivation behind the research, emphasizing its significance in the dynamic landscape of education. The report addresses the project’s relevance in optimizing learning environments, delivering personalized education, and addressing challenges in remote learning scenarios.The implementation overview details the integration of key components and the calculation of real-time metrics related to student attention. Practical applications of the system are discussed, highlighting its role in educational adaptability, early intervention, and the generation of data-driven insights for continuous improvement in teaching methodologies. The conclusion summarizes key findings and discusses the potential implications of the Analysis of Attention Span of Students using Deep Learning. This research work aims to contribute to the discourse surrounding educational technology, emphasizing adaptability and personalization for an enriched learning experience and accuracy is measured. Moreover, end user interface is implemented, and impact is analyzed.",Yes,"논문 초록에서 딥러닝 기법을 활용한 학생 주의 집중도 분석이라는 독창적인 연구 내용을 다루고 있으며, 실시간 지표 계산과 시스템 구현, 정확도 측정 등 구체적인 연구 기여가 포함되어 있다. 따라서 본 논문은 직접 기여하는 연구 논문에 해당한다."
An Empirical Overview of Stress Detection Based on Photoplethysmography Signal Using Various Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912261,"The rapid speed of modern life and the shift towards cognitive work have contributed to a rising prevalence of mental stress across all professions. Many sedentary occupations, particularly in fields like Information Technology (IT), causes prolonged periods of intense mental effort that can significantly elevate stress levels. Extended exposure to such mental stress is linked to an increased risk of serious conditions, such as cardiovascular diseases and mental health disorders. Consequently, there is a critical need for non-intrusive tools capable of detecting mental stress. Early detection is essential for preventing stress-related health concerns and continuous observation of stress levels. Numerous methods are currently available for stress detection, but they face limitations like inaccuracies due to individual differences and environmental factors as well as difficulties with real-time monitoring. In this discussion, several techniques are explored that offer better insights with stress detection, providing valuable perspectives for future researchers. This survey discusses various deep learning techniques, highlighting their advantages and limitations. It also suggests potential improvements to tackle these limitations in the future. Initially, this survey categorizes Deep Learning techniques into five techniques: Long ShortTerm Memory (LSTM), Convolutional Neural Network (CNN), Transfer Learning (TL), Attention-Based Mechanisms, and Transformer-based approaches are discussed in depth. Subsequently, it identifies research gaps in traditional Deep Learning approaches to guide future innovative model development. The study reviews 25 research papers focusing on various methods for stress detection using DL. Finally, this survey evaluates performance metrics, datasets, techniques used for research, and model achievements. CNNs emerge as the predominant approach, and Python is frequently used for stress detection in DL. Additionally, accuracy is highlighted as a key analytical metric.",No,초록에서 해당 논문은 기존 연구들을 종합적으로 검토하고 분석하는 서베이(리뷰) 논문임을 명확히 밝히고 있습니다. 직접적인 실험이나 독창적인 연구 결과를 제시하기보다는 기존 연구들의 장단점과 연구 공백을 논의하는 데 중점을 두고 있습니다.
Adaptive Security in a Connected World: Review of Machine Learning for IoT Intrusion Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10774622,"In IoT Security risks are, however, presented to devices and services by this integration. To give machine learning (ML) credit for being a novel approach, this paper surveys Intrusion Detection Systems (IDS) for the Traditional Network and Internet of Things. With 41 articles review, Machine Learning can adapt to the changing IoT environment, Deep Learning (DL) in particular shows promise as a method. IoT device with security problem with complex IoT landscape is too much for traditional rule-based IDS to handle, which has led to a rise in interest in machine learning approaches. To solve issues used different techniques are investigated. The research discusses important feature selection techniques related to IoT intrusion detection, emphasizing the value of feature engineering and classification. With newly created datasets both normal and malicious scenarios use performance measures such as F1-score, accuracy, precision, recall, and AUC-ROC.",No,"본 논문은 41개의 기존 연구를 리뷰하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않고 기존 연구들을 종합하여 분석하는 내용에 초점이 맞춰져 있습니다. 따라서 직접 기여하는 연구 논문으로 보기 어렵습니다."
A Smart Operator Assistance System Using Deep Learning for Angle Measurement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599587,"Manual workstations play a critical role in flexible assembly lines by enabling human responses to reconfiguration that is faster than machine responses. As a result, human is more adaptive and sometimes unreplaceable by machines in complex assembly. However, with the increasing complexity of tasks, product quality has become highly susceptible to human error due to increments in operators’ cognitive load. One of the errors that affect assembly quality is the operator’s use of a handheld tool with an unfavorable working angle when handling the workpiece. In this case, an assistive mechanism to remind workers about the wrong working angle is necessary to support the process. To this end, this study proposes an angle monitoring system to inspect the working angle of handheld tools and provide feedback in real time with minimal interruption to the assembly process. The proposed system consists of an angle measurement model and an action recognition model, which are both built using deep-learning-based object detection algorithm. Besides, the system was designed with flexibility that it is applicable to different tools and assembly tasks. A case study on fastening a high-end graphical processing unit card is investigated to evaluate their performance. Results show 95.83% and 99.83% accuracies of the models. In practice, the proposed study is expected to facilitate assembly quality by preventing the failure of angle-related operations in a timely and reliable manner.",Yes,논문은 딥러닝 기반의 각도 측정 및 동작 인식 모델을 개발하여 작업자의 작업 각도를 실시간으로 모니터링하고 피드백을 제공하는 시스템을 제안하고 있다. 이는 독창적인 연구 내용과 실험 결과(정확도 평가)를 포함한 직접적인 연구 기여로 판단된다.
Developing and Validating the Contextual Technology Andragogy/Pedagogy Entrepreneurship Work Content Knowledge Model: A Framework for Vocational Education,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10676317,"Purpose: The present study aimed to integrate artificial intelligence technology (ChatGPT) with an in-depth literature review to explore knowledge dimensions of professional teaching and information and communication technology (ICT) integration models in TVET, and utilize the Delphi technique and ChatGPT to examine, enhance, and validate a newly proposed model for professional teaching and ICT integration in TVET.Methods: Three rounds of the Delphi technique were applied to develop and validate this new model. Artificial intelligence tool (ChatGPT) with an in-depth literature review were used to explore knowledge dimensions for TVET education. In Round 1, ChatGPT was used to transform the technological pedagogical content knowledge (CK) model for TVET instruction. A new suggested model was developed called the contextual technology andrology/pedagogy entrepreneurship work CK (CTA/PEWCK) model. Ten experts from the TVET sector participated in Round 2, and 39 participated in Round 3 to validate the new suggested model.Findings: The findings revealed that the fifteen knowledge dimensions extracted from this new suggested model presented essential knowledge for TVET education.Conclusion: Applying the CTA/PEWCK model offers professional development opportunities for TVET teachers that focus on hands-on experiences to develop competencies for sustainable development (ESD), enabling an integrated approach to knowledge dimensions, procedures, and attitude.",Yes,"본 논문은 Delphi 기법과 인공지능 도구(ChatGPT)를 활용하여 새로운 교육 모델(CTA/PEWCK 모델)을 개발하고 검증하는 과정을 포함하고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문으로 판단된다. 또한, 전문가 검증을 통해 모델의 타당성을 평가한 점에서 연구 논문의 요건을 충족한다."
An Empirical study of the IoT arrhythmia detection methods: Review and research gaps,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402472,"Cardiac arrhythmia is a condition of irregular heartbeat, which causes sudden loss of life, which insists the necessity for the continuous monitoring of the arrhythmia patients so that an appropriate medical procedure is provided, for which the smart healthcare plays a major role. Cardiac arrhythmia is one among a type of cardiovascular diseases (CVDs), which reports 12% of total deaths all-over the world. Even though there is a lot of growth in IoT-health monitoring, the manual method suffers from a lot of drawbacks. Hence, there is a need for an automatic method in health-care specifically, for classification of arrhythmia. In this research, we analyze various automatic methods used for classifying arrhythmia in IoT network. This research presents the detailed review of 25 research papers suggesting the methodologies based on various Machine Learning Methods, Deep learning method, nature-inspired methodologies, and so on, for the automatic prediction of Arrhythmia. The collected papers are classified based on different prediction and clustering classification techniques. The research gaps and the challenges faced by the existing techniques are listed and elaborated, which helps the researchers to upgrade the future works. The works utilize various datasets, software tools, performance evaluation measures, prediction techniques, which are analyzed and the performance attained by different techniques are discussed and the research gaps for further contributions are identified in such a way that it enables the researcher to contribute much for the benefit of the society.",No,"본 논문은 25편의 기존 연구를 종합적으로 검토하고 연구 격차를 제시하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 포함하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 분석하고 정리한 문헌 리뷰에 해당합니다."
A Wearable High Blood Pressure Classification Processor Using Photoplethysmogram Signals through Power Spectral Density Features,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869847,"High blood pressure (BP) is a major source of health problems related to mental stress, cardiac issues, kidney problems, vision, and brain. High BP bursts can damage and rupture blood vessels and cause strokes. Therefore, it is quite important to continuously monitor it for high BP patients. Conventional BP monitoring devices a) can cause discomfort and b) not suitable for intermittent monitoring. The photoplethysmographic (PPG) signals measure the volume changes in the human blood through human skin. This work presents a high BP classification processor using PPG signals through an artificial intelligence (AI) based boosted circuit. A data set of 25 participants was collected. Ten out of the 25 participants were high blood pressure patients with systolic BP (SBP) and, diastolic BP (DBP) values higher than 140mmHg and 90mmHg, respectively. The AI boosted circuit calculates the power spectral densities, power spectral densities difference, and the sum of the consecutive difference between PPG signals. The features are forwarded to a small 3-level Decision Tree (DT) classifier. The decision tree classifier classifies the high SBP and DBP as high or normal/low with 96.2% classification accuracy. The SBP values ≥ 130mmHg and < 130mmHg were classified as HIGH SBP or LOW/NORMAL SBP respectively. Similarly, the DBP values ≥ 80mmHg and < 80mmHg were classified as HIGH DBP or LOW/NORMAL DBP, respectively. The system was implemented on an Artix-7 FPGA which consumes power of ≈18.23uW @ 50 MHz.",Yes,"본 논문은 PPG 신호를 이용한 고혈압 분류 프로세서를 인공지능 기반 회로로 구현하고, 실제 데이터 수집 및 분류 정확도 평가를 포함한 독창적인 연구 내용을 다루고 있다. 또한 FPGA 구현 및 전력 소모 측정까지 수행하여 직접적인 연구 기여가 명확하다."
Development and Research of English Defect Reporting Software Based on Natural Language Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10788281,"This paper aims to explore an innovative English defect reporting software that utilizes natural language processing (NLP) techniques, particularly hybrid neural networks and attention mechanisms, to improve the accuracy and readability of English defect reporting. The research focuses on English pronunciation defect recognition and aims to provide software developers with an intelligent defect report writing and review tool by integrating advanced algorithms and simulation techniques. Firstly, this study proposes an English pronunciation defect recognition model based on hybrid neural networks and attention mechanisms. The model uses deep learning algorithms to automatically identify and correct pronunciation errors in reports, while using attention mechanisms to improve the ability to capture key information in the text. This integrated approach effectively improves the language quality of defect reporting and reduces the communication barriers caused by language problems. Secondly, a comprehensive natural language processing system is developed, which not only includes the pronunciation defect recognition module, but also includes grammar checking, vocabulary suggestion and context understanding. Through the organic combination of these functions, the system can provide users with a full range of language support to ensure the professional and clear defect report. Finally, the simulation experiment of the system algorithm is carried out to evaluate its performance in practical application. The experimental results show that the developed system performs well in identifying pronunciation defects and providing language recommendations, significantly improving the quality of defect reporting.",Yes,"본 논문은 자연어 처리 기반의 영어 결함 보고 소프트웨어 개발과 관련된 독창적인 연구 내용을 포함하고 있으며, 하이브리드 신경망과 어텐션 메커니즘을 활용한 모델 제안과 시스템 구현, 그리고 시뮬레이션 실험을 통한 성능 평가를 수행하고 있다. 이는 직접적인 연구 기여를 나타내는 연구 논문으로 판단된다."
A Domain-Adapted Machine Learning Approach for Visual Evaluation and Interpretation of Robot-Assisted Surgery Skills,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9808117,"In this study, we present an intuitive machine learning-based approach to evaluate and interpret surgical skills level of a participant working with robotic platforms. The proposed method is domain-adapted, i.e., jointly utilizes an end-to-end learning approach for smoothness detection and domain knowledge-based metrics such as fluidity and economy of motion for extracting skills-related features within a given trajectory. An advantage of our approach compared to similar stochastic or deep learning models is its intuitive and transparent manner for extraction and visualization of skills-related features within the data. We illustrate the performance of our proposed method on trials of the JIGSAWS data set as well as our own experimental data gathered from Phantom Premium 1.5A Haptic Device. This approach utilized t-SNE technique and provides visualized low-dimensional representation for different trials that highlights nuanced information within the executive task and returns unusual or faulty trials as outliers far away from their normal skill or participant clusters. This information regarding the input trajectory can be used for evaluation and education applications such as learning curve analysis in surgical assessment and training programs.",Yes,"본 논문은 로봇 수술 기술 평가를 위한 도메인 적응형 머신러닝 방법을 제안하고, 이를 기존 데이터셋과 실험 데이터에 적용하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Fingerprint generation and authentication though Adaptive convolution generative adversarial network (ADCGAN),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178664,"Fingerprints are crucial in identification of humans. The uniqueness of finger prints makes it an interesting subject. Fingerprints are termed as a technique used to define, assess, and quantify a person's physical and behavioral property. Deep learning has made its application in all the major fields such as natural language processing, computer vision and speech processing. Deep learning has also found its application in the important subject of fingerprint synthesis and biometric. The ever-growing complexity of fingerprint authentication issues, from cellphone authentication to airport security systems, seems to be best handled by these models. In recent years, deep learning-based models have been used more and more to raise the accuracy of various fingerprint recognition systems. The persuasive capacity of Generative Adversarial Networks (GANs) to generate believable instances can be credibly taken from an existing distribution of samples. GAN exhibits exceptional performance on data generation-based tasks and also encourages study in privacy and security. In this work, using Adaptive Deep Convolution Generative Adversarial Networks (ADCGAN), we develop a model that generates and authenticate the fingerprints. A Socofing dataset was trained on ADGAN model. The model gave 92% accuracy. The conduct of fingerprint research has been made possible due to ADGAN, without restrictions related to the confidential nature of biometric data.",Yes,"논문 초록에서 Adaptive Deep Convolution Generative Adversarial Networks (ADCGAN)을 사용하여 지문 생성 및 인증 모델을 개발하고 92% 정확도를 달성했다고 명시하고 있어, 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 알 수 있습니다. 이는 기존 연구를 바탕으로 한 새로운 모델 제안과 성능 평가를 포함하는 직접적인 연구 기여에 해당합니다."
Analysis and Evaluation of Sentiments in Online Communities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522446,"In this research paper, we focus on understanding people's feelings and opinions in online social forums, like social media platforms and discussion boards. These forums are crucial for conversations and sharing information online. Our main goals are to explain what sentiment analysis is, discuss the unique challenges of analyzing sentiments in social forums, review different methods used for sentiment analysis, and see how well they work in this context. We highlight the growing importance of sentiment analysis for grasping public opinions, spotting trends, and making better decisions in the fast-paced world of social forums. We explore the difficulties in sentiment analysis here, like dealing with casual language, understanding context, and the specifics of different topics discussed. By looking at various sentiment analysis techniques, such as rule-based methods, machine learning, and combinations of both, we aim to understand which ones are most useful for social forums. We also talk about the measures used to evaluate these techniques and stress the importance of having diverse and representative data to train these models. We provide examples of how sentiment analysis can be used, like monitoring brands or understanding political discussions. Finally, we discuss where future research in sentiment analysis for social forums could go, including improving models to handle messy text, making them work better across different topics, and considering ethical issues like privacy and bias. Our paper aims to contribute to the ongoing progress in sentiment analysis, offering insights into the challenges and opportunities in understanding user feelings in online social forums. We hope this will help guide future research to develop better sentiment analysis models for these platforms, which are both effective and ethical.",No,"초록 내용은 기존 연구들을 설명하고 평가하는 리뷰 성격이 강하며, 독창적인 연구 결과나 새로운 실험, 데이터 분석에 대한 직접적인 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 연구 논문보다는 개관 또는 리뷰 논문에 더 가깝다고 판단됩니다."
Ethical Impact Identification of a Dementia Behaviour Monitoring System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581932,"The identification of ethical impacts is the first phase of Ethical Impact Assessments, which are used to evaluate the ethical implications of new technologies. Using a structured methodology, this paper explores the identification of ethical impacts of a video-based tool for monitoring dementia-related behaviours. By reflecting on autonomy, dignity, non-maleficence, beneficence, justice and privacy, our work contributes to a broader understanding of the ethical landscape surrounding the use of Artificial Intelligence (AI) and computer vision in healthcare. Our contribution goes beyond the specific aspects of this system as we address consent, privacy and autonomy, factors that are relevant to any technology for people with dementia. We call for the integration of ethical considerations into the design and implementation of AI technologies, ensuring that innovations not only advance clinical care, but also respect fundamental ethical principles.",Yes,"본 논문은 치매 행동 모니터링 시스템의 윤리적 영향 식별에 대해 구조화된 방법론을 사용하여 직접적인 연구를 수행하고 있으며, AI와 컴퓨터 비전 기술의 윤리적 측면을 분석하는 독창적인 기여를 포함하고 있다. 따라서 연구 논문으로 판단된다."
Artificial Intelligence and Students Learning: A Study on Outcome of Technological Usage (AI) In Higher Education Platform,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113027,"Artificial intelligence (AI) is presumptively driving technological force of the primary this century, and may work nearly every trade, if not human endeavors at large. Businesses and governments worldwide are running huge sums of cash into an awfully big range of implementations, and dozens of start-ups square measure being funded to the tune of billions of dollars. AI is one altogether those aspects of recent life relating to that the majority individuals have some awareness, and notwithstanding acknowledge we've little information. In fact, for several AI is similar with golem robots, which might be as a result of news relating to AI is sort of regularly illustrated with a picture of a golem or a digital brain. Meanwhile, the dystopian footage of front robots keep firmly within the realm of phantasy. This study discusses on what student ought to learn within the age of AI, however will AI enhance and remodel education. The analysis is administered with well-structured form and picked up responses from Teaching colleges and students experiencing computer science in education. the current study aiming at application of AI in higher education, an empirical approach has been wanting to verify the possible analysis findings. exploitation convenient sampling, around 320 responses were collected from AI users. Results shows Outcome of Technological Usage of AI (OTUAI) has been dominantly factorized into 3 dominant factors. there’s vital distinction among demographic profile teams in factors of OTUAI. Impact of demographic profile, usage of technology and medium of instruction on overall OTUAI has been identified.",Yes,"본 논문은 인공지능(AI)의 고등교육 플랫폼 내 활용 결과를 실증적으로 분석하고, 320명의 응답자를 대상으로 데이터를 수집하여 기술 사용의 결과를 도출하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Artificial Intelligence Agents to Support Data Mining for SoS Modeling of Space Systems Design,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172802,"The complex and multidisciplinary nature of space systems and mission architectures is especially evident in early stage of design and architecting, where systems stakeholders have to keep into account all the aspects of a project, including alternatives, cost, risk, and schedule and evaluate various potentially conflicting metrics with a high level of uncertainty. Though aerospace engineering is a relatively young discipline, stakeholders in the field can rely on a vast body of knowledge and good practices for space systems design and architecting of space missions. These guidelines have been identified and refined over the years. However, the increase in size and complexity of applications in the aerospace discipline highlighted some gaps in this approach: first, the amount of available information is now very large and originates from multiple sources, often with diverse representations, and useful data for trade space analysis or analysis of all potential alternatives can be easily overlooked; second, the variety and complexity of the systems involved and of the different domains to be kept into account can generate unexpected interactions that cannot be easily identified; third, continuous advancements in the field of aerospace resulted in the development of new approaches and methodologies, for which a common knowledge database is not existing yet, thus requiring substantial effort upfront. To address these gaps and support both decision making in early stage of space systems design and increased automation in extraction of necessary data to feed working groups and analytical methodologies, we propose the training and use of Artificial Intelligence agents. These agents can be trained to recognize not only information coming from standardized representations, for example Model Based Systems Engineering diagrams, but also descriptions of systems and functionalities in plain English. This capability allows each agent to quantify the relevance of publications and documents to the query for which it is trained. At the same time, each agent can recognize potentially useful information in documents which are only loosely connected to the systems or functionalities on which the agent has been trained, and which would possibly be overlooked in a traditional literature review. The search for pertinent sources can be further refined using keywords, that let the user specify more details about the systems or functionality of interest, based on the intended use of the data. In this work we illustrate the use of Artificial Intelligent agents to sort space habitat subsystems into NASA Technology Roadmaps categories and to identify relevant sources of data for these subsystems. We demonstrate how the agents can support the retrieval of complex information required to feed existing System-of-Systems analytic tools and discuss challenges of this approach and future steps.",Yes,"본 논문은 인공지능 에이전트를 활용하여 우주 시스템 설계 초기 단계에서 데이터 마이닝과 의사결정을 지원하는 새로운 방법론을 제안하고 있으며, 이를 통해 기존 방법의 한계를 극복하고자 하는 독창적인 연구 내용을 포함하고 있다. 또한, 실제 사례를 통해 제안된 방법의 적용 가능성을 시연하고 있어 연구 논문에 해당한다."
Batch-mode active learning for technology-assisted review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363867,"In recent years, technology-assisted review (TAR) has become an increasingly important component of the document review process in litigation discovery. This is fueled largely by dramatic growth in data volumes that may be associated with many matters and investigations. Potential review populations frequently exceed several hundred thousands documents, and document counts in the millions are not uncommon. Budgetary and/or time constraints often make a once traditional linear review of these populations impractical, if not impossible — which made ""predictive coding"" the most discussed TAR approach in recent years. A key challenge in any predictive coding approach is striking the appropriate balance in training the system. The goal is to minimize the time that Subject Matter Experts spend in training the system, while making sure that they perform enough training to achieve acceptable classification performance over the entire review population. Recent research demonstrates that Support Vector Machines (SVM) perform very well in finding a compact, yet effective, training dataset in an iterative fashion using batch-mode active learning. However, this research is limited. Additionally, these efforts have not led to a principled approach for determining the stabilization of the active learning process. In this paper, we propose and compare several batch-mode active learning methods which are integrated within SVM learning algorithm. We also propose methods for determining the stabilization of the active learning method. Experimental results on a set of large-scale, real-life legal document collections validate the superiority of our method over the existing methods for this task.",Yes,"본 논문은 기존 연구의 한계를 극복하기 위해 SVM 기반의 배치 모드 능동 학습 방법을 제안하고, 능동 학습 과정의 안정화 판단 방법도 새롭게 제시하는 등 독창적인 연구 기여를 포함하고 있다. 또한, 실제 대규모 법률 문서 집합을 대상으로 실험을 수행하여 제안 방법의 우수성을 검증하였다."
Sports Video Data Classification using Yolov5 Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10249850,"The machine learning along with computer vision has helped widely in classification of sports videos. Deep learning techniques are also being used to perform research in this domain. The video frames are the most significant components of the sports classification system. There are many models that have been used to classify sports videos. The objective of this work is to develop a classifier for different sports activity recognition using video data with high accuracy and probability. Sports dataset has been used from Kaggle [10]. This framework has been created for applications related to sports, object detection, game identification, recognition, analysis, players’ tracking, and performance. This framework consists of various intermediary processes. At first, preprocessing has been carried out in this framework by converting input sports video into video frames. Then skeletonization has been carried out using computer vision. Finally, feature extraction and classification has been done using the Yolov5 model that has been trained with the dataset. Accuracy and loss graphs have been generated using prediction metrics for this model to evaluate the accuracy of this framework. The accuracy of the framework is more than 90% according to the results we have obtained.",Yes,"논문은 스포츠 비디오 분류를 위한 Yolov5 기반 분류기 개발이라는 구체적이고 독창적인 연구 내용을 포함하고 있으며, 데이터셋을 활용한 모델 학습과 평가 결과(정확도 90% 이상)를 제시하고 있다. 이는 직접적인 연구 기여를 나타내므로 연구 논문에 해당한다."
Detection Mechanism of Money Laundering based on Random Walk and Skip-Grim Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9909113,"Money laundering means that criminals use the services provided by banks to transfer a large amount of illegal funds to untraceable destination accounts. Most of the related works are rule-based and machine learning based anti-money laundering systems. However, the anti-money laundering systems based on machine learning are affected by the data scale of money laundering transactions. The rule-based anti-money laundering systems require a lot of manual work and cannot adapt to the changing money laundering behavior. Therefore, this paper designs a money laundering detection mechanism based on random walk and skip-grim model. This detection mechanism preferentially constructs an account transfer graph. Then, on this basis, it generates random transfer trajectories for each account using the random walk algorithm. Thereafter, the transfer characteristics of each user are automatically analyzed from the random transfer trajectories utilizing the skip-grim model. Finally, it compares the extracted transfer characteristics of different users and combines the cosine similarity to identify illegal money laundering users. Last, we use python programming language and cbank dataset to evaluate the performance of the proposed scheme, and we compare our work with the related works, flowscope and Martin jullum. The extensive simulation results validate that, the proposed scheme has better identification effect, correctly identifying more than 80% of the money laundering accounts, and the misjudgment rate is less than 20 %.",Yes,"논문은 랜덤 워크와 스킵-그림 모델을 활용한 새로운 자금세탁 탐지 메커니즘을 설계하고, 이를 실제 데이터셋과 비교 실험을 통해 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구를 단순히 요약한 것이 아니라 직접적인 기여가 있는 연구 논문으로 판단된다."
"Deep Learning for Aspect-Level Sentiment Classification: Survey, Vision, and Challenges",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726353,"This survey focuses on deep learning-based aspect-level sentiment classification (ASC), which aims to decide the sentiment polarity for an aspect mentioned within the document. Along with the success of applying deep learning in many applications, deep learning-based ASC has attracted a lot of interest from both academia and industry in recent years. However, there still lack a systematic taxonomy of existing approaches and comparison of their performance, which are the gaps that our survey aims to fill. Furthermore, to quantitatively evaluate the performance of various approaches, the standardization of the evaluation methodology and shared datasets is necessary. In this paper, an in-depth overview of the current state-of-the-art deep learning-based methods is given, showing the tremendous progress that has already been made in ASC. In particular, first, a comprehensive review of recent research efforts on deep learning-based ASC is provided. More concretely, we design a taxonomy of deep learning-based ASC and provide a comprehensive summary of the state-of-the-art methods. Then, we collect all benchmark ASC datasets for researchers to study and conduct extensive experiments over five public standard datasets with various commonly used evaluation measures. Finally, we discuss some of the most challenging open problems and point out promising future research directions in this field.",No,"이 논문은 기존 연구들을 종합적으로 정리하고 평가하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 방법론을 제시하지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
A Chest X-ray Image Retrieval System for COVID-19 Detection using Deep Transfer Learning and Denoising Auto Encoder,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458223,"The COVID-19 pandemic is the defining global health crisis of our time which is currently challenging families, communities, health care systems, and government all over the world. It is critical to detect and isolate the positive cases as early as possible for timely treatment to prevent the further spread of the virus. It was found in few early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. In the current context, a rapid, accessible and automated screening tool based on image processing of chest X-rays (CXRs) would be much needed as a quick alternative to PCR testing, especially with commonly available X-ray machines and without the dedicated test kits in labs and hospitals. Several classifications based approaches have been proposed recently with encouraging results to detect pneumonia based on CXRs using supervised deep transfer learning techniques based on Convolutional Neural Networks (CNNs). These black box approaches are mainly non-interactive in nature and their prediction represents just a cue to the radiologist. This work focuses on issues related to the development of such an automated system for CXRs by performing discriminative feature learning using deep neural networks with a purely data driven approach and retrieving images based on an unknown query image and performing retrieval evaluation on currently available benchmark datasets towards the goal of realistic comparison and real clinical integration. The system is trained and tested on an image collection of 1700 CXRs obtained from two different resources with encouraging results based on precision and recall measures in individual deep feature spaces. It is hoped that the proposed system as diagnostic aid would reduce the visual observation error of human operators and enhance sensitivity in testing for Covid-19 detection.",Yes,"논문은 COVID-19 검출을 위한 흉부 X선 이미지 검색 시스템을 제안하며, 딥러닝과 오토인코더를 활용한 독창적인 방법론과 실험 결과를 포함하고 있다. 이는 기존 연구를 바탕으로 한 직접적인 연구 기여와 실험적 검증을 포함한 연구 논문으로 판단된다."
A Comparative Study of Deep Image Retrieval Models Leveraging Deep Features,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10739143,"This review investigates how deep learning methods can be utilized for efficient image retrieval based on content. Obtaining accurate images from vast digital collections poses significant challenges, motivating research in CBIR. The effectiveness of these methods varies depending on the dataset’s type and size, with certain algorithms excelling with specific dataset characteristics. An extensive and well-structured review of successful image retrieval techniques is given in this paper. Our primary objective is to evaluate various deep learning models applied by researchers and compare their performance based on the outcome of evaluation matrices. These models encompass CNNs, DBNs, and other deep architectures tailored for image retrieval tasks. By synthesizing insights from this review, researchers can make informed decisions regarding model selection and potentially enhance retrieval performance by leveraging advanced deep learning features. The importance of deep features in image retrieval is the ability to capture complex visual patterns and semantic information that cannot be easily extracted by traditional handcrafted features. With the increasing volume of online images, Image retrieval using deep learning has become crucial for applications like object recognition, image retrieval, and image search engines.",No,"초록에서 이 논문은 기존 연구들을 종합하고 비교하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 제시하지 않는다. 따라서 연구 논문보다는 문헌 리뷰에 해당한다."
Construction of a Meteorological Data Support Vector Machine Drought Prediction Model Based on Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10699072,"In recent years, the proliferation of machine learning algorithms in drought prediction has revolutionized the field, leveraging their robust data processing and pattern recognition abilities to offer fresh perspectives and methodologies. This article specifically employs machine learning to develop a support vector machine (SVM) drought prediction model grounded in meteorological data, aiming to enhance prediction accuracy. Following a comprehensive literature review outlining the fundamentals of machine learning and its applications in drought forecasting, the article delves into the construction process of the SVM-based model in detail. Experimental analysis validates the model’s strengths, revealing that despite Decision Tree and Linear Regression algorithms demonstrating improved accuracy trends (ranging from 0.7-0.86 and 0.65-0.87, respectively), the SVM algorithm consistently outperforms them, achieving superior prediction accuracy. This underscores the conclusion that the SVM algorithm is a highly effective tool for drought prediction, offering precision that surpasses traditional methods.",Yes,논문은 기계 학습 알고리즘을 활용해 SVM 기반 가뭄 예측 모델을 직접 구축하고 실험 분석을 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 새로운 모델 개발과 성능 평가를 다룬 연구 논문으로 판단된다.
Fine-Grained In-Context Permission Classification for Android Apps Using Control-Flow Graph Embedding,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298328,"Android is the most popular operating system for mobile devices nowadays. Permissions are a very important part of Android security architecture. Apps frequently need the users' permission, but many of them only ask for it once—when the user uses the app for the first time—and then they keep and abuse the given permissions. Longing to enhance Android permission security and users' private data protection is the driving factor behind our approach to explore fine-grained context-sensitive permission usage analysis and thereby identify misuses in Android apps. In this work, we propose an approach for classifying the fine-grained permission uses for each functionality of Android apps that a user interacts with. Our approach, named DroidGem, relies on mainly three technical components to provide an in-context classification for permission (mis)uses by Android apps for each functionality triggered by users: (1) static inter-procedural control-flow graphs and call graphs representing each functionality in an app that may be triggered by users' or systems' events through UI-linked event handlers, (2) graph embedding techniques converting graph structures into numerical encoding, and (3) supervised machine learning models classifying (mis)uses of permissions based on the embedding. We have implemented a prototype of DroidGem and evaluated it on 89 diverse apps. The results show that DroidGem can accurately classify whether permission used by the functionality of an app triggered by a UI-linked event handler is a misuse in relation to manually verified decisions, with up to 95% precision and recall. We believe that such a permission classification mechanism can be helpful in providing fine-grained permission notices in a context related to app users' actions, and improving their awareness of (mis)uses of permissions and private data in Android apps.",Yes,"이 논문은 Android 앱의 권한 사용을 정밀하게 분류하는 새로운 방법론(DroidGem)을 제안하고, 이를 구현 및 평가한 연구 결과를 포함하고 있다. 따라서 독창적인 연구 내용과 실험적 검증이 포함된 연구 논문에 해당한다."
A Review of Power System Transient Stability Analysis and Assessment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8942834,"The failure of power system transient stability is one of the main factors causing catastrophic accidents of power systems. Therefore, it is of great significance to evaluate the transient stability of a power system. This paper first introduces the evaluation methods of power system transient stability, including the assessment methods based on time domain simulation, direct method, artificial intelligence-based methods and the probabilistic assessment method. The key challenges in power system transient stability assessment are reviewed and analyzed, including the stability evaluation of power-electronized power system and the main elements of artificial intelligence method used in transient stability assessment. Last, the future research directions and conclusions are discussed.",No,초록에서 해당 논문은 기존 연구들을 종합하여 전력 시스템 과도 안정도 평가 방법들을 소개하고 주요 도전과제 및 향후 연구 방향을 논의하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 독창적인 연구 결과나 직접적인 실험·분석 기여가 포함된 연구 논문으로 보기 어렵습니다.
Creating Realities: An In-Depth Study of AI-Driven Image Generation with Generative Adversarial Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10726281,"The field of artificial intelligence has witnessed significant advancements with the advent of Generative Adversarial Networks, revolutionizing image generation techniques. The literature review covers seminal works and recent advancements, highlighting the evolution of image generation techniques. Taxonomy is developed to classify methods based on their underlying principles and applications across various domains including art, fashion, healthcare, and entertainment. Comparative analysis evaluates the effectiveness, efficiency, and applicability of different models, utilizing metrics corresponding Inception Score and Frechet Inception Distance. By conducting a thorough review of existing literature and technologies, we explore the evolution of GAN architectures, from their inception to the latest innovations. We examine various GAN models, including DCGAN, StyleGAN, and CycleGAN, assessing their effectiveness in generating high-fidelity images across diverse applications. In addition to this comprehensive analysis, we present our own implementation of a GAN model tailored for specific image generation tasks. Our model is tested on multiple datasets to evaluate its performance, highlighting improvements in training stability and image quality. We discuss the architectural choices, optimization techniques, and training procedures employed in our implementation. Furthermore, we address the challenges faced in the field, such as mode collapse, training instability, and the need for largescale datasets. Our findings offer valuable insights into optimizing GAN performance and propose potential directions for future research. This comprehensive analysis serves as a foundational reference for researchers and practitioners aiming to leverage GANs for advanced image generation tasks, pushing the boundaries of what is visually conceivable through artificial intelligence.",Yes,"논문 초록에서 기존 연구들을 종합적으로 분석하는 동시에, 저자들이 직접 구현한 GAN 모델과 그 성능 평가 결과를 제시하고 있어 독창적인 연구 기여가 포함되어 있음을 알 수 있습니다. 따라서 연구 논문에 해당합니다."
Exploring Pretrained Models and Transfer Learning Techniques for Image Retrieval,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307272,"In recent years, the field of content-based image retrieval (CBIR) has witnessed significant advancements and garnered immense attention. With the increase of digital imagery across various domains, the need for effective image search and retrieval techniques has become increasingly important. Pre-trained deep neural networks have arisen as effective tools for image analysis and feature extraction due to their ability to learn complex visual representations from vast amounts of data. By harnessing the power of pre-trained deep neural networks, such as AlexNet, ResNet, VGG16, and GoogLeNet, researchers seek to enhance CBIR systems. This paper explores the significance of utilizing pre-trained networks for content-based image retrieval (CBIR). It includes a literature review discussing similar CNN architectures & their applications as well as identifying research gaps. The paper conducts a comparative study of the four pre-trained networks: AlexNet, ResNet, VGG16, and GoogLeNet, evaluating their performance and suitability for CBIR. It also demonstrates transfer learning techniques to fine tune AlexNet and VGG16 models using MNIST and Fashion MNIST dataset. Evaluation using LRAP as a metric revealed good performance for both models, with AlexNet consistently outperforming VGG16, albeit with longer execution times.",Yes,"논문은 사전학습된 네트워크를 활용한 이미지 검색 성능 비교와 전이학습 기법을 적용한 실험을 포함하고 있어, 독창적인 연구 내용과 실험적 기여가 명확히 드러난다. 따라서 단순 문헌 리뷰가 아닌 직접적인 연구 논문으로 판단된다."
Sentiment Analysis and Opinion Mining on Social Media Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696144,"Sentiment analysis on social media is a Natural Language Processing practice used to extract subjective information and opinions from user-generated content on various social media platforms, such as Twitter, Facebook, and Instagram. The goal of the proposed work is to perform sentiment analysis on social media data related to a particular topic or brand, such as a product launch or a social issue. Social media data will be collected using relevant APIs or web scraping tools and pre-processed by cleaning and filtering out irrelevant or spam content This data is helpful for users as well as for the management to make informed decisions. Because micro-blog posts are usually very brief and informal, traditional opinion mining algorithms struggle to handle this type of content, making the subject challenging to tackle. Semantic and syntactic analysis in Sentiment Net was addressed in the previous lexicon-based sentiment analysis approach. The proposed suggestion is to produce an automation-based sentiment analysis method, which is less expensive in spite of this system's requirement is least. Consequently, in this work, introduces a novel system architecture that is capable of automatically analyzing the sentiments contained in these communications. We use this algorithm in conjunction with manually annotated social media data for sentiment analysis. This system's machines are capable of autonomously have knowledge to extract data. In the proposed system, a sentiment analysis model, such as a lexicon-based machine learning model, is applied to classify the sentiment of the content as positive, negative, or neutral. Twitter data is utilized for training, validating and testing the tweets based on their emotions, the emojis too are classified. The experimental findings validate our system's effectiveness in sentiment analysis in real-world social media applications.",Yes,"논문 초록에서 제안된 시스템 아키텍처와 자동화된 감성 분석 방법, 그리고 수집된 트위터 데이터를 활용한 모델 학습 및 검증 과정을 명확히 기술하고 있어 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단됩니다."
Review of Security in Mobile Edge Computing with Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714349,"Mobile edge computing is an emerging concept, introduced to bring the cloud services and resources to the user proximity and exploring them at the edge of the network. This helps to meet the requirements of low latency, location awareness, and mobility support. In this paper, we discuss the related work done in mobile edge computing and how it allows the cloud application services to be hosted alongside mobile network elements and facilitates the radio information in real-time. We have also explored the various machine learning methods/algorithms used by applications and services that are hosted on MEC platform. Deep learning method is one of the machine learning technique that have been studied in this paper to incorporate security measures in Mobile edge computing environments. Various categories of threats are analyzed and studied in terms of its strengths and implementation.",No,본 논문은 모바일 엣지 컴퓨팅과 딥러닝을 활용한 보안에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 분석과 정리에 중점을 두고 있습니다.
A Critical Analysis of Applying Computer Vision In Building an Intelligent System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10182958,"The terminology ""computer vision"" refers to the method that leverages technologies of intelligent systems such as artificial intelligence (AI) to enable computers to gather useful information from visual sources. The automated activities are demonstrated previously using computer vision understanding. The definition, examples, as well as uses of computer vision, are covered in this article. For computer vision to be totally successful there must be a huge collection. When they lack the knowledge required to execute the task at hand, these systems periodically discover trends, which cause this to occur. Hence, the research has evaluated the notion of computer vision in diverse fields of operations through the literature review as well as in the analysis and discussion portion of the study. Since the literature review and data analysis of the research is done through secondary sources, an imperative notion of review has been gathered and presented. Therefore, the significance of computer vision in assembling intelligent systems like AI and other new technologies has given discussed in the research.",No,"초록에서 제시된 내용은 문헌 리뷰와 기존 연구의 분석에 기반한 개념적 고찰에 불과하며, 직접적인 실험, 데이터 수집, 또는 독창적인 연구 결과를 제시하지 않는다. 따라서 본 논문은 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵다."
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620168,"Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, existing research on HAII is limited, fragmented, and inconsistent. We present here a survey of that literature and recommendations for research best practices that should improve the field. We divided our investigation into the following areas: 1) terms used to describe HAII, 2) primary roles of AI-enabled systems, 3) factors that influence HAII, and 4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, seven factors influence HAII: user characteristics (e.g., user personality), user perceptions and attitudes (e.g., user biases), user expectations and experience (e.g., mismatched user expectations and experience), AI interface and features (e.g., interactive design), AI output (e.g., perceived accuracy), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments). HAII is most measured with user-related subjective metrics (e.g., user perceptions, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.",No,"이 논문은 기존 문헌을 체계적으로 검토하고 연구 동향과 공백을 분석하는 문헌 리뷰(survey) 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 평가하는 데 초점이 맞춰져 있습니다."
Optimization of Power in Power Converters Using Machine Learning: An Overview,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932361,"The rapid evolution in power electronics have brought significant attention to the optimization of power converters, which are essential for efficient energy conversion and management. Traditional techniques for optimizing power converters often struggle with the increasing complexity of modern systems, including nonlinear dynamics, diverse operating conditions, and stringent efficiency requirements. Last few decades, it has been observed that machine learning has evolved as hopeful solution with data driven methodologies to address various challenges. This survey provides the integration of machine learning for optimizing power converter, reviewing new approaches, key procedures and application in different domain. This categorizes different types of machine learning techniques and evaluate their usage in multiple fields like real-time control, fault detection, thermal management and system design. The paper also discusses different datasets, evaluation metrics and implementation framework in field of power converters. Lastly, challenges like computational complexity, data security and real-world deployment are highlighted along with direction towards future research to enhance the integration between machine learning and power converters. The comprehensive review aims to provide insight of researchers carried out to develop an intelligent power system.",No,"본 논문은 머신러닝을 활용한 전력 변환기 최적화에 관한 기존 연구들을 종합적으로 정리한 개관(survey) 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구들을 리뷰하는 성격이 강하다."
Using Unsupervised Machine Learning Methods and the Nasa Snowex Swesarr Instrument to Study How Snow Water Equivalent is Changing with Climate Change,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10641609,"The Snow Water Equivalent Synthetic Aperture Radar and Radiometer (SWESARR) is a dual microwave instrument meant to fill in information gaps in the remote sensing data of Snow Water Equivalent (SWE). The aim of this work is to improve and validate SWESARR measurements of SWE for areas with tree canopy using unsupervised machine learning methods. This information is critical to NASA’s SnowEx mission for understanding the spatial and temporal variability of snow. SWE is an integral part of the climate system and affects many other climate-related processes, thus an accurate understanding of how SWE is changing with climate change is crucial for future water resource management. We have made use of a suite of parameters to help in identifying features most important in predicting SWE in areas that have missing satellite data due to vegetation. Our aim is to validate and improve SWESARR measurements using unsupervised machine learning clustering algorithms with the goal of being able to better quantify spatial and temporal changes of SWE due to climate change.Other relevant datasets that have been central to identifying parameters for predicting SWE and data validation have been from the ASO (Airborne Snow Observatory), NSIDC (National Snow and Ice Data Center), and ground snow pit observations taken by the SnowEx field work team. This work is a result of NASA’s SnowEx team based out of Goddard Space Flight Center within the Climate Change Research Initiative at NASA GISS.",Yes,"본 논문은 SWESARR 측정값을 개선하고 검증하기 위해 비지도 학습 기법을 적용하는 독창적인 연구를 수행하고 있으며, 이를 통해 기후 변화에 따른 눈수분량(SWE)의 공간적·시간적 변화를 정량화하는 새로운 방법론을 제시하고 있다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단된다."
Towards Better ML-Based Software Services: An Investigation of Source Code Engineering Impact,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234350,"In recent years, the development of machine learning-based solutions for software services, particularly for source code, has grown rapidly. It is witnessed that many machine learning models for software services require the input of source code snippets in a desired form of abstract syntax tree (AST), which is mostly generated from an external tool. However, such data pre-processing tasks could be done by different engineering tools, and the impact of these tools towards final models is often neglected. In this work, we aim to investigate the source code engineering impacts towards machine learning-based software services. Three different types of parsing tools are identified, which are parser generator, parsing library and parser developed for a certain purpose. They are thoroughly evaluated towards the impacts on the prediction model of Code2Vec for the prediction task of the method name in Java language. The collective result on the Java-small dataset shows that the generated ASTs differ a lot in terms of source code structures and contents when using different parsing tools. The difference could influence the performance of the trained model significantly. Our result suggests that when machine learning models are implemented for software services, especially for code-related tasks, the selection of parsing tools should be thoroughly considered during the data pre-processing stage. While there are some interesting findings on Java-med and Java-small, we anticipate this work could provide some insights for better ML-based software service solutions from the perspective of source code engineering.",Yes,"본 논문은 소스 코드 엔지니어링 도구가 머신러닝 기반 소프트웨어 서비스의 성능에 미치는 영향을 실험적으로 평가하고, 그 결과를 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 기존 연구를 단순히 정리한 리뷰가 아니라 직접적인 실험과 분석을 수행한 연구 논문으로 판단된다."
Machine Learning-Based Security Test Model and Evaluation for SIP-Based DoS Attacks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894217,"In recent years, with the development of IP-based systems, circuit-switched systems have rapidly started to be replaced by packet-switched systems in communication infrastructures and operators have started to prefer VoIP systems more due to their advantages such as cost and resource efficiency. However, since VoIP (Voice over Internet Protocol) systems are equally open to all threats to which IP-based systems are open, researchers have proposed different methods for obtaining strong security solutions. Recently, rule-based systems have been replaced by machine learning-based systems in many areas and different machine learning-based solutions have been suggested for VoIP security.In this study, a machine learning-based solution was proposed for detecting SIP flooding attacks within the scope of DoS (Denial of Service) attacks which is one of the current threats to VoIP infrastructure. For this purpose, a test environment was created on a previously developed simulation infrastructure, primarily normal traffic and attack traffic were generated, and then the effectiveness of certain machine learning methods in the classification of traffic was tested with the labeled data obtained.When the results are evaluated with the parameters based on the working conditions, it is observed that the related methods can produce meaningful results.",Yes,"본 논문은 SIP 기반 DoS 공격 탐지를 위한 머신러닝 기반 보안 테스트 모델을 제안하고, 시뮬레이션 환경에서 정상 및 공격 트래픽을 생성하여 머신러닝 기법의 효과를 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
System Level Knowledge Representation for Complexity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447091,"To develop systems capable of high level cognitive functions such as intelligence, it is necessary to formally capture different types of knowledge, so that they can be used to support complex processes, such as inference and reasoning. The design and engineering of Intelligent Systems to support large distributed socio technical processes increasingly leverages converging techniques from Artificial Intelligence, Knowledge Representation (KR) and Cognitive Architectures. This is resulting in multi layered architectures and AI technologies which one the one hand offer unprecedented capabilities, on the other hand present innumerable, often inconceivable risks. Sophisticated conceptual structures are necessary not only to support the modeling, validation and explanation of complex engineered systems, but primarily to support cognition and conceptualization of the complexities involved, for designers, developers, end users and any stakeholder. Depending on the cognitive makeup of observers, and on the knowledge available, complexity can be conceptualized and traversed following a diversity of methods and patterns. Sometimes complexity can be broken down into cognitively accessible chunks, in other cases however, it cannot be broken down without losing essential information about the system as a whole. Addressing the need to develop cognitive artifacts, methods and techniques that can capture and represent complexity, this paper proposes the outline of conceptual structure that bridges existing approaches which tend to distinguish between cognitive engineering and Knowledge Representation, with the aim to integrate technical and socio technical systems dimensions. The paper presents considerations about cognitive aspects of complex systems theory and practice. It anticipates a convergence between cognitive architectures and KR, introduces the notion of System Level Knowledge Representation and applies it to navigate socio technical complexity in systems engineering. A summary of related work where the System Level Knowledge Representation is being developed and evaluated is also provided.",Yes,"논문 초록에서 제안하는 개념적 구조와 System Level Knowledge Representation의 개발 및 평가에 대해 언급하고 있어, 독창적인 연구 내용과 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 기존 접근법을 통합하고 새로운 개념을 제시하는 점에서 직접적인 연구 기여가 있다고 판단됩니다."
Deep Learning in Mental Health: An In-Depth Analysis of Prediction Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421590,"Millions of people worldwide continue to suffer from a variety of diseases that frequently go undetected or untreated, making mental health disorders a major public health concern. In addition to a full examination of the many types of mental disorders, symptoms, diagnostics, and investigations of mental disorders, this review paper gives a detailed description of the mental disorder detection utilizing deep learning-based methodologies. The review starts out by going over the frequency, importance, and difficulties in timely diagnosing mental health illnesses. The system design for employing deep learning to identify mental disorders early on is then further explored. In addition, the comparative analysis component of the article, which is covered in the literature review section, provides a critical evaluation of the datasets and evaluation metrics currently in use for training and evaluating deep learning models for the identification of mental disorders. The advancements in this field, including the identification of depression, anxiety, schizophrenia, and other mental health issues, are demonstrated through a thorough assessment of recent studies and their findings. This work also discusses future possibilities and challenges in the field of deep learning for the diagnosis of mental disorders, highlighting the importance of large-scale, diversified, and representative datasets.",No,초록에서 해당 논문은 기존 연구들을 종합하고 비교 분석하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험을 제시하는 연구 논문에 해당하지 않습니다.
5G D2D Transmission Mode Selection Performance & Cluster Limits Evaluation of Distributed AI and ML Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530792,"5G D2D Communication promises improvements in energy and spectral efficiency, overall system capacity, and higher data rates. However, to achieve optimum results, it is essential to select wisely the Transmission mode of the D2D Device to form clusters in the most advantageous positions in terms of Sum Rate and Power Consumption. Towards this end, this paper investigates the use of Distributed Artificial Intelligence (DAI) and innovative D2D, Machine Learning (ML) approaches (i.e., DAIS, FuzzyART, DBSCAN and MEC) to achieve satisfactory results in terms of Spectral Efficiency (SE), Power Consumption (PC) and execution time, with the creation of clusters and back-hauling links in D2D network under existing Base Station. Additionally, this paper focuses on a small number of Devices (i.e., <=200), targeting the identification of the limits of each approach in terms of the low number of devices. More specifically, we investigate when an operator must consider implementing a D2D network (that requires extra complexity), therefore when the cluster members are sufficient enough to achieve better results than the classic mobile network. So, this research identifies where it is beneficial to form a cluster, investigate the critical point that gains increases rapidly and in the end, examine the applicability of 5G requirements. Additionally, prior work presented a Distributed Artificial Intelligence (DAI) Solution/Framework in D2D, and a DAIS Transmission Mode Selection (TMS) plan was proposed. In this paper, DAIS is further examined, improved in terms of thresholds evaluation (i.e., Weighted Data Rate (WDR), Battery Power Level (BPL)), evaluated, and compared with other approaches (AI/ML). The results obtained demonstrate the exceptional performance of DAIS and FuzzyART, compared to all other related approaches in terms of SE, PC, execution time and cluster formation. Also, results show that the investigated AI/ML approaches are also beneficial for Transmission Mode Selection (TMS) in 5G D2D communication, even with fewer devices (i.e., >=5 for clustering, >=50 for backhauling) as lower limits.",Yes,"논문 초록에서 Distributed AI 및 ML 기법을 활용한 5G D2D 전송 모드 선택 및 클러스터링 한계 평가에 대해 구체적인 방법론 개선과 성능 평가 결과를 제시하고 있어, 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문임을 알 수 있습니다. 또한 기존 연구를 확장하고 비교 분석한 점도 연구 논문임을 뒷받침합니다."
A Small-Scale Temperature Forecasting System using Time Series Models Applied in Ho Chi Minh City,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994437,"Urban living benefits greatly from weather forecasting since it may lower weather-related losses, safeguard public health and safety and promote both economic growth also quality of life. The main goal of this work is to develop a small-scale temperature forecasting system employing a cutting-edge time series model. In order to do so, data on Ho Chi Minh City's temperature is gathered. The performance of several time series models based on machine learning and deep learning is then evaluated for input data of various lengths. To create a small-scale temperature forecasting system, the best model is chosen. The suggested approach is particularly well suited for a smart agricultural indoor temperature forecasting system, which cannot be accomplished with any large-scale temperature forecasting systems.",Yes,본 논문은 호치민시의 온도 데이터를 수집하고 여러 시계열 모델을 평가하여 최적의 모델을 선정하는 등 직접적인 연구 방법론과 실험 결과를 포함하고 있다. 이는 독창적인 연구 내용을 포함한 연구 논문에 해당한다.
Prediction of Insurance Premium using Machine Learning with an Adaptive Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307009,"The insurance market is very large and expanding day by day. There are many parameters to consider before deciding on insurance premiums. Sometimes it becomes difficult to browse all the documents before applying for insurance, so it is necessary to understand the insurance industry and point out issues related to competition in that industry. This type of company is very interested in forecasting. The goal of this article is to find accurate predictions based on considering different dimensions of machine learning to reduce the company's financial losses. Machine learning helps companies to optimize their services with greater accuracy and fewer losses. It can also help insurance companies effectively screen cases, evaluate them more accurately, and make accurate cost forecasts. This research work uses machine learning-based methods like linear regression, KStar, and Random Forest and suggests a suitable method to produce results with high accuracy and less relative error. In addition to this, it demonstrates how to create a specific data subset that can be used to test and train a machine learning system. The effectiveness of the suggested strategy is assessed by contrasting the estimated value with the actual value of the simulated data. Insurance firms are capable to construct consistent financial structures, such as monthly premiums or payroll taxes, to provide funds to pay for the medical benefit agreements that are defined in insurance policies by calculating the whole risk of the expenses associated with health care and the medical system.",Yes,"본 논문은 머신러닝 기법을 활용하여 보험료 예측 모델을 개발하고, 다양한 알고리즘의 성능을 비교 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 데이터셋 생성 및 모델 검증 과정을 통해 실제 적용 가능성을 검토하는 등 직접적인 연구 기여가 명확하다."
A proposed Crypto-Ransomware Early Detection(CRED) Model using an Integrated Deep Learning and Vector Space Model Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9031182,"Crypto-ransomware is a malware category that targets user-related files to encrypt them and hold them to ransom. The irreversible effect of crypto-ransomware attacks entails early detection before it starts encrypting the files. Although several works have been proposed to detect such attacks at the pre-encryption phase before the encryption takes place, the main limitation of these works is the way in which they define the boundaries of the pre-encryption phase. That is, these studies determine the pre-encryption boundaries based on tracking the first call of any cryptography-related Application Programming Interface (API). However, relying on the first call of cryptography-related APIs to delineate the pre-encryption boundaries is not accurate as these APIs might be related to other (normal) tasks done by the crypto-ransomware, such as unpacking and/or decrypting the metamorphic payload, before the ransomware starts the malicious activities. In that case, the collected pre-encryption data lack many relevant pre-encryption attack patterns that come after the mistakenly-identified pre-encryption boundary. Such data insufficiency adversely affects the accuracy of the detection model and increases the rate of false alarms. To overcome such limitations, this paper proposes an early detection model (CRED) that can determine the pre-encryption boundaries and collect the data related to this phase more accurately. Unlike the extant research, the CRED model employs data-centric and process-centric detection approaches to combine both IRP and API data. These data will then be used to train a deep learning-based model. The CRED model will be evaluated using a data-benchmark collected by executing real-world crypto-ransomware samples downloaded from a widely-used repository. The performance of the detection model will be validated using the k-fold cross validation and compared against the models proposed by the existing works.",Yes,"본 논문은 기존 연구의 한계를 극복하기 위해 새로운 암호화 랜섬웨어 조기 탐지 모델(CRED)을 제안하고, 실제 데이터를 활용해 딥러닝 기반 모델을 학습 및 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
BMI-based framework for teaching and evaluating robot skills,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6907749,"Brain Machine Interface systems provide ways of communication and control of a variety of devices that range from domestic appliances to humanoid robots. Most BMI systems are designed exclusively to control devices using low-level commands, or high-level commands when devices with pre-programmed functionalities are available. In this paper, we build on our previous work on BMI-based Learning System in which we presented a different approach for designing BMI systems that incorporate learning capabilities that relieve the user from tedious low-level control. In this work, we extend the capabilities of our framework to allow a user to be able to teach and evaluate a robotic system by using a BMI. We provide general system architecture and demonstrate its applicability in new domains such as teaching a humanoid robot object manipulation skills and evaluating its performance. Our approach consists of 1) tele-operating robot's actions while robot's camera collects object's visual properties, 2) learning manipulation skills (i.e. push-left, lift-up, etc.) by approximating a posterior probability of commonly performed actions when observing similar properties, and 3) evaluating robot's performance by considering brain-based error perception of the human while he/she passively observes the robot performing the learned skill. This technique consists of monitoring EEG signals to detect a brain potential called error related negativity (ERN) that spontaneously occurs when the user perceives an error made by the robot. By using human error perception, we demonstrate that it is possible to evaluate robot actions and provide feedback to improve its learning performance. We present results from five human subjects who successfully used our framework to teach a humanoid robot how to manipulate diverse objects, and evaluate robot skills by visual observation.",Yes,"본 논문은 BMI 기반 학습 시스템을 확장하여 로봇에게 조작 기술을 가르치고 평가하는 새로운 프레임워크를 제안하고, 이를 실제 인간 피험자 실험을 통해 검증한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Multi-mission technical subsystem management measures taken and lessons learned,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747637,"The primary focus of the Instrument Operations Subsystem (IOS) within NASA's Advanced Multi-Mission Operations System (AMMOS) is to provide adaptable multi-mission operations core capabilities (ground data systems tools and services) supporting mission instrument operations planning, decision-making, and performance assessment for a wide variety of instrument types (cameras, spectrometers, robotic components, etc.) for deep space planetary exploration (orbiter, in-situ, fly-by) and observatory missions. This paper addresses, from the technical program leadership perspective, the measures taken and lessons learned relative to managing the system engineering, development, and cost of IOS tools and services which provide payload instrument data processing, management, and distribution (primarily in the form of science imagery and higher order data products) in support of mission operations. Items to be covered include measures taken to: a) draft IOS functional processes/procedures within the boundaries of institutional and managing/oversight organizations, b) create a software and infrastructure management plan for the IOS team with roles and responsibilities, c) develop a strategic plan and roadmap for expanded and/or new tool capabilities which build upon an existing tool-base to support upcoming missions (such as Mars2016) and additional/new instrument types, d) define subsystem level functional work-breakdown-structure and related requirements, e) formulate an adjustable/verifiable cost model for developing/adapting functional components, f) achieve Capability Maturity Model Integration (CMMI) level 3, and g) lessons learned along with suggestions for improvement when accommodating competing agendas from a multitude of stakeholders and contributors.",No,"본 논문은 NASA의 Instrument Operations Subsystem 관리 및 운영에 관한 기술적 조치와 교훈을 다루고 있으며, 주로 시스템 관리, 계획, 절차 수립 및 조직적 측면에 초점을 맞추고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 데이터 분석을 포함한 연구 논문으로 보기 어렵습니다."
A Systematic Review on Intelligent Intrusion Detection Systems for VANETs,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8970942,"Vehicular Ad hoc Networks (VANETs) are a growing area that continues to gain interest with an increasing diversity of applications available. These are the underlying network for Intelligent Transportation Systems (ITS), a set of applications and services that aim to provide greater security and comfort to drivers and passengers. However, the characteristics and size of a VANET make it a security challenge. It has been a subject of study, with several research works aimed at this problem, usually involving cryptography. There are, however, some attacks that cannot be solved using traditional methodologies. For example, Sybil attack, Denial of Service (DoS), Black Hole, etc. are not preventable using cryptographic tools. Nonetheless, using an Intrusion Detection System (IDS) can help to detect malicious behavior, preventing further damage. This work presents a Systematic Literature Review (SLR) that aims to evaluate the feasibility of this type of solution. Additionally, it should provide information about the most common approaches, allowing the identification of the most used Machine Learning (ML) algorithms, architectures and datasets.",No,"본 논문은 VANETs의 지능형 침입 탐지 시스템에 관한 기존 연구들을 체계적으로 검토하는 문헌 리뷰(Systematic Literature Review)로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 종합하고 분석하는 리뷰 논문에 해당합니다."
Implementation and Comparison of U-net networks for Automatic COVID-19 Lung Infection Segmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955782,"With the big number of COVID-19 patients, efficient detection tools are necessary. In this work, we proposed an automatic detection and quantification tool based on deep learning model. The architecture used is U-Net architecture, one of the most known for medical applications. This network was introduced as a binary semantic segmentation tool. It uses a dataset of 100 images, seventy-two of them for training, ten for validation, and eighteen for testing. The model will be compared with other deep learning models and evaluated using several evaluation metrics. The results have shown an accuracy of 0.958, sensitivity of 0.900, dice coefficient of 0.726, and a specificity of 0.962. Compared with other related works, our network has the best accuracy and specificity. The obtained results show the ability of the model as a binary segmentation tool to separate infection tissue and healthy tissue.",Yes,"본 논문은 U-Net 기반의 딥러닝 모델을 사용하여 COVID-19 폐 감염 부위를 자동으로 분할하는 새로운 도구를 제안하고, 이를 다른 모델과 비교 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Cloud Based AIVR Training System for Emergency Rescue,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10858818,"This paper mainly discusses the current situation and existing problems of training exercises in emergency medicial scenarios, as well as the importance and necessity of developing an AI training system for emergency medical scenarios. First, we introduce the current status of emergency medical training and emphasize the potential value of AI technology in this field. Then, the goal of developing an AI training system is defined as improving the efficiency and quality of medical education and training. We summarize the advantages of AI technology in emergency medical exercises and make a review and selection analysis of its related technologies. In the design and implementation stage, we describe in detail the requirement analysis, function design, system architecture, and core module development of the emergency medical AI training system. After system integration and testing, we verified the feasibility and effect of the system. In the process of evaluation and optimization, we evaluated the performance and user experience of the system and put forward optimization strategies and security considerations for the problems. In the application case analysis, we collect user feedback to verify its efficiency and carry out an in-depth analysis of the advantages and disadvantages of the system. Finally, in the conclusion and prospect part, we summarize the research results, point out the existing problems and improvement directions and look forward to the future development trends and challenges of AI training systems for emergency medical scenarios.",Yes,"논문 초록에서 시스템의 요구분석, 기능 설계, 아키텍처 및 핵심 모듈 개발, 통합 및 테스트, 성능 평가와 최적화 전략 제시 등 구체적인 연구 및 개발 과정이 포함되어 있어 독창적인 연구 내용이 포함된 연구 논문으로 판단됩니다. 또한, 사용자 피드백 수집과 효율성 검증을 통해 실질적인 기여를 하고 있음을 알 수 있습니다."
Artificial Intelligence for Traffic Prediction and Estimation in Intelligent Cyber-Physical Transportation Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269085,"A cyber-communication infrastructure is required for physical components like control systems, sensors, actuators, and the environment to communicate and collaborate in real-time effectively. This is what cyber-physical systems (CPSs) provide. Deep Learning (DL) techniques have sped up and improved the progress of Intelligent Transportation Systems (ITS) in recent years, especially in problem domains traditionally addressed with analytical or statistical solutions. In addition to advancing driverless vehicle development to a new level, the improvements brought about by DL applications have led to better traffic organization and scheduling, increased security and safety on transit roads, reduced maintenance costs, and optimized performance for public transportation ride-sharing companies. The primary goal of this work is to offer a complete study and awareness of the uses of DL models on ITS, as well as to demonstrate the development in ITS research that has been owing to DL studies. This article briefly introduces the reader to various DL methods before thoroughly examining and describing how these methods are now being used in the transportation sector. Deep learning models are trained on this real-world traffic information to detect better and forecast the probability of crashes. This work aims to do just that by providing a multi-perspective assessment of deep learning-based techniques for traffic forecasting. This paper offers a summary and taxonomy of current traffic forecast systems. We provide a compilation of the most cutting-edge methods currently used for traffic forecasting. In addition, we assess the efficacy of various approaches using a public, real-world dataset and provide an evaluation and analysis of our findings. These findings demonstrate that, related to state-of-the-art shallow models, a deep model is superior at traffic detection and achieves equivalent results in terms of traffic prediction. We suggest a new deep learning framework, an Attention-based hybrid Convolutional Neural network with Long Short Term Memory (LSTM) (AHCNLS), to perform real-time traffic prediction from a data mining approach to enhance driver and passenger safety. The suggested approach considers the spatial and temporal connection between GPS trajectories and contextual elements. Using a publicly available dataset, our proposed technique is evaluated and shown to have advantages over competing methods.",Yes,"본 논문은 딥러닝 기반의 새로운 교통 예측 프레임워크(AHCNLS)를 제안하고, 공개된 실제 데이터셋을 사용해 기존 방법들과 비교 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Innovative Approaches to FPGA Security: Leveraging LUT Locking and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10872400,"As the digital landscape continues to evolve, Field Programmable Gate Arrays (FPGAs) have become indispensable across sectors such as telecommunications, automotive, and consumer electronics due to their adaptability and performance. This paper presents a comprehensive examination of the security challenges associated with FPGAs, with a focus on advanced hardware obfuscation techniques, particularly Look-Up Table (LUT) locking. It underscores the increasing necessity to protect FPGAs from sophisticated cyber threats, highlighting innovative strategies for safeguarding intellectual property and ensuring system integrity. A review of current literature identifies existing vulnerabilities in hardware obfuscation and logic locking, em-phasizing the limitations of traditional security measures against advanced attacks. The efficacy of LUT locking is demonstrated in enhancing security without compromising performance. Additionally, this study outlines prospective research directions, including the integration of machine learning for dynamic routing optimization, the development of quantum-resistant cryptographic protocols, and the establishment of secure multi-tenant FPGA environments.",Yes,"논문 초록에서 LUT 락킹의 효능을 직접 입증하고, 기존 보안 기법의 한계를 분석하는 등 독창적인 연구 내용과 실험적 기여가 포함되어 있음을 알 수 있습니다. 또한, 미래 연구 방향을 제시하며 새로운 접근법을 탐구하는 점도 연구 논문임을 뒷받침합니다."
Data mining techniques used for uterus fibroid diagnosis and prognosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526439,"The availability of massive amounts of medical data leads to the requirement for powerful data analysis tools to extract useful knowledge. Researchers have long been committed applying statistical and data processing tools to spice up data analysis on large data sets. Health problem identification is one in every of the applications where data mining tools are proving flourishing results. Uterus Fibroid diagnosis and Prognosis square measure two medical applications cause a good challenge to the researchers. The employment of machine learning and method techniques has revolutionized the entire process of Fibroid diagnosis and Prognosis. The diagnosis of fibroid present within the different parts of the female internal reproductive organ distinguishes it's eliminated or detain the female internal reproductive organ. Fibroid Prognosis predicts once Fibroid is probably going to recur in patients that have had their cancers excised. Thus, these two issues are mainly within the scope of the classification issues. This study paper summarizes numerous data mining techniques, review and technical articles on Fibroid diagnosis and prognosis. During this paper we tend to present an outline of the present research being carried out using the data mining techniques to reinforce the Fibroid diagnosis and prognosis.",No,초록에서 본 논문은 자궁 섬유종 진단과 예후에 사용된 데이터 마이닝 기법들을 요약하고 리뷰하는 개요 논문으로 보입니다. 직접적인 독창적 연구 결과나 실험 데이터보다는 기존 연구들을 정리하는 데 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Prediction of Type 2 Diabetes using Metagenomic Data and Identification of Taxonomic Biomarkers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10600811,"Nowadays, different molecular levels of -omics data on diseases are generated and analyzing these data with machine learning methods is one of the popular research topics. Among these data, the use of metagenomic data to facilitate the diagnosis, detection and treatment of diseases is increasing day by day. Type 2 diabetes (T2D) is a chronic disease characterized by insulin resistance and progressive dysfunction of pancreatic beta cells. While the number of people with diabetes is increasing by around 8% annually, the cost of treating the disease is rising by 18% per year. Therefore, the number of studies on the diagnosis, development and progression of T2D is increasing over time. The aim of this study is to achieve higher machine learning performance by using fewer metagenomic features and to achieve better classification performance by reducing computational costs. In this study, we compare the performance of three different methods using T2D-related metagenomic data. First, the MetaPhlAn tool is used to calculate the taxonomic species and their relative abundances in each sample. The SVM-RCE, RCEIFE and microBiomeGSM tools used in this study are methods that perform classification by grouping and scoring features and are known to work well on complex datasets. In this study, the best results were obtained with the RCE-IFE tool with an AUC of 0.72 with an average of 125 features information. In addition, key taxonomic species identified by these tools as associated with T2D are presented in comparison to the literature.",Yes,"본 논문은 T2D 관련 메타게놈 데이터를 활용하여 머신러닝 기법을 적용하고, 성능 비교 및 주요 생물종을 도출하는 독창적인 연구를 수행하고 있다. 이는 직접적인 연구 기여와 새로운 분석 결과를 포함하는 연구 논문에 해당한다."
Performance Analysis of Various Deep Learning Models in Lung Cancer Detection and Classification Using Medical Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10748430,"Lung cancer remains a prevalent and deadly disease, accounting for millions of deaths globally each year. Early detection and accurate diagnosis of lung cancer are pivotal in improving patient outcomes and survival rates. Traditional diagnostic methods often face limitations in sensitivity and specificity, necessitating the exploration of advanced technological solutions. This research paper explores the application of deep learning models, specifically Convolutional Neural Networks (CNNs), ResNet50, MobileNetV2 and VGG-16 architectures, in the detection and classification of lung cancer from medical imaging data. The primary objective of this study is to develop and evaluate deep learning-based models capable of identifying lung cancer patterns in radiological images, notably X-rays and CT scans. These models leverage the inherent capacity of deep learning to automatically extract intricate features and patterns from complex image data, aiding in the precise identification of cancerous regions. The paper reviews the strengths and limitations of existing approaches examining their efficacy in identifying and classifying lung nodules in achieving reliable and robust results. Furthermore, it addresses the challenges associated with utilizing deep learning in medical diagnosis, such as data scarcity, bias, and interpretability. Through meticulous analysis and experimentation, we aim to evaluate the potential of deep learning models to revolutionize lung cancer detection, paving the way for improved patient outcomes and early intervention strategies.",Yes,논문 초록에서 다양한 딥러닝 모델을 개발하고 평가하는 실험적 연구가 포함되어 있음을 명확히 언급하고 있습니다. 이는 기존 연구를 단순히 리뷰하는 것이 아니라 직접적인 연구 기여와 성능 분석을 수행한 연구 논문임을 나타냅니다.
Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223613,"The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for Io'Ivbotnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes.",Yes,"논문 초록에서 Variational Auto-encoder와 cost-sensitive learning을 활용하여 IoT 봇넷 탐지 모델을 개발하고, 다양한 딥러닝 모델의 성능을 평가한 독창적인 연구 내용을 포함하고 있음을 확인할 수 있습니다. 이는 기존 연구에 직접 기여하는 새로운 방법론과 실험 결과를 제시하는 연구 논문에 해당합니다."
Determining the optimum TMS and PS of overcurrent relays using the Firefly Algorithm for solving the relay coordination problem,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9418021,"To determine optimal solutions, Artificial intelligence (AI) was used to a number of problems by evolutionary search and metaheuristic algorithms. They are being used in numerous problems related to modern power systems, such as economic load dispatch and coordination of relays. Overcurrent relays (OCRs) coordination is a major issue in the distribution system (DS) protection. To minimize the overall operating time of the relays and set the optimum values of the relay, the coordination should be done correctly. Make sure that when a fault happens, the least damage is incurred. The application of the firefly algorithm (FA) for optimal coordination of relays is presented in this paper. Using MATLAB, the algorithm was implemented and evaluated upon various systems, two of which are discussed in this work. The obtained results by the firefly algorithm (FA) and genetic algorithm (GA) are analyzed and compared.",Yes,"본 논문은 과전류 계전기의 최적 조정 문제를 해결하기 위해 파이어플라이 알고리즘을 적용하고, MATLAB을 통해 알고리즘을 구현 및 평가한 연구 내용을 포함하고 있다. 이는 직접적인 알고리즘 개발 및 성능 비교를 통한 독창적인 연구 기여로 판단된다."
Variational Autoencoders and Wasserstein Generative Adversarial Networks for Improving the Anti-Money Laundering Process,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446893,"There has been much recent work on fraud and Anti Money Laundering (AML) detection using machine learning techniques. However, most algorithms are based on supervised techniques. Studies show that supervised techniques often have the limitation of not adapting well to new irregular fraud patterns when the dataset is highly imbalanced. Instead, unsupervised learning can have a better capability to find anomalous and irregular patterns in new transaction. Despite this, unsupervised techniques also have the disadvantage of not being able to give state-of-the-art detection results. We propose a suite of unsupervised and deep learning techniques to implement an anti-money laundering and fraud detection system to resolve this limitation. The system leverages three deep learning models: autoencoder (AE), variational autoencoder (VAE), and a generative adversarial network. We preprocess the given dataset to separate the Transaction Date attribute into its base components to capture time-related fraud patterns. Also, Wasserstein Generative Adversarial Network (WGAN) is used to generate fraud transactions, which are then mixed with the base dataset to form a more balanced mixed dataset. These two datasets are used to train the AE and VAE models. We built two versions of the AE model (single-loss and multi-loss) besides a novel method of calculating the anomaly score threshold, called Recall-First Threshold (RFT), which helps enhance the model's performance. Experimental results demonstrated that the False Positive Rate (FPR) drops down to as low as 7% in the proposed multi-loss AE model. In comparison, we achieved an accuracy of 93%, with 100% of the fraud transactions recalled successfully.",Yes,"본 논문은 기존 연구의 한계를 극복하기 위해 변분 오토인코더(VAE), Wasserstein GAN 등 여러 딥러닝 모델을 활용한 새로운 비지도 학습 기반 AML 탐지 시스템을 제안하고, 실험을 통해 성능을 검증하였다. 이는 독창적인 연구 방법론과 실험 결과를 포함한 연구 논문에 해당한다."
Analysis of Deep Learning-based MIMO Detectors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393665,"Multiple-Input Multiple-Output (MIMO) communication systems have become a fundamental technology in modern wireless networks due to their ability to enhance data rates and system capacity. However, traditional MIMO detection algorithms face significant challenges, including increasing complexity and performance degradation with growing system dimensions. Deep learning has shown great promise in various domains in recent years, leading researchers to explore its potential in addressing MIMO detection capability. This paper provides a comprehensive overview of deep-learning-based MIMO detection techniques, presenting an extensive literature review and taxonomy of approaches. We conduct a comparative analysis with conventional techniques and evaluate the detection performance of deep learning-based approaches. The paper also compares the required number of FLOPs operations to identify the potential of each detection method.",No,"논문 초록은 기존 연구들을 종합적으로 검토하고 비교 분석하는 리뷰 논문임을 나타내며, 직접적인 독창적 연구 결과나 새로운 실험적 기여가 포함되어 있지 않다. 따라서 본 논문은 연구 논문보다는 문헌 조사 및 분석에 중점을 둔 개관 논문으로 판단된다."
Use of Compression Analytics to Detect Deception,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386884,"This paper describes the development and application of compression-based machine learning (ML) combined with part of speech (PoS) analysis to detect deception. Multiple data sets, including research associated with papermills, social media, and online reviews were used to test these methods. Our research is especially concerned with achieving reliable performance with minimal parameters and data preprocessing. The core idea draws from scientific literature suggesting that subtle, yet predictable linguistic cues can be indicative of intentionally deceptive writing. Part of speech parsing was chosen as a means to represent how the person was communicating as opposed to the content of the writing. The contribution of this paper beyond existing literature is demonstrating the robustness of the application of compression-based ML across a variety of data sets and using explainability to validate that the results are consistent with the underlying psychology. These methods have relatively few parameters and are easy to apply while still providing the best performance among the methods we tested. We present the results of studies over six data sets. In addition to the classification results, we also perform an explainability analysis that shows what parts of speech contributed the most to the performance. for the compression-base ML. These results are largely consistent between data sets and match the underlying psychology research.",Yes,"논문은 압축 기반 머신러닝과 품사 분석을 결합하여 기만 탐지를 수행하는 독창적인 연구 방법을 개발하고 다양한 데이터셋에 적용한 결과를 제시하고 있습니다. 또한, 성능 평가와 설명 가능성 분석을 통해 연구 기여를 명확히 하고 있어 직접적인 연구 기여가 포함된 논문으로 판단됩니다."
A Review of Artificial Intelligence-Enhanced Security Solutions for the Internet of Things,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10859551,"The rapid development of the Internet of Things (IoT) is fundamentally transforming various industries, with many IoT systems increasingly driven by artificial intelligence (AI). Advances in AI introduce new changes and breakthroughs in IoT, while the swift progress of IoT technology also presents significant security and privacy challenges. This paper reviews the major security issues and solutions within AI-driven IoT environments, focusing on key management protocols, intrusion detection systems, physical layer authentication, and data encryption. It examines the advantages and limitations of symmetric and asymmetric key management, the roles of network-based and host-based intrusion detection systems, and how physical layer authentication enhances device security through wireless signal characteristics. Additionally, the paper discusses the role of homomorphic encryption and privacy protection technologies, suggesting that integrating these methods represents an effective strategy for addressing IoT security challenges. Future research directions include developing scalable security protocols, advanced threat detection systems, and incorporating AI to enhance the resilience and efficiency of IoT security measures.",No,"본 논문은 AI 기반 IoT 보안 솔루션에 대한 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리와 분석에 중점을 두고 있습니다."
An Overview of Artificial Intelligence Ethics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844014,"Artificial intelligence (AI) has profoundly changed and will continue to change our lives. AI is being applied in more and more fields and scenarios such as autonomous driving, medical care, media, finance, industrial robots, and internet services. The widespread application of AI and its deep integration with the economy and society have improved efficiency and produced benefits. At the same time, it will inevitably impact the existing social order and raise ethical concerns. Ethical issues, such as privacy leakage, discrimination, unemployment, and security risks, brought about by AI systems have caused great trouble to people. Therefore, AI ethics, which is a field related to the study of ethical issues in AI, has become not only an important research topic in academia, but also an important topic of common concern for individuals, organizations, countries, and society. This article will give a comprehensive overview of this field by summarizing and analyzing the ethical risks and issues raised by AI, ethical guidelines and principles issued by different organizations, approaches for addressing ethical issues in AI, and methods for evaluating the ethics of AI. Additionally, challenges in implementing ethics in AI and some future perspectives are pointed out. We hope our work will provide a systematic and comprehensive overview of AI ethics for researchers and practitioners in this field, especially the beginners of this research discipline.",No,"초록의 내용을 보면 이 논문은 AI 윤리에 관한 기존 연구와 지침, 문제점들을 종합적으로 정리하고 분석하는 개관(overview) 논문으로 보입니다. 독창적인 연구 결과나 새로운 실험, 데이터 분석 등의 직접적인 연구 기여가 포함되어 있지 않은 것으로 판단됩니다."
Deep Coupled Joint Distribution Adaptation Network: A Method for Intelligent Fault Diagnosis Between Artificial and Real Damages,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288756,"Deep learning techniques have been widely applied for intelligent fault diagnosis. However, these techniques require large amounts of labeled data from a particular machine, which is demanding for real-world applications. Alternatively, models can be developed based on artificial damages and be applied for industrial data with real damages. In that case, a major challenge arises since the distributions of those artificial and real damages are greatly different, which results in severe performance degradation of conventional deep models. In this work, a model named deep coupled joint distribution adaptation network (DCJDAN) is proposed to address the large domain discrepancy between artificial and real damages. By utilizing two untied deep convolutional networks, the proposed method allows the source- and target-stream networks to focus on learning domain-representative features, providing flexibility for explicitly modeling the domain discrepancy. To ensure a more effective knowledge transferring, a regulation term is adopted to force the untied coupled networks to stay similar since the source domain and the target domain are related. The joint distribution adaptation module is further adapted to minimize the domain discrepancy, which considers both the marginal and conditional distribution differences and provides more accurate distribution matching. The effectiveness of the proposed method is evaluated based on three bearing data sets with artificial and real damages. As reported, the proposed method achieves an average accuracy of 98.17% for all tasks, which outperforms several state-of-the-art deep domain adaptation models and improves the diagnosis performance compared with the conventional deep learning models.",Yes,"본 논문은 인공 손상과 실제 손상 간의 도메인 차이를 극복하기 위한 새로운 딥러닝 모델(DCJDAN)을 제안하고, 이를 통해 지능형 고장 진단 성능을 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법의 효과를 실제 데이터셋을 통해 검증하여 직접적인 연구 기여를 하고 있음을 보여준다."
Analyzing Transformer Insulation Paper Prognostics and Health Management: A Modeling Framework Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506492,"In the era of Industry 4.0, digital transformation has spurred the swift advancement of technologies, including intelligent predictive maintenance scheduling, prognostics and health management. The accurate prediction of remaining useful life plays a crucial role in these technologies as it extends power equipment’s safe operational duration and decreases the maintenance costs associated with unforeseen shutdowns. Also, the increased accessibility of data for monitoring system conditions has paved the way for the more immense adoption of machine learning models in prognostics and health management for power transformers. At the moment, with the ever-increasing demand for electricity, there is a corresponding increase in the degradation processes of power transformers. Transformers insulation system and more importantly, the paper insulation happens to be the principal part where the degradation is prominent. Therefore, an accurate prediction of the insulating paper condition through its degree of polymerization is required to guarantee the reliability of power transformers. In this regard, the predictions, reliability, and health monitoring of this power equipment can be actualized by modeling the degradation of transformer insulation paper through several machine learning frameworks. In this view, this review paper has been drafted not only to serve as a guide for researchers interested in the fields of transformer insulation system fault prognosis but also to offer insights into potential research directions as existing literature in modeling and evaluating transformer paper insulation is presented.",No,초록에서 해당 논문은 기존 문헌을 종합하고 연구 방향을 제시하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험을 포함한 연구 논문으로 보기 어렵습니다.
Tool Wear and Surface Quality Monitoring Using High Frequency CNC Machine Tool Current Signature,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8472037,In this paper a machine learning approach for tool wear monitoring (TWM) and surface quality detection is proposed using high frequency current samples of a CNC turning machine main terminal. Significant frequency based features related to tool wear and surface quality are selected by univariate filter methods. Supervised machine learning methods including Support Vector Machine (SVM) and Random Forest Ensemble (RFE) are used to estimate tool wear and surface quality. Best hyper-parameter combinations of the proposed models are evaluated and found by grid search methods. Experimental studies are conducted on a CNC turning machine using a test work piece and the classification and accuracy results are presented. The presented methodology makes the set up of an on-line system for tool condition monitoring and an estimation of the work piece surface quality by the use of inexpensive and easy to install measurement hardware possible.,Yes,"본 논문은 CNC 기계의 고주파 전류 신호를 이용한 공구 마모 및 표면 품질 모니터링을 위한 기계 학습 모델을 제안하고, 실험을 통해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
An Efficient Ensemble Ranking Model for IoT-based Crop-Precision-Prediction(CPP) in Agriculture,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10690002,"In recent years, agriculture has undergone substantial technical transformation using Internet of Things and Artificial Intelligence, to realize smoothly production scenarios. In this work, a new disease identification application by machine learning is proposed for IoT microcontrollers and plant diseases, which has a high degree of accuracy reaching to 91.45%. In this research we also review the most common traditional and innovative technique used to promote and sustain the agricultural growth, focusing on the Smart Farming systems based on IoT. This research aims to compare several traditional to modern methods, detailing the evolution of technologies and their benefic impact on agricultural productivity. Alongside this in-depth review, the research also engages in a comparative analysis of different approaches to smart farming, with a particular focus on building and evaluating small pilot systems. This assessment is crucial in determining the most strategic approach to developing and enhancing these systems, enabling researchers to achieve a certain degree of systemic optimization. Integrating IoT is one of the main benefits of this study, with the following information further highlighting the main application in agricultural sector: In the field of agriculture, the integration of Internet of Things such as sensors can reduce the need for manual labor and maximize efficiency in farms. For instance, IoT devices can smartly perform planting processes and monitor soil information in real time. Then, the collected information can be distributed to related devices for decision-making and resource management. This could lead to improved agricultural productivity. Furthermore, the study also describes the effectiveness of the new machine learning application as it reached a 98.45 per cent accuracy in detecting more than four types of plant diseases which could be useful in agriculture for helping to prevent the spread of disease.",Yes,"논문은 IoT 기반 농업에서 작물 정밀 예측을 위한 새로운 기계 학습 질병 식별 애플리케이션을 제안하고, 91.45%에서 98.45%에 이르는 높은 정확도를 보고하고 있다. 이는 기존 연구를 검토하는 것뿐만 아니라 직접적인 실험과 성능 평가를 포함한 독창적인 연구 기여로 판단된다."
An analysis of machine learning techniques for multimodal recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550676,"This study delves into the realm of machine learning (ML). Recognizing the significance of ML in modality recognition, such as in Facial Expression Recognition (FER), Eye Movement Detection (EMD), and Voice Identification (VI), researchers have proposed varied ML techniques. However, existing research encounters constraints such as time and financial limitations, impeding a comprehensive comparison and selection of optimal ML techniques for modality recognition. To bridge this gap, our study conducts a preliminary investigation, comparing ML techniques based on their accuracy levels for modality recognition. Utilizing a Systematic Literature Review (SLR) approach, our study focuses on publications from the past five years. Specifically concentrating on ML applications in FER, EMD, and VI, we consider accuracy measures as key criteria. This study provides a comprehensive overview of ML techniques applied for FER, EMD, and VI. Notable achievements include Pure Convolutional Neural Network (CNN)'s exceptional accuracy on specific datasets, but limitations such as inadequate representation of facial expressions in some datasets are identified. Future research suggestions include sophisticated model testing and the use of Gaussian approaches for enhanced accuracy. Also, EMD highlights a variety of ML techniques achieving accuracy rates between 66% and 100%, with challenges prompting recommendations for advanced models, transfer learning, and addressing issues like sunglasses hindering facial feature detection. For VI, techniques like CNN, EDT, MLP, and others achieve accuracy rates spanning from 15% to 98.13%, with proposed solutions including dataset scaling, hybrid model creation, and combining EfficientNet with acoustic/linguistic knowledge to categorize a greater number of new words. In conclusion, this preliminary investigation lays the groundwork for optimizing ML techniques in modality recognition. The systematic comparison of various ML techniques and their accuracies provides valuable insights for researchers and practitioners. Identified limitations and proposed future research directions contribute to the ongoing evolution of ML applications across diverse domains.",No,"본 논문은 기존 연구들을 체계적으로 문헌 검토(SLR)하여 머신러닝 기법들의 정확도를 비교하는 개관적 연구로, 직접적인 독창적 실험이나 새로운 알고리즘 개발 등의 연구 기여가 포함되어 있지 않다. 따라서 연구 논문보다는 리뷰 논문에 가까운 성격을 가진다."
Optimization Techniques in Reinforcement Learning for Healthcare: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10774698,"One paradigm for machine learning that is transforming is reinforcement learning, or RL, promising significant healthcare improvements through personalized treatment recommendations, optimized resource allocation, and enhanced predictive modeling. Despite these successes, challenges such as model interpretability and real-world integration persist. This paper reviews various optimization techniques employed in RL, focusing on Proximal Policy Optimization (PPO) due to its stability, efficiency, and applicability in healthcare. Applications of PPO include adaptive treatment strategies, personalized medication management, and optimized clinical workflows. Additionally, the paper discusses case studies demonstrating RL’s potential to reduce medication errors, enhance treatment protocols, predict adverse events, and personalize radiation therapy. A comprehensive database search initially retrieved 708 articles; after removing duplicates and screening based on titles and abstracts, 23 articles were considered, ultimately narrowing down to 15 articles following the application of exclusion criteria. The findings demonstrate RL’s significant impact on healthcare, enhancing treatment effectiveness, optimizing service selection, and improving patient care through advanced techniques like Deep RL and multi-agent systems. Future research should focus on developing sophisticated adaptive treatment models and validating RL applications through clinical trials, underscoring RL’s potential to revolutionize healthcare by making it more effective, efficient, and patient centric.",No,"본 논문은 강화학습 최적화 기법에 대한 리뷰 논문으로, 기존 연구들을 종합하여 정리하고 분석하는 내용을 담고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문에 해당하지 않습니다."
Rate Insight: A Comparative Study on Different Machine Learning and Deep Learning Approaches for Product Review Rating Prediction in Bengali Language,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055515,"In this contemporary era of digital marketing, ecommerce has emerged as one of the most preferred methods for day-to-day shopping. Ever since the COVID-19 pandemic, online shopping behavior has forever changed to less or no human-to-human interaction. As a result, it is getting more difficult for e-commerce enterprises to observe and evaluate market trends, particularly when done through consumer behavior analysis. To identify behavioral patterns and customer review-rating discrepancies, extensive analysis of product reviews is a substantial research field. Lack of benchmark corpora and language processing techniques, predicting review ratings in Bengali has become increasingly problematic. This paper thoroughly analyzes the approach to product review rating prediction for Bengali text reviews exploiting our own constructed dataset that was collected from an e-commerce website called DarazBD1. We acquired product reviews with labels known as ratings of five sentiment classes, from ""1"" to ""5"". It is noteworthy that we established a well-balanced dataset using our automated scraping system and a significant amount of time and effort is spent to maintain quality standards through the human annotation process. Exploration of multiple approaches to machine learning models such as logistic regression, random forest, multinomial naïve Bayes, and support vector machine, the best classification accuracy score of 78.63% is achieved by SVM. Subsequently, using Word2Vec, FastText, and GloVe embeddings with three deep neural network(DNN) architectures: CNN, Bi-LSTM, and a combination of CNN and Bi-LSTM, CNN+Bi-LSTM gave the highest accuracy score of 75.25% among the DNN architectures.",Yes,"본 논문은 벵골어 제품 리뷰 평점 예측을 위한 자체 구축 데이터셋을 활용하여 다양한 머신러닝 및 딥러닝 모델을 비교 분석하는 독창적인 연구를 수행하고 있다. 또한, 데이터 수집, 전처리, 모델 평가 등 직접적인 연구 기여가 포함되어 있어 연구 논문에 해당한다."
Realtime Safety Analysis System using Deep Learning for Fire Related Activities in Construction Sites,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987855,"The era of digital transformation focuses on the integration of digital and AI based technology in construction industry for sustainable economic growth and high quality of life. This paper aims to provide a real-time detection and tracking of various construction activities and provide immediate practical safety guidelines and alert for probable accidental scenarios to ensure the safety of construction site and workers by using deep learning algorithms with vision-based edge devices and smartphone. Proposed paper develops a hybrid algorithm using scene classification first, and dependent object detection and tracking second to analyze vast category of fire related activities from video and images in real-time using computationally challenging devices. To cover the ever-changing construction location, easy to move smartphone-based applications were developed with AI as an API solution. The review of the results confirms superior real-time performance in successfully identifying and providing clear safety guidelines for indoor and outdoor fire related activities such as welding work and fire safety equipment and workers safety gear such as hardhat helmet. The study validated the practicality of IoT and deep learning-based solutions for construction jobsites with indoor and outdoor locations.",Yes,"본 논문은 딥러닝 알고리즘을 활용한 실시간 화재 관련 활동 감지 및 안전 분석 시스템을 개발하고, 이를 스마트폰 기반 애플리케이션으로 구현한 독창적인 연구 내용을 포함하고 있다. 또한, 실제 건설 현장에 적용 가능한 IoT 및 딥러닝 솔루션의 실용성을 검증한 점에서 직접적인 연구 기여가 있다고 판단된다."
The Use of Arabic Language COVID-19 Tweets Analysis in IoT Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693080,"Social media platforms have become one of the most powerful tools for organizations and individuals to publish news and express thoughts or feelings. With the increasingly enormous number of internet users in Saudi Arabia, the need raised to analyze Arabic posts. Since the emergence of COVID-19 in the latest 2019, it lefts economies and businesses counting the cost while governments fight the spread of the virus with new compartmentalization measures. Keeping in view the importance of quick and timely data analysis and sharing for policy actions, Artificial intelligence (AI) has played a crucial role in facilitating the exchange of views and information between scientists and decision-makers during the Coronavirus pandemic, and they continue to do so. This work mined to these content-related tweets to see how people’s feelings and expressions are changing. The results of this analysis can be used with integration with several IoT technologies to reduce the impact of covid-19 and drive new decisions in this field. For this goal, we proposed a Machine Learning (ML) models that can classify both of the sentiment and topic of Modern Standard Arabic (MSA) tweets and achieve high accuracy results.",Yes,"논문 초록에서 COVID-19 관련 아랍어 트윗을 분석하기 위해 머신러닝 모델을 제안하고 높은 정확도를 달성했다고 명시하고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 분석 결과를 IoT 기술과 통합하여 새로운 의사결정에 활용하는 방안도 제시하고 있습니다."
Deep Q-Learning for Channel Optimization in MRCP BMI Systems: A Teleoperated Robot Implementation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10539979,"Brain–machine interface (BMI) systems utilize brain signals to control external devices. Such systems can assist brain injury survivors who are partly or entirely unable to move the affected parts of the body. Electroencephalogram (EEG), a non-invasive method for recording brain signals in different locations of the subject’s scalp, is commonly used in such applications due to its cost-effectiveness and portability. Although EEG signals provide high temporal features, the signals are not robust due to both internal and external noises. Using all available EEG channels causes the system’s performance to deteriorate. Therefore, it is important to select the most informative channels to improve the system performance while reducing computation complexity. In this work, we propose a new Deep Q-Network (DQN)-based method to identify the best EEG channel combination for motor-related cortical potential tasks. The deep learning model is used to evaluate the DQN’s selected channels and provide feedback in terms of recognition rate. To evaluate the DQN’s performance, we compared the results with Genetic Algorithm and Backward Elimination channel optimization methods. Confusion matrix and recognition rates shows that the proposed DQN-based EEG channel optimization outperforms other methods. In addition, the results demonstrated that the DQN approach significantly reduced the number of channels while improving the BMI recognition rates. Furthermore, the EEG signals of optimized channels are used to control a teleoperated robotic hand in real time. The results of this work demonstrate the effectiveness of EEG channel optimization for the Internet of Things implementation of BMI systems.",Yes,"본 논문은 Deep Q-Network 기반의 새로운 EEG 채널 최적화 방법을 제안하고, 이를 기존 방법들과 비교 평가하여 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있습니다. 또한 최적화된 채널을 이용해 원격 조작 로봇을 제어하는 실시간 구현까지 수행하여 직접적인 연구 기여를 보여줍니다."
Interpreting Training Aspects of Deep-Learned Error-Correcting Codes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10206783,"As new deep-learned error-correcting codes continue to be introduced, it is important to develop tools to interpret the designed codes and understand the training process. Prior work focusing on the deep-learned TurboAE has both interpreted the learned encoders post-hoc by mapping these onto nearby ""interpretable"" encoders, and experimentally evaluated the performance of these interpretable encoders with various decoders. Here we look at developing tools for interpreting the training process for deep-learned error-correcting codes, focusing on: 1) using the Goldreich-Levin algorithm to quickly interpret the learned encoder; 2) using Fourier coefficients as a tool for understanding the training dynamics and the loss landscape; 3) reformulating the training loss, the binary cross entropy, by relating it to encoder and decoder parameters, and the bit error rate (BER); 4) using these insights to formulate and study a new training procedure. All tools are demonstrated on TurboAE, but are applicable to other deep-learned forward error correcting codes (without feedback).",Yes,"논문 초록에서 새로운 해석 도구를 개발하고, 훈련 과정을 이해하기 위한 방법론을 제안하며, 이를 바탕으로 새로운 훈련 절차를 연구한다고 명시하고 있습니다. 이는 기존 연구를 단순히 요약하거나 리뷰하는 것이 아니라 독창적인 연구 기여를 포함한 연구 논문임을 나타냅니다."
Digital intelligent virtual assistant (DIVA) with natural speech and accent recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770703,"A Virtual Assistant (VA) system is a computer program that recognizes designated language voice commands and executes tasks on the user's behalf. This paper aims to conduct a state-of-the-art review for Digital Intelligent Virtual Assistant (DIVA) and their extended use for diverse users. Furthermore, the paper proposes a workflow scheme which takes into account accent recognition, voice registration and gives more manual control to the users. The proposed design considers the user ease and their struggles with technology; hence the proposed system is destined to serve as a mobile application. The VA encompasses techniques such as Natural Language Processing, Speech Pattern Recognition, Machine Learning and Language Data Statistical Analysis to accomplish the aforementioned user goals. Subsequently, this research will indulge in data analysis using questionnaires filled out anonymously and the results of this survey will allow us to validate our hypothesis. The analysis results are expected to demonstrate how users view their Digital Virtual Assistants and address their concerns regarding security, accent support, control and ease of use. Finally the paper summarizes the findings, methodology and results obtained through questionnaires and highlighting the potential for future works.",No,"본 논문은 Digital Intelligent Virtual Assistant에 대한 최신 동향을 리뷰하고 설계 방안을 제안하는 개념적 연구에 가깝습니다. 직접적인 실험 결과나 독창적인 연구 데이터보다는 설문조사 분석을 통해 사용자 인식을 평가하는 내용이 주를 이루고 있어, 독창적인 연구 기여가 포함된 연구 논문으로 보기 어렵습니다."
A Survey on Audio-Video Based Defect Detection Through Deep Learning in Railway Maintenance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795283,"Within Artificial Intelligence, Deep Learning (DL) represents a paradigm that has been showing unprecedented performance in image and audio processing by supporting or even replacing humans in defect and anomaly detection. The railway sector is expected to benefit from DL applications, especially in predictive maintenance applications, where smart audio and video sensors can be leveraged yet kept distinct from safety-critical functions. Such separation is crucial, as it allows for improving system dependability with no impact on its safety certification. This is further supported by the development of DL in other transportation domains, such as automotive and avionics, opening for knowledge transfer opportunities and highlighting the potential of such a paradigm in railways. In order to summarize the recent state-of-the-art while inquiring about future opportunities, this paper reviews DL approaches for the analysis of data generated by acoustic and visual sensors in railway maintenance applications that have been published until August 31st, 2021. In this paper, the current state of the research is investigated and evaluated using a structured and systematic method, in order to highlight promising approaches and successful applications, as well as to identify available datasets, current limitations, open issues, challenges, and recommendations about future research directions.",No,본 논문은 딥러닝을 활용한 철도 유지보수 분야의 음성 및 영상 기반 결함 탐지 연구 동향을 체계적으로 정리한 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과보다는 기존 연구들을 종합하고 평가하는 내용에 초점이 맞춰져 있습니다.
Optimal Transmission Expansion Planning in Deregulated Power Markets Using AI-Enhanced Cost-Based Pricing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10545953,"Transmission expansion planning plays a critical role in ensuring the reliability and efficiency of power systems in deregulated markets. In such markets, optimal expansion planning becomes complex due to uncertainties in electricity demand, generation technologies, and market dynamics. This research paper reviews and analyses some research papers that focus on the utilization of Artificial Intelligence (AI)-enhanced cost-based pricing strategies for achieving optimal transmission expansion planning in deregulated power markets. The paper categorizes and evaluates these studies based on methodologies, case studies, AI techniques employed, and their contributions to the field tested on radial 5-bus system. The synthesis of these works highlights the growing significance of AI techniques in addressing the intricate challenges posed by transmission expansion planning within deregulated power markets.",No,초록에서 본 논문은 기존 연구들을 리뷰하고 분석하는 문헌 고찰(review) 성격임을 명확히 하고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 방법론 제시보다는 기존 연구들의 분류와 평가에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Machine Learning Algorithms for Detecting Phishing Websites,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10910759,"Despite the quick growth of the digital world, phishing attempts still constitute a serious threat to the security of online banking transactions. The goal of this research project is to evaluate the usage of machine learning algorithms to detect websites that are used to spread phishing messages. This work specifically considers complex feature engineering and algorithm selection approaches. The detection method is enhanced by the application of optimal model, which is utilized to extract and evaluate text-based elements from phishing websites. It is done with optimal model. DNSPython and Python-Whois are the technologies utilized to finish gathering domain-related data. However, Scikit-learn makes the process of putting machine learning models into practice easier. The objectives that AutoML aids in achieving include efficient model selection and optimization. This has the important advantage of making it possible to automatically identify the algorithms with the highest performance levels. The objective of this research endeavour is to enhance the accuracy and robustness of phishing detection systems through the utilization of diverse methodologies and technologies. Additionally, the study makes an effort to clarify how machine learning would be able to successfully counteract the increasingly complex phishing methods. Through improving the automatic identification of phishing-related websites, our program seeks to contribute to the creation of safer online environments.",Yes,"논문 초록에서 머신러닝 알고리즘을 활용한 피싱 웹사이트 탐지 방법을 평가하고, 최적 모델 적용 및 다양한 기술을 통해 탐지 정확도와 강건성을 향상시키려는 구체적인 연구 내용이 포함되어 있다. 이는 독창적인 연구 기여를 목적으로 한 연구 논문임을 나타낸다."
Optical Impairment Compensation in Fiber Communication Systems Based on Artificial Intelligence: A Comprehensive Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509919,"The global demand for high-speed communication has increased dramatically over the past few years when data beginning to dominating of the traffic according to the Cisco Visual Networking Index (VNI). Data traffic is triple between 2014 and 2020, mainly, due to developing applications that consume bandwidth such as cloud services, HD video, high quality of real-time video transmission, virtual- augmented reality (VR-AR), online- games (video games), exchange of multimedia via smartphones and the more like. In fact, in 2020, more than a million minutes of multimedia (video) content is transiting the IP network every second according to the VNI; and the demands will exceed the capability of the current (core) internet backbone systems, in which optical communications are the main infrastructure. In this paper, the focus was on reviewing the mechanisms used for the most important and most effective techniques used to increase the capacity of optical transmission systems, namely Nonlinear Compensation (NLC) which work to reduce the nonlinear impairments that represent the main intrinsic challenges and the main capacity limitations facing the optical systems. The traditional NLC techniques were determined based on the approximate solution of the Nonlinear Schrodinger Equation (NLSE) through Digital Back Propagation (DBP), or Split- step Fourier Method (SSFM). however, their implementation demands excessive signal processing resources, and high-level accurate knowledge. A completely new approach that uses artificial intelligence (AI) algorithms to identify and solve these impairments has been studied in this paper. Traditional NLC techniques are reviewed in the first part to mitigation the nonlinearities and estimate the quality of transmission (QoT). Whereas in the second part, we review the uses of AI techniques that have been studied in applications related to monitoring performance, reduce nonlinearity, and quantify QoT. Finally, this paper presents a summary with a conclusion and outlook for development and challenges in optical fiber communication systems where AI is predictable to represent a hot major role in the near future.",No,"본 논문은 기존의 광섬유 통신 시스템에서 인공지능을 활용한 광학적 손상 보상 기법에 대한 포괄적인 리뷰(survey) 논문으로, 직접적인 실험이나 새로운 연구 결과를 제시하지 않고 기존 연구들을 정리하고 분석하는 데 중점을 두고 있습니다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Very Large Scale Distributed Training Data and Federated Learning: A Case Study in DAS,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223813,"FOTAS is an optoelectronic system used to measure acoustic signal values along the length of a fiber optic cable. By creating an acoustic profile along the length of the fiber optic cable, the system uses deep learning to detect and classify events in the physical environment. It provides high detection accuracy over long distances, it is reliable against electromagnetic interference, and it is also reliable during harsh working conditions. In order to train deep learning models known to the FOTAS system, the data must be stored in a central location. However, the migration of data is difficult because of its large size. After installing sensors in various regions and collecting data, it is very expensive to migrate this data to a server for model training. In addition to the difficulties that may emerge from data migration, problems related to data security may also occur. Federated learning enables model training on a given device by using data from server and client devices, which eliminates the need for a problematic data migration process. Client devices process data locally and share model weights with the server device. Therefore, there is no need to transfer large amounts of data. In this study, we apply the federated learning method on the FOTAS system and compare the results with the results from using classical methods. By selecting two client devices and one central device, we transferred model weights between remote devices. By choosing asynchronous device features, we observed how the federated learning system responds to asynchronous conditions. Based on the results of our study, we can conclude that federated learning can be used in commercial systems.",Yes,"본 논문은 FOTAS 시스템에 연합 학습(federated learning) 방법을 적용하고, 기존 방법과의 성능을 비교하는 실험적 연구를 수행하여 직접적인 연구 기여를 하고 있다. 또한, 비동기 조건에서의 연합 학습 반응을 관찰하는 등 독창적인 연구 내용을 포함하고 있다."
A novel fast face recognition algorithm based on multi-dimension neural network model and boundary feature extraction technique,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117727,"The manuscript addresses the subject of measured learning, which is the learning of a dissimilarity work from an arrangement of similar or dissimilar example pairs. The domain plays an important part in many machine learning applications, for example, those related to face acknowledgment or face retrieval. All the more specifically, themanuscript expands on the late Improved Measure Knowledge (IMK) strategy. Improved Measure Knowledge (IMK) has been appeared to perform exceptionally well for face retrieval tasks, however the algorithm depends on the computation of a weak measured which is extremely tedious. Themanuscript demonstrates how, by bringing scatter into the weak projectors, the meeting time can be decreased up when compared to Improved Measure Knowledge (IMK), with no performance misfortune. The manuscript also acquaints an unequivocal way with control the rank of the so-obtained measurements, allowing settling in advance the measurement of the feature space. The proposed ideas are experimentally validated on a face retrieval task with three unique signatures.",Yes,"논문 초록에서 제안된 알고리즘이 기존 방법(IMK)의 계산 시간을 단축하고 성능 손실 없이 개선된 측정 학습 기법을 제시하며, 이를 얼굴 검색 작업에 대해 실험적으로 검증했다고 명시하고 있습니다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문임을 나타냅니다."
Privacy protection strategies in mobile crowdsensing from the framework perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10718827,"In the era of Industrial 5.0, privacy protection in mobile crowdsensing (MCS) becomes even more important to achieve the goals of being human-centric, resilient, and sustainable. To address privacy challenges in MCS environments, this paper first conducts a systematic literature review to identify the research gap in MCS privacy protection and classify privacy protection strategies in terms of key phases of a MCS process. Then a six-dimensional framework for MCS privacy protection integrating user perspective, interaction perspective, and system security perspective is proposed. This comprehensive framework addresses the multifaceted privacy protection limitations identified in MCS process by examining the intersections between these dimensions. Then its effectiveness is demonstrated by case study ‘MCS for personalized healthcare’. This framework balances privacy and data utility, empowers users with transparent policies and control interfaces, and employs AI-driven adaptive security measures. Additionally, it provides a research roadmap for sustainable and resilient privacy protection in the context of Industrial 5.0, offering a comprehensive and user-centric solution to evolving privacy challenges.",Yes,"본 논문은 체계적인 문헌 검토를 통해 연구 격차를 식별하고, 모바일 크라우드센싱(MCS) 개인정보 보호를 위한 6차원 프레임워크를 제안하는 등 독창적인 연구 내용을 포함하고 있다. 또한 제안된 프레임워크의 효과를 사례 연구로 입증하여 직접적인 연구 기여를 하고 있음을 보여준다."
Multi-Task Fusion for Improving Mammography Screening Data Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618960,"Machine learning and deep learning methods have become essential for computer-assisted prediction in medicine, with a growing number of applications also in the field of mammography. Typically these algorithms are trained for a specific task, e.g., the classification of lesions or the prediction of a mammogram’s pathology status. To obtain a comprehensive view of a patient, models which were all trained for the same task(s) are subsequently ensembled or combined. In this work, we propose a pipeline approach, where we first train a set of individual, task-specific models and subsequently investigate the fusion thereof, which is in contrast to the standard model ensembling strategy. We fuse model predictions and high-level features from deep learning models with hybrid patient models to build stronger predictors on patient level. To this end, we propose a multi-branch deep learning model which efficiently fuses features across different tasks and mammograms to obtain a comprehensive patient-level prediction. We train and evaluate our full pipeline on public mammography data, i.e., DDSM and its curated version CBIS-DDSM, and report an AUC score of 0.962 for predicting the presence of any lesion and 0.791 for predicting the presence of malignant lesions on patient level. Overall, our fusion approaches improve AUC scores significantly by up to 0.04 compared to standard model ensembling. Moreover, by providing not only global patient-level predictions but also task-specific model results that are related to radiological features, our pipeline aims to closely support the reading workflow of radiologists.",Yes,"본 논문은 기존의 모델 앙상블 방식과 달리 다중 작업별 모델을 퓨전하는 새로운 딥러닝 파이프라인을 제안하고, 이를 공개된 유방촬영 데이터셋에 적용하여 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
AgTech Adoption for Irrigation Systems: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176798,"Agriculture is the backbone of our country's economy, and irrigation is the major phase of agriculture throughout the crop life cycle and crop production. Many scientists and scholars are breaking their necks to hatch legitimate solutions in contemplation of enhancing the efficiency of water used for irrigation. This could be achieved using various AgTech adoption strategies. As per an estimate, around 70 percent of the freshwater on earth is consumed specifically for irrigation purposes. To reinforce the efficiency of freshwater management and slash water wastage in the act of irrigation, further advancements are being made in this field. It takes into consideration several environmental parameters, most commonly humidity, temperature, and soil moisture, thereby impacting crop production and water management simultaneously. The present paper reviews much of this research that incorporates the use of various IoT devices, such as microcontrollers, microprocessors, actuators, etc., together as a system for optimizing water requirements and usage. But deploying sensors in the field and collecting data takes considerable time, cost, and effort. To agree upon that, the paper has proposed the use of evapotranspiration as a parameter to measure water loss from crop fields. To calculate evapotranspiration using weather parameters, it is suggested to use the FAO 56 Penman-Monteith equation (PME). PME is considered a standard empirical method for the calculation of reference evapotranspiration. This method can further be used to measure the crop water requirements based on the crop coefficient of any particular crop. Also, machine learning and deep learning (ML/DL) models can be used for the predictive analysis of evapotranspiration. Thus, empirical as well as DL/ML models can replace the IoT-based models, or they can reduce the cost of existing IoT models by working simultaneously.",No,"본 논문은 기존 연구들을 종합하여 AgTech 도입 현황과 방법들을 검토하는 리뷰 논문으로, 직접적인 실험이나 새로운 연구 결과를 제시하지 않고 있다. 따라서 독창적인 연구 내용이나 실증적 기여가 포함된 연구 논문으로 보기 어렵다."
Secure Multi-Party Computation for Machine Learning: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498135,"Machine learning is a powerful technology for extracting information from data of diverse nature and origin. As its deployment increasingly depends on data from multiple entities, ensuring privacy for these contributors becomes paramount for the integrity and fairness of machine learning endeavors. This review looks into the recent advancements in secure multi-party computation (SMPC) for machine learning, a pivotal technology championing data privacy. We evaluate these applications from various aspects, including security models, requirements, system types, and service models, aligning with the IEEE’s recommended practices for SMPC. Broadly, SMPC systems are divided into two categories: homomorphic-based systems, which facilitate computations on encrypted data, ensuring data remains confidential, and secret sharing-based systems, which disseminate data across parties in fragmented shares. Our literature analysis highlights certain gaps, such as security requisites, streamlined information exchange, incentive structures, data authenticity, and operational efficiency. Recognizing these challenges lead to envisioning a holistic SMPC protocol tailored for machine learning applications.",No,"본 논문은 최신 연구 동향과 기술을 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분석하고 평가하는 데 중점을 두고 있다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Parcel Damage Classification using Computer Vision: A Deep Learning Approach for Shipment Quality Assessment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503363,"The provision of parcel quality is critical in today’s shipping and logistic fields. Quick and accurate identification of parcel damage plays a significant role in preventing financial losses and customer satisfaction. This paper develops a novel classification for package damage which is enabled using computer vision and the deep learning (DL) approach. In this study, a combination of a convolutional neural network (CNN) with a support vector machine (SVM) was used to classify multiple parcel damages like scratches, deformation as well and cracks. This is an alternative means that gives a true picture of the reliability of ships. Firstly, this proposed work concentrates on the parcel’s assessment in the field of logistics – a demand for reliable and automatic technologies. Several key quantitative measures are used rigorously to evaluate the efficiency of the employed DL algorithm in correctly classifying/identifying and categorizing various package damages. These results show that DL has an impressive accuracy rate of 98.8% in the classification of package defects and may be practically applicable in field applications. Finally, the implications on shippers, logistics suppliers as well as those related to e-business are assessed. Issues arising from the results entail less cargo loss, improved quality control, and enhanced customer happiness among others. In particular, computer vision and DL have excellent technique for classifying parcel damage to contribute to the domain. This research will be important for the logistics and shipping business within a short period. This methodology is designed to automate the assessment of the quality of shipments that meet an important requirement along this chain. This could be a game changer whereby it would boost operational efficiency and ensure better service delivery.",Yes,"논문은 딥러닝과 컴퓨터 비전 기법을 활용하여 소포 손상 분류를 위한 새로운 방법론을 제안하고, 실험을 통해 높은 정확도를 입증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Error Vulnerabilities and Fault Recovery in Deep-Learning Frameworks for Hardware Accelerators,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203738,"Hardware accelerators such as GP-GPUs, Tensor Cores, and Deep-Learning Accelerators (DLA) are increasingly being used in real-time settings such as autonomous vehicles (AVs). In such deployments, any software errors and process failures in hardware systems can lead to critical faults in AVs. Therefore, assessing and mitigating hardware accelerator faults are critical requirements for safety-critical systems. Past work on this subject focused on simulated and injected software and hardware faults to understand and analyze the behavior of the software stack and the entire system. However, programming errors and process failures caused when using software frameworks must also be considered. In this paper, we present experiments which show that widely used deep-learning frameworks are vulnerable to programming mistakes and errors. We first focus on memory-related programming errors caused by applications using deep-learning frameworks that facilitate high-performance inferencing. We next find that a reset to recover from any fault imposes significant time penalties in reloading a pre-trained deep neural network model. To reduce these fault recovery times, we propose fault recovery mechanisms that checkpoint and resume the network based on the inference stage when an error is detected. Finally, we substantiate the practical feasibility of our approach and evaluate the improvement in recovery times11A demo video clip demonstrating our recovery algorithm has been uploaded to Youtube: https://www.youtube.com/watch?v=xwUYdJdA5oM.. We use a case-study with real-world applications on an Nvidia GeForce GTX 1070 GPU and an Nvidia Xavier embedded platform, which is commonly used by multiple automotive OEMs.",Yes,"본 논문은 딥러닝 프레임워크의 프로그래밍 오류 취약성을 실험적으로 분석하고, 오류 복구 시간을 줄이기 위한 새로운 복구 메커니즘을 제안 및 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Enhancing Sentiment Analysis Accuracy on IMDB Reviews Through Ensemble Machine Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417873,"In the rapidly evolving field of sentiment analysis, the IMDB movie review dataset has become one of the key benchmarks for evaluating the performance of machine learning models. This paper presents a comprehensive study of various machine learning models applied to the dataset, focusing on positive and negative reviews. We delve into the intricacies of advanced models such as BERT, LSTM, and GRU. The novelty of this work lies in the application of ensemble methods, specifically voting functions and stacking, to improve the accuracy of sentiment classification. We propose original techniques that leverage the strengths of individual models, mitigating their weaknesses through a collaborative approach. The ensemble methods used outperform single-model approaches, demonstrating improved accuracies in the classification of both positive and negative reviews. This research contributes to the ongoing discourse in sentiment analysis, offering fresh perspectives and techniques that enhance sentiment classification accuracy. The findings underscore the potential of ensemble methods in machine learning.",Yes,"논문 초록에서 제안된 앙상블 기법과 새로운 투표 및 스태킹 방법이 독창적인 연구 기여로 명시되어 있으며, 기존 모델들의 성능을 개선하는 새로운 접근법을 제시하고 있기 때문에 연구 논문에 해당한다."
A Comprehensive Review of Machine Learning Privacy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10823914,"The integration of machine learning (ML) into various domains has raised significant concerns regarding the privacy of individuals. As datasets grow larger and more complex, the potential for sensitive information leakage during the learning process becomes a critical issue. This paper provides a comprehensive review of the current state of machine learning privacy, examining the theoretical foundations, practical challenges, and emerging solutions. It delineates the context of ML privacy issues, focusing on the data-centric approach of ML, its wide-ranging applications, and the resultant privacy exposures. The paper thoroughly describes common privacy attack methodologies, which include membership inference, model inversion, and poisoning attacks. The review then catalogues current privacy-preserving techniques, encompassing encryption strategies, differential privacy, and homomorphic encryption. It evaluates privacy safeguarding approaches designed for distinct ML environments, spanning centralized, distributed, and federated learning architectures. Ultimately, the paper forecasts the trajectory of future ML privacy research, highlighting the imperative for innovation in technology, collaborative interdisciplinary work, and the refinement of legal and regulatory frameworks to enhance privacy in ML contexts.",No,논문 초록에서 해당 논문은 머신러닝 프라이버시에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 평가에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Recurrent Neural Network-Based Prediction of TCP Transmission States from Passive Measurements,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548064,"Long Short-Term Memory (LSTM) neural networks are a state-of-the-art techniques when it comes to sequence learning and time series prediction models. In this paper, we have used LSTM-based Recurrent Neural Networks (RNN) for building a generic prediction model for Transmission Control Protocol (TCP) connection characteristics from passive measurements. To the best of our knowledge, this is the first work that attempts to apply LSTM for demonstrating how a network operator can identify the most important system-wide TCP per-connection states of a TCP client that determine a network condition (e.g., cwnd) from passive traffic measured at an intermediate node of the network without having access to the sender. We found out that LSTM learners outperform the state-of-the-art classical machine learning prediction models. Through an extensive experimental evaluation on multiple scenarios, we demonstrate the scalability and robustness of our approach and its potential for monitoring TCP transmission states related to network congestion from passive measurements. Our results based on emulated and realistic settings suggest that Deep Learning is a promising tool for monitoring system-wide TCP states from passive measurements and we believe that the methodology presented in our paper may strengthen future research work in the computer networking community.",Yes,"본 논문은 LSTM 기반 RNN을 활용하여 TCP 전송 상태를 예측하는 새로운 모델을 제안하고, 기존 기법 대비 성능 우위를 실험적으로 입증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Detecting polarization in ratings: An automated pipeline and a preliminary quantification on several benchmark data sets,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258231,"Personalized recommender systems are becoming increasingly relevant and important in the study of polarization and bias, given their widespread use in filtering information spaces. Polarization is a social phenomenon, with serious consequences, in real-life, particularly on social media. Thus it is important to understand how machine learning algorithms, especially recommender systems, behave in polarized environments. In this paper, we study polarization within the context of the users' interactions with a space of items and how this affects recommender systems. We first formalize the concept of polarization based on item ratings and then relate it to the item reviews to investigate any potential correlation. We then propose a domain independent data science pipeline to automatically detect polarization using the ratings rather than the typical properties used to detect polarization, such as item's content or social network topology. We perform an extensive comparison of polarization measures on several benchmark data sets and show that our polarization detection framework can detect different degrees of polarization and outperforms existing measures in capturing an intuitive notion of polarization. Our work is an essential step toward quantifying and detecting polarization in ongoing ratings and in benchmark data sets, and to this end, we use our developed polarization detection pipeline to compute the polarization prevalence of several benchmark data sets. It is our hope that this work will contribute to supporting future research in the emerging topic of designing and studying the behavior of recommender systems in polarized environments.",Yes,"논문은 편향 탐지를 위한 새로운 데이터 과학 파이프라인을 제안하고, 여러 벤치마크 데이터셋에서 이를 검증하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 방법들과 비교 분석을 수행하여 연구 기여를 명확히 하고 있어 연구 논문에 해당한다."
Analyzing Public Sentiment: A Deep Dive into Twitter Discourse on the 2022 No Confidence Motion in Pakistan,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473299,"Twitter, among other widely used social media platforms, is a prominent space for the global expression of feelings and opinions. In Pakistan, political parties actively utilize Twitter to connect with the public, fostering a continuous exchange of viewpoints. This results in a substantial daily volume of millions of tweets covering diverse political topics. This study conducts sentiment analysis on tweets gathered from the X platform (formerly Twitter) concerning a significant political event in Pakistan during the year 2022. The event was related to the No Confidence Motion which resulted in the dismissal of sitting Prime Minister Imran Khan. This work prepares dataset which comprises of 10000 tweets and is publicly available on GitHub. Tweets are classified as positive, negative, or neutral by using machine learning, deep learning, and transformers. Then, the comparison is made between these approaches using benchmark evaluation measures such as recall, precision, and F1-score. Multilingual BERT, a deep learning approach, turns out to be the best classifier for the collected tweets with the highest F1-score of 77.27%.",Yes,"본 논문은 2022년 파키스탄의 No Confidence Motion과 관련된 트윗 데이터를 수집하고, 머신러닝 및 딥러닝 기법을 활용해 감성 분석을 수행하는 독창적인 연구를 포함하고 있다. 또한 다양한 분류 기법을 비교 평가하여 최적의 모델을 제시하는 등 직접적인 연구 기여가 명확하다."
Cross-Evaluation of Deep Learning-based Network Intrusion Detection Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10410724,"Network Intrusion Detection Systems are essential tools for protecting networks against attacks. Deep Learning approaches are increasingly employed in developing these systems due to their versatility and effectiveness. However, the common procedure for training and testing Deep Learning models typically leverages traffic data entirely collected from the operational network managed by a single organization, posing privacy and security concerns in sharing these data. As a result, the assessment of the performance of these models in real-world scenarios is significantly hindered. On the other hand, given the wide variety of existing attacks and the emergence of new attack types, it is crucial to evaluate the robustness of Intrusion Detection Systems when the network context varies. Indeed, it is highly desirable that the effectiveness of trained Deep Learning models is not severely impacted when ported into other networks.To this aim, in this work, we exploit various single-modal and multimodal Deep Learning approaches and leverage a cross-evaluation procedure to assess their capability to distinguish malicious from benign traffic in different network contexts. Furthermore, we investigate the impact of various informative fields extracted from traffic on the generalization capability of models. Our cross-evaluation leverages three recent public-available network attack datasets related to diverse scenarios. The results obtained suggest that the availability at training time of traffic generated by attacks conducted in the operational network is crucial for designing a robust Intrusion Detection System that keeps working with minimal Fl-score degradation, when the network context changes.",Yes,"본 논문은 다양한 딥러닝 기반 침입 탐지 시스템을 여러 네트워크 환경에서 교차 평가하는 실험적 연구를 수행하며, 모델의 일반화 능력과 성능 저하 문제를 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
On Sparsity Measures In Deep Subspace Clustering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9776918,"Traditional clustering methods groups data points according to attributes such as similarity, continuity, neighbor-hood information, etc. overlooks the structural properties of the data. Consequently, prevalent clustering approaches to below-par performance in real-world applications. Unlike traditional clustering approaches, subspace clustering methods attempt to group datapoints keeping the inherent structure and rank-related properties of the data into account. Despite the rapid growth in deep learning-based approaches, very few works have utilized deep learning for the subspace clustering task. This work introduced an auto-encoder-based deep learning architecture consisting of a self-expressive layer for the deep subspace clustering task. We use smoothed L2, L0.5 and Frobenius norms instead of the actual measures for ease of optimization task. We also explored the efficacy of sparsity measures that characterize the self-representation coefficient matrix of the self-expressive layer. The experiments conducted on standard datasets suggest that the application of efficient sparsity measures improves the performance of the subspace clustering approach and results in superior performance compared to the previous deep subspace clustering approaches.",Yes,"본 논문은 딥러닝 기반의 자기 표현 계층을 포함한 새로운 아키텍처를 제안하고, 다양한 희소성 측정 기법을 적용하여 성능을 향상시킨 실험 결과를 제시하고 있다. 이는 기존 연구와 차별화된 독창적인 연구 내용을 포함하고 있어 연구 논문에 해당한다."
Driver Drowsiness Detection Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10169941,"Driver drowsiness is a major cause of road accidents worldwide, and a drowsy driver is a serious threat to road safety [1]. To address this issue, researchers have developed various driver drowsiness detection systems that can alert the driver before a mishap occurs. In this paper, we present a comprehensive review of driver drowsiness detection systems, including their underlying techniques, advantages, and limitations. We examine the most commonly used techniques for detecting driver drowsiness, such as physiological measures, eye-tracking, and machine learning approaches. We also highlight the challenges associated with the development and implementation of such systems, including variability in individual sleep patterns, changing environmental conditions, and the trade-off between accuracy and user-friendliness. This review aims to provide a critical analysis of the state-of-the-art in driver drowsiness detection systems, and to identify research gaps and future directions for improving road safety. The document describes the functional and non-functional requirements of the system, including video input, eye detection, blink detection, EAR calculation, drowsiness detection, alert mechanism, user settings, accuracy, speed, robustness, portability, security, and user-friendliness. This study presents a driver drowsiness detection system based on machine learning algorithms and physiological signals, including electroencephalogram (EEG) and electrocardiogram (ECG).",Yes,논문 초록에서 기존 연구들을 리뷰하는 내용과 함께 EEG 및 ECG 신호를 활용한 머신러닝 기반 졸음 운전 탐지 시스템을 제안하는 직접적인 연구 기여가 포함되어 있음을 명시하고 있습니다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 판단됩니다.
Pavement Defect Detection With Deep Learning: A Comprehensive Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10288367,"Pavement defect detection is of profound significance regarding road safety, so it has been a trending research topic. In the past years, deep learning based methods have turned into a key technology, with advantages of high accuracy, strong robustness, and adaptability to complex pavement environments. This paper first reviews the methods based on image processing and 3D imaging. As for image-based processing techniques, they can be classified into three types based on how to label the collected data: fully supervised learning, unsupervised learning, and other methods. Different methods are further classified and compared with each other. Second, the pavement detection methods based on 3D data are sorted out, thereby summarizing their benefits, drawbacks, and application scenarios. Third, the study proposed the major challenges in the field of pavement defect detection, introduced validated datasets and evaluation metrics. Finally, on the basis of reviewing the literature in pavement defect detection, the promising direction is put forward.",No,"본 논문은 기존 연구들을 종합적으로 검토하고 분류하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 연구 논문보다는 연구 동향을 정리한 리뷰 논문에 해당합니다."
The Role of Technology in Enhancing Adolescent Online Safety: Current Trends and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500143,"Adolescent online safety has become a great concern in today's digital landscape, necessitating innovative approaches to address online risks, such as cyber-bullying, cyber-stalking, exposure to inappropriate content, internet addiction, and more. This systematic review paper aims to explore how technology can contribute to enhancing online safety for adolescents. By conducting an extensive survey of 21 research papers published between 2015 and 2023, the study aims to identify and assess prevailing trends, challenges, and potential applications of modern technology in addressing online risks specific to the adolescent demographic. The paper offers insights into the efficacy of existing technological strategies and underscores prospective research paths for the development of resilient online safety measures tailored for adolescents. The review covers various technologies such as robust content filters, AI -driven sentiment analysis, gamified educational programs for online safety, adaptive parental control technologies, and secure communication platforms. These strategies aim to address challenges like privacy concerns, over-restriction, rapid evolution of online platforms, and digital literacy disparities. The findings highlight the importance of comprehensive education programs, adaptive technological solutions, tailored support, and empowering adolescents in ensuring continuous enhancement of online safety.",No,"이 논문은 21개의 기존 연구를 체계적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구 동향과 기술 적용 사례를 종합하여 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 평가에 해당합니다."
Enhancing Text-Independent Speaker Verification through Advanced Deep Representation Feature Analysis with Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10739109,"Speaker Verification (SV) identifies individuals II. RELATED WORK by evaluating their distinctive vocal qualities. Text-independent speaker authentication is an important task in speech recognition and security applications. This proposes an extended fine-grained analysis of deep representation features for text-independent speaker verification using neural networks. This paper uses a Combination of CNN and RNN Neural Network Models for verification. Two Machine Learning Models Random Forest Classifier and AdaBoost Classifier was implemented on speaker verification system, but the neural network model outperforms both the models. The model achieves the accuracy of 96.4%. The approach has numerous practical uses across different security and access control systems, as well as in speech recognition technologies.",Yes,"논문은 텍스트 독립 화자 검증을 위해 CNN과 RNN을 결합한 신경망 모델을 제안하고, 기존 머신러닝 모델과 비교하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문으로 판단된다."
Predicting Student Performance Through Classroom Concentration: A Review on Machine Learning and Computer Vision Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10563245,"Student performance prediction is a complex problem in which a computer predicts student’s future performance while they engage in their studies. To enable effective educational interventions during a course, accurate early forecasts of a student’s performance is essential. The review proposed a silent method and the latest data for capturing student behaviour is classroom video monitoring. For student performance prediction dynamics to be understood and subsequently improved, it is essential to understand students’ attention spans and what kinds of behaviours may advise the absence of attention. The ability to detect student attention is one of the features of computer vision systems used to monitor classrooms. This review included the pertinent Educational Data Mining literature on identifying dropouts and students at risk from classroom data. For example, during class hours students are to concentrate, focus and give all their attention in class. A machine learning approach can be used to rate each student’s concentration with the help of computer vision algorithms since most studies use student data from colleges such as quiz marks and exam scores. The assessment outcomes revealed that a variety of Machine Learning (ML) and computer vision techniques are developed to understand and address the main problems, including guessing about to-fail learners and student dropouts. It has been validated that machine learning systems are important for detecting at-risk students and dropout frequencies which help for improving student success. To find the basics that can have an impact on how well students perform academically machine learning algorithms can be used to study a student’s daily routine such as their study habits, social interactions, and extracurricular activities. Since it applies the most relevant advanced methods for each specific task, this student concentration score using computer vision offers the ideal answer for monitoring classrooms. The integration of facial recognition algorithms and machine learning provides an assuring approach for monitoring classroom engagement and enhancing educational interventions.",No,"본 논문은 머신러닝과 컴퓨터 비전 기법을 활용한 학생 집중도 및 성과 예측에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하지 않고, 기존 문헌을 분석하는 데 중점을 두고 있어 연구 논문에 해당하지 않는다."
Machine Learning Applied to Kidney Disease Prediction: Comparison Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944799,"Machine learning has earned a remarkable position in healthcare sector because of its capability to enhance the disease prediction in healthcare sector. Artificial intelligence and Machine learning techniques are being used in healthcare sector. Nowadays, one of the world's crucial health related problem is kidney disease. It is increasing day by day because of not maintaining proper food habits, drinking less amount of water and lack of health consciousness. So we need some technique that will continuously monitor health condition effectively. Here, we have proposed an approach for real time kidney disease prediction, monitoring and application (KDPMA). Our aim is to find an optimized and efficient machine learning (ML) technique that can effectively recognize and predict the condition of chronic kidney disease. In this work, we used ten most popular machine learning technique to predict kidney disease. In this process, the data has been divided into two sections. In one section train dataset got trained and another section got evaluated by test dataset. The analysis results show that Decision Tree Classifier and Gaussian Naive Bayes achieved highest performance than the other classifiers, obtaining the accuracy score of 100% and 1 recall(Sensitivity) score. Now we are developing mobile application based on the best output results classifier technique to predict Kidney Disease from patient report.",Yes,"본 논문은 만성 신장 질환 예측을 위해 10가지 기계학습 기법을 적용하고 성능을 비교하는 독창적인 연구를 수행하였으며, 최적의 분류기를 선정하는 실험적 기여가 포함되어 있다. 또한, 연구 결과를 바탕으로 모바일 애플리케이션 개발 계획도 제시하여 직접적인 연구 기여가 명확하다."
Unveiling MIMETIC: Interpreting Deep Learning Traffic Classifiers via XAI Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527948,"The widespread use of powerful mobile devices has deeply affected the mix of traffic traversing both the Internet and enterprise networks (with bring-your-own-device policies). Traffic encryption has become extremely common, and the quick proliferation of mobile apps and their simple distribution and update have created a specifically challenging scenario for traffic classification and its uses, especially network-security related ones. The recent rise of Deep Learning (DL) has responded to this challenge, by providing a solution to the time-consuming and human-limited handcrafted feature design, and better clas-sification performance. The counterpart of the advantages is the lack of interpretability of these black-box approaches, limiting or preventing their adoption in contexts where the reliability of results, or interpretability of polices is necessary. To cope with these limitations, eXplainable Artificial Intelligence (XAI) techniques have seen recent intensive research. Along these lines, our work applies XAI-based techniques (namely, Deep SHAP) to interpret the behavior of a state-of-the-art multimodal DL traffic classifier. As opposed to common results seen in XAI, we aim at a global interpretation, rather than sample-based ones. The results quantify the importance of each modality (payload- or header-based), and of specific subsets of inputs (e.g., TLS SNI and TCP Window Size) in determining the classification outcome, down to per-class (viz. application) level. The analysis is based on a publicly-released recent dataset focused on mobile app traffic.",Yes,"본 논문은 딥러닝 기반 트래픽 분류기의 해석을 위해 XAI 기법을 적용한 독창적인 연구를 수행하고 있으며, 구체적인 기법(Deep SHAP)과 데이터셋을 활용한 실험 결과를 제시하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
A Review of Motor Brain-Computer Interfaces Using Intracranial Electroencephalography Based on Surface Electrodes and Depth Electrodes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579835,"Brain-computer interfaces (BCIs) provide a communication interface between the brain and external devices and have the potential to restore communication and control in patients with neurological injury or disease. For the invasive BCIs, most studies recruited participants from hospitals requiring invasive device implantation. Three widely used clinical invasive devices that have the potential for BCIs applications include surface electrodes used in electrocorticography (ECoG) and depth electrodes used in Stereo-electroencephalography (SEEG) and deep brain stimulation (DBS). This review focused on BCIs research using surface (ECoG) and depth electrodes (including SEEG, and DBS electrodes) for movement decoding on human subjects. Unlike previous reviews, the findings presented here are from the perspective of the decoding target or task. In detail, five tasks will be considered, consisting of the kinematic decoding, kinetic decoding,identification of body parts, dexterous hand decoding, and motion intention decoding. The typical studies are surveyed and analyzed. The reviewed literature demonstrated a distributed motor-related network that spanned multiple brain regions. Comparison between surface and depth studies demonstrated that richer information can be obtained using surface electrodes. With regard to the decoding algorithms, deep learning exhibited superior performance using raw signals than traditional machine learning algorithms. Despite the promising achievement made by the open-loop BCIs, closed-loop BCIs with sensory feedback are still in their early stage, and the chronic implantation of both ECoG surface and depth electrodes has not been thoroughly evaluated.",No,"본 논문은 기존 연구들을 종합하여 분석한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 기존 문헌을 정리하고 비교하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 연구 동향과 현황을 요약하는 성격이 강합니다."
Deep Learning-Based Intrusion Detection System for Detecting IoT Botnet Attacks: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829842,"The proliferation of Internet of Things (IoT) devices has brought about an increased threat of botnet attacks, necessitating robust security measures. In response to this evolving landscape, deep learning (DL)-based intrusion detection systems (IDS) have emerged as a promising approach for detecting and mitigating botnet activities in IoT environments. Therefore, this paper thoroughly reviews existing literature on botnet detection in the IoT using DL-based IDS. It consolidates and analyzes a wide range of research papers, highlighting key findings, methodologies, advancements, shortcomings, and challenges in the field. Additionally, we performed a qualitative comparison with existing surveys using author-defined metrics to underscore the uniqueness of this survey. We also discuss challenges, limitations, and future research directions, emphasizing the distinctive contributions of our review. Ultimately, this survey serves as a guideline for future researchers, contributing to the advancement of botnet detection methods in IoT environments and enhancing security against botnet threats.",No,"본 논문은 기존 연구들을 종합하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 문헌 조사 및 정리 목적의 리뷰 논문에 해당합니다."
Transfer Adaptation Learning for Target Recognition in SAR Images: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612244,"Synthetic aperture radar (SAR) target recognition is a fundamental task in SAR image interpretation, which has made tremendous progress with the advancement of artificial intelligence technology. However, SAR imaging is sensitive to the operating conditions of platforms, resulting in large distribution discrepancy for the data collected on different platforms. Moreover, SAR target images are difficult to annotate due to the blurry textures, resulting in insufficient labeled data to train a model. Therefore, subject to the data distribution discrepancy and insufficient labeled data, SAR target recognition becomes a highly challenging task. Transfer adaptive learning (TAL) is a learning paradigm aimed at completing target tasks by transferring knowledge from relevant source domains, which is a promising technique for solving the aforementioned problems in SAR target recognition. However, there is currently no comprehensive survey about the application of transfer adaptation learning in SAR target recognition. To this end, we comprehensively summarized the development of transfer adaptive learning in SAR target recognition, and provided systematic guidance for future research. In this article, we first summarized the electromagnetic features and visual features of SAR images used for target recognition, which can be potentially used for knowledge transfer. Then, we systematically reviewed the related literature according to the homogeneity of the transfer domains, the modality of the data in the source domain, and the category of the TAL methods. The available datasets that can be used to validate the TAL methods for SAR target recognition were also summarized for the researcher's convenience. We also conducted comparative experiments on these data to demonstrate the performance of TAL methods. Finally, we analyzed the main challenges of the current methods and pointed out several directions worth studying in the future.",No,"본 논문은 SAR 표적 인식에서 전이 적응 학습 기법에 대한 종합적인 서베이 논문으로, 기존 연구들을 정리하고 비교 실험을 수행하였으나, 독창적인 연구 결과나 새로운 방법론을 제안하는 직접적인 연구 기여는 포함하지 않았다. 따라서 연구 논문보다는 리뷰 논문에 해당한다."
Artificial Intelligence Approach in the Detection of Lung Diseases Developing Post-COVID-19 with Lung Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10416075,"Although it is known that the COVID-19 process is over, the subsequent damage caused by COVID-19 on the body is undeniable. The SARS-CoV-2 virus has been shown to be responsible for causing acute respiratory distress in a large number of COVID-19 cases. When the literature is reviewed, Thorax Computed Tomography is recommended to evaluate permanent lung damage in individuals recovering from COVID-19. When tomography images are examined, the respiratory systems of patients who have had COVID-19 are significantly affected by the virus. In the study, thorax CT images of patients who had COVID-19 and then came back to the hospital and were diagnosed with COPD and Bronchiectasis were examined with artificial intelligence methods. The study consists of two stages. First, COVID-19, COPD, and Bronchiectasis datasets were trained with pre-trained deep-learning models. Then, only the COVID-19 dataset was trained with the GAN algorithm, and the model weights were recorded. Tests were performed on the COPD and Bronchiectasis dataset with the recorded model weight. The performance metric ratios of the training results obtained with the COVID-19 dataset and test trained individually and the combination of three classes was compared in the test environment. As a result of the proposed study, the highest performance rate for the classification method made in the first step was found to be 99% in the pre-trained DenseNet201 model. In the second step of the study, model weights were recorded on the COVID-19 dataset trained with the GAN algorithm and tested on COPD and Bronchiectasis datasets. Lower loss rates are observed in the GAN algorithm compared to the models in the first stage of the study.",Yes,"본 논문은 COVID-19 이후 발생하는 폐질환을 인공지능 기법으로 진단하는 독창적인 연구를 수행하였으며, 딥러닝 모델과 GAN 알고리즘을 활용한 실험적 결과를 제시하고 있다. 이는 직접적인 연구 기여와 새로운 실험적 분석을 포함하는 연구 논문에 해당한다."
A Review on Deep Learning Techniques for the Diagnosis of Novel Coronavirus (COVID-19),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351923,"Novel coronavirus (COVID-19) outbreak, has raised a calamitous situation all over the world and has become one of the most acute and severe ailments in the past hundred years. The prevalence rate of COVID-19 is rapidly rising every day throughout the globe. Although no vaccines for this pandemic have been discovered yet, deep learning techniques proved themselves to be a powerful tool in the arsenal used by clinicians for the automatic diagnosis of COVID-19. This paper aims to overview the recently developed systems based on deep learning techniques using different medical imaging modalities like Computer Tomography (CT) and X-ray. This review specifically discusses the systems developed for COVID-19 diagnosis using deep learning techniques and provides insights on well-known data sets used to train these networks. It also highlights the data partitioning techniques and various performance measures developed by researchers in this field. A taxonomy is drawn to categorize the recent works for proper insight. Finally, we conclude by addressing the challenges associated with the use of deep learning methods for COVID-19 detection and probable future trends in this research area. The aim of this paper is to facilitate experts (medical or otherwise) and technicians in understanding the ways deep learning techniques are used in this regard and how they can be potentially further utilized to combat the outbreak of COVID-19.",No,초록에서 해당 논문은 COVID-19 진단을 위한 딥러닝 기법들을 종합적으로 정리하고 분류하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구들을 요약하고 분석하는 데 중점을 둔 논문입니다.
Behavior Identification in ATM using Different Methods: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917965,"The rapid evolution of Automated Teller Machines (ATMs) has intensified the need for effective behavior identification methods to combat fraudulent activities and enhance security. This review provides a comprehensive analysis of current methodologies used in ATM behavior identification, including machine learning, deep learning, and video surveillance techniques. Despite the advancements, there remains a critical research gap in addressing the integration and effectiveness of these methods in diverse real-world scenarios. This paper evaluates and compares the efficacy of approaches such as YOLOv3-based detection, Convolutional LSTM networks, and heuristic algorithms. It identifies existing gaps, such as the lack of comprehensive datasets and the need for real-time detection capabilities. By synthesizing recent advancements and identifying these gaps, the paper aims to offer valuable insights into improving behavior identification strategies and to propose future research directions for enhancing ATM security systems.",No,"본 논문은 ATM 행동 식별 방법에 대한 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 방법들의 비교와 연구 동향 분석에 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 현황 분석 및 향후 연구 방향 제시에 초점이 맞춰져 있습니다."
Decoding Gender on Social Networks: An In-depth Analysis of Language in Online Discussions Using Natural Language Processing and Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386655,"In today’s digital era, the internet is an indispensable platform for self-expression, facilitating communication, idea sharing, and community formation. Language, a pivotal tool in these online interactive spaces, is vital in reflecting personal identities, notably gender identification. This paper investigates gender identification on online discussion platforms, recognizing the crucial role of language in reflecting personal identities. The study employs Natural Language Processing techniques and machine learning algorithms to analyze data from a public discussion website. Beginning with a comprehensive literature review, the research explores the nexus between gender and language in online and offline contexts. The methodology involves data gathering, extensive preprocessing, and in-depth exploratory analysis, employing statistical methods and graphical representations. The study then rigorously evaluates their accuracy and effectiveness by applying diverse algorithms and models for gender-based text categorization. Results indicate the superior performance of transformer models, particularly distilBERT, in categorizing gender accurately. Additionally, the research underscores the challenges of gender-neutral analysis, emphasizing the need for inclusive methodologies in non-binary gender classification. The study contributes to the broader field of gender studies, providing valuable insights for future research and discussions on the interplay of gender and language in online spaces.",Yes,"논문은 자연어 처리와 머신러닝 기법을 활용해 온라인 토론 데이터에서 성별을 식별하는 독창적인 연구를 수행하고 있으며, 다양한 알고리즘의 성능 평가와 새로운 인사이트 제공을 포함하고 있어 직접 기여하는 연구 논문에 해당한다."
2D Wavelet-Scalogram Deep-Learning for Seizures Pattern Identification in the Post-Hypoxic-Ischemic EEG of Preterm Fetal Sheep,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340425,"Neonatal seizures after an hypoxic-ischemic (HI) event in preterm newborns can contribute to neural injury and cause impaired brain development. Preterm neonatal seizures are often not detected or their occurrence underestimated. Therefore, there is a need to improve knowledge about preterm seizures that can help establish diagnostic tools for accurate identification of seizures and for determining morphological differences. We have previously shown the superior utility of deep-learning algorithms for the accurate identification and quantification of post-HI microscale epileptiform transients (e.g., gamma spikes and sharp waves) in preterm fetal sheep models; before the irreversible secondary phase of cerebral energy failure starts by the bursts of high-amplitude stereotypic evolving seizures (HAS) in the signal. We have previously developed successful deep-learning algorithms that accurately identify and quantify the micro-scale transients, during the latent phase. Building up on our deep-learning strategies, this work introduces a real-time deep-learning-based pattern fusion approach to identify HAS in the 256Hz sampled post-HI data from our preterm fetuses. Here, for the first time, we propose a 17-layer deep convolutional neural network (CNN) classifier fed with 2D wavelet-scalogram (WS) images of the EEG patterns for accurate seizure identification. The WS-CNN classifier was cross-validated over 1812 manually annotated EEG segments during ~6 to 48 hours post-HI recordings. The classifier accurately recognized HAS patterns with 97.19% overall accuracy (AUC = 0.96).Clinical relevance—The promising results from this preliminary work indicate the ability of the proposed WS-CNN pattern classifier to identify HI-related seizures in the neonatal preterm brain using 256Hz EEG; the frequency commonly used clinically for data collection.",Yes,본 논문은 2D wavelet-scalogram 기반의 17층 딥러닝 CNN 분류기를 개발하여 미리 수집된 EEG 데이터를 이용해 신생아 뇌전증 패턴을 정확히 식별하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 바탕으로 새로운 딥러닝 모델을 제안하고 검증한 직접적인 연구 기여로 판단된다.
Modeling and Assessment of Machine Learning Models for Solar Radiation Forecast,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10328258,"Solar radiation significantly impacts the energy received from the sun in a specific area, crucial for planning non-conventional renewable energy power plants like solar photovoltaic or solar thermal systems. Variability in this resource, influenced by climate and geography, poses challenges for solar integration planning. Numerical models estimate solar resource but lack realtime and future responses. Machine Learning (ML) offers heuristic predictive tools, using extensive datasets and algorithms for quantifying and forecasting solar radiation. A proposed ML model incorporates geolocation and links primary resource with climate data from diverse Colombian cities. It consists of three stages: clustering, estimation, and response, utilizing ML predictors selected by criteria and literature review. Model response is validated using statistical methods, providing accurate solar resource predictions.",Yes,"논문 초록에서 제안된 머신러닝 모델의 설계, 적용, 그리고 통계적 검증 과정을 명확히 기술하고 있어 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 다양한 도시의 기후 데이터를 활용한 예측 모델 개발은 새로운 연구 결과로 판단됩니다."
Diagnosis of Sepsis Based on Potential Immune-Related Biomarker and Machine Learning Method,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013813,"Sepsis is known as an extreme response by the human body to an infection which often leads to life-threading complications in patients. In this work, we propose a novel method to identify potential biomarkers and effective algorithms for diagnosing sepsis based on blood transcriptome data. A panel including fifteen immune-related genes has been identified as an effective gene combination for the diagnosis of sepsis. The proposed immune-related gene biomarkers have been selected carefully through a sequential selection procedure, which combines immune-related gene selection, differential gene expression analysis, and gene importance calculated from machine learning algorithms to address the most informative gene expression markers. The cross-validation procedure in combination with different machine-learning algorithms is applied for estimating diagnosis performance related to gene combinations and machine-learning algorithms. The selected gene combinations corresponding to each machine learning model are then tested separately using various machine learning methods. The performance results on the testing data, which are an AUC score of 99.54%, a sensitivity of 98.09%, a specificity of 89.65%, and an accuracy of 96.26%, show that the identified gene biomarkers and proposed algorithm are effective and have the potential for practical applications in clinical diagnosis.",Yes,"본 논문은 혈액 전사체 데이터를 기반으로 새로운 면역 관련 유전자 바이오마커를 식별하고, 이를 활용한 기계 학습 알고리즘을 제안하여 패혈증 진단 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Exploring the Application of Large Language Models in Detecting and Protecting Personally Identifiable Information in Archival Data: A Comprehensive Study*,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386949,"This comprehensive study investigates the application of Large Language Models (LLMs) for detecting and protecting Personally Identifiable Information (PII) in archival data, a pressing concern for archives under the mandate to increase public access while safeguarding personal privacy. The paper juxtaposes traditional supervised learning methods against LLMs’ unsupervised capabilities in PII detection, unveiling LLMs as viable alternatives capable of achieving satisfactory performance levels without the need for extensive training datasets. Through empirical analysis, the study validates the feasibility of LLMs in identifying sensitive information within large volumes of archival material. The findings highlight LLMs’ significant interpretability, providing understandable rationale behind PII identification—a feature that not only enhances trust in AI applications but also aids archival staff in the review process. This research contributes novel insights into the intersection of AI and archival science, presenting LLMs as powerful tools for addressing the twin challenges of data accessibility and privacy.",Yes,"본 논문은 LLM을 활용한 PII 탐지 및 보호에 대한 실증적 분석과 기존 방법과의 비교를 통해 새로운 연구 결과를 제시하고 있어, 독창적인 연구 내용을 포함한 연구 논문에 해당한다. 또한 AI와 기록학의 교차점에서 새로운 통찰을 제공하는 점도 연구 논문임을 뒷받침한다."
"Hybrid POF-VLC Systems: Recent Advances, Challenges, Opportunities, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10856321,"Hybrid Polymer Optical Fiber and Visible Light Communication (POF-VLC) systems are emerging as a promising solution for high-speed, interference-free connectivity, especially in environments where traditional RF communication is constrained. This paper investigates key nonlinear impairments in POF-VLC systems, such as chromatic dispersion (CD), self-phase modulation (SPM), cross-phase modulation (XPM), four-wave mixing (FWM), and stimulated scattering, which severely degrade signal quality and limit transmission range. We review advanced modulation techniques like Orthogonal Frequency Division Multiplexing (OFDM) and Discrete Multitone Modulation (DMT), alongside traditional methods like Non-Return-to-Zero (NRZ) and On-Off Keying (OOK), evaluating their effectiveness in overcoming these challenges. Furthermore, the application of machine learning, particularly neural network-based equalizers like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), is highlighted for their potential to enhance signal quality and system performance. This review emphasizes the transformative role these advanced strategies can play in optimizing hybrid POF-VLC systems, paving the way for their integration into high-demand communication environments. Moreover, this paper presents several promising research directions, such as optimizing training algorithms, exploring deeper neural network architectures, and integrating POF-VLC systems with emerging technologies like beyond 5G, improving energy efficiency, and addressing scalability and complexity in real-time adaptive POF-VLC systems.",No,초록 내용은 기존 연구들을 종합적으로 검토하고 향후 연구 방향을 제시하는 리뷰 논문으로 보입니다. 직접적인 실험 결과나 새로운 방법론의 제안 없이 기존 연구를 요약하고 분석하는 데 중점을 두고 있습니다.
Identifying multiscale hidden states to decode behavior,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513242,"A key element needed in a brain-machine interface (BMI) decoder is the encoding model, which relates the neural activity to intended movement. The vast majority of work have used a representational encoding model, which assumes movement parameters are directly encoded in neural activity. Recent work have in turn suggested the existence of neural dynamics that represent behavior. This recent evidence motivates developing dynamical encoding models with hidden states that encode movement. Regardless of their type, encoding models have vastly characterized a single scale of activity, e.g., either spikes or local field potentials (LFP). In our recent work we developed a multiscale representational encoding model to simultaneously characterize and decode discrete spikes and continuous field activity. However, learning a multiscale dynamical model from simultaneous spike-field recordings in the presence of hidden states is challenging. Here we present an unsupervised learning algorithm for estimating a multiscale state-space model with hidden states and validate it using spike-LFP activity during a reaching movement. We use the learned multiscale statespace model and a corresponding decoder to identify hidden states from spike-LFP activity. We then decode the movement trajectories using these hidden states. We find that the identified states can accurately decode the trajectories. Moreover, we demonstrate that adding LFP to spikes improves the decoding accuracy, suggesting that our unsupervised learning algorithm incorporates information across scales. This learning algorithm could serve as a new tool to study encoding across scales and to enhance future BMI systems.",Yes,"논문은 다중 스케일 신경 활동 데이터를 이용한 숨겨진 상태를 추정하는 새로운 비지도 학습 알고리즘을 제안하고, 이를 통해 운동 궤적을 정확히 디코딩하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
A survey of automatic target recognition technology based on multi-source data fusion,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855956,"With the rapid advancement of big data, the Internet of Things, and artificial intelligence, informatization, and intelligent warfare have become central themes in modern conflict. Warfare has evolved from traditional methods to unmanned and intelligent operations, such as swarming and wolf-pack tactics, presenting significant challenges for the development of new-era combat systems and intelligent unmanned platforms. Military ground unmanned platforms leverage their scalability to incorporate various measuring instruments and sensors, thereby broadening their application to meet diverse combat requirements. Currently, the most prevalent and significant application of these platforms is in the reconnaissance and identification of targets, systematic threat evaluation, casualty avoidance, and enhancement of combat personnel's battlefield survival rates. Consequently, both domestic and international scholars have increasingly focused on developing various Automatic Target Recognition (ATR) technologies, with extensive research dedicated to ATR based on multi-source data fusion. To elucidate the current state of research, this paper provides a detailed review of various multi-source data fusion algorithms and automatic target recognition algorithms, offering insights and future research directions.",No,"본 논문은 다양한 다중 소스 데이터 융합 알고리즘과 자동 표적 인식 알고리즘에 대한 상세한 리뷰를 제공하는 조사 연구(survey)로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구를 종합하고 분석하는 리뷰 논문에 해당합니다."
"Performance, Efficiency, and Target Setting for Bank Branches: Time Series With Automated Machine Learning",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004502,"Setting targets for the bank branches and distribution of annual targets to the branches and portfolio managers, quarterly, is a crucial process for decision making and strategic planning in the banking industry. Performance of the bank branches and portfolio managers are also evaluated by the quarterly divided targets to the branches and portfolio managers. In this study, the focus is on performance prediction by using state-of-art machine learning algorithms. A novel automated machine learning approach with combined algorithm selection and hyperparameter optimization is also applied for each of the branches since all the branches might have different customer segmentation and behavior. Moreover, the postconditions are executed to finalize the target calculation and distribution over the performance predictions. The study shows the success of the methodology with a successful application of 98% accuracy in the prediction and majority of branch target calculations. An end-to-end solution found to the seasonality and periodicity problem, which is the biggest problem faced by branches while achieving their goals. Also, the novel approach increases the success of branch targets by 10% in overall. The most significant innovation this study provides to the literature and practitioners is that, unlike classical studies, it solves the seasonality and periodicity problem through multiple time series modeling. The target setting procedure was employed by the largest financial institution in Turkey, Ziraat Bank, to evaluate the operating performance of its branches. The empirical study demonstrates the applicability of the proposed model in the banking sector. The outputs of the study are implemented in real life for all retail branches of Ziraat Bank. In addition, the study awarded the most innovative use of AI/ML, the most innovative project for in-house implementation related to the innovative aspect of the work, by the Global FinTech Innovation Awards 2022.",Yes,"본 논문은 은행 지점의 성과 예측을 위해 최신 머신러닝 알고리즘과 자동화된 기법을 적용한 독창적인 연구를 수행하였으며, 계절성과 주기성 문제를 다중 시계열 모델링으로 해결하는 새로운 접근법을 제시하고 있다. 또한 실제 금융기관에 적용되어 실증적 결과를 도출한 점에서 직접 기여하는 연구 논문으로 판단된다."
DrunaliaCap: Image Captioning for Drug-Related Paraphernalia With Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184936,"Image captioning is a process of generating textual descriptions of images. In recent years, research on publicly available large-scale datasets and deep learning-based algorithms has promoted the development of this field. However, little research has been conducted on captioning images of drug-related paraphernalia that, despite being an important topic for both drug prevention and police enforcement, is not covered by existing image captioning studies. In this paper, we propose DrunaliaCap—a deep learning-based system for autogenerating both “factual” (what is in the image) and “functional” (the usage of each paraphernalia during drug-taking) descriptions of images of drug-related paraphernalia. We constructed a new dataset containing 20 categories of drug-related items and trained deep learning-based models for the proposed system. We further proposed a method to evaluate and optimize the generation of captions to prevent them from missing important knowledge. Experiments were conducted to validate the performance of the newly proposed dataset and method. We analyzed the experimental results and discussed the significance, limitations, and potential applications of our work.",Yes,"본 논문은 약물 관련 도구 이미지에 대한 새로운 데이터셋을 구축하고, 딥러닝 기반의 이미지 캡셔닝 시스템을 제안하여 직접적인 연구 기여를 하고 있다. 또한, 캡션 생성 최적화 방법을 제안하고 실험을 통해 성능을 검증하는 등 독창적인 연구 내용이 포함되어 있다."
A Review on AI-powered Advancements in Climate Finance and its Impact,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498959,"As humanity works progressively to address climate change, the combination of artificial intelligence (AI) and climate financing offers a hopeful path for significant progress. The objective of this project is to examine the strategic implementation of AI technology to improve many areas of climate financing. This research evaluates the capacity of AI algorithms to streamline and automate operations in climate financing organizations, resulting in improved operational efficiency and decreased administrative costs. Furthermore, we have examined the possible uses of AI-driven algorithmic trading in the realm of environmentally-friendly finance. We have assessed its influence on market behavior, the availability of funds, and the distribution of resources towards sustainable investments. In addition, we examine the ethical and regulatory factors related to the implementation of AI in this particular setting. The study examines the influence of algorithmic trading on green finance, with a focus on potential opportunities for collaboration between the public and private sectors. This research offers useful insights into utilizing AI technology to tackle climate-related concerns and promote sustainability in financial decision-making, with a specific emphasis on practical applications.",No,논문 초록에서 해당 연구는 AI와 기후 금융의 발전 현황을 검토하고 평가하는 리뷰 성격임을 명확히 하고 있습니다. 독창적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구와 기술을 종합적으로 분석하는 내용으로 보입니다.
A Review Study of an Intelligent Strategy Towards Higher Education Examination Management Structure Based on Fog Computing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10215412,"Universities currently employ manual and partial digital methods for managing examinations, which involve extensive paperwork and consume significant time. This approach raises significant issues, such as time-consuming processes, errors, poor exam and result quality, submission delays, plagiarism, inaccurate grading, and a lack of security. Lecturers face challenges when creating exam questions for every subject every semester, often reusing previous questions without considering their quality, failing educational quality standards. The marking of exam scripts is often rushed to meet deadlines, leading to substandard grading, and the entire process is time-consuming and requires unnecessary effort. Un considerable amount of paper is utilized, and the storage of exam scripts occupies substantial physical space, posing the risk of damage or loss. A novel approach was proposed via Intelligent Green Examination Structure based on fog computing to address these issues. This innovative structure aims to transform traditional examination procedures into an automated, intelligent, and secure system that reduces human involvement and ensures high-quality outcomes. This research focuses on evaluating the effectiveness of an Artificial Intelligence (AI)-powered digital exam management structure based on Fog Computing in enhancing the efficiency and security of exam administration. The structure of this paper begins with a problem discussion, hypothesis, objectives, significance of the research, literature review, and research methodology.",No,"논문 초록에서 제안된 내용은 기존 문제를 해결하기 위한 새로운 접근법을 제안하고 평가하는 리뷰 연구로 보이며, 직접적인 실험 결과나 독창적인 연구 데이터가 포함되어 있지 않습니다. 또한, 초록에 연구 방법론과 문헌 검토가 포함되어 있으나, 구체적인 연구 수행 및 결과에 대한 언급이 부족합니다."
Transformative Impact of Artificial Intelligence and Cybersecurity on Bitcoin's Trajectory,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10563543,"This study delves into the intertwined roles of Artificial Intelligence (AI) and cybersecurity in the evolution of Bitcoin, analyzing their pivotal influence on the cryptocurrency's trajectory. By employing a mixed-methods approach, including a literature review, quantitative data analysis, and expert interviews, the paper explores how AI enhances transactional efficiency and cybersecurity measures fortify the integrity of Bitcoin's blockchain. The research investigates the dual impact of AI-driven market analysis and automated trading systems, alongside the development of robust cybersecurity protocols essential for fraud detection and network defense. The findings illustrate the dynamic interplay between technological innovation and financial security, revealing a transformative effect on Bitcoin's market behavior and regulatory landscape.",Yes,"본 논문은 AI와 사이버보안이 비트코인의 발전에 미치는 영향을 분석하기 위해 문헌 검토, 정량적 데이터 분석, 전문가 인터뷰 등 혼합 방법론을 사용하여 직접적인 연구를 수행하고 있다. 이는 독창적인 연구 내용과 실증적 분석을 포함한 연구 논문에 해당한다."
Statistical Features Versus Deep Learning Representation for Suspicious Human Activity Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9942440,"For many years, researchers have been interested in human activity recognition for many reasons, including preventing crimes before they occur, preventing acts of sabotage, and securing people and facilities. Human activity recognition using deep learning is the focus of many researchers. Techniques for human activity recognition that rely on sensors worn by a person with his limbs and torso are impractical and the only useful way to recognize the human activity especially in public places is through video clips of surveillance cameras. In this paper i compared the performance of CNN with different models such as Inception, ResNet, Inception-ResNet, MobileNet V2, NASNet and PNASNet and the performance of handcrafted features such as statistical features (shape moments like mean, Skew, Kurtosis, etc.) in terms of their human activity recognition accuracy and also compared the results with the state-of-the-art methods. In my research i used suspicious activities included in the HMDB data set (falling to the floor, punching, kicking and shooting a gun) to evaluate the statistical features and also to evaluate the different modern CNN architectures. The experimental results in the case of using statistical features confirmed the superiority of the Support vector machine (SVM) than other classifiers. My experimental results indicated that the CNN with NASNet architecture achieves the best performance of the six CNN architectures but when comparing the performance with the statistical features method, I found the superiority of statistical features with a support vector machine classifier. This paper contributes to studying the effectiveness of using modern CNN architectures in recognizing suspicious human activity and found that these techniques depend on the quality of images, whether the activity is individual or group. This study enables the researcher to evaluate and compare the different modern CNN architectures in suspicious human activity recognition and compare them also with models that use the hand-crafted features represented in the statistical features on an objective and fair basis The remainder of this paper is organized as follows: Section 1 provides the related work. Section 2 describes an action recognition system using Hand-crafted features. Section 3 presents the experimental results and Section 4 concludes the paper.",Yes,"본 논문은 다양한 CNN 아키텍처와 통계적 특징 기반 방법을 비교 평가하는 실험 연구를 수행하여 인간 활동 인식 성능을 분석하고, 새로운 비교 결과를 제시하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Thermal cut off & SoC estimation of Battery for EV Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837700,"State-of-Charge (SOC) estimation plays a pivotal role in optimizing the performance, lifespan, and safety of rechargeable batteries in various applications. This paper presents a comprehensive review of SOC estimation techniques employed in Battery Management Systems (BMS). The study encompasses an in-depth analysis of traditional methods, such as voltage and current integration, as well as advanced approaches like Kalman filtering, neural networks, and machine learning algorithms. Furthermore, it discusses the challenges faced in accurate SOC estimation, including temperature variations, aging effects, and dynamic loads. The review critically evaluates the advantages and limitations of existing techniques and provides insights into potential future research directions aimed at enhancing the precision and reliability of SOC estimation methods.The thermal management of batteries is a critical aspect of ensuring their safe operation and longevity. This paper investigates various thermal cutoff mechanisms implemented in battery systems to prevent overheating, thermal runaway, and potential catastrophic failures. The study explores passive methods, such as heat sinks and phase change materials, as well as active techniques, including cooling fans and liquid cooling systems. Additionally, it delves into the role of advanced thermal cutoff devices like Positive Temperature Coefficient (PTC) thermistors and thermal fuses in safeguarding batteries from excessive temperatures. The paper also discusses the impact of operating conditions, such as charge-discharge rates and ambient temperature, on the effectiveness of thermal cutoff mechanisms.",No,"초록에서 제시된 내용은 SOC 추정 기법과 열 차단 메커니즘에 대한 기존 연구들의 종합적인 리뷰에 초점이 맞춰져 있으며, 새로운 실험 결과나 독창적인 연구 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 직접적인 연구 결과를 제시하는 연구 논문이라기보다는 리뷰 논문으로 판단됩니다."
Sleep Analysis for Fatigue of Crewmembers with Deep Multi-Task Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9986945,"This paper targets to investigate the potential performance of crewmembers given the sleep quality surveillance. The fatigue of crewmembers can significantly affect flight safety and service quality. The quantification and prediction of fatigue is highly desired in practice, and lead to better control of the crewmember arrangement and scheduling. Though the working status can be related to many inherent causal factors, sleep quality plays a major role in fatigue. In this work, we propose to develop an end-to-end system to collect the sleep data with wearable devices, and explore its relations with the popular fatigue indexes, e.g., Karolinska Sleepiness Scale (KSS), Psychomotor Vigilance Task (PVT), and NASA Task Load Index (NASA-TLX). Specifically, we collected the sleep time and quality measurement of 256 crewmembers for three days before the flight, and the corresponding KSS, PVT, and NASA-TLX during or just after the flight. Then, a multi-task deep learning framework is developed to learn the mapping from sleep to fatigue indexes. We trained and evaluated on 200 and 56 crewmembers with the quantified results showing that fatigue is highly predictive with the sleep data. The learned deep learning model can potentially be applied to the pre-flight crewmember screen to avoid the fatigue driving and service.",Yes,본 논문은 웨어러블 기기를 통해 수집한 수면 데이터와 피로 지수 간의 관계를 다중 작업 딥러닝 모델로 분석하는 독창적인 연구를 수행하였다. 실험을 통해 256명의 승무원 데이터를 수집하고 모델을 학습 및 평가한 점에서 직접적인 연구 기여가 포함되어 있다.
"Neural Networks in Human Activity Recognition: Trends, Challenges, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915132,"Human Action Analysis is a procedure in tracking individual gestures, motion and actions which finds applications, including healthcare, surveillance, security and human-computer communication. With the proliferation of sensors and deep learning techniques, HAR has marked remarkable development in the past few years. This review paper gives a brief survey of the current studies in Activity Analysis, focusing on deep learning-based approaches. The various sensors employed in HAR, including accelerometers, gyroscopes, and vision-based sensors were used in acquiring signals. The main objective is to design an appropriate model for recognizing/classifying human actions effectively to support elderly and differently abled people. The evolution of deep learning architectures used in detection such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTM) networks can be utilized for action recognition. The characterization of these algorithms is evaluated using various metric indices including precision, accuracy, recall, Fl-score, and mean average precision. The challenges and limitations of existing HAR systems, including data purity, imbalance nature of class, and positioning of sensor were also presented. This review aims to provide a clear picture in Action Recognition, highlighting the Pros and cons of different approaches. By identifying the prospects and obstacles, this review seeks to inspire future research in behaviour analysis, ultimately leading to the development of more accurate, efficient, and robust systems.",No,"초록에서 해당 논문은 기존 연구들을 종합하여 정리한 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험 기여가 포함되어 있지 않다. 따라서 연구 논문보다는 현황과 동향을 분석하는 서베이 논문에 해당한다."
Kidney Disease Classification and Diagnosis: A Comprehensive Review of Current AI Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638841,"Accurate and early diagnosis of kidney disease (KD) is crucial for effective treatment and improved patient outcomes. computer aided and AI techniques offers a promising solution in this domain, analyzing medical images to identify disease patterns. This research paper explores the potential of ML, particularly deep learning (DL) and transfer learning, for classifying kidney conditions from CT scans. We review existing research to identify the most effective ai algorithms for this task, evaluate performance metrics used to assess accuracy, and explore challenges like data availability and interpretability. By discussing future directions, including integration of additional data sources, this paper aims to provide a comprehensive analysis of AI techniques-based KD classification, paving the way for more accurate and efficient diagnostic tools.",No,초록에서 본 논문은 기존 연구들을 종합적으로 검토하고 평가하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험 기여보다는 기존 연구의 분석과 정리에 중점을 두고 있습니다.
Sparse Sample Train Axle Bearing Fault Diagnosis: A Semi-Supervised Model Based on Prior Knowledge Embedding,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262175,"Data-driven fault diagnosis models often exhibit limited generalization abilities when trained on small sample sizes, as commonly encountered in complex working environments such as train bearing systems. To address this challenge, incorporating domain knowledge into machine learning models has been proposed. Specifically, prior knowledge in the form of wavelet packet decomposition and information entropy features was extracted to characterize fault-related physical phenomena. Convolutional neural networks (CNNs) supplemented this approach by learning general features. An attention mechanism fused the two feature types, emphasizing the prior knowledge. Extreme gradient boosting (XGBoost) classification was then applied for fault discrimination. In addition, a self-training method semi-supervised the model during training. The approach was evaluated on three bearing datasets against deep learning and machine learning baselines. Results showed the method achieved over 70% accuracy with only 30 labeled samples per class, outperforming the semi-supervised baseline. This lightweight, knowledge-driven solution provides a foundation for efficient fault diagnosis from limited data in complex fault scenarios. Overall, the study demonstrated an effective method for addressing small sample size challenges in machine fault diagnosis.",Yes,"본 논문은 제한된 샘플 수 문제를 해결하기 위해 도메인 지식을 포함한 반지도학습 모델을 제안하고, 이를 실제 베어링 결함 진단에 적용하여 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
An Innovative Anomaly Driving Detection Strategy for Adaptive FCW of CNN Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377289,"As a road safety issue, research on driver distraction is still in its infancy. Although people talk about distraction as if they know what it means, it is poorly defined. Less is known about the patterns of driver exposure to the various sources of distraction that exist or the impact of these on driver performance, either individually or in combination. Thus, there are many sectors from the community with a vested interest in preventing and mitigating the potential effects of distracted driving. To assist analysts with developing the required advances around this field, this article gives an extensive writing study of work tending to the issue of human characteristics acknowledgment in a driving environment. A considerable amount of literature has been published on this topic. These studies efficiently survey the writing back to 2000s and recognized more than 40 peer review articles in this field. The review for each approach and procedure is to quantify and perceive characteristics with regards to driving behavior. Over the writing, discovery on solid inclination toward encouraging states related with abnormal behavior and drastic action while checking the various states of taxonomy for unsafe driver. Human body movement while in fatigue or distraction condition and utilizing supervised artificial intelligence to systematically surmise the underlying human affective. For instance, visual face features as a signal for driver assistance system. Overall, these articles highlighted the beneficial effects of the current work along with publicly available resources such as datasets and tools to enable new scholars to begin in this field. Besides, it also discerns new research potential to assist in the advancement and improvement of driving systems.",No,"초록은 기존 문헌을 광범위하게 검토하고 요약하는 리뷰 논문임을 나타내며, 직접적인 독창적 연구 결과나 실험적 기여에 대한 언급이 없다. 따라서 본 논문은 연구 논문보다는 문헌 조사 및 정리 목적의 리뷰 논문으로 판단된다."
COVID-19 Detection Using Fusion-Based Deep Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346927,"The COVID-19 pandemic has been one of the most challenging crises attacking the world in the last three years. Many systems have been introduced in the field of COVID-19 detection. In this research, deep learning models for the detection of COVID-19 with a probability of the presence of COVID-19 are proposed. The COVID-19 dataset is split into 70% training and 30% testing using the holdout method. For the deep learning model, the CT images are fed into the model without feature extraction, and three different deep learning DL models (convolutional neural networks (CNN), GoogleNet, and residual nets (ResNet50)) are trained and evaluated. The three deep learning models are fused to obtain high accuracy, and get better performance. The trained models are evaluated using accuracy, precision and recall evaluation metrics. The experiments show that the best model is the deep-based fusion model by which the system achieved 96.4%, 96.2%, and 96.2% for accuracy, precision, and recall, respectively. Comparing our study with the related work proves the powerful and high performance of the proposed methodologies.",Yes,"본 논문은 COVID-19 검출을 위한 딥러닝 모델을 제안하고, 세 가지 모델을 융합하여 성능을 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한, 실험을 통해 모델의 정확도, 정밀도, 재현율을 평가하여 연구 결과를 직접 제시하고 있으므로 연구 논문에 해당한다."
CPSoSaware: Cross-Layer Cognitive Optimization Tools & Methods for the Lifecycle Support of Dependable CPSoS,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155036,"Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and networked computing elements as well as human users. Their increasingly stringent demands on efficient use of resources, high service and product quality levels and, of course low cost and competitiveness on the world market introduce big challenges related to the design operation continuum of dependable connected CPSs. The CPSoSaware project aims at developing the models and software tools to allocate computational power/resources to the CPS end devices and autonomously determining what cyber-physical processes will be handled by the devices' heterogeneous components (CPUs, GPUs, FPGA fabric, software stacks). The project relies on Artificial Intelligence (AI) support to strengthen reliability, fault tolerance and security at system level and also to lead to CPS designs that work in a decentralized way, collaboratively, in an equilibrium, by sharing tasks and data with minimal central intervention. The CPSoSaware system will interact with the human users/operators through extended reality visual and touchable interfaces increasing situational awareness. The CPSoSaware system will be evaluated: i) in the automotive sector, in mixed traffic environments with semi-autonomous connected vehicles and ii) in the manufacturing industry where inspection and repair scenarios are employed using collaborative robots.",Yes,"논문 초록에서 CPSoSaware 프로젝트가 인공지능을 활용한 모델과 소프트웨어 도구를 개발하고, 이를 자동차 및 제조업 분야에서 평가하는 구체적인 연구 활동을 다루고 있음을 알 수 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 시사합니다."
COPPTCHA: COPPA Tracking by Checking Hardware-Level Activity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9049424,"User privacy is an extremely important concern for mobile applications. Recently, the Federal Trade Commission (FTC) has penalized multiple mobile application developers, such as TikTok and BabyBus for violating privacy regulations. Privacy concerns are more critical for children, who do not comprehend the risks associated with transmitting private information like geospatial location. The Children's Online Privacy Protection Act (COPPA) is an online privacy regulation platform to monitor data usage by mobile applications designed for children. Existing research on detecting whether an application complies with certain privacy regulations is performed either by analyzing the application binary or by dynamic monitoring of network at runtime. However, as explained in related work, both methods have their respective demerits. We propose COPPTCHA, a Hardware performance counter (HPC)-based technique to detect whether a children's app abides by the COPPA regulations. HPCs are special purpose registers found in all processors that measure system level events. Since the proposed method is hardware-based, it is difficult to undermine it compared to software-based COPPA compliance detection. COPPTCHA has no hardware overhead, since HPC data collection is integral to all industry standard processors. The HPC readings of applications running on a smartphone are classified using machine learning based classifiers to detect COPPA compliance. Our experiments employing a Moto-G4 smartphone shows that COPPTCHA can detect COPPA-violating apps with ≥ 99% accuracy.",Yes,"이 논문은 COPPA 규정을 준수하는지 여부를 하드웨어 성능 카운터를 이용해 탐지하는 새로운 기법인 COPPTCHA를 제안하고 있으며, 이를 위해 머신러닝 분류기를 활용한 실험 결과도 제시하고 있다. 따라서 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단된다."
StressAlly: A Smartphone-Based Stress Companion Recommender System for Students,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10437258,"Stress has become an increasing concern among college students. Passive sensing techniques allow the extraction of stress-related parameters from a student. These techniques use highly resource-intensive machine learning algorithms to predict the stress levels of a student from these parameters. However, the current techniques do not provide any social communication solution for students suffering from stress. In this work, we propose StressAlly, a stress companion recommender system. The system comprises two modules, the stress score predictor and the stress companion recommender. The stress score predictor incorporates edge computing and deploys lightweight in-app inferences. The stress score predictor calculates the stress level in the scale 0–4 in the smartphone using the Artificial Neural Network (ANN) Regressor and sends it to the server. The Stress companion recommender provides similar stress levels of students to each student using the User-User Collaborative Filtering technique. We achieved training MAE and loss of 0.7478 and 1.0298, respectively. We get a test Mean Absolute Error (MAE) of 0.737 on the unseen data. We evaluate the CPU, memory, time delay, and network performance of StressAlly on the server and the Android smartphone. StressAlly utilizes 188 MB memory and 25% CPU on the smartphone.",Yes,"본 논문은 스트레스 예측을 위한 인공신경망 회귀모델 개발과 사용자 간 협업 필터링 기반 추천 시스템 설계 등 독창적인 연구 내용을 포함하고 있습니다. 또한, 성능 평가 지표와 시스템 자원 사용량을 구체적으로 제시하여 직접적인 연구 기여가 있음을 보여줍니다."
Pilot Workload Prediction from ECG Using Deep Convolutional Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802158,"Pilot workload monitoring plays an important role in aviation safety. Heart rate, heart rate variability, and respiration rate have been shown to correlate with pilot workload and can be measured using electrocardiograms (ECG). Traditional machine learning approaches rely on manually extracting features from ECG, which is a difficult and time-consuming process. Recent years witnessed the success of deep neural networks, especially deep convolutional neural networks (CNNs), in computer vision and related domains; however, the application of deep CNNs onto the ECG data faces challenges on both data insufficiency and lack of tailored CNN architectures. With a small training set, this work proposes the use of transfer learning with pre-trained deep CNNs for the prediction of pilot workload. Two ECG-derived visual representations, spectrograms and scalograms, are compared for their performance on the prediction. Experimental results indicate that the scalograms perform better (at 51.35%) than spectrograms (at 45.85%) in predicting three levels of pilot workload. With the scalograms, using the pre-trained deep CNNs as ""off-the-shelf"" feature extractors yields better performance than fine-tuning the deep CNNs (at 42.44%) for the ECG data. The deep features are visualized using dimension reduction with t-sne.",Yes,"본 논문은 조종사 작업 부하 예측을 위해 ECG 데이터를 활용한 딥러닝 모델을 제안하고, 전이 학습과 두 가지 시각적 표현 방법을 비교하는 실험적 연구를 수행하였다. 이는 기존 방법의 한계를 극복하고자 하는 독창적인 연구 내용과 실험 결과를 포함하고 있어 연구 논문에 해당한다."
Software Agents Situated in Primary Distribution Networks: A Cooperative System for Fault and Power Restoration Management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4302509,"In this paper, extended research upon the potentials of implementing distributed artificial intelligence technology to achieve high degrees of independency in distribution network protection and restoration processes is presented. The work that has already been done in the area of agent-based and/or knowledge-based applications and expert systems is briefly reviewed. The authors justify the need to distribute activities in contradiction to the centralized methodologies. A proper model of the real environment is introduced in order to define the designing parameters of a prototype agent entity, which is the part of a cooperative network-management system. The system's goal is to autonomously perform effective fault management upon medium-voltage power distribution lines. The structure of the agent entity is then described by means of the agent behaviors being implemented. The cooperative operations of the proposed system and its computer simulation are presented. Simulation results are being evaluated. Finally, general conclusive remarks are made.",Yes,"논문 초록에서 제안된 시스템의 설계, 에이전트 행위 구현, 그리고 컴퓨터 시뮬레이션 결과 평가 등 구체적인 연구 방법과 결과가 포함되어 있어 독창적인 연구 내용을 다루고 있음을 알 수 있습니다. 따라서 본 논문은 직접 기여하는 연구 논문에 해당합니다."
WIP: Building a Research Experience for Undergraduates in Quantum Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893516,"This work in progress research-to-practice study describes the development of a new undergraduate research training site on Quantum Machine Learning (QML), hosted at Arizona State University, a large Hispanic-Serving Institution. The objectives of this project are to a) recruit and prepare students from diverse pathways to increase representation of those traditionally underrepresented in QML research, b) increase awareness of career opportunities in the QML field, c) engage students in theoretical and experimental quantum information processing and machine learning (ML), d) motivate students to continue QML research into graduate school, and e) provide professional development training including presenting to stakeholders, developing publications/patents, and building an awareness on social implications, ethics, and privacy. The project adopts an integrative theory, application, and hands-on training approach by immersing undergraduate students in ML algorithm and quantum computing studies with hands-on quantum circuit design tasks. Participants are embedded in research labs, guided by graduate students and faculty mentors on quantum computing research studies. The program is evaluated by both the Center for Evaluating the Research Pipeline (CERP) and an independent evaluator. Formative and summative assessments include pre- and post-surveys, a mid-point check-in survey, and a document review of program deliverables. Findings are described in a final evaluation report. This paper describes the importance of introducing QML research at the undergraduate level, methods for recruiting a diverse group of participants, program format, research projects, and preliminary program evaluation results.",No,"본 논문은 양자 머신러닝 분야의 학부생 연구 경험 프로그램 개발과 평가에 관한 연구 진행 보고서로, 직접적인 독창적 연구 결과나 새로운 연구 발견을 제시하지 않습니다. 주로 교육 프로그램 설계와 평가에 초점을 맞추고 있어 연구 논문으로 보기 어렵습니다."
Empirical Evaluation of Embedding Models in the Context of Text Classification in Document Review in Construction Delay Disputes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825997,"Text embeddings are numerical representations of text data, where words, phrases, or entire documents are converted into vectors of real numbers. These embeddings capture semantic meanings and relationships between text elements in a continuous vector space. The primary goal of text embeddings is to enable the processing of text data by machine learning models, which require numerical input. Numerous embedding models have been developed for various applications. This paper presents our work in evaluating different embeddings through a comprehensive comparative analysis of four distinct models, focusing on their text classification efficacy. We employ both K-Nearest Neighbors (KNN) and Logistic Regression (LR) to perform binary classification tasks, specifically determining whether a text snippet is associated with 'delay' or 'not delay' within a labeled dataset. Our research explores the use of text snippet embeddings for training supervised text classification models to identify delay-related statements during the document review process of construction delay disputes. The results of this study highlight the potential of embedding models to enhance the efficiency and accuracy of document analysis in legal contexts, paving the way for more informed decision-making in complex investigative scenarios.",Yes,"본 논문은 여러 임베딩 모델을 비교 평가하고, 이를 활용한 텍스트 분류 모델을 직접 학습 및 실험하여 건설 지연 분쟁 문서 검토에 적용하는 독창적인 연구 내용을 포함하고 있다. 따라서 새로운 실험적 결과와 분석을 제시하는 연구 논문으로 판단된다."
Personalizing the Prediction: Interactive and Interpretable machine learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768705,"While many applications with machine learning provide enough utilities for users, they mostly target average of users. Although it might be acceptable in certain domains, there are domains such as health and medical-care where it is crucial to provide personalized service. In such cases, personalization of machine learning model usually does not depend on end users to make change to the system. As machine learning models are black-box, the only information that the users can acquire is input and output of certain decision made by the model. Thus, with no reason behind specific prediction provided by the system, users cannot understand how the system works and make amendments to the system. This shortcoming is directly related to users' credibility in the system. In this paper, we present an interface where the system provides users the reason behind the decision made by the machine learning model and users provide feedback to the model. Moreover, we present the principle behind the suggested interface and prototype that instantiates the suggested interface. Our interface's effectiveness is evaluated through users' surveys regarding two main attributes: (1) how well users understand the system and more importantly, (2) how it influences users to trust in the system.",Yes,"논문은 사용자 맞춤형 머신러닝 모델을 위한 인터페이스를 제안하고, 그 원리와 프로토타입을 구현하여 사용자 이해도와 신뢰도에 미치는 영향을 평가하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구와 차별화된 직접적인 기여를 하는 연구 논문으로 판단된다."
SAARSNet: A Deep Neural Network for COVID-19 Cases Diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9436536,"The global spread of the COVID-19 is a continuously evolving situation and it is still a major risk on the health of people around the world. A huge number of people are infected by this deadly virus and the number is still getting increased day by day. At this time, no specific vaccines or treatments of COVID-19 are found. Numerous ways are offered to detect COVID-19 such as swab test, CDC and RT-PCR tests. All of them can detect corona virus in different ways but they are not recommended by the reason of their limited availability, inaccurate results, high false-negative rate predicates, high cost and time consuming. Hence, medical radiography and Computer Tomography (CT) images were suggested as the next best alternative of RT -PCR and other tests for detecting Covid-19 cases. Recent studies found that patients with COVID-19 cases are present abnormalities in chest X-Ray images. Motivated by this, many researchers propose deep learning systems for COVID-19 detection. Although, these developed AI systems have shown quite promising results in terms of accuracy, they are closed source and unavailable to the research community. Therefore, in the present work, we introduced a deep convolutional neural network design (SAARSNet) designed to detect COVID-19 cases from chest X-Ray images. 1292 X-Ray images have been used to train and test the proposed model. the images have been collected from two open-source datasets. The input images are progressively resized into (220 by 150 by 3) in order to decrease the training time of the system and improve the performance of the SAARSNet architecture. Furthermore, we also investigate how SAARSNet makes predictions under three different scenarios with the aim of distinguishing COVID-19 class from both Normal and Abnormal classes as well as gaining deeper perceptions into critical factors related to COVID-19 cases. We also used the confusion metrics for evaluating the performance of SAARSNet CNN in an attempt to measure the true and false identifications of the classes from the tested images. With the proposed architecture promising results has been achieved in all of the three different scenarios. Although, there are some misclassified cases of COVID-19, the corresponding performance was best in detecting both Normal and Abnormal cases correctly. Furthermore, in the three classes scenario, normal class has been achieved 100% positive predictive value while optimistic results have been investigated in detecting COVID-19 and abnormal classes.",Yes,"본 논문은 COVID-19 진단을 위한 새로운 딥러닝 모델(SAARSNet)을 제안하고, 공개된 X-Ray 이미지 데이터를 사용해 모델을 학습 및 평가한 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 시나리오에서 모델의 성능을 분석하고 결과를 제시하여 직접적인 연구 기여를 하고 있음을 보여준다."
"IoT and AI driven sustainable practices in airlines as enabler of passenger confidence, satisfaction and positive WOM : AI and IoT driven sustainable practice in airline",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395850,"Covid-19 has changed way of living. Airline operations were completely stopped and almost all airlines have grounded their fleets. After opening of aviation sector also, passengers are afraid of pandemic and avoid travelling. However, Modern, and innovative technologies have capability to disrupt operations and service offerings of airline industry also, like any other industry. This literature review-based study analyzes the impact of modern technology adoption by airlines for enhancing the safety features for passengers in post-covid-19 and its impact on passenger confidence, satisfaction and positive WOM (Word of Mouth). The literature-based study proposes that by adopting internet of things and artificial intelligence driven sustainable practices airlines can enhance passenger’s confidence while travelling. The study also proposes that AI and IoT enabled systems has capability to turn old traditional processes at the airport and in flight to modern service capabilities which enhances passengers’ trust as well as satisfaction as passenger received services of better quality. Although the study is based on previous research, it gives proposition and opens a door of opportunities for the future research. In future, the propositions can be converted into testable hypotheses and findings can be empirically validated using robust methodology.",No,"본 논문은 기존 문헌을 바탕으로 한 문헌고찰 연구로, 직접적인 실험이나 데이터 분석을 통한 독창적인 연구 결과를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 향후 연구 방향을 제안하는 개념적 논문에 해당합니다."
Using Machine Learning Algorithms to Improve Education Process,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9960034,"One of the main goals of any educational institution is high quality education, taking into account the shortcomings of the education system. Currently, in the field of education, one of the urgent problems is the identification of signs that affect student performance. Machine learning algorithms are used to determine these features. Machine learning algorithms are being implemented in various spheres of society today. Giants from all over the world, such as Amazon, Google, Apple, identify customer-related problems using machine learning algorithms. In addition, a number of machine learning methods in economics, banking and medicine, manufacturing and industry can improve the work of specialists. Aiming at better performance and better understanding of the obtained results, binding models had been applied at the data selection and integration stages. The article discusses the collection of databases, learning algorithms, analysis of algorithms and the definition of features. In the research work, a database is created that includes the physiological and psychological characteristics of students. The program is executed in the Python programming environment. Predictions are based on databases with machine learning algorithms. Linear regression methods, the Support Vector Machine method and Random Forest are used, and work is devoted to determine the best algorithm. Algorithms are evaluated using regression evaluation metrics. The results of the study provide the best algorithm and the necessary marks for individual academic performance.",Yes,"논문 초록에서 학생들의 생리적, 심리적 특성을 포함한 데이터베이스를 구축하고, 여러 머신러닝 알고리즘을 적용하여 최적의 알고리즘을 도출하는 연구 과정을 명확히 기술하고 있습니다. 이는 직접적인 데이터 수집, 알고리즘 적용 및 평가를 포함한 독창적인 연구 내용임을 보여줍니다."
A Comprehensive Review of Machine Learning in Multi-objective Optimization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9515233,"In the real world, it is challenging to calculate a trade-off alternative with traditional classical methods for complex non-linear systems, which always involve multiple conflicting objectives. Such complicated systems urgently desire advanced methods to conquer the multi-objective optimization problems (MOPs). As a promising AI method, the development and application of Machine Learning (ML) attract increasingly more attention from researchers. The natures of ML methods, such as parallel computation possibility, no need for any priori assumptions, etc., ensure the effectiveness and efficiency for solving MOPs. However, as we know, there is no literature related to the comprehensive review of ML in multi-objective optimization domain until now. This literature review aims to provide researchers a global view of mainstream ML methods for MOO in a general domain and a reference for applying ML methods to solve a specific type of MOPs. In this paper, the general ML mainstream methods are summarized, based on which the literature relating to ML on MOPs are retrieved in comprehensive domains. The relevant literature is categorized according to the emphasis of object types, purposes and methods, and the categorization results are finally analyzed and discussed.",No,이 논문은 머신러닝을 활용한 다목적 최적화에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이 아니라 기존 문헌을 정리하고 분석하는 데 중점을 두고 있습니다.
Hybrid Quantum-Classical Machine Learning for Near Real-time Space to Ground Communication of ISS Lightning Imaging Sensor Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099338,"Near real-time sensor data is growing exponentially faster than our ability to sense and make sense of it as the current classical computing and supercomputing approaches require an enormous volume of computational resources, storage, and training time. Quantum computing is posed to exponentially outperforms today's high-performance computers and accelerate the evolution of information occurring in classical systems and sensor networks. The Lightning Imaging Sensor (LIS) mounted on the International Space Station (ISS) locates, senses, and detects lightning activities from low Earth orbit and measures radiant energy at millisecond timing over a broad regional spectrum. In this paper, we are introducing a space-to-ground hybrid quantum-classical machine learning architecture to demonstrate the application potential and the feasibility of Hybrid Quantum Neural Networks using the ISS LIS lightning dataset and a corresponding background dataset as inputs (1) to train a classical deep neural network model (2) where the feature extraction outputs are encoded as quantum states using multiple calibrated quantum encoding patterns (3) then used as part of a quantum feature mapping process, (4) allowing for enhanced hybrid QNN training, optimization, and auto-differentiation, setting the path towards polynomial advantage. The following is how the rest of the paper is structured: The introduction is covered in section I, and the motivation and our contribution are covered in section II. Section III presents related works. Section IV describes the NRT lightning imaging sensor. Section V describes the space- to-ground hybrid quantum architecture. Section VI. Space-to-ground hybrid quantum-classical machine learning architecture. Section VII describes quantum machine learning. Section VIII presents a performance analysis. Finally, section XI concludes the research paper.",Yes,"논문 초록에서 제안된 하이브리드 양자-고전 기계학습 아키텍처와 그 적용 가능성, 성능 분석 등이 포함되어 있어 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 구체적인 방법론과 실험 데이터셋을 활용한 연구임을 명확히 하고 있습니다."
Exploring Technological Success Factors of Big Data in E-Learning Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10218073,"Big data plays an important role in the development of e-learning systems. There are many factors affecting its implementation and success with e-learning systems. This study aims to identify the implementation success factors related to big data within e-learning systems. To identify these elements, a comprehensive examination of the literature was done by using exploratory and single-case research approaches, the study used Basrah University as the case study to assess the success factors of big data in their e-learning system. Additionally, survey and expert interviews were conducted to validate the literature review’s conclusions and identify further factors. The collected data were analyzed using NVivo software to identify themes and sub-themes. To assess the quantitative survey data, machine learning methods are combined with qualitative analysis using a Random Forest Classifier. The finding showed that five factors should be considered which can aid in the creation of more efficient e-learning systems namely: Positive Impact on Students, Faculty/Staff Support and Training, Effective Content Design, System Functionality and Usability, and Assessment and Feedback. The results of this study can be used to improve learning outcomes by developing more efficient big data analytics-integrated e-learning systems.",Yes,"본 논문은 빅데이터가 적용된 이러닝 시스템의 성공 요인을 식별하기 위해 문헌 검토, 사례 연구, 설문조사, 전문가 인터뷰 등 다양한 연구 방법을 사용하여 직접 데이터를 수집하고 분석한 독창적인 연구 내용을 포함하고 있다. 또한, 정성적 및 정량적 분석 기법을 활용하여 구체적인 성공 요인을 도출함으로써 연구에 실질적인 기여를 하고 있다."
Ship Space-Time AIS Trajectory Data Compression Method,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760355,"As the ship Automatic Identification system (Automatic Identification Systems, AIS) mandating the use of a large number of static and dynamic information related to the ship automatically through the shore-based or spaceborne terminal is passed to the maritime system and the shipping company, as an effective water traffic monitoring and management provides the necessary data to support. But because the ship is most of the time when navigating in high speed to or only minor adjustments, that receives a huge ship AIS trajectory data is invalid and redundant information, such a huge data increases the cost of computer memory, reduce the efficiency of data server calls. Therefore, it is important to study how to compress invalid and redundant information based on the main structural features of the AIS trajectory. This paper addresses the problem of many invalid and redundant information generated by Automatic Identification Systems (AIS), and conducts a study to effectively compress invalid and redundant information while preserving the main structural features of ship AIS trajectories. The main research work of this paper is to construct a compression method of ship space-time AIS trajectory data based on Douglas-Peucker (DP) algorithm and select the multiple thresholds to compress the upbound and down-bound ship AIS trajectories respectively. The DTW algorithm is used to evaluate the compression effect, and obtain the optimal compression threshold. Experimental results show that DP algorithm can efficiently compress ship AIS track data and retain pivotal feature points.",Yes,"본 논문은 AIS 궤적 데이터 압축을 위한 새로운 방법을 제안하고, Douglas-Peucker 알고리즘과 DTW 알고리즘을 활용하여 최적의 압축 임계값을 도출하는 실험적 연구를 수행하였다. 이는 기존 연구와 차별화된 독창적인 연구 내용과 실험 결과를 포함하고 있어 연구 논문에 해당한다."
Scalable Multimodal Learning and Multimedia Recommendation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10429923,"Driven by the rapid growth of multimedia big data, multimodal learning (especially multimodal deep learning) has gained its significant importance and achieved biggest success in various multimedia computing related applications. The complexity and scale of modern multimedia recommender system often require much more sophisticated statistical model, learning architecture and data processing algorithms to facilitate effective and efficient content understanding and analysis than ever before. In this work, we discuss several major research challenges of the future multimedia recommender system supported by advanced multimodal learning. We also, 1) introduce why multimodal learning is important for large scale multimedia recommendation, 2) review various limitations of the current generation of learning model and architecture, and 3) review key challenges and technical issues in developing and evaluating modern multimedia recommender systems with multimodal learning under different contexts. We hope that our discussion and prediction provide an impetus for further research on this important direction.",No,"초록 내용은 주로 기존 연구의 한계와 미래 연구 방향에 대한 논의 및 리뷰에 초점을 맞추고 있으며, 독창적인 실험 결과나 새로운 연구 방법론의 제시가 포함되어 있지 않습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Research on Sustainable Management of Grid Companies Based on Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150589,"With the advancement of power systems, grid operations monitoring has become more complex. To ensure the sustainable operation of the grid, it is important to quickly and dynamically extract a small amount of data from a large amount of real-time data, closely related to grid security, and accurately identify system security risks in a timely manner. In this regard, this paper proposes an artificially intelligent sustainable management system for the grid, which has achieved remarkable results. The main contribution of this paper is the development of a risk identification model for microgrids using artificial intelligence and the DEMATEL method. Firstly, the paper identifies the risk factors involved in the microgrid from four aspects, including the generation side, distribution side, demand side, and human factors, through a comprehensive literature review, expert survey, and brainstorming. Secondly, an artificial intelligence and DEMATEL-based microgrid risk factor identification method is employed to clarify the importance and perturbation relationship of each risk factor of the microgrid. Finally, the paper classifies all factors into two categories, cause factors, and effect factors, and ranks the importance of each factor. To further demonstrate the effectiveness of the proposed model, a wind power prediction algorithm based on data mining technology and an improved SVM algorithm, and a PV power prediction algorithm based on a deep neural network are established. After comparing and analyzing the performance of the constructed algorithms with other algorithms, the DBN prediction model is proposed, which has an absolute error probability of 62.9% within 1%, surpassing the other algorithms and meeting the engineering needs. Moreover, the paper proposes risk control measures on the power generation side, which can significantly reduce the risks involved in power generation. In summary, the paper proposes an artificially intelligent sustainable management system for the grid, which includes a risk identification model for microgrids, and prediction models for wind and PV power generation. The paper's findings provide valuable insights for researchers and practitioners in the field of grid operation and management. The proposed models have shown remarkable results, and the risk control measures can significantly reduce the risks involved in power generation, contributing to the sustainable operation of the grid.",Yes,"본 논문은 인공지능과 DEMATEL 기법을 활용한 마이크로그리드 위험요인 식별 모델 개발, 풍력 및 태양광 발전 예측 알고리즘 구축 등 독창적인 연구 내용을 포함하고 있습니다. 또한 제안된 모델의 성능 비교 및 위험 제어 방안 제시를 통해 실질적인 연구 기여를 하고 있음을 알 수 있습니다."
Evaluation of Resting-State EEG Patterns in Female Schizophrenia and Addiction Patients Using Convolutional Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837566,"Machine learning aids in accurately diagnosing psychiatric disorders, impacting individuals' quality of life and society, by identifying and classifying them efficiently. Traditional diagnoses of psychiatric disorders relied on subjective assessments, which are time-consuming, costly, and human biased; machine learning offers faster, cheaper, and more accurate solutions. The aim of this study was to evaluate patterns of relationship in female schizophrenic and addictive disorder patients by the application of computational techniques to analyze resting-state electroencephalography (EEG) data signals obtained via machine learning with a view to identify distinctive patterns or features that can help medical practitioners to differentiate between the two patient groups. The study utilized data on beta waves collected from the forehead of the human subjects with the electrodes in positions fp1 and fp2. A review of published literature in the field of diagnosis of mental health disorders using machine learning was also conducted in order to identify current trends on the subject and identify possible research gaps for further research. The results show the relationships between age, education, IQ, and AB.D.beta a.FP1/FP2 assessed using correlation coefficients and p-values show a weak negative relationship between age and AB.D.beta a.FP1/FPI/FP2. The correlation coefficients are -0.1054, -0.1722, and -0.0975, respectively. The p-values for these relationships are 0.6407, 0.4435, and 0.4962, indicating that the correlations are not significant. Age, education and IQ did not appear to have a significant impact on the AB.D.beta a.FP1/FPI/FP2 patterns in the studied female patients. Our study investigates the efficacy of convolutional neural networks (CNNs) in diagnosing psychiatric disorders in females. We achieved an accuracy of 84.3% using CNNs for evaluating EEG patterns.",Yes,"본 논문은 여성 정신분열증 및 중독 환자의 휴지기 EEG 데이터를 CNN을 활용해 분석하고, 두 환자군을 구분하는 독창적인 연구를 수행하였으며, 실험 결과와 정확도 수치를 제시하고 있다. 이는 직접 기여하는 연구 내용이 포함된 연구 논문에 해당한다."
Type Learning for Binaries and Its Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588310,"Binary type inference is a challenging problem due partly to the fact that during the compilation much type-related information has been lost. Most existing research work resorts to program analysis techniques, which can be either too heavyweight to be viable in practice or too conservative to be able to infer types with high accuracy. In this paper, we propose a new approach to learning types for binary code. Motivated by “duck typing,” our approach learn types for recovered variables from their features and properties (e.g., related representative instructions). We first use machine learning to train a classifier with basic types as its levels from binaries with debugging information. The classifier is then used to learn types for new and unseen binaries. While for composite types, such as pointer and struct, a points-to analysis is performed. Finally, several experiments are conducted to evaluate our approach. The results demonstrate that our approach is more precise, both in terms of correct types and compatible types, than the commercial tool Hex-Rays, the open source tool Snowman, and a recent tool EKLAVYA using machine learning. We also show that the type information our proposed system learns is capable of helping detect malware.",Yes,"논문은 기존 연구의 한계를 지적하고, 머신러닝 기반의 새로운 이진 코드 타입 추론 방법을 제안하며, 이를 평가하는 실험 결과를 포함하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 나타낸다."
Proposing Causal Sequence of Death by Neural Machine Translation in Public Health Informatics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744474,"Each year there are nearly 57 million deaths worldwide, with over 2.7 million in the United States. Timely, accurate and complete death reporting is critical for public health, especially during the COVID-19 pandemic, as institutions and government agencies rely on death reports to formulate responses to communicable diseases. Unfortunately, determining the causes of death is challenging even for experienced physicians. The novel coronavirus and its variants may further complicate the task, as physicians and experts are still investigating COVID-related complications. To assist physicians in accurately reporting causes of death, an advanced Artificial Intelligence (AI) approach is presented to determine a chronically ordered sequence of conditions that lead to death (named as the causal sequence of death), based on decedent's last hospital discharge record. The key design is to learn the causal relationship among clinical codes and to identify death-related conditions. There exist three challenges: different clinical coding systems, medical domain knowledge constraint, and data interoperability. First, we apply neural machine translation models with various attention mechanisms to generate sequences of causes of death. We use the BLEU (BiLingual Evaluation Understudy) score with three accuracy metrics to evaluate the quality of generated sequences. Second, we incorporate expert-verified medical domain knowledge as constraints when generating the causal sequences of death. Lastly, we develop a Fast Healthcare Interoperability Resources (FHIR) interface that demonstrates the usability of this work in clinical practice. Our results match the state-of-art reporting and can assist physicians and experts in public health crisis such as the COVID-19 pandemic.",Yes,"본 논문은 인공지능 기반의 신경망 기계 번역 모델을 활용하여 사망 원인의 인과 순서를 예측하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 의료 도메인 지식과 데이터 상호운용성 문제를 해결하는 방법을 제안하며, 임상 실무 적용을 위한 인터페이스 개발까지 다루고 있어 연구 논문에 해당합니다."
Post COVID-19 Twitter user’s Emotions Classification using Deep Learning Techniques in India,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395899,"Social Media platforms contain a huge data collection of shared and personal thinking with respect to a wide assortment of subjects, communicated and spread ceaselessly by their users. Among those platforms, Twitter is gaining immense popularity. This research work proposes a system to computationally measure the emotions of live tweets by their users and emotions regarding the government's decision on extending the lockdown. The system consists of dashboard with various functionalities. Main dashboard has country-wise data visualization of the emotions derived from the tweets, it has clickable map of India which shows state-wise data visualization as well. Live emotion prediction of tweets is achieved using Deep Learning tools. Tweet fetching is dynamic to get up-to-date data automatically. Resources tab is available for COVID-19 related statistics and news. 250 plus days since the initial stage COVID-19 case in the World, and 180+ days into the most punctual Lockdown Order of India, how is the people thinking in circumstances such as these? The Corona Virus imperils our physical wellbeing to be sure, however close by, social separating additionally represents a danger to our enthusiastic steadiness. Accordingly, it is vital to comprehend public emotions under COVID-19.",Yes,"본 논문은 딥러닝 기법을 활용하여 인도 내 트위터 사용자들의 감정을 실시간으로 분류하는 시스템을 제안하고 있으며, 데이터 수집과 감정 예측을 위한 구체적인 방법론을 포함하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문으로 판단된다."
Exploring the Potential of Locally Run Large Language (AI) Models for Automated Grading in Introductory Computer Science Courses,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892816,"This innovative practice full paper describes the effectiveness of self-hosted large language models (LLMs) in assisting with the automatic grading of CSI assignments. Educators often rely on automated review of student code submissions in larger courses. Despite recent advancements, current systems primarily focus on assessing functionality, with important aspects such as code structure, efficiency, and style often relegated to secondary foci. LLMs provide an increasingly attractive addition to these systems to enhance those overlooked areas. Prior research has shown LLM's capable of assisting students in understanding and resolving programmer error messages, correcting syntax errors, providing enhanced explanations of code segments, or even generating code. The absence of freely available, purpose-designed LLMs for grading and providing feedback on code submissions prevents widespread adoption by educators. Remotely-hosted systems, such as fine-tuned GPT models, have shown promise, yet the associated risks of privacy breaches, ethical considerations, and recurring costs make this approach unfeasible as a universal solution. To mitigate these concerns, self-hosted open-source models are an alternative that can operate on consumer-grade hardware and prevent some privacy and security concerns. While no purpose-built solution yet exists, it is unclear if any existing models are powerful enough to facilitate automated grading. To explore these questions, we present a two-phase analysis, leveraging real grading data from a semester length, introductory CSI course with 124 students and nine programming projects. Nine stable LLM models were selected and repeatedly prompted to grade student submissions using the same context that a human teaching assistant (TA) was given. This paper analyzes 1,172,383 API requests, totaling 33.4 days of active runtime, evaluating model consistency, ability to adhere to specified constraints, and comparison to human-generated grades. The results show various models' inability to consistently grade assignments, albeit with some exceptions. The importance of providing comprehensive context to models was highlighted, as incomplete contexts resulted in worse performance. Other models struggled with longer prompts, delivering less consistent results. Despite disparities between AI-generated and human-assigned grades, the potential for refinement is clear; improved rubrics or selective fine-tuning could enhance model output. Future work will focus on analyzing models' qualitative justifications for grades, refining rubrics, training on domain-specific datasets, and fine-tuning the highest performing models to potentially improve grading accuracy.",Yes,"본 논문은 자체 호스팅된 대형 언어 모델을 활용하여 컴퓨터 과학 입문 과제의 자동 채점 가능성을 실험하고 평가하는 독창적인 연구를 수행하였다. 실제 학생 과제 데이터를 기반으로 여러 모델의 성능을 분석하고, 모델 개선 방향을 제시하는 등 직접적인 연구 기여가 포함되어 있다."
Prognostics and Health Management: A Review of Vibration Based Bearing and Gear Health Indicators,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115325,"Prognostics and health management is an emerging discipline to scientifically manage the health condition of engineering systems and their critical components. It mainly consists of three main aspects: construction of health indicators, remaining useful life prediction, and health management. Construction of health indicators aims to evaluate the system's current health condition and its critical components. Given the observations of a health indicator, prediction of the remaining useful life is used to infer the time when an engineering systems or a critical component will no longer perform its intended function. Health management involves planning the optimal maintenance schedule according to the system's current and future health condition, its critical components and the replacement costs. Construction of health indicators is the key to predicting the remaining useful life. Bearings and gears are the most common mechanical components in rotating machines, and their health conditions are of great concern in practice. Because it is difficult to measure and quantify the health conditions of bearings and gears in many cases, numerous vibration-based methods have been proposed to construct bearing and gear health indicators. This paper presents a thorough review of vibration-based bearing and gear health indicators constructed from mechanical signal processing, modeling, and machine learning. This review paper will be helpful for designing further advanced bearing and gear health indicators and provides a basis for predicting the remaining useful life of bearings and gears. Most of the bearing and gear health indicators reviewed in this paper are highly relevant to simulated and experimental run-to-failure data rather than artificially seeded bearing and gear fault data. Finally, some problems in the literature are highlighted and areas for future study are identified.",No,본 논문은 진동 기반 베어링 및 기어 건강 지표에 대한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 정리와 향후 연구 방향 제시에 중점을 두고 있습니다.
Cobots for FinTech,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591113,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",Yes,"논문은 팬데믹 상황에서 금융 서비스용 임베디드 장치의 소프트웨어 개발을 가속화하기 위해 로봇 공학, IoT, AI를 활용한 자동화 시스템을 개발하고 적용한 구체적인 연구 내용을 다루고 있다. 이는 독창적인 기술적 접근과 구현 사례를 포함한 연구 논문으로 판단된다."
Predicting nuclear fuel parameters by using machine learning techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650410,"Today, machine learning techniques are widely used to solve complex problems at computing level. One of these techniques is the vector support machine which has been used in various applications. This technique will be used to solve a problem relating to the nuclear industry and its control through the Treaty on the Non-Proliferation of Nuclear Weapons (NPT). Specifically, this work seeks to create a model capable of predicting certain parameters of importance in irradiated fuels, under an analysis of a number of characteristics. For this purpose, a simulated database, with a significant number of irradiated fuels, was used. The analysis process began with a treatment of the information, with the purpose of using machine learning tools. Subsequently, a validation process of the built model took place, where experimentation will contrast two models with different amounts of fuel characteristics. Finally, an Boruta analysis is carried out to analyze and obtain a measure of the importance over the importance parameters. The results showed that the two models obtained a high perform even when the number of features were substantially different. Even models with the use of hyperparameter tuning improve performance, as demonstrated below.",Yes,"논문은 핵연료의 특정 파라미터를 예측하기 위해 머신러닝 기법을 적용하고, 시뮬레이션 데이터베이스를 활용하여 모델을 구축 및 검증하는 과정을 다루고 있다. 이는 직접적인 연구 방법론과 실험 결과를 포함한 독창적인 연구 내용으로 판단된다."
P1522: a formal standard for testability and diagnosability measures,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=800409,"Members of the Maintenance and Diagnostic Control subcommittee of IEEE's Standards Coordinating Committee 20 (SCC20) are developing a standard for testability and diagnosability characteristics and metrics. The objective of this standard, P1522, is to provide notionally correct, useful, and mathematically precise definitions of testability measures that may be used to either measure or predict the testability of a system. Notionally correct means that the measures are not in conflict with intuitive and historical representations. The end purpose is to provide an unambiguous source for definitions of testability and diagnosability metrics. In this paper, we present a summary of the work completed so far on P1522 and a roadmap for its completion. We cover the organization of the standard, the sources of the measures, how these measures relate to the other AI-ESTATE standards, and information modeling.",No,"본 논문은 IEEE 표준 개발 작업에 대한 요약과 로드맵을 제시하고 있으며, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 표준화 작업 보고서에 가깝다고 판단됩니다."
QSAnglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585733,"Developing sophisticated artificial intelligence (AI) systems requires AI researchers to experiment with different designs and analyze results from evaluations (we refer this task as evaluation analysis). In this paper, we tackle the challenges of evaluation analysis in the domain of question-answering (QA) systems. Through in-depth studies with QA researchers, we identify tasks and goals of evaluation analysis and derive a set of design rationales, based on which we propose a novel approach termed prismatic analysis. Prismatic analysis examines data through multiple ways of categorization (referred as angles). Categories in each angle are measured by aggregate metrics to enable diverse comparison scenarios. To facilitate prismatic analysis of QA evaluations, we design and implement the Question Space Anglyzer (QSAnglyzer), a visual analytics (VA) tool. In QSAnglyzer, the high-dimensional space formed by questions is divided into categories based on several angles (e.g., topic and question type). Each category is aggregated by accuracy, the number of questions, and accuracy variance across evaluations. QSAnglyzer visualizes these angles so that QA researchers can examine and compare evaluations from various aspects both individually and collectively. Furthermore, QA researchers filter questions based on any angle by clicking to construct complex queries. We validate QSAnglyzer through controlled experiments and by expert reviews. The results indicate that when using QSAnglyzer, users perform analysis tasks faster (p <; 0.01) and more accurately (p <; 0.05), and are quick to gain new insight. We discuss how prismatic analysis and QSAnglyzer scaffold evaluation analysis, and provide directions for future research.",Yes,"본 논문은 QA 시스템 평가 분석을 위한 새로운 시각적 분석 도구(QSAnglyzer)와 분석 방법론(프리즘 분석)을 제안하고, 이를 실험과 전문가 평가를 통해 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Meta-Learning Applications in Digital Image Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145478,"In recent decades, advances in capture devices and the increase of available digital image data have stimulated the creation of methodologies for data processing that produce various forms of valuable models, such as descriptors, classifiers, approximations and visualizations. These models are often developed in the field of machine learning, which is characterized by a large number of available algorithms. These algorithms often do not have guidelines to identify the most appropriate one based on specific data to which they will be applied and the nature of the problem under analysis. There is a knowledge that allows to relate the features of the algorithms and data that present a good performance to fulfill a specific task, known as Meta-Knowledge, which can include information on algorithms, evaluation metrics to calculate similarity of datasets or relation of tasks. Being Meta-Learning the study of methods based on principles that explore the Meta-Knowledge to obtain efficient models and solutions, adapting the processes of Machine Learning and Data Mining. The research carried out in this work analyzes the applications and advantages offered by Meta-Learning in the field of digital image processing. To carry out this task, different types of images, characterizers, and feature analysis techniques are used. The results obtained show that methodology based on Meta-Learning is efficient when applied in the processing of digital images for identification and storage of experience generated by developing methodologies for classification of different types of images, obtaining a high performance with respect to an evaluation metrics. This statement means that Meta-Learning allows recommending the most appropriate methodology to perform the processing of a specific type of image based on features of the dataset under analysis and the type of specific task to be performed.",Yes,"논문 초록에서 Meta-Learning을 활용한 디지털 이미지 처리 방법론을 분석하고, 다양한 이미지 유형과 특성 분석 기법을 적용하여 성능 평가 결과를 제시하고 있다. 이는 기존 연구를 종합하는 수준이 아니라, 직접적인 실험과 평가를 통해 새로운 연구 결과를 도출한 독창적인 연구임을 나타낸다."
Implementation of Deep Joint Source-Channel Coding on 5G Systems for Image Transmission,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10333673,"Deep joint source-channel coding (JSCC) has been attracting attention for achieving task-oriented communication. It replaces traditional information source coding and channel coding with a deep learning-based autoencoder, directly mapping information sources such as images to IQ symbols. For images, it is claimed to avoid the cliff effect and achieve a higher peak signal noise ratio (PSNR) even in low SNR regions. While related work has assumed various propagation channel models and validated the effectiveness of Deep JSCC, there are few reports confirming its principles through experiments. Specifically, to the best of our knowledge, there are no reported examples of experiments of Deep JSCC in 5G systems. In this paper, we present a proof-of-concept of Deep JSCC in a 5G system. We modified commercially available 5G base stations (gNB) and 5G terminals to enable input and output of IQ data from external devices. We connect the 5G devices using coaxial cables and attenuators, transmit and receive JSCC signals, and evaluate the PSNR. The results demonstrate that even when communicating at power levels lower than the minimum receiver sensitivity specified in the receiver’s datasheet, the image can be successfully restored with less than 1 dB degradation in PSNR compared with the simulation result.",Yes,본 논문은 Deep JSCC를 5G 시스템에 실제로 구현하고 실험을 통해 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 기존 연구의 시뮬레이션 결과를 넘어 실제 5G 장비를 개조하여 실험을 수행한 점에서 직접적인 연구 기여가 있다고 판단된다.
Image Colorization: Comprehensive Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10742818,"This paper has conducted a detailed and extensive analysis of the algorithms applied in image colorization. Specifically this paper analyses the application of the algorithms combinations of the approaches the performance of the algorithms considering different parameters. This paper also reviews and contrasts the research methodologies suggested. The analysis is conducted to understand how the algorithms really work what are the drawbacks of the applications how the systems operate under different conditions. This paper also suggests possible areas for further research such as integration of different image features. The implementation of multi modal pictures for matching and colorizing, the study of super resolution. Overall this paper aimed to provide a comprehensive guide for further development of algorithms in the image colorization area. The paper also highlights new directions and challenges in the field of image colorization including the development of real time colorization systems for interactive applications the exploration of novel evaluation metrics for evaluating colorization quality and the integration of machine learning techniques with domain specific knowledge. This paper acts as a roadmap for improving the state of the art in image colorization encouraging interdisciplinary collaboration and ultimately improving the visual content creation process across various domains by synthesizing existing research findings and suggesting directions for future exploration.",No,"본 논문은 이미지 컬러화 알고리즘에 대한 기존 연구들을 종합적으로 분석하고 리뷰하는 내용을 담고 있으며, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하고 있지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
The contribution of Machine Learning and Eye-tracking technology in Autism Spectrum Disorder research: A Review Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493357,"According to Diagnostic and Statistical Manual of Mental Disorders, Autism spectrum disorder (ASD) is a developmental disorder characterised by reduced social interaction and communication, and by restricted, repetitive, and stereotyped behaviour. An important characteristic of autism, referred in several diagnostic tests, is a deficit in eye gaze. The objective of this study is to review the literature concerning machine learning and eye-tracking in ASD studies conducted since 2015. Our search on PubMed identified 18 studies which used various eye-tracking instruments, applied machine learning in different ways, distributed several tasks and had a wide range of sample sizes, age groups and functional skills of participants. There were also studies that utilised other instruments, such as Electroencephalography (EEG) and movement measures. Taken together, the results of these studies show that the combination of machine learning, and eye-tracking technology can contribute to autism identification characteristics by detecting the visual atypicalities of ASD people. In conclusion, machine learning and eye-tracking ASD studies could be considered a promising tool in autism research and future studies could involve other technological approaches, such as Internet of Things (IoT), as well.",No,"이 논문은 2015년 이후의 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 요약하고 평가하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 동향과 가능성을 논의하는 성격입니다."
Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475720,"The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs’ writing proficiency within AES.To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adversarial Weights Perturbation (AWP) and Metric-specific AttentionPooling (6 kinds of AP) for each label in the competition.The primary focus of this research is to investigate the impact of hyperparameters, particularly the adversarial learning rate, on the performance of the model. By fine-tuning the hyperparameter tuning process, including the influence of 6AP and AWP, the resulting models can provide more accurate evaluations of language proficiency and support tailored learning tasks for ELLs. This work has the potential to significantly benefit ELLs by improving their English language proficiency and facilitating their educational journey.",Yes,본 논문은 DeBERTa 모델에 Adversarial Weights Perturbation과 Metric-specific AttentionPooling 기법을 적용하여 자동 에세이 채점 성능을 향상시키는 구체적인 방법론과 하이퍼파라미터 튜닝 결과를 제시하고 있다. 이는 기존 연구를 바탕으로 한 독창적인 실험과 분석을 포함한 연구 논문으로 판단된다.
Multi-Layout Unstructured Invoice Documents Dataset: A Dataset for Template-Free Invoice Processing and Its Evaluation Using AI Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9481217,"The daily transaction of an organization generates a vast amount of unstructured data such as invoices and purchase orders. Managing and analyzing unstructured data is a costly affair for the organization. Unstructured data has a wealth of hidden valuable information. Extracting such insights automatically from unstructured documents can significantly increase the productivity of an organization. Thus, there is a huge demand to develop a tool that can automate the extraction of key fields from unstructured documents. Researchers have used different approaches for extracting key fields, but the lack of annotated and high-quality datasets is the biggest challenge. Existing work in this area has used standard and custom datasets for extracting key fields from unstructured documents. Still, the existing datasets face some serious challenges, such as poor-quality images, domain-related datasets, and a lack of data validation approaches to evaluate data quality. This work highlights the detailed process flow for end-to-end key fields extraction from unstructured documents. This work presents a high-quality, multi-layout unstructured invoice documents dataset assessed with a statistical data validation technique. The proposed multi-layout unstructured invoice documents dataset is highly diverse in invoice layouts to generalize key field extraction tasks for unstructured documents. The proposed multi-layout unstructured invoice documents dataset is evaluated with various feature extraction techniques such as Glove, Word2Vec, FastText, and AI approaches such as BiLSTM and BiLSTM-CRF. We also present the comparative analysis of feature extraction techniques and AI approaches on the proposed multi-layout unstructured invoice document dataset. We attained the best results with BiLSTM-CRF model.",Yes,"본 논문은 다중 레이아웃의 비정형 송장 문서 데이터셋을 새롭게 구축하고, 이를 다양한 AI 기법으로 평가하는 독창적인 연구 내용을 포함하고 있다. 데이터셋 제작과 평가 방법론 제시 및 성능 비교 분석을 통해 직접적인 연구 기여를 하고 있으므로 연구 논문에 해당한다."
"IoT based Weather, Soil, Earthquake, and Air Pollution Monitoring System",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083932,"This research study shows how IoT technology can screen local meteorological conditions and share that information globally. Weather shifts cause extreme rainfall. A flood monitoring system uses NODEMCU ESP8266 to store and retrieve data and inform authorities of rising water levels using ultrasonic sensors and LEDs. Soil moisture affects crop growth. Its microprocessor and sensor improve soil moisture monitoring. An earthquake warning system can detect the slightest vibration before a major earthquake. Due to industry and autos, air quality is getting worse. Air quality and chemical content must be assessed using IoT since it has changed so much. Connected devices and enhanced sensor technology have transformed traditional environmental monitoring into a cutting-edge Smart Environment Monitoring System (SEMS). This paper evaluates SEM aids and research investigations, including air quality, weather, soil, and seismic monitoring systems. SEM applications segment the examination, with a deeper dig into each section's sensors. Discussion findings and analyzed research patterns form the basis for the in-depth analysis that follows the comprehensive review and suggests key SEMS research implications. The authors studied how IoT, machine learning, and other sensorbased advancements have made environmental monitoring smart.",No,논문 초록은 IoT 기반 환경 모니터링 시스템에 대한 기존 연구와 기술들을 평가하고 종합적으로 검토하는 리뷰 성격을 띠고 있습니다. 독창적인 실험 결과나 새로운 연구 기여보다는 기존 연구 동향 분석과 시스템 구성에 중점을 둔 것으로 보입니다.
"DDoS Attacks: Detection Techniques, Challenges, and Modern Practices",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10940527,"Internet, one of the essential components of today's civilization, serves numerous purposes for individuals, businesses, and society. However, its extensive use has sparked concerns, especially regarding privacy and cybersecurity. Furthermore, cyber dangers are becoming more dangerous, intense, and complicated. Distributed Denial of Service (DDoS) attacks have evolved as a prevalent and substantial danger to cybersecurity that may disable the network infrastructures of targeted companies and providers. To guard from DDoS assaults, a many security measures are used, like firewalls and intrusion detection systems. Improving the protective capabilities of IDS frameworks using machine and deep learning, other associated technology, is a popular topic currently. Nevertheless, regardless of considerable improvements, identifying DDoS assaults using machine and deep learning, other associated techs remain a difficulty, particularly when dealing with new DDoS attack. Consequently, this review aims to comprehensively discuss about DDoS attacks by going through the contemporary efforts made in the literature to counter the danger due to the DDoS attacks. First, we investigate certain DDoS attack-related solutions suggested by today’s investigators. Finally, we delve deeper by identifying the domains wherein the DDoS attacks are prone to take place; common challenges in recognizing the DDoS attacks in the IoT circumstance; advantages of Software-Defined Networking (SDN); state-of-the-art practices in the academic community to counter DDoS attack attempts in any networks of IoT or SDN or web-connected devices.",No,"초록에서 해당 논문은 기존 문헌을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험적 기여에 대한 언급이 없다. 따라서 본 논문은 연구 논문보다는 기존 연구를 정리하고 분석하는 리뷰 논문으로 판단된다."
Decoding of Auditory Imagination Activity Based on Machine Learning Methods,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075201,"The decoding of brain signals has important research value and is also full of challenges. In recent years, the fMRI technology is more and more widely used in the brain signals decoding because of its high spatial resolution and non-invasive characteristic. The existing brain region template provides a powerful tool for brain region decoding research, and the obtained functional connectivity matrix quantifies the correlation between brain regions. The development of pattern recognition technology has created a favorable technical foundation for the study of fMRI data, including a variety of mature machine learning methods such as support vector machine. In this paper, the fMRI data of 24 healthy subjects was measured when they performed the auditory imagination under different sound and scene conditions, then the functional connectivity matrices of brain regions were obtained and to be classified by several pattern recognition methods, such as optimized support vector machine, naive Bayes, and logistic regression. At last, the classification methods were compared according to the classification accuracy. Compared with related works, Methods of data acquisition and the accuracy of the results are better than previous job. The method can help psychologists and neuroscientists perform the brain signal decoding with high efficiency and quality. At the same time, the research is helpful to reveal the neural mechanism of auditory imagination and auditory perception, deepen our understanding of human brain auditory information processing, help the computer to simulate human hearing, and use the correlation of brain signals to help the deaf and mute people to carry out related activities, which has good social benefits.",Yes,"본 논문은 fMRI 데이터를 이용해 청각 상상 활동을 기계학습 방법으로 분류하는 실험을 수행하고, 여러 분류 기법의 성능을 비교하는 독창적인 연구 내용을 포함하고 있다. 또한, 기존 연구 대비 데이터 획득 방법과 결과 정확도가 향상되었음을 명시하여 직접적인 연구 기여가 있음을 보여준다."
Evaluation of fruit selection with ensemble model in the Peruvian industry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10326089,"This paper allows to evaluate fruits with the ensemble algorithm with artificial vision, besides, it evaluates the systematic review with the PRISMA methodology thought the last 10 years. Our findings are the application of TensorFlow and self-supervised multi-network fusion classification models. Some researchers describe classification errors. The deep learning methodology is also an efficient way of classifying and applying algorithms that optimally analyze features such as color, shape, and texture of the object. On the other hand, we can implement artificial intelligence to this work since in recent years it has been developing in all industrial sectors and is bringing positive results. Finally, it uses a case study with the limitations to admit safe practices in a company dedicated to agricultural production and also over time that when studying the last cycle and at the same time working, the availability of time is limited to be able to use it in a work area. The results are an mean absolute error for size evaluation of 92.92%, color evaluation with 93.23%, and the assessment of the maturity, defects and quality of 97.72% after the evaluation of 1271 research articles with the systematic review process.",Yes,"논문은 인공신경망과 앙상블 모델을 활용한 과일 평가 방법을 제안하고, 실제 사례 연구를 통해 성능 지표를 제시하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 딥러닝 기반 분류 모델의 적용과 평가 결과를 구체적으로 다루고 있어 연구 논문에 해당한다."
An Estimation of the Performance of Deep Learning Based Hard Link Boot Caffe Neural Network for Network Anomaly Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452430,"As internet technology has rapidly advanced in recent years, so too has the range of cyber-attacks. In the current state of affairs in the cyber world, the ability to detect such attacks is more crucial than ever. Researchers from a wide variety of fields have increasingly looked to machine learning (ML) and deep learning (DL) techniques to help them solve their difficulties. In this study, we provide a system for identifying network attacks using deep learning. In this work, initially the database was retrieved and data preprocessing was done by using the Minmax error splash method. Then the attack related features are extracted using the Sigmoid polychain component analysis. Then Stochastic convergence Adam optimization algorithm was used for feature selection. The purpose of using optimization is to improve the classifier accuracy. This technique extracts a specialized features that are related to malicious code that can be used to classify the attack. The extracted features are given as a input to the Hard link boot caffe memory neural network classifier. The presented method is evaluated on a NSL-KDD dataset, achieving a high attack detection accuracy, which makes it the best among the competing approaches.",Yes,"본 논문은 네트워크 공격 탐지를 위한 딥러닝 기반 시스템을 제안하고, 데이터 전처리, 특징 추출, 최적화 알고리즘 적용, 그리고 새로운 신경망 분류기(Hard link boot caffe memory neural network)를 사용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Analysis and review of current deep learning techniques for dental image segmentation with a novel deep neural network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235850,"Dental image segmentation is an important task in dental image analysis and plays a crucial role in various dental applications, such as diagnosis, treatment planning, and clinical research. With the advent of different imaging modalities, there has been a significant increase in the complexity and variability of dental images, thus making the dental image segmentation task more challenging. Recent advancements in deep learning techniques have led to significant improvements in dental image segmentation accuracy and efficiency. In this paper, some of the current deep-learning techniques that are used for dental image segmentation on various imaging modalities have been discussed. A comprehensive review is carried out, which provides the details of current deep learning methodologies, including types of deep architecture used, different imaging modalities, and datasets along with their applications and shortcomings. Additionally, a novel deep learning methodology based on a dual stream encoder and decoder architecture is proposed for automatically segmenting panoramic images. The evaluation is carried out on 1000 image dataset and is measured by dice coefficient, jaccard index, accuracy, precision and recall. The proposed methodology performs better than the state-of-the-art deep segmentation models.",Yes,"논문은 현재 딥러닝 기법들에 대한 종합적인 리뷰를 제공하는 동시에, 새로운 이중 스트림 인코더-디코더 아키텍처 기반의 독창적인 딥러닝 방법론을 제안하고 평가한 연구 결과를 포함하고 있다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문에 해당한다."
"Machine Learning in the Prevention, Diagnosis and Management of Diabetic Foot Ulcers: A Systematic Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246496,"Diabetic foot ulcers (DFUs) are a serious complication for people with diabetes. They result in increased morbidity and pressures on health system resources. Developments in machine learning (ML) offer an opportunity for improved care of individuals at risk of DFUs, to identify and synthesise evidence about the current uses and accuracy of ML in the interventional care and management of DFUs, and, to provide a reference for areas of future research. PubMed, Google Scholar, Web of Science and Scopus were searched using the Preferred Reporting Items for a Systematic Review and Meta-analysis of Diagnostic Test Accuracy Studies (PRISMA-DTA) guidelines for papers involving ML and DFUs. In order to be included, studies needed to mention ML, DFUs, and report relevant outcome measures regarding ML algorithm accuracy. Bias in included studies was assessed using the quality assessment tool for diagnostic accuracy (QUADAS-2). 37 out of 3769 papers were included after applying eligibility criteria. Included papers reported accuracy measures for multiple types of ML algorithms in DFU studies. Whilst varying across the ML algorithm used, all studies reported at least 90% accuracy compared to gold standards using a minimum of one reported ML algorithm for processing or recording data. Applications where ML had positive effects on DFU data analysis and outcomes include image segmentation and classification, raw data analysis and risk assessment. ML offers an effective and accurate solution to guide analysis and procurement of data from interventions which are designed for the care of DFUs in small samples and study conditions. Current research is limited, and, for the development of more applicable ML algorithms, future research should address the following: direct comparison of ML applications with current standards of care, health economic analyses and large scale data collection. There is currently no evidence to confidently suggest that ML methods in DFU diagnosis are ready for implementation and use in healthcare settings.",No,"본 논문은 머신러닝을 이용한 당뇨병성 족부궤양 연구에 대한 기존 문헌을 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합과 평가에 초점을 맞추고 있습니다."
Laparoscope Manipulating Robot (LMR) Navigation using Deep Learning-based Surgical Instruments Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9980059,"During laparoscopic surgery, several items, including the laparoscope and all the surgical instruments must be manipulated at the same time. This paper presents the implementation of a surgical instrument detection system based on the deep learning methodology to work with the Laparoscope Manipulating Robot (LMR). Two procedures in this work are object detection on surgical instruments and then laparoscope navigation. The position information of detected surgical instruments is feed-backed to navigate the laparoscope automatically. The object detection comprises three main processes, the training, validating, and testing processes. Dataset in the experiments were gathered from 5 various public YouTube video clips related to gynecologic surgery. The previous work has compared various algorithms on surgical instrument detection. YOLOv4 provided the best performance with the validation result of the detection model across the experiment datasets with F1-score of 93.50% at the Confidence Threshold over 48% on the average during the training process. Hence YOLOv4 has been chosen to be assigned to the object detector. And as for the navigation process in the real-time surgical operation, once all surgical instruments have been detected, the focus of the laparoscope should be in the center among those instruments. The center position of each detected surgical instrument has been used to calculate the center of the laparoscope. To validate the viability of on-the-fly surgical instrument detection and laparoscope navigation, the proposed system has been implemented and tested on two test-cases of gynecologic surgery with the soft-tissue cadavers. The real tests have shown that the proposed method works well with two and three surgical instruments detection, but not with a single instrument detection, as the information on the position of a single surgical instrument detected is not enough to navigate the laparoscope to a designated object.",Yes,본 논문은 딥러닝 기반의 수술 기구 검출 및 복강경 내비게이션 시스템을 구현하고 실제 테스트를 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다.
Comparative Analysis of Control Strategies for Microgrid Energy Management with a Focus on Reinforcement Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749831,"The depletion of fossil fuel reserves and the urgent need to cut greenhouse gas emissions are driving a significant shift in the global energy sector. This transformation necessitates advanced energy management strategies in microgrids to integrate renewable energy sources efficiently. Traditional methods, including classical, metaheuristic, model predictive, stochastic, and robust control techniques, have been extensively studied. However, these approaches often struggle with slow performance and high computational demands, making them less effective for real-time applications due to the need for frequent re-optimization. In contrast, reinforcement learning, a branch of machine learning, excels by continuously learning and optimizing through real-time interactions This approach offers greater flexibility and adaptability in complex and dynamic environments. This paper provides a comparative review of various control strategies for microgrid energy management, examining their strengths and limitations. We place a particular focus on reinforcement learning algorithms and their application in microgrids. This paper conducts a detailed analysis of studies on reinforcement learning-based energy management systems, evaluating their potential to enhance energy efficiency, optimize usage, and contribute to the sustainability of modern energy infrastructures. This review aims to enhance understanding in this field and offer insights for future research on smart energy management systems.",No,"본 논문은 다양한 제어 전략에 대한 비교 분석과 강화학습 기반 에너지 관리 시스템 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 분석하는 데 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 이해와 향후 연구 방향 제시에 초점이 맞춰져 있습니다."
"On the Evaluation, Management and Improvement of Data Quality in Streaming Time Series",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845398,"The Internet of Things (IoT) technologies plays a key role in the Fourth Industrial Revolution (Industry 4.0). This implies the digitisation of the industry and its services to improve productivity. To obtain the necessary information throughout the different processes, useful data streams are obtained to provide Artificial Intelligence and Big Data algorithms. However, strategic decision-making based on these algorithms may not be successful if they have been developed based on inadequate low-quality data. This research work proposes a set of metrics to measure Data Quality (DQ) in streaming time series, and implements and validates a set of techniques and tools that allow monitoring and improving the quality of the information. These techniques allow the early detection of problems that arise in relation to the quality of the data collected; and, in addition, they provide some mechanisms to solve these problems. Later, as part of the work, a use case related to industrial field is presented, where these techniques and tools have been deployed into a data management, monitoring and data analysis platform. This integration provides additional functionality to the platform, a Decision Support System (DSS) named DQ-REMAIN (Data Quality REport MAnagement and ImprovemeNt), for decision-making regarding the quality of data obtained from streaming time series.",Yes,논문은 스트리밍 시계열 데이터의 품질을 평가하고 개선하기 위한 새로운 지표와 기법을 제안하고 이를 구현 및 검증하는 연구 내용을 포함하고 있다. 또한 실제 산업 현장에 적용한 사례를 통해 독창적인 연구 기여를 명확히 보여주고 있다.
Feature Extraction and Selection for Identifying Faults in Contactors Using Fiber Bragg Grating,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192542,"Switching devices are used in a wide application field to control and protect electrical systems. Failures in such equipment cause a loss of reliability in electrical facilities, which can lead to catastrophic consequences. The main advantage of using optical sensors is their immunity to the electromagnetic field, allowing installation in unfeasible locations compared to other technologies presented in related works. Consequently, the proposed approach consists of a new application employing fiber Bragg grating (FBG) to measure dynamic strain signals while switching a low-voltage contactor and develop a signal processing algorithm to extract and select features for classification using supervised learning methods. The models were trained and validated with different measurement sets, dividing them into intermediate and critical wear-out stages. The test procedures were carried out in a controlled manner replacing the contactor’s main internal components. Two feature extraction methods were evaluated. The first calculates the power spectral density (PSD) and the switching time, while the second considers the coefficients generated by the wavelet scattering transform (WST). With maximum relevance and minimum redundancy (mRMR) and the support vector machine (SVM) algorithms, it was possible to identify components states, obtaining an accuracy of 99.4% for cross validation, 100% for validation dataset, and 86.4% for the new test dataset. The results demonstrate that the proposed system can recognize critical faults and is promising to be applied in other types of commutation equipment in future applications striving to increase the complexity of the evaluated devices.",Yes,"본 논문은 광섬유 브래그 격자(FBG)를 이용한 동적 변형 신호 측정과 신호 처리 알고리즘 개발, 특징 추출 및 선택, 그리고 감독 학습 기반 분류 모델 훈련 및 검증을 포함한 독창적인 연구 내용을 다루고 있다. 또한, 실험적 테스트와 성능 평가를 통해 새로운 접근법의 유효성을 입증하고 있어 연구 논문에 해당한다."
Artificial intelligence and computer science in education: From kindergarten to university,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7757570,"Artificial Intelligence (AI) already plays a major role in our daily life (e.g. intelligent household appliances like robotic vacuum cleaners or AI-based applications like Google Maps, Google Now, Siri, Cortana, ...). Sound knowledge about AI and the principles of computer science will be of vast importance for future careers in science and engineering. Looking towards the near future, jobs will largely be related to AI. In this context literacy in AI and computer science will become as important as classic literacy (reading/writing). By using an analogy with this process we developed a novel AI education concept aiming at fostering AI literacy. The concept comprises modules for different age groups on different educational levels. Fundamental AI/computer science topics addressed in each module are, amongst others, problem solving by search, sorting, graphs and data structures. We developed, conducted and evaluated four proof-of-concepts modules focusing on kindergarten/primary school as well as middle school, high school and university. Preliminary results of the pilot implementations indicate that the proposed AI education concept aiming at fostering AI literacy works.",Yes,"논문은 AI 교육 개념을 새롭게 개발하고, 이를 다양한 교육 단계에 적용하여 파일럿 모듈을 설계, 실행, 평가한 연구 내용을 포함하고 있다. 이는 독창적인 연구 기여와 실험적 검증을 포함하는 연구 논문으로 판단된다."
Paddy Leaf Disease Detection Using Fine-tuned EfficientNetB4 Convolutional Neural Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10726047,"Agriculture remains the backbone of many countries since it acts as the main source of income and food. In this regard, different food crops are significant in the global economy, where rice occupies a very central position as one of the most important staple foods in many Asian countries. However, rice cultivation encounters various diseases at the different stage affecting either the quality or the rate of growth in the crop. Diagnosis of these diseases can be tiresome by applying traditional methods and use of naked eyes because symptoms of most diseases are often related. Therefore, the use of automated systems and the integration of such systems adds more realization to disease detection, hence informing the farmer to take appropriate measures on time to minimize crop or plant losses in order to yield quality produce. In the last several years different approaches utilizing image processing, machine learning, and deep learning algorithms have been investigated by researchers to create effective automatic systems. Therefore, this paper aims at optimizing several of the existing highly accurate deep learning-based state-of-art Convolutional neural networks for the early detection of different rice crop diseases, also known as paddy leaf diseases. This work also pointed out a comparative assessment of the utilized techniques through precision, accuracy, recall, and F1-score metrics with a finetuned EfficientNetB4 model with an optimum F1-score of approximately 99. 7%.",Yes,"논문은 기존의 딥러닝 기반 컨볼루션 신경망 모델을 최적화하고, 이를 통해 벼 잎 질병을 조기 탐지하는 독창적인 연구를 수행한 내용을 담고 있습니다. 또한, 다양한 성능 지표를 통해 모델을 평가한 점에서 직접적인 연구 기여가 포함된 연구 논문으로 판단됩니다."
Adapting Software Architectures to Machine Learning Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825909,"Unique developmental and operational characteristics of machine learning (ML) components as well as their inherent uncertainty demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings and (iii) a survey to quantitatively validate the challenges and their solutions. We compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along with new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability. Using the survey we were able to establish a link between architectural solutions and software quality attributes, which enabled us to provide twenty architectural tactics used to satisfy individual quality requirements of systems with ML components. Altogether, the results of the study can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.",Yes,"본 논문은 체계적 문헌 검토, 인터뷰, 설문조사를 통해 머신러닝 컴포넌트를 통합하는 소프트웨어 아키텍처의 도전과 해결책을 실증적으로 연구하고 있으며, 이를 바탕으로 20개의 아키텍처 전술을 제시하는 등 독창적인 연구 결과를 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Distributed Robotic Systems in the Edge-Cloud Continuum with ROS 2: a Review on Novel Architectures and Technology Readiness,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10062523,"Robotic systems are more connected, networked, and distributed than ever. New architectures that comply with the de facto robotics middleware standard, ROS 2, have recently emerged to fill the gap in terms of hybrid systems deployed from edge to cloud. This paper reviews new architectures and technologies that enable containerized robotic applications to seamlessly run at the edge or in the cloud. We also overview systems that include solutions from extension to ROS 2 tooling to the integration of Kubernetes and ROS 2. Another important trend is robot learning, and how new simulators and cloud simulations are enabling, e.g., large-scale reinforcement learning or distributed federated learning solutions. This has also enabled deeper integration of continuous interaction and continuous deployment (CI/CD) pipelines for robotic systems development, going beyond standard software unit tests with simulated tests to build and validate code automatically. We discuss the current technology readiness and list the potential new application scenarios that are becoming available. Finally, we discuss the current challenges in distributed robotic systems and list open research questions in the field.",No,"본 논문은 새로운 아키텍처와 기술 동향을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석하는 개관 논문에 해당합니다."
Patient's Pain Recognition by Using Deep Models Based on Transfer Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10076192,"Undetected pain threatens the quality of human life by prevents them from understanding the nature of their pain. Therefore, an urgent need to find an alternative that can recognize pain in patients suffering from difficulty in express their pain for reasons related to age or the ability of explain feeling of pain. The structure of a proposed system depends on the transfer learning technique, where two pre-trained deep convolution neural network (ResNet-50and Xception) models, have been used as a feature extractor. These models receive the images from the database, which was collected especially for this work from 100 subjects of (10–55) ages in different times and natural environments. Extracted features, which formed a features vector, were taken from the Global Average Pooling (GAP) layer. These feature vectors are passed to a supervised classification method such as Support Vector Machine (SVM), K-Nearest Neighbour (KNN), and build a Deep Neural Network (DNN) classifier from scratch. The proposed model's performance was evaluated by comparing it with related work, which adopted the concept of a pre-trained model and used the database collected in a natural environment in the training and testing process. The proposed model demonstrates outstanding results by the combination of exploiting the Transfer Learning (TL) technique in the Xception model and building the DNN as a classifier. The accuracy of the proposed model was 98.17% which demonstrates promising results.",Yes,"본 논문은 환자의 통증 인식을 위해 전이 학습 기반의 딥러닝 모델을 제안하고, 직접 수집한 데이터셋을 활용하여 여러 분류기와 비교 평가를 수행하는 독창적인 연구 내용을 포함하고 있다. 따라서 새로운 방법론과 실험 결과를 제시하는 연구 논문에 해당한다."
Deep Learning Transfer with AlexNet for chest X-ray COVID-19 recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451239,"The COVID-19 is a new disease from the virus SARS-CoV-2, the infection can cause respiratory illness with symptoms such as cough, fever, and, in severe cases, pneumonia. Early diagnosis is crucial for the correct treatment to reduce as much as possible the stress in the healthcare system. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease. In this study, we have applied learning transfer to a convolutional neural network known as AlexNet for binary chest X-ray recognition (COVID-19 vs Healthy). We have fine-tunned AlexNet for our specific problem. The first layer, which works with RGB images, is replaced for images in a single intensity (grayscale). 11,312 chest X-ray images from six public databases were used to train the network. Among them are samples of healthy people and samples that present the effect of pneumonia and COVID-19 diseases. The results prove that deep learning with chest X-ray images can extract significant biomarkers related to COVID-19, since the obtained accuracy, sensitivity and specificity were 96.5%, 98.0%, and 91.7%, respectively. ROC analysis and confusion matrices are used to validate the results of the fine-tunned AlexNet network.",Yes,"본 논문은 AlexNet을 활용한 딥러닝 전이학습 기법을 적용하여 COVID-19 진단을 위한 흉부 X선 이미지를 분류하는 독창적인 연구를 수행하였으며, 구체적인 데이터셋과 실험 결과(정확도, 민감도, 특이도 등)를 제시하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Enhanced study on Deep learning model for kidney segmentation using DCE-MRI,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368681,"Medical imaging is capturing pictures of bodily components for diagnostic or research reasons. Because of advancements in image-handling techniques, which include picture recognition, examination, and upgrading, clinical imaging is expanding swiftly. Picture division is a vital subject in picture handling and PC vision with applications, for example, scene grasping, clinical picture examination, mechanical discernment, video observation, expanded reality, and picture pressure, among numerous others. Numerous picture segmentation methods have been created and are available in the literature. There has been a lot of work done to develop techniques for segmenting pictures using deep-learning models since they have proved effective in various vision applications. Deep learning has turned into a functioning exploration point in the field of clinical picture examination. Compared to the laborious and lengthy conventional technique, deep learning (DL) automatic detection algorithms can speed up diagnostics, increase test accuracy, lower expenses, and lessen the workload on the radiologist. Kidney segmentation is an integral part of any non-intrusive computer-assisted diagnostic (CAD) system for rapidly recognising severe renal repudiation because of the intensity of inhomogeneity brought on by errors in the image capture process, efficient kidney segmentation is challenging in evaluating and therapeutic planning of kidney-related disorders. This article addresses recent developments in DL-based kidney tumor segmentation systems. We highlight their fundamental components and numerous approaches as we discuss the main medical image kinds, segmentation methods, and evaluation standards for segmentation outcomes in kidney tumor++ segmentation.",Yes,"논문 초록에서 딥러닝 기반 신장 종양 분할 시스템의 최근 발전과 다양한 접근법, 평가 기준 등을 다루고 있어 직접적인 연구 기여와 새로운 방법론 제시가 포함된 연구 논문임을 알 수 있습니다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 판단됩니다."
Review on Ultrasound-Image Analysis of Fetus Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10126500,"Deep learning Techniques are rapidly becoming the industry level for analyzing fetal ultrasound pictures. Despite the significant number of survey studies that have previously been published on this subject, the majority focus on image analysis-related Medical Science or do not include all fetal Ultrasound deep learning applications. We have collected 30 research papers released after 2016; this study examines the most recent work on the subject. Each study is evaluated and commented on from both a technique and an application standpoint. The studies were considered for the analysis of anatomical fetus structure. The primary limits and open concerns for each category are mentioned. Summary tables are supplied to make it easier to compare the various techniques. Also presented are publicly accessible data sets and key performance metrics indicators widely used to evaluate algorithmic Behaviour regarding deep learning techniques used. Here studies wrap up with a condemning overview of the present date in Deep learning algorithms for fetal Ultrasound image processing and a conclusion of nowadays problems that researchers working in the field must address to transform research techniques into practical clinical practice.",No,"본 논문은 기존 연구들을 수집하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 중점을 두고 있습니다."
Smart Safety Surveillance System: Personal Protective Equipment and Drowsiness Detection in Industrial Environments,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724032,"The Intelligent Safety Surveillance System (ISSS) for Industrial Environments is a comprehensive solution designed to enhance workplace safety by monitoring Personal Protective Equipment (PPE) compliance and detecting worker drowsiness. Leveraging computer vision and machine learning technologies, the ISSS provides real time monitoring and proactive alerting mechanisms. This system employs strategically positioned cameras to detect and recognize PPE items such as helmets, safety masks, and gloves. Deep learning algorithms analyze video feeds to identify instances of noncompliance, triggering immediate alerts to workers and supervisors for corrective action. In addition to PPE compliance monitoring, the ISSS utilizes cameras equipped with infrared sensors to monitor workers’ facial expressions and eye movements for signs of drowsiness. Machine learning models are trained to recognize patterns indicative of drowsiness, such as drooping eyelids or prolonged eye closure. Upon detecting these signs, the system alerts workers and supervisors, mitigating the risk of accidents caused by fatigue-related errors. By combining advanced technologies with proactive safety measures, the ISSS aims to create a safer work environment in industrial settings. Its ability to detect non-compliance and drowsiness in real-time enables timely intervention, ultimately reducing the risk of workplace accidents and injuries.",Yes,본 논문은 개인 보호 장비 착용 여부와 졸음 감지를 위한 컴퓨터 비전 및 머신러닝 기반의 실시간 안전 감시 시스템을 제안하고 구현한 연구 내용을 포함하고 있다. 이는 독창적인 기술 적용과 알고리즘 개발을 통해 산업 현장의 안전성을 향상시키는 직접적인 연구 기여로 판단된다.
Network Intrusion Detection for IoT Security Based on Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629941,"Pervasive growth of Internet of Things (IoT) is visible across the globe. The 2016 Dyn cyberattack exposed the critical fault-lines among smart networks. Security of IoT has become a critical concern. The danger exposed by infested Internet-connected Things not only affects the security of IoT but also threatens the complete Internet eco-system which can possibly exploit the vulnerable Things (smart devices) deployed as botnets. Mirai malware compromised the video surveillance devices and paralyzed Internet via distributed denial of service attacks. In the recent past, security attack vectors have evolved bothways, in terms of complexity and diversity. Hence, to identify and prevent or detect novel attacks, it is important to analyze techniques in IoT context. This survey classifies the IoT security threats and challenges for IoT networks by evaluating existing defense techniques. Our main focus is on network intrusion detection systems (NIDSs); hence, this paper reviews existing NIDS implementation tools and datasets as well as free and open-source network sniffing software. Then, it surveys, analyzes, and compares state-of-the-art NIDS proposals in the IoT context in terms of architecture, detection methodologies, validation strategies, treated threats, and algorithm deployments. The review deals with both traditional and machine learning (ML) NIDS techniques and discusses future directions. In this survey, our focus is on IoT NIDS deployed via ML since learning algorithms have a good success rate in security and privacy. The survey provides a comprehensive review of NIDSs deploying different aspects of learning techniques for IoT, unlike other top surveys targeting the traditional systems. We believe that, this paper will be useful for academia and industry research, first, to identify IoT threats and challenges, second, to implement their own NIDS and finally to propose new smart techniques in IoT context considering IoT limitations. Moreover, the survey will enable security individuals differentiate IoT NIDS from traditional ones.",No,초록에서 해당 논문은 기존 연구들을 분류하고 비교하는 서베이(조사) 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구의 리뷰와 분석에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
SLEEP-SEE-THROUGH: Explainable Deep Learning for Sleep Event Detection and Quantification From Wearable Somnography,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102486,"Evidence is rapidly accumulating that multifactorial nocturnal monitoring, through the coupling of wearable devices and deep learning, may be disruptive for early diagnosis and assessment of sleep disorders. In this work, optical, differential air-pressure and acceleration signals, acquired by a chest-worn sensor, are elaborated into five somnographic-like signals, which are then used to feed a deep network. This addresses a three-fold classification problem to predict the overall signal quality (normal, corrupted), three breathing-related patterns (normal, apnea, irregular) and three sleep-related patterns (normal, snoring, noise). In order to promote explainability, the developed architecture generates additional information in the form of qualitative (saliency maps) and quantitative (confidence indices) data, which helps to improve the interpretation of the predictions. Twenty healthy subjects enrolled in this study were monitored overnight for approximately ten hours during sleep. Somnographic-like signals were manually labeled according to the three class sets to build the training dataset. Both record- and subject-wise analyses were performed to evaluate the prediction performance and the coherence of the results. The network was accurate (0.96) in distinguishing normal from corrupted signals. Breathing patterns were predicted with higher accuracy (0.93) than sleep patterns (0.76). The prediction of irregular breathing was less accurate (0.88) than that of apnea (0.97). In the sleep pattern set, the distinction between snoring (0.73) and noise events (0.61) was less effective. The confidence index associated with the prediction allowed us to elucidate ambiguous predictions better. The saliency map analysis provided useful insights to relate predictions to the input signal content. While preliminary, this work supported the recent perspective on the use of deep learning to detect particular sleep events in multiple somnographic signals, thus representing a step towards bringing the use of AI-based tools for sleep disorder detection incrementally closer to clinical translation.",Yes,"본 논문은 웨어러블 센서를 통해 수집된 신호를 기반으로 딥러닝 모델을 개발하고, 이를 이용해 수면 이벤트를 분류 및 정량화하는 독창적인 연구 내용을 포함하고 있다. 또한, 모델의 설명 가능성을 높이기 위한 기법들을 제안하고 실험을 통해 성능을 평가한 점에서 직접 기여하는 연구 논문으로 판단된다."
Extracting Human Levels of Trust in Human–Swarm Interaction Using EEG Signals,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423920,"Trust is an essential building block of human civilization. However, when it relates to artificial systems, it has been a barrier to intelligent technology adoption in general. This article addresses the gap in determining levels of trust in scenarios that include humans interacting with a swarm of robots. Electroencephalography (EEG) recordings of the human observers of the different swarms allow for extracting specific EEG features related to different trust levels. Feature selection and machine learning methods comprise a classification system that would allow recognition of different levels of human trust in those human–swarm interaction scenarios. The results of this study suggest that EEG correlates of swarm trust exist and are distinguishable in machine learning feature classification with very high accuracy. Moreover, comparing common EEG features across all human subjects used in this study allows for the generalization of the classification method, providing solid evidence of specific areas and features of the human brain where activations are related to levels of human–swarm trust. This work has direct implications for effective human–machine teaming with applications to many fields, such as exploration, search and rescue operations, surveillance, environmental monitoring, and defense. In these applications, quantifying levels of human trust in the deployed swarm is of utmost importance because it can lead to swarm controllers that adapt their output based on the human's perceived trust level.",Yes,"본 논문은 EEG 신호를 이용해 인간-스웜 상호작용에서 신뢰 수준을 추출하는 독창적인 연구를 수행하였으며, 머신러닝 기반 분류 시스템을 개발하여 신뢰 수준을 정확히 인식하는 결과를 제시하고 있다. 이는 직접적인 실험과 데이터 분석을 포함한 연구 논문에 해당한다."
Using Business Intelligence Dashboard for Sustainable Urban Water Security Monitoring,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150119,"This paper presents the findings from the experts on the effective factor of monitoring dashboards for urban water security. The urban water security dashboard monitors the water quality, availability, and other relevant indicators. Although monitoring dashboards exist, there are potential improvements in the context of urban water security. The study integrates the technological factors in the domain area. An expert review is conducted to get confirmation from professionals in the chosen sector about the effective factors for the dashboards from the IT perspective. This methodology uses qualitative via face-to-face and online sessions. Thematic analysis and content validity index (CVI) have been used to evaluate the experts' review. The findings show security and knowledge discovery is the main factor emphasised. The experts deemed visual design the least priority when creating a business intelligence dashboard. Experts also recommend embedding Artificial Intelligence (AI) with the data analysis for dashboards. The discussion identified challenges and future work for this research. The discussion identified three potential areas of future research in water security and informatics: i) investigating other dimensions of effective dashboards, ii) conducting additional research on the environmental dashboard, and iii) investigating dashboards' real-world application in urban water security.",No,"본 논문은 전문가 의견을 수집하고 기존 모니터링 대시보드의 효과적인 요소를 검토하는 연구로, 직접적인 실험이나 새로운 기술 개발 등 독창적인 연구 결과를 제시하지 않습니다. 따라서 기존 연구를 종합하고 향후 연구 방향을 제안하는 성격이 강해 연구 논문으로 보기 어렵습니다."
Empowered Artificial Intelligence Approach using Intuitionistic Fuzzy based Deep Neural Network for Chronic Diseases Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10582074,"Managing the increasing prevalence of chronic illnesses is a major worldwide health concern. It's important to understand the medical problem, its stages, types, and available treatments. The most important component in the early diagnosis of chronic diseases might enhance the diagnostic procedure more successfully. Medical professionals can diagnose a process with the help of numerous artificial intelligence tools. However, ambiguity, irrelevance, and inconsistency in the disease dataset have an impact on the classification accuracy. Conventional machine learning techniques are inadequate for managing scenarios of uncertainty that arise in datasets related to chronic illnesses. To anticipate chronic diseases, each characteristic in the suggested work is represented with a hesitation index. In order to compute the hesitation degree as a more informative procedure in determining the association among features, the belongingness and non-belongingness aspects of the intuitionistic fuzzy concept are induced in the prediction process. This work effectively manages the large chronic illness dataset by developing an intuitionistic fuzzy hesitation index-based similarity measure to weed out unimportant and strongly associated characteristics and enhance the classification model. Utilizing the scored feature information, the deep neural network modifies its hyper parameter to address the overfitting issues that frequently arise in DNNs. The performance of the empowered intuitionistic fuzzy-based deep neural network (EIF-DNN) is analyzed using five distinct chronic conditions, including hypertension, diabetes, hepatitis, cancer, and stroke. Comparing the proposed EIF-DNN against the current versions of multi-layered networks, the simulation results show that it produces promising results in predicting chronic diseases at an early stage.",Yes,논문 초록에서 제안된 EIF-DNN 모델은 직관주의 퍼지 이론과 딥 뉴럴 네트워크를 결합하여 만성 질환 예측을 위한 새로운 방법론을 개발하고 성능을 평가한 연구임을 알 수 있습니다. 이는 기존 기법의 한계를 극복하고자 하는 독창적인 연구 기여를 포함하고 있으므로 연구 논문에 해당합니다.
Machine Learning-Based Drone Detection and Classification: State-of-the-Art in Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846214,"This paper presents a comprehensive review of current literature on drone detection and classification using machine learning with different modalities. This research area has emerged in the last few years due to the rapid development of commercial and recreational drones and the associated risk to airspace safety. Addressed technologies encompass radar, visual, acoustic, and radio-frequency sensing systems. The general finding of this study demonstrates that machine learning-based classification of drones seems to be promising with many successful individual contributions. However, most of the performed research is experimental and the outcomes from different papers can hardly be compared. A general requirement-driven specification for the problem of drone detection and classification is still missing as well as reference datasets which would help in evaluating different solutions.",No,"본 논문은 드론 탐지 및 분류에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하고 있지 않습니다. 따라서 연구 논문보다는 현황 분석 및 문헌 조사에 해당합니다."
Ambulatory Behavior Assessment Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340595,"This work leverages a custom implementation of a deep neural network-based object detection algorithm to detect people and a set of assistive devices relevant to clinical environments. The object detections form the basis for the quantification of different ambulatory activities and related behaviors. Using features extracted from detected people and objects as input to machine learning models, we quantify how a person ambulates and the mode of ambulation being used.Clinical relevance— This system provides the data required for clinicians and hospitalized patients to work together in the creation, monitoring, and adjustment of ambulatory goals.",Yes,"논문 초록에서 딥러닝 기반 객체 검출 알고리즘을 구현하고 이를 통해 보행 행동을 정량화하는 독창적인 연구 방법을 제시하고 있습니다. 또한, 추출된 특징을 활용해 보행 양상을 평가하는 기계학습 모델을 개발한 점에서 직접적인 연구 기여가 포함된 논문으로 판단됩니다."
HealthMine: A Tool for Social Media Text Mining in Health,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9297002,"Social media has become a major source of information in recent years, with millions of posts every minute if not seconds. Containing information on various topics like health, politics and sports, one cannot deny that social media has become a good leverage for the field of data analytics. The objective of this work was to apply techniques of text mining, data analytics, and machine learning to Implement a web application, HealthMine, which extracts and classifies relevant data collected from social media platforms, namely Twitter and MedHelp, following a health-related user query. This would spare the user the burden of filtering out irrelevant information and focusing more on what is relevant. However, since content on social media is user-generated, the reliability is dubitable and the advice of a certified medical practitioner is always recommended. The main purpose of the tool is to allow users to share experiences. The tool was evaluated using 1400 tweets and 1800 MedHelp posts and it was found that Naive Bayes Classifier yielded the best accuracy among other classifiers, with an accuracy of 86.3 and 76.6 for Twitter and Medhelp respectively.",Yes,"본 논문은 소셜 미디어에서 건강 관련 데이터를 추출하고 분류하는 웹 애플리케이션인 HealthMine을 구현하고, 다양한 분류기 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, Naive Bayes 분류기를 사용하여 정확도를 측정하는 실험적 결과를 제시하고 있어 연구 논문에 해당한다."
Emerging Applications and Challenges in Quantum Computing: A Literature Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645271,"Quantum computing is at the cutting edge of technological innovation, with the potential to transform a wide array of scientific and practical fields. This literature survey addresses the critical need for a structured understanding of recent advancements and identifies research gaps. It comprehensively reviews recent advancements in quantum computing, emphasizing non-standard architectures, applications in material sciences, and the integration of quantum and classical machine learning models. The survey also examines quantum algorithms for real-time systems, explores cryptographic protocols beyond Shor's algorithm, and discusses resource management in cloud computing. Key findings highlight significant progress in algorithm efficiency, practical applications in 2D materials and topological insulators, and the hybridization of quantum and classical models. Challenges such as scalability, resource optimization, and ethical concerns still remain. This paper guides future research to focus on optimizing algorithms, efficient resource allocation, and enhancing security measures. This work synthesizes current knowledge, guiding future research efforts and contributing to the advancement of the quantum computing research community.",No,"본 논문은 최신 연구 동향과 연구 격차를 종합적으로 정리한 문헌 조사(literature survey)로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 체계적으로 요약하는 리뷰 논문에 해당합니다."
End-to-end Gesture Recognition Framework for the Identification of Allergic Rhinitis Symptoms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9881698,"Human Gesture Recognition (HGR) using smart wearable IoT devices has emerged as a new field in human-centered computing regarding various domains. Though there are many research works related to data processing methodologies and Neural Networks architectures in this field, a lack of research on how to efficiently identify and interpret the AI models’ exports into human gestures is observed. This paper proposes an innovative end-to-end approach of how to solve and evaluate effectively a major part of HGR problems in a real-world scenario, in real-time. This is achieved with the effective utilization of data processing methods, the adoption, and extension of a cutting-edge Deep Learning model architecture, as well as the introduction and implementation in practice of innovative methods, both for interpretation and evaluation, that increase the trustworthiness of the model’s predictions.As a case study, we deployed the introduced pipeline into a real-world scenario of gestures’ identification and classification regarding allergic symptoms. We adopted multidisciplinarity by collaborating with recognized allergists that validated the whole approach in real patients via two pilot phases. As a result, by delivering a real-world application of our approach, we achieved a superior performance concerning the reliability of the pipeline, being 91.6% in our laboratory pilot phase and 81.4% in patients’ pilot data. Lastly, it is worth mentioning here that our framework can be employed in most HGR problems with minor modifications in data processing and learning procedure configuration.",Yes,"논문은 새로운 end-to-end 제스처 인식 프레임워크를 제안하고, 실제 환자 데이터를 활용한 파일럿 테스트를 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 데이터 처리 방법과 딥러닝 모델 아키텍처의 확장 및 해석 방법을 도입하여 실질적인 기여를 하고 있음을 보여줍니다."
Link Prediction in Social Networks: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664178,"Link prediction in social networks has garnered significant attention due to its potential applications in various domains. This review paper synthesizes and evaluates recent advancements in link prediction methodologies across diverse research studies. It provides a comprehensive overview of the strategies proposed by different authors, emphasizing key methodologies, metrics, and their advantages. The review encompasses proximity-based algorithms, dynamic user relationship analysis, node similarity-based approaches, multiplex network methods, temporal node similarity weighting, centrality measures, negative link prediction techniques, machine learning-based comparisons, and temporal preferential attachment models. Each methodology is dissected to elucidate its core principles and effectiveness in predicting links within social networks. Furthermore, the paper discusses the experimental setups, evaluation metrics, and comparative analyses employed by researchers to validate their proposed methodologies. Through this comprehensive examination, the review paper aims to offer insights into the evolving landscape of link prediction techniques and guide future research directions in social network analysis.",No,"본 논문은 다양한 링크 예측 기법들을 종합적으로 검토하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 정리하고 분석하는 개관 논문에 해당합니다."
Complex Network Measures for Data Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533608,"Complex networks have become an increasingly relevant research topic in machine learning, with many learning systems in the literature successfully exploring complex network properties and measures. In data classification, the use of complex networks allows the detection of structural and topological patterns related, for example, to the formation pattern of the input data. Some measures of complex networks have already been used in this sense. However, a systematic study capable of characterizing such measures in the context of data classification is lacking in the literature. In this work, we evaluate comparatively the predictive performance of some measures. Specifically, eight complex network measures were selected from the literature, namely: assortativity, average local clustering coefficient, average degree, betweenness, average shortest path length, closeness, global clustering coefficient and eigenvector centrality. For our analyses, both artificial and real-world data sets were considered. The results show that measures such as average shortest path and assortativity, besides presenting high predictive capability, are also more robust to the variation of the network structure. In summary, this research paves a way to support other related works in selecting more appropriate complex network measures for data classification.",Yes,"논문은 복잡한 네트워크 측정치를 데이터 분류에 적용하고, 여덟 가지 네트워크 측정치의 예측 성능을 비교 평가하는 독창적인 연구를 수행하고 있다. 이는 기존 문헌에서 부족한 체계적 연구를 보완하는 직접적인 연구 기여로 판단된다."
Design of a data acquisition system to be used in fault diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483298,"In Machine learning, the availability of reliable datasets to be used by training algorithms is a widely posed problem. In this perspective, this work represents a design of a data acquisition system that allows the collection of data from a real world industrial machine (Direct Current motor machines). The goal of this data collection is the construction of a fault diagnosing tool by using heterogeneous data. Those heterogeneous data are collected from different types of sensors measuring different types of variables that are directly related to the industrial system. Owing to this data collection, one can build machine learning models such as Bayesian networks, Artificial Neural Networks, etc. Those models can be used in fault detection, diagnosis and prognosis.",Yes,"본 논문은 실제 산업용 기계에서 데이터를 수집하기 위한 데이터 획득 시스템을 설계하는 내용을 다루고 있으며, 이를 통해 고장 진단 도구 개발에 필요한 데이터를 제공하는 독창적인 연구 기여를 포함하고 있다. 따라서 연구 논문으로 판단된다."
Development of a Fuzzy Variable Rate Irrigation Control System Based on Remote Sensing Data to Fully Automate Center Pivots,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10316268,"Growing agricultural demands for the global population are unlocking the path to developing innovative solutions for efficient water management. Herein, an intelligent variable rate irrigation system (fuzzy-VRI) is proposed for decision-making to achieve optimized irrigation in various delimited zones. The proposed system automatically creates irrigation maps for a center pivot irrigation system for a variable rate application of water. Primary inputs are satellite imagery on remotely sensed soil moisture (SSM), soil-adjusted vegetation index (SAVI), canopy temperature (CT), and nitrogen content (NI). The system relates these inputs to set reference values for the rotation speed controllers and individual openings of each central pivot sprinkler valve. The results showed that the system can detect and characterize the spatial variability of the crop and further, the fuzzy logic solved the uncertainties of an irrigation system and defined a control model for high-precision irrigation. The proposed approach is validated through the comparison between the recommended irrigation and actual irrigation at two field sites, and the results showed that the developed approach gives an accurate estimation of irrigation with a reduction in the volume of irrigated water of up to 27% in some cases. Future research should implement the fuzzy-VRI real-time during field trials in order to quantify its effect on irrigation use, yield, and water use efficiency. Note to Practitioners—This work is motivated by the objective of managing irrigation more efficiently. It will be a site-specific irrigation management tool and we proposed a theoretical framework that aims an artificial intelligence approach to automatically create optimal control maps for a center pivot irrigation system. At the heart of this system will be the fuzzy logic, which will define the reference values for the rotation speed controllers and the individual opening of each center pivot sprinkler valve. Currently, there is a lack of these types of systems which ends up generating an increase in demand for more intelligent, automated, and accurate systems. The proposed system will be based on decision-making - whether to apply more or less water - and will use remote sensing data, therefore, the innovative irrigation system will efficiently describe the spatial variability of the crop. The results indicate that edaphoclimatic variables, when well combined with fuzzy logic, can resolve uncertainties and nonlinearities of an irrigation system and define a control model for high precision irrigation. However, it will not always be possible to reduce water consumption, but this technology has many uses to increase farm profitability.",Yes,"논문은 원격탐사 데이터를 활용한 퍼지 논리 기반의 가변 관개 제어 시스템을 개발하고, 이를 실제 필드에서 검증한 연구 내용을 포함하고 있어 독창적인 연구 기여가 명확하다. 또한, 시스템 설계, 구현, 성능 평가를 통해 새로운 인공지능 기반 관개 제어 모델을 제안하고 있으므로 연구 논문에 해당한다."
Techniques to Evaluate and Enhance Security Measures in Cloud Computing Environments,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10867380,"In this research Cloud computing has emerged as a critical infrastructure component, offering flexible, scalable, and cost-effective data processing and storage solutions. However, this widespread adoption exposes sensitive data to various cyberthreats, posing significant security challenges. This study aims to address these obstacles by evaluating and enhancing security protocols in cloud computing environments. It seeks to identify new cyberthreats, comprehensively review existing security measures, and devise innovative methods for continuous assessment. The research methodology incorporates a literature review, analysis of current protocols, and the development of novel approaches leveraging advanced technologies such as blockchain and artificial intelligence. Key findings underscore the importance of proactive defensive strategies, highlighting how adaptive security protocols and sophisticated threat detection systems bolster cloud security resilience. The study emphasizes the necessity for ongoing evaluation and refinement to effectively mitigate emerging risks and vulnerabilities. Robust security protocols are imperative for ensuring the availability, confidentiality, and integrity of data within cloud systems. By advocating for proactive defensive techniques to maintain trust amidst evolving threat landscapes, this research contributes to the advancement of cloud security practices.",Yes,"논문 초록에서 기존 보안 프로토콜 분석과 함께 블록체인 및 인공지능을 활용한 새로운 방법론 개발을 명시하고 있어, 독창적인 연구 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 새로운 사이버 위협 식별과 지속적인 평가 방법 제시 등 직접적인 연구 활동이 수행되었음을 보여줍니다."
First CE Matters: On the Importance of Long Term Properties on Memory Failure Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020671,"Dynamic random access memory failures are a threat to the reliability of data centres as they lead to data loss and system crashes. Timely predictions of memory failures allow for taking preventive measures such as server migration and memory replacement. Thereby, memory failure prediction prevents failures from externalizing, and it is a vital task to improve system reliability. In this paper, we revisited the problem of memory failure prediction. We analyzed the correctable errors (CEs) from hardware logs as indicators for a degraded memory state. As memories do not always work with full occupancy, access to faulty memory parts is time distributed. Following this intuition, we observed that important properties for memory failure prediction are distributed through long time intervals. In contrast, related studies, to fit practical constraints, frequently only analyze the CEs from the last fixed-size time interval while ignoring the predating information. Motivated by the observed discrepancy, we study the impact of including the overall (long-range) CE evolution and propose novel features that are calculated incrementally to preserve long-range properties. By coupling the extracted features with machine learning methods, we learn a predictive model to anticipate upcoming failures three hours in advance while improving the average relative precision and recall for 21% and 19% accordingly. We evaluated our methodology on real-world memory failures from the server fleet of a large cloud provider, justifying its validity and practicality.",Yes,"본 논문은 메모리 고장 예측을 위해 장기적인 Correctable Errors(CEs) 특성을 분석하고, 이를 기반으로 새로운 특징을 제안하여 기계 학습 모델을 개발하는 독창적인 연구 내용을 포함하고 있다. 또한 실제 클라우드 서버 데이터를 활용해 제안 방법의 유효성을 평가한 점에서 직접적인 연구 기여가 명확하다."
Machine Learning Approaches for Anomaly Detection in IoT Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601954,"The exploration paper explores the application of machine literacy ways for anomaly discovery within Internet of Effects (IoT) networks. With the rapid expansion of IoT bias, icing network security becomes decreasingly grueling. Traditional security measures frequently fall suddenly in detecting arising pitfalls and anomalies in the vast and dynamic IoT terrain. thus, this study investigates the efficacity of colorful machine learning algorithms in relating abnormal geste reflective of implicit security breaches or system malfunctions. Through a comprehensive review of the literature and empirical analysis, the paper examines the strengths and limitations of different machine learning approaches, including supervised, unsupervised, and semisupervised styles, in detecting anomalies within IoT networks. The findings give perceptivity into the feasibility and effectiveness of employing machine literacy for enhancing the security and trustability of IoT ecosystems.",No,"초록에서 해당 논문은 기존 문헌을 종합적으로 검토하고 기계 학습 알고리즘의 효능을 분석하는 탐색적 연구(exploration paper)임을 명시하고 있습니다. 이는 직접적인 독창적 연구 결과보다는 문헌 리뷰와 경험적 분석에 초점을 맞추고 있어, 새로운 연구 기여를 포함한 연구 논문으로 보기 어렵습니다."
Admission control with online algorithms in SDN,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502884,"By offloading the control plane to powerful computing platforms running on commodity hardware, Software Defined Networking (SDN) unleashes the potential to operate computation intensive machine learning tools and solve complex optimization problems in a centralized fashion. This paper studies such an opportunity under the framework of the centralized SDN Admission Control (AC) problem. We first review and adapt some of the key AC algorithms from the literature, and evaluate their performance under realistic settings. We then propose to take a step further and build an AC meta-algorithm that is able to track the best AC algorithm under unknown traffic conditions. To this aim, we exploit a machine learning technique called Strategic Expert meta-Algorithm (SEA).",Yes,"논문 초록에서 기존 알고리즘을 검토하고 평가한 후, 새로운 Admission Control 메타 알고리즘을 제안하여 미지의 트래픽 조건에서 최적 알고리즘을 추적하는 기법을 개발한 점이 명확히 독창적인 연구 기여를 포함하고 있음을 보여줍니다. 따라서 본 논문은 직접적인 연구 결과를 제시하는 연구 논문으로 판단됩니다."
Day-ahead forecasting of regional photovoltaic production using deep learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9300538,"Power production based on solar energy is directly related to the state of the atmosphere. As the atmospheric state is undergone, this connection makes the solar energy a non-dispatchable source as opposed to controllable renewable sources such as hydroelectricity. In a context of growing photovoltaic generation, accurate forecast tools at regional scale are then increasingly important to grid operators. Indeed, forecasts allow getting information about future production over the next minutes, hours and days. Forecasting tools offer the possibility of a better grid management strategy specifically for transmission system operator (TSO) that are responsible for balancing renewable power production. High forecast accuracy could also lead to reduced costs for energy trading. In light of this situation, this study focuses on the development and analysis of a regional forecasting tool based on a deep learning approach. The selected model consists in a combination of a convolutional neural network (CNN) with a long short-term memory architecture (LSTM). The CNN layers allow extracting spatial features from Numerical Weather Prediction outputs while the LSTM part supports the temporal relationship. The day ahead regional forecast for Germany is chosen as a case study. The CNN-LSTM is compared to the classical Random Forest model known to be one of the reference techniques for this kind of problematic. Simpler deep learning models are also tested to validate the improvement brought by the CNN-LSTM architecture. All the comparisons are based on the classical root mean squared error (RMSE) metrics. The main result of this study shows that CNN-LSTM model can improve forecast accuracy when compared to state-of-the-art Random Forest. As expected, this improvement is strongly correlated to the amount of historical data which must cover several years according to the sensitivity study realized in this work.",Yes,"본 논문은 딥러닝 기반의 CNN-LSTM 모델을 개발하고 이를 기존의 랜덤 포레스트 모델과 비교 분석하는 독창적인 연구를 수행하고 있다. 또한, 모델의 성능 향상과 데이터 민감도에 대한 실험적 결과를 제시하여 직접적인 연구 기여를 포함하고 있다."
Machine translation survey for Punjabi and Urdu languages,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344667,"Computer technology plays a major role in human's life, so the automatic translation perspectives are important for making communication possible among each other. Translation from one regional language to another national or international language is important to understand the information and technology expressed in that language. This led to the inception of machine translation from regional language to national or international language. Machine translation is a field of Natural Language Processing (NLP) and Artificial Intelligence (AI), where it deals with translation from one language to another language. The statistical machine translation approach is popular in automatic translation research area with good accuracy. This paper gives a review of the work done in Punjabi and Urdu languages. To the best of our knowledge, no work is available on Punjabi to Urdu MTS methodology, but Urdu to Punjabi MTS is already developed [Umrinder Pal Singh et. al, 2016]. Punjabi is a regional language as well as official language of Indian state, Punjab. All state government related work within Punjab is done in Punjabi language only, written in two scripts i.e. Gurmukhi and Shahmukhi. Urdu is an international language, where most of the Muslim states/countries use Urdu language only. This paper also surveys the various Indian Machine Translation systems and existing methods for evaluating the translated output of various MTS.",No,초록에서 해당 논문은 Punjabi와 Urdu 언어에 대한 기계 번역 연구를 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 방법론 제시보다는 기존 연구들을 조사하고 정리하는 데 중점을 두고 있습니다.
Modeling Machine Learning to Solve Distribution Problems and the Number of Backlogs in Maintenance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10406640,"The purpose of this research is the need to reduce backlogs and distribution for technician problems in the maintenance process. The distribution of work to technicians for maintenance will be effective in meeting the goals of the maintenance system of the University of the Thai Chamber of Commerce to be more efficient while reducing delays in maintenance by implementing Machine Learning (ML) using the form Support Vector Machine (SVM), while research is developed on the application UTCC-CMMS is able to work more comprehensively by allowing the application to act as a staff. The work related to maintenance can receive maintenance requests on the algorithm VSM to suitable technicians. and evaluate the performance as well. Also, contact purchasing or support external technicians in maintenance, resulting in faster maintenance there is a lot of backlog work reduced than before, allowing the technician and everyone to work together as well. The evaluation of operations in the past found that it received a score of 2.44. After the adoption of ML, it was found that the score was 4.44, representing an increase of 81.96%.",Yes,"논문 초록에서 머신러닝(SVM)을 활용하여 유지보수 작업 분배 문제를 해결하고 백로그를 줄이는 구체적인 연구 방법과 성과를 제시하고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문으로 판단됩니다."
An Empirical Approach of Movie Recommendation System Using Machine Learning (NLP),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541796,"Recommendation systems play a significant role in helping users make informed automated decisions. These systems facilitate users in discovering valuable information from a vast pool of available data. In the context of movie recommendation systems, recommendations are generated based on user similarities (Collaborative Filtering) or by taking into account an individual user’s interests (Content-Based Filtering). To address the limitations of both collaborative and content-based filtering, a hybrid approach is often employed to create a more effective recommendation system. Additionally, various similarity measures are used to determine user similarities for recommendations. This paper provides an overview of state-of-the-art techniques in Content-Based Filtering, Collaborative Filtering, Hybrid Approaches, and Deep Learning-Based Methods for movie recommendations. Furthermore, we discuss different similarity measures [1] [2].Various organizations, such as Instagram, LinkedIn, Spotify, Disney + Hotstar, and Amazon, utilize recommendation systems to enhance their profits and cater to their customers’ needs. This paper primarily focuses on a concise review of diverse techniques and their methods for movie recommendations, thereby encouraging further research in the domain of recommendation systems.",No,"초록에서 본 논문은 영화 추천 시스템에 사용되는 다양한 기존 기법들에 대한 개요와 리뷰를 제공하는 데 중점을 두고 있으며, 독창적인 연구 결과나 새로운 방법론의 제안이 명확히 드러나지 않습니다. 따라서 직접 기여하는 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Deep Learning Model for Option Pricing - Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10816734,"The Indian options market and in particular, the National Stock Exchange (NSE) segment of this marketspace poses a number of challenges when it comes to pricing option trades under high volatility regimes, low liquidity environments as driven by retail volume traders/ long positions and also rapidly changing regulatory ecosystems amongst others. These dynamics are often not captured by traditional pricing models such as the Black-Scholes model, therefore leading to inefficiencies in their prices. Deep learning, which can capture the non-linearities in data and handle quite a large set of features backed by parallel processing power promises to fill that void. There are several factors that any valid pricing model will have to take into account. These factors are underlying asset, strike price, time to expiration, volatility, interest rates, expected dividends and still there is a lot need in order to be able simulate option prices. which then encompass their significance when pricing an option. Furthermore, given their ability to integrate real-time market data (incorporating the India VIX), liquidity measures and economic indicators, these models can tackle some of the shortcomings that plague conventional methodologies. The other ultimate tools to achieve this goal are Option Greeks which plays a very important role in Options Pricing & Risk Management. This paper presents a detailed literature review of latest deep learning architectures applied on option pricing. This study addresses these issues while also bringing novel contributions to an important area of research that has not received attention yet, e.g., market specific models nuanced with the Indian scenario and necessity for a hybrid deep learning approach combining state-of-the-art volatility forecasting alongside data driven methods.",No,"초록에서 본 논문은 최신 딥러닝 아키텍처를 적용한 옵션 가격 책정에 대한 문헌 리뷰를 제공한다고 명시하고 있으며, 직접적인 실험이나 독창적인 연구 결과보다는 기존 연구들을 종합하고 분석하는 데 중점을 두고 있다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵다."
Rough set theory and its practice in knowledge discovery,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=862504,"As an effective mathematical tool to deal with vagueness and uncertainty, the rough set theory has got much attention in the fields of artificial intelligence, pattern recognition, knowledge process and so on. This paper introduces the Pawlak rough set theory model (1992, 1995) and the generalized rough set theory model, discusses the general methods used in knowledge discovery in database, and measures the plausibility with the generalized model. Finally, it also reviews the research and applications briefly.",No,초록에서 이 논문은 Pawlak의 이론과 일반화된 모델을 소개하고 기존 연구와 응용을 리뷰하는 내용임을 알 수 있습니다. 따라서 독창적인 연구 결과나 새로운 실험적 기여보다는 이론 및 방법론의 정리와 개관에 중점을 둔 논문으로 보입니다.
Research Progress on Distributed Photovoltaic Roof Extraction Methods Based on UAV LiDAR Point Clouds,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10875130,"With the growing global demand for renewable energy, distributed photovoltaic (PV) systems have become a significant direction for new energy development. Building roofs are essential carriers for distributed PV systems, and the extraction of roof is closely linked to the evaluation of solar energy potential. This paper reviews the research progress of building roof extraction methods based on UAV LiDAR point clouds. Firstly, the application of traditional methods in point cloud segmentation of building roof was reviewed, such as morphological algorithms, active contour models, region growing algorithms, random sample consensus algorithms (RANSAC), and clustering-based algorithms. These methods, while effective, face certain limitations when processing large-scale point cloud data. In contrast, PointNet and its variants, which can directly process point clouds, have emerged as key deep learning methods. Recently, Transformer-based models have shown advanced performance in point cloud classification and segmentation tasks. Finally, the challenges and future trends of UAV LiDAR-based roof extraction are discussed, providing technical support for evaluating the potential of urban distributed PV systems and formulating distributed PV power generation plans.",No,"본 논문은 UAV LiDAR 점군을 이용한 분산형 태양광 지붕 추출 방법에 대한 기존 연구들을 종합적으로 리뷰하는 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 연구 동향과 기술적 과제를 논의하는 내용에 초점이 맞춰져 있습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리 및 전망 제시에 해당합니다."
Sentiment Analysis of Social Media Data using Fuzzy-Rough Set Classifier for the Prediction of the Presidential Election,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754173,"As an interdisciplinary research field, sentiment analysis is one of the momentous applications in Natural Language Processing, for quantifying the emotional value in vast data in the form of text available in social media networks to gain an understanding of the attitudes, opinions, and emotions expressed. There is a great deal of literature on the various approaches to address sentiment analysis with social media and this research focuses on Machine Learning techniques with Twitter data analysis. Special attention is drawn towards the classifiers based on the Fuzzy Set and Rough Set approach which are two powerful mathematical components of computational intelligence with its new dimension involved in the field of sentiment analysis. However, there is a minimal number of review papers discussing rough-fuzzy classifier involvement in sentiment analysis and there is a plethora of work that must be done with text mining in natural language processing. The mission of this study is to develop a sentiment-based classifier using machine learning and fuzzy-rough set theory. Further, it carries automatic sentiment classification with Twitter corpus collected during September 1st and November 15th, 2019 (two months before the election) regarding the case study for the prediction of results at the presidential election 2019, Sri Lanka. The fuzzy rough classifier is developed using the Fuzzy Rough Nearest Neighbor algorithm. The accuracy of the fuzzy rough set-based classifier is higher compared to other classifiers. The actual results of the presidential election of 2019 are tally with the predicted results of the classifier. Therefore, the current state of the art for the prediction of political sentiment with microblogging is probable with the social media data as witnessed with this case study and this can be used in other cases as well.",Yes,"본 논문은 퍼지-러프 집합 이론을 활용한 새로운 감성 분류기를 개발하고, 이를 트위터 데이터를 이용해 대통령 선거 결과 예측에 적용한 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 분류기의 정확도를 다른 분류기와 비교하여 성능을 평가한 점에서 직접적인 연구 기여가 있다고 판단된다."
Anamoly Detection in Very Large Scale System using Big Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10059870,"Big data refers to a term that is used to describe vast amounts of data that have multiple kinds of Vs: velocity, variety, and volume. It could be semi-structured, unstructured, or even structured, making data analysis difficult. New architecture, methodologies, algorithms, and analytics are needed to extract hidden data and identify assaults on enormous amounts of data. It is quite challenging to identify assaults using conventional methods. This study provides a thorough analysis of malware detection in several sectors using deep learning and provides an overview of deep learning data. In networked computers, there have been more attacks. To protect a network, a strong intrusion detection system (IDS) is necessary. Reviewing the literature reveals that while some research has been conducted in this area, a thorough and in-depth investigation has not yet been carried out. For unanticipated and unpredictable assaults, many academics suggested an IDS employing deep learning, but not for big data. The present research design is based on three ensemble methods, Randam Forest, Decision tree regression, and Gradient Boosting Tree, as well as a deep learning-based intrusion detection system for large datasets named RNN that runs for 1,000 epochs with a learning rate complexity and diversity [0.01-0.5]. It is employed in the creation of the hybrid, safe, and scalable, which is based on big data and deep learning methods. In comparison to using just one classifier, the suggested classifiers provide a more accurate classification. Detection rate (99 percentage), false positive rate (1.5 percent), accuracy (99 percentage), and F-Measure (99.03%) are the experimental results. The results show that new anomaly detection methods work better in the big data context.",Yes,"본 논문은 빅데이터 환경에서 딥러닝과 앙상블 기법을 활용한 새로운 침입 탐지 시스템을 제안하고, 실험을 통해 성능을 평가한 연구 내용을 포함하고 있다. 이는 기존 연구와 차별화된 독창적인 방법론과 실험 결과를 제시하는 직접 기여하는 연구 논문으로 판단된다."
From Vision to Precision: The Dynamic Transformation of Object Detection in Autonomous Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485026,"This paper explores the evolution of self-driving cars, from early radio-controlled prototypes to today's advanced autonomous vehicles, highlighting key milestones such as Houdina Radio Control's 1920s efforts and Carnegie Mellon University's Navlab project in the 1980s. Recent advancements driven by industry leaders like Google, Tesla, and Uber incorporate advanced technologies, including sensors, cameras, radar, and lidar, empowering self-driving cars for autonomous navigation. The literature review in Frontiers in Future Transportation focuses on performance metrics for automated driving systems, discussing methodologies for quantifying obstacle threats and enhancing human-like perception and response mechanisms, with a significant emphasis on deep learning methods such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). Object detection algorithms' dynamic evolution, influenced by CNNs, is explored, and the integration of hardware accelerators like GPUs, FPGAs, and specialized AI accelerators is highlighted for real-time responsiveness. Deep learning applications in self-driving cars, including obstacle detection, lane recognition, and traffic sign and light recognition, are outlined. Vision sensor-based obstacle detection using monocular and binocular approaches is discussed, emphasizing the importance of binocular vision technology. Lane recognition's significance for deviation warning, collision prevention, and path planning is highlighted, along with deep learning approaches such as John et al.'s lane detection algorithm. Traffic sign and light recognition are explored for safety and regulatory compliance, utilizing CNNs and Deep Neural Networks (DNNs). The discussion on sensors covers simultaneous localization and mapping (SLAM) using both passive (digital cameras) and active sensors (LIDAR, RADAR), emphasizing the integration of sensors and pre-existing maps to enhance the SLAM process and contribute to the overall perception of autonomous vehicles.",No,"초록 내용은 자율주행차의 발전 과정과 관련 기술, 기존 연구 및 기술 동향에 대한 종합적인 문헌 리뷰에 초점을 맞추고 있으며, 독창적인 연구 결과나 새로운 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 논문에 더 가깝다고 판단됩니다."
Data Poisoning Attacks on Federated Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618642,"Federated machine learning which enables resource-constrained node devices (e.g., Internet of Things (IoT) devices and smartphones) to establish a knowledge-shared model while keeping the raw data local, could provide privacy preservation, and economic benefit by designing an effective communication protocol. However, this communication protocol can be adopted by attackers to launch data poisoning attacks for different nodes, which has been shown as a big threat to most machine learning models. Therefore, we in this article intend to study the model vulnerability of federated machine learning, and even on IoT systems. To be specific, we here attempt to attacking a popular federated multitask learning framework, which uses a general multitask learning framework to handle statistical challenges in the federated learning setting. The problem of calculating optimal poisoning attacks on federated multitask learning is formulated as a bilevel program, which is adaptive to the arbitrary selection of target nodes and source attacking nodes. We then propose a novel systems-aware optimization method, called as attack on federated learning (AT2FL), to efficiently derive the implicit gradients for poisoned data, and further attain optimal attack strategies in the federated machine learning. This is an earlier work, to our knowledge, that explores attacking federated machine learning via data poisoning. Finally, experiments on several real-world data sets demonstrate that when the attackers directly poison the target nodes or indirectly poison the related nodes via using the communication protocol, the federated multitask learning model is sensitive to both poisoning attacks.",Yes,"본 논문은 연합 학습(federated learning)에서 데이터 중독 공격(data poisoning attacks)에 대한 모델 취약성을 연구하고, 최적의 공격 전략을 도출하는 새로운 최적화 방법(AT2FL)을 제안하는 등 독창적인 연구 내용을 포함하고 있다. 또한 실제 데이터셋을 활용한 실험을 통해 제안 기법의 유효성을 검증하고 있어 연구 논문에 해당한다."
An Empirical Performance Evaluation of Semantic-Based Similarity Measures in Microblogging Social Media,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606643,"Measuring textual semantic similarity has been a subject of intense discussion in NLP and AI for many years. A new area of research has emerged that applies semantic similarity measures within Twitter. However, the development of these measures for the semantic analysis of tweets imposes fundamental challenges. The sparsity, ambiguity, and informality present in social media are hampering the performance of traditional textual similarity measures as ""tweets"", have special syntactic and semantic characteristics. This paper reviews and evaluates the performance of topological, statistical, and hybrid similarity measures, in the context of Twitter analysis. Furthermore, the performance of each measure is compared against a naïve keyword-based similarity computation method to assess the significance of semantic computation in capturing the meaning in tweets. An experiment is designed and conducted to evaluate the different measures through examining various metrics, including correlation, error rates, and statistical tests on a benchmark dataset. The potential weaknesses of semantic similarity measures in relation to Twitter applications of textual similarity assessment and the research contributions are discussed. This research highlights challenges and potential improvement areas for the semantic similarity of tweets, a resource for researchers and practitioners.",Yes,논문은 트위터 텍스트의 의미적 유사성 측정 방법들을 실험적으로 평가하고 비교하는 독창적인 연구를 수행하고 있다. 다양한 유사성 측정 기법을 실험 설계와 통계적 검증을 통해 분석함으로써 직접적인 연구 기여를 포함하고 있다.
Hyperspectral Image Classification Using Random Forest and Deep Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165588,"One of the purposes of hyperspectral remote sensing is to differentiate and identify the materials present on the Earth's surface by the spectral behavior of each object in the different regions of the electromagnetic spectrum. Such differentiation and identification can be accomplished through different image classification algorithms. However, there is no perfect classifier, since every algorithm has labeling errors. With the advent of orbital and aerial images of very high spatial and spectral resolution, the recognition of the materials present in urban environments is increasingly accurate. Thus, we thoroughly study different methodologies to identify the algorithm that presents the best results in the characterization of urban objects. The hyperspectral image used in the present study represents an area over Houston University - Texas and its surroundings, containing 48 spectral bands, with a spatial resolution of 1 meter and spectral range of 380 nm to 1050 nm. For the identification of 21 classes present in the study area, this paper analyzes two different classification methods: Deep Learning and Random Forest. To improve classification accuracy, performed the feature extraction. To obtain such preliminary results we used tools available in specific software as Normalized Difference Vegetation Index (NDVI), Minimum Noise Fraction (MNF), Principal Component Analysis (PCA) and Soil Adjusted Vegetation Index (SAVI). The image segmentation was performed using two different methods known as Multiresolution Segmentation and Spectral difference. Multiresolution segmentation needs parameters related to form and compactness. The best results were obtained with the values of form = 0.7 and compactness = 0.5, besides the scale of 10. From this, samples of all classes contained in the study area were selected for the training of the algorithms. This step is of paramount importance, as sample collection directly impacts the result of the classifications. After performing these steps, the information obtained from sample collection is entered into the data mining software (WEKA 3.8) to train the classification algorithms. The analysis of the results was performed by cross-validation, thus obtained the confusion matrix, calculated the Overall Accuracy (OA) and Kappa Index. The classification by the Random Forest method had an overall accuracy of 84.72% and a Kappa Index of 0.83. In turn, the Deep Learning algorithm had an overall accuracy of 81.32% and a Kappa index of 0.80. In this case, the classification by the Random Forest method presented better results for the hyperspectral image classification than the Deep Learning method. The accuracy difference obtained between the methods is not considered significant, so it is suggested for future work to analyze other complementary issues such as processing time.",Yes,"본 논문은 하이퍼스펙트럼 이미지 분류를 위해 랜덤 포레스트와 딥러닝 알고리즘을 적용하고, 다양한 전처리 및 특징 추출 기법을 사용하여 분류 정확도를 평가하는 독창적인 연구 내용을 포함하고 있습니다. 또한 실험 결과와 성능 비교를 통해 새로운 지식을 제공하고 있어 연구 논문에 해당합니다."
Student and Lecturer Performance Enhancement System using Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9315981,"The proposed research work develops a system to enhance the performance of university students and lecturers by providing an excellent statistical insight. Already existing research works have attempted to solve independent classroom challenges that are related to measuring the student attention and marking student attendance but the existing research works have not combined theimportant aspects into one system. Hence, the proposed research wor has been carried out on various main aspects such as attendance register, monitoring student behavior as well as lecturer performance and lecture summarization. The system will incorporate tools and technologies in the different domains of artificial intelligence, machine learning, and natural language processing. After implementing and testing the proposed method it has been concluded that the student activity recognition process has been performed much better than the other emotion and gaze components by providing 94.5% results. The proposed system can determine the lecturer's physical activities and the quality of the lecture content with a reasonable accuracy. The summarized lecture has showed 70% similarity to actual lecture content and student attendance by using Face Recognition was marked with 83% accuracy. This research concludes that the automation of major classroom activities will impact the students and lecturers positively. Also, this system yields valuable results and increases the productivity of higher education institutions in the future.",Yes,"본 논문은 인공지능, 머신러닝, 자연어처리 기술을 활용하여 학생과 강사의 성과 향상을 위한 시스템을 개발하고, 구현 및 테스트 결과를 제시하고 있다. 이는 기존 연구와 차별화된 통합 시스템을 제안하며, 직접적인 연구 개발과 성능 평가를 포함한 독창적인 연구 내용으로 판단된다."
Towards Sustainable Deep Learning for Wireless Fingerprinting Localization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9838464,"Location based services, already popular with end users, are now inevitably becoming part of new wireless infrastructures and emerging business processes. The increasingly popular Deep Learning (DL) artificial intelligence methods perform very well in wireless fingerprinting localization based on extensive indoor radio measurement data. However, with the increasing complexity these methods become computationally very intensive and energy hungry, both for their training and subsequent operation. Considering only mobile users, estimated to exceed 7.4 billion by the end of 2025, and assuming that the networks serving these users will need to perform only one localization per user per hour on average, the machine learning models used for the calculation would need to perform 65×1012 predictions per year. Add to this equation tens of billions of other connected devices and applications that rely heavily on more frequent location updates, and it becomes apparent that localization will contribute significantly to carbon emissions unless more energy-efficient models are developed and used. This motivated our work on a new DL-based architecture for indoor localization that is more energy efficient compared to related state-of-the-art approaches while showing only marginal performance degradation. A detailed performance evaluation shows that the proposed model produces only 58% of the carbon footprint while maintaining 98.7% of the overall performance compared to state of the art model external to our group. Additionally, we elaborate on a methodology to calculate the complexity of the DL model and thus the CO2 footprint during its training and operation.",Yes,"논문은 기존 무선 지문 인식 위치 추정 방법의 에너지 효율성을 개선한 새로운 딥러닝 아키텍처를 제안하고, 성능 평가와 탄소 발자국 계산 방법론을 포함한 독창적인 연구 내용을 담고 있다. 이는 직접적인 연구 기여를 포함하는 연구 논문에 해당한다."
A Decision Support System For Retinal Image Defect Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153446,"Deep learning has become the de facto method for image classification. In this work, a common framework for decision support system is presented that can be reused for diagnosing multiple retinal clinical conditions. Retinal fundus images provide a non-invasive way to diagnose eye-related diseases like glaucoma and diabetic retinopathy (DR). State-of-the-art deep learning methods focus on the detection of key regions of the retina including fundus, optic disc and retinal vessels individually. In order to achieve acceptable precision and recall for a clinically deployable system, a decision support system that combines state-of-the-art deep learning system and relevant explainable features are built. The proposed method is tested on two retinal pathology use cases - glaucoma and for the detection of hard exudates that is critical in diagnosing DR. The proposed model is validated using DRIVE dataset with average Jaccard index of more than 96% for fundus, around 98% for OD and around 90% in identifying retinal vessels using a five-fold cross-validation. For disease detection, the above key regions are combined and validated using standard datasets with good outcomes.",Yes,"본 논문은 망막 영상 결함 탐지를 위한 의사결정 지원 시스템을 제안하고, 딥러닝 기반의 새로운 통합 프레임워크를 개발하여 여러 임상 조건 진단에 적용한 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 모델을 다양한 데이터셋으로 검증한 결과를 제시하여 직접적인 연구 기여가 명확하다."
Data Security Patterns for Critical Big Data Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366149,"With the words' current growth in technologies and digitalization, protecting data in massive data-driven systems has become a new challenge to overcome. Therefore, many measures, algorithms, and protocols are used and still under development to minimize risks of Data loss, manipulation, sniffing, or spying… In order to gain more in terms of security, privacy, and integrity levels. As important as this issue is, combined efforts from different disciplines (Network Security, Security Operations Center Analysis (SOC Analysis), Machine Learning (ML), and Deep Learning (DL) algorithms) are necessary to enhance Data protection. In this paper, we will expose some natural threats of Big Data, based on its definition, ethics, and suggestions to avoid these threats, or at least limit their effects. We will also show works related to big data security based on ML and shed light on Artificial Neural Networks (ANN) applications for data classification and threat detection. Then focus on Convolutional Neural Networks (CNN) to optimize parameters. We will try to find solutions for CNN limitations of hyperparameters tuning and rotating image feature extraction. Furthermore, we will discuss state of art results to understand the matter in a deeper sense and expose high-accuracy solutions with improvement path suggestions. Our work is going to come up with some natural threats, compare some existing methods, expose, and discuss results, and then suggest optimal ways to help increase data security, especially through improving CNN limitations.",Yes,"논문 초록에서 CNN의 하이퍼파라미터 튜닝과 이미지 특징 추출 최적화 등 구체적인 방법론과 개선 방안을 제시하며, 기존 방법들과의 비교 및 결과 분석을 포함하고 있어 독창적인 연구 기여가 포함된 연구 논문으로 판단됩니다."
Computer-aided quality control of radiological systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=744753,"The problem of quality control of radiological systems is discussed in relation to the management and quantitative tools normally applied to it. An alternative approach is to consider the problem in terms of a control loop in which the sets of quality control measurements are compared with their previous values in the context of a numerical model of the system to produce quantitative corrections. A preliminary study of the method applied to mammographic quality control is discussed. By modelling the system with an appropriate set of simultaneous linear equations relating changes in the controlled parameters to the quality control measurements obtained, it is possible to identify quantitative changes in the controlled parameters from measured quality control data using the inverse of the model in matrix form. This method essentially encapsulates knowledge about the system in the form of the linear model. This is distinct from current work attempting to apply artificial intelligence knowledge-based techniques to the problem.",Yes,"논문 초록에서 제시된 방법은 방사선 시스템의 품질 관리를 위한 수치 모델과 선형 방정식을 이용한 새로운 접근법을 제안하고 있으며, 이는 기존 방법과 구별되는 독창적인 연구 내용임을 보여준다. 또한, 실제 적용 사례(유방촬영 품질 관리)에 대한 예비 연구도 포함되어 있어 직접적인 연구 기여가 있다고 판단된다."
Traffic Light Recognition Using Deep Learning and Prior Maps for Autonomous Cars,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851927,"Autonomous terrestrial vehicles must be capable of perceiving traffic lights and recognizing their current states to share the streets with human drivers. Most of the time, human drivers can easily identify the relevant traffic lights. To deal with this issue, a common solution for autonomous cars is to integrate recognition with prior maps. However, additional solution is required for the detection and recognition of the traffic light. Deep learning techniques have showed great performance and power of generalization including traffic related problems. Motivated by the advances in deep learning, some recent works leveraged some state-of-the-art deep detectors to locate (and further recognize) traffic lights from 2D camera images. However, none of them combine the power of the deep learning-based detectors with prior maps to recognize the state of the relevant traffic lights. Based on that, this work proposes to integrate the power of deep learning-based detection with the prior maps used by our car platform IARA (acronym for Intelligent Autonomous Robotic Automobile) to recognize the relevant traffic lights of predefined routes. The process is divided in two phases: an offline phase for map construction and traffic lights annotation; and an online phase for traffic light recognition and identification of the relevant ones. The proposed system was evaluated on five test cases (routes) in the city of Vitória, each case being composed of a video sequence and a prior map with the relevant traffic lights for the route. Results showed that the proposed technique is able to correctly identify the relevant traffic light along the trajectory.",Yes,"논문은 딥러닝과 사전 지도를 결합한 새로운 교통 신호등 인식 시스템을 제안하고, 이를 실제 테스트 케이스에 적용하여 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Anti-Corrosion Capacity Validation for Anti-Sulfur Type Electronic Passive Components by Way of Various Flower-of-Sulfur (FoS) Methodologies,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696582,"Hardware reliability of Information Technology (IT) equipment can be easily affected by corrosive gases, moisture, contaminants and particulate matter. It can potentially cause electrical open failures due to sulfur corrosion (Ag2S) on the inner electrode of electronic passive components. In order to improve the robustness against sulfur corrosion, many vendors adopted gold-based and silver-palladium-based inner electrode designs (noble metal) as well as other solutions, including passivation cover and reverse structure designs. New IT equipment applications, including Artificial Intelligence (AI), Big Data, 5G, Internet of Things (IoT) and Edge Computing in recent years has proliferated the need of passive electronic components and the industry is facing passive component shortage. Therefore, quality control is becoming more critical in reducing poor quality risk. Flowers of Sulfur (FoS) is a popular method to validate the anti-sulfur corrosion capability of electronic passive components. However, the condition of single corrosive gas (sulfur vapor) is not enough to represent the accelerated corrosion exposure in field environments. It has been stipulated that typical testing methods as outlined in ASTM B809 and EIA-977 may not be totally effective in driving Anti-Sulfur Resistor (ASR) failure occurrence at 105°C/750 hours (test to pass), or even longer duration. Despite testing at those conditions, end-customers have reported sulfur corrosion-related failures. Therefore, it is necessary to develop an effective accelerated method for anti-sulfur corrosion capacity validation for anti-sulfur type electronic passive components. In this paper, three conditions of FoS test were carried out to validate the anti-sulfur corrosion capability of ASR components. We introduced chlorine-gas as another acceleration factor, and benchmarked it against ASTM B809 and EIA-977 FoS tests. Several analytical methods were used in this work, including, high resolution 3D X-Ray Microscope (3D X-Ray), Cross section polisher, (CP), Plasma Focused Ion Beam, (PFIB), Scanning Electron Microscopy (SEM) and Energy-Dispersive X-ray spectroscopy (EDX). Finally, we found that FoS with chlorine-gas condition is the most aggressive of all conditions.",Yes,"본 논문은 기존의 Flowers of Sulfur (FoS) 테스트 방법에 새로운 가속 인자인 염소 가스를 도입하여 전자 수동 부품의 내황 부식 능력을 검증하는 독창적인 실험 연구를 수행하고 있습니다. 다양한 분석 기법을 활용하여 실험 결과를 도출하였으며, 이는 직접적인 연구 기여를 포함한 논문임을 보여줍니다."
Unsupervised Domain Adaptation via Subspace Interpolating Deep Dictionary Learning: A Case Study in Machine Inspection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094819,"With the advent of industry 4.0, data-driven techniques have gained a lot of popularity for machine condition monitoring, ensuring reliable and safe operation of the machines. In most practical application scenarios, domain discrepancy may arise between the training (source domain) and test (target domain) data due to various factors like changes in the operating conditions, different sensor locations, etc. Traditional data-driven techniques fail to address this domain shift, and hence domain adaptation techniques are required to ensure reliable performance. This work presents an unsupervised domain adaptation method where labeled data is available only in the source domain via subspace interpolation using deep dictionary learning. Deep dictionaries learn rich representations from the data and hence are used for subspace interpolation to capture the domain shift and form a shared feature space for cross-domain analysis. The proposed method is evaluated for the challenging scenario of adaptation between different but related machines. Experimental results obtained with two publicly available bearing fault datasets are promising; the proposed method significantly outperforms all the state-of-the-art methods.",Yes,"논문은 기존 문제(도메인 불일치)를 해결하기 위한 새로운 비지도 도메인 적응 방법을 제안하고, 딥 딕셔너리 학습을 활용한 서브스페이스 보간 기법을 개발하여 독창적인 연구 기여를 하고 있다. 또한, 공개 데이터셋을 이용한 실험을 통해 제안 방법의 우수성을 입증하고 있어 연구 논문에 해당한다."
Multi-Lingual Information Retrieval Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493789,"The task of finding data files related to an information need from a group of information resources is known as Information Retrieval. In this work, the author propose a multi-lingual information retrieval system using deep learning. Input to the system is a question in sentencing form that can be processed by NLP tools. In the preprocessing phase, part-of-speech tagging of the input sentence is performed. A three layer neural network is used for creating word to vector representation. The word2vec model continuous-bag-of-words (CBOW) is used for this purpose. Then related words are obtained via word-2-vec using deep learning RNN. RNN is the recurrent neural network. Finally, results are obtained by calculating the cosine similarity score. For multi-lingual results, bilingual mapping is performed using CFILTs bilingual corpus. The tourism dataset is used for experimentation purposes.",Yes,"논문 초록에서 다국어 정보 검색 시스템을 딥러닝을 활용해 직접 설계하고 실험한 내용을 포함하고 있어 독창적인 연구 기여가 있다고 판단됩니다. 또한, 구체적인 모델 구성과 실험 데이터셋 사용이 명시되어 있어 연구 논문에 해당합니다."
A Comprehensive Review of Machine Learning Algorithms in Optimizing Power Consumption in Smart Buildings,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10944340,"The increasing global energy demand and the need for sustainable solutions have driven the evolution of smart buildings equipped with advanced technologies to optimize energy consumption. Machine learning (ML) algorithms are pivotal in enabling these technologies, facilitating energy consumption forecasting, anomaly detection, and autonomous control systems. This paper offers a comprehensive review of state-of-the-art ML algorithms in the context of energy optimization within smart buildings. It categorizes and evaluates supervised, unsupervised, and reinforcement learning methods, focusing on their applications, advantages, and challenges. Furthermore, it discusses recent advancements and outlines future research directions, emphasizing the integration of innovative techniques like federated learning and adaptive energy management systems to enhance efficiency, scalability, and sustainability.",No,"본 논문은 머신러닝 알고리즘을 스마트 빌딩 에너지 최적화에 적용한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않고, 기존 연구들을 평가하고 정리하는 데 중점을 두고 있습니다."
Pollen Grains Classification with a Deep Learning System GPU-Trained,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662170,"Traditional approaches to automatic classification of pollen grains consisted of classifiers working with feature extractors designed by experts, which modeled pollen grains aspects of special importance for biologists. Recently, a Deep Learning (DL) algorithm called Convolutional Neural Network (CNN) has shown a great improvement in performance in many computer vision tasks such as classification, due to this great performance the computational requirements have increased considerably; therefore, it is advisable to use new platforms such as the Graphics Processing Unit (GPU), which offer large computational resources for the development of new systems with CNN. This paper presents the GPU-Trained implementation of a DL system with the CNN algorithm, proposing a CNN model capable of running on a GPU in real-time for the automatic classification of 19 different pollen grains belonging to 14 different families, which are found in high concentrations in Mexico, and which are large interest in areas such as beekeeping, paleoecology, botany, allergology, agriculture among others. These areas seek to improve the collection of palynological data in terms of time and accuracy. In order to evaluate our model, evaluation tests were performed in the NVIDIA Jetson TX2 Developer Kit GPU. Experimental results achieves around 90% in CCR and Sensitivity in the proposed model. Additionally, the proposed model works at a processing speed of 6,826 Frames Per Second (FPS) and has approximately 50% fewer parameters than reported in related works.",Yes,"논문은 CNN 기반의 딥러닝 모델을 GPU에서 실시간으로 구동할 수 있도록 구현하고, 19종의 꽃가루를 분류하는 새로운 시스템을 제안하며 실험 결과를 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문에 해당한다."
Research on several major diseases based on machine learning models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9948344,"Due to the continuous growth of disease types and past cases, it is more and more difficult to diagnose diseases only by manpower. Machine learning is a model mechanism that is sensitive to data and relies on a large amount of data to complete training. It is very suitable for medical diagnosis. Many scholars have tried to use ML to develop medical diagnosis systems, but they are basically not used in the real world at this stage. This article reviews the work related to medical detection of three major diseases (heart disease, cancer, and COVID-19), aiming to summarize previous experiences to help future scholars conduct research. Specifically, this paper summarizes the research status of the prediction of these three types of diseases based on machine learning methods, evaluate the accuracy and universality of the corresponding prediction models based on time as a clue, and use a comparative method to find out the progress researchers have made in this area and limitations still exist at this stage. And at the end of the article, the results and some potential work fields of the future in these studies are summarized.",No,"본 논문은 기존 연구들을 종합하여 요약하고 비교하는 리뷰 논문으로 보이며, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
Different Approaches in Depression Analysis : A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200001,"Clinical depression has been a common but a serious mood disorder nowadays affecting people of any age group. Since depression affects the mental state, the patient will find it difficult to communicate his/her condition to the doctor. Commonly used diagnostic measures are interview style assessment or questionnaires about the symptoms, laboratory tests to check whether the depression symptoms are related with other serious illness. With the emergence of machine learning and convolutional neural networks, many techniques have been developed for supporting the diagnosis of depression in the past few years. Since depression is a multifactor disorder, the diagnosis of depression should follow a multimodal approach for its effective assessment. This paper presents a review of various unimodal and multimodal approaches that have been developed with the aim of analyzing the depression using emotion recognition. The unimodal approach considers either of the attributes among facial expressions, speech, etc. for depression detection while multimodal approaches are based on the combination of one or more attributes. This paper also reviews several depression detection systems that use facial feature extraction methods that use eigenvalue algorithm, fisher vector algorithm, etc. and speech features such as spectral, acoustic feature, etc. The survey covers the existing emotion detection research efforts that use audio and visual data for depression detection. The survey shows that the depression detection using multimodal approach and deep learning techniques achieve greater performance over unimodal approaches in the depression analysis.",No,초록에서 해당 논문은 다양한 우울증 분석 기법들을 정리한 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이 아니라 기존 연구들을 종합하여 분석한 문헌 리뷰에 해당합니다.
Predicting Board Performance Using Classification Algorithms and Time Series Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10636861,"Higher Education Institutions are increasingly recognizing the value of Educational Data Mining (EDM), a method that collects and analyzes educational data. EDM can employ machine learning techniques to assess and predict students’ academic outcomes. This paper aims to apply EDM, integrating time series analysis and classification algorithms, to assist a university in the Philippines in improving its students’ performance on the Licensure Examination for Teachers. The study involved an in-depth literature reviews and applied feature selection techniques to determine the significance of specific attributes. Subsequently, a comparative analysis was undertaken among three classification algorithms to ascertain the most accurate model in predicting student’s examination performance. Various validation metrics were used to validate the results. Finally, the most optimal model was selected and integrated into the system. Moreover, the Rapid Application Development (RAD) model was employed to develop the “Board Prophet” system, which offers various functionalities. These includes an online dashboard for accessing historical data, time series analysis capabilities for predicting the university’s board performance, individual predictions of a students’ success or failure in the licensure examination, and proposed intervention plans based on the system’s predictions. Furthermore, the study’s findings revealed that the student’s General Weighted Average and Review Center were the two most significant attributes, with Decision tree outperforming other classification algorithms. The system features were assessed using ISO/IEC 25010 standards and the Technology Acceptance Model (TAM), with users showing high acceptance. These findings will empower the university make better decisions that will improve their students’ licensure examination performance.",Yes,"본 논문은 교육 데이터 마이닝 기법을 적용하여 학생들의 시험 성과를 예측하는 모델을 개발하고, 여러 분류 알고리즘을 비교 분석하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 개발된 시스템의 성능 평가와 사용자 수용도 분석까지 수행하여 직접적인 연구 기여가 명확하다."
Bias Detection and Correction Methods for Machine Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10654404,"Machine learning algorithms are powerful tools for solving complex problems and making data-driven decisions. However, they can also inherit or amplify biases from the data, the design, or the deployment of the algorithms, leading to unfair or discriminatory outcomes for certain groups or individuals. In this paper, we review the sources, types, and measures of bias in machine learning algorithms, and survey the existing methods for detecting and correcting bias at different stages of the algorithmic pipeline. We also discuss the challenges and limitations of these methods, and highlight some open questions and future directions for research on bias in machine learning algorithms.",No,"논문 초록은 기존 연구들을 리뷰하고 정리하는 내용으로, 직접적인 독창적 연구 결과나 새로운 방법론 제시보다는 기존 방법들의 조사와 논의에 초점을 맞추고 있습니다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 논문에 가깝다고 판단됩니다."
Private Synthetic Data Generation for Mixed Type Datasets,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825249,"In the face of escalating threats from privacy attacks on machine learning models, we propose a system that can artificially generate data that imitates real data but doesn’t contain any sensitive or personally identifiable information. The generated data, called synthetic data, will have the same semantic and statistical distribution as the original dataset but provide privacy guarantees. Compared to previous works that dealt with either structured or unstructured data separately, our work develops a complete hybrid pipeline for generating private synthetic datasets from complex datasets that consist of both structured (numerical or categorical) and unstructured data. The private synthetic data generated can be analyzed by collaborators and third parties without increasing the risks of leakage of sensitive data. We evaluate our system on Yelp reviews and drug side-effects datasets and calculate metrics for both quality and privacy. We introduce a context-aware exposure metric to quantify context-dependent memorization and use it along with exposure to evaluate privacy. Our evaluations demonstrate that our system generates meaningful private synthetic datasets that achieve good performance in characteristic similarity, utility, as well as privacy. Given these results, the generated synthetic data can be used by data scientists, researchers, and developers to address challenges related to data privacy, scarcity, diversity, and model testing in a wide range of applications including healthcare, insurance, and financial systems that rely on sensitive data.",Yes,"논문은 기존 연구와 차별화된 하이브리드 파이프라인을 개발하여 구조화 및 비구조화 데이터를 모두 포함하는 복합 데이터셋에서 프라이빗 합성 데이터를 생성하는 독창적인 시스템을 제안하고 평가하고 있다. 또한, 새로운 맥락 인지 노출 지표를 도입하여 프라이버시를 정량화하는 등 직접적인 연구 기여가 포함되어 있다."
Mining unstructured log files for recurrent fault diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990536,"Enterprise software systems are large and complex with limited support for automated root-cause analysis. Avoiding system downtime and loss of revenue dictates a fast and efficient root-cause analysis process. Operator practice and academic research have shown that about 80% of failures in such systems have recurrent causes; therefore, significant efficiency gains can be achieved by automating their identification. In this paper, we present a novel approach to modelling features of log files. This model offers a compact representation of log data that can be efficiently extracted from large amounts of monitoring data. We also use decision-tree classifiers to learn and classify symptoms of recurrent faults. This representation enables automated fault matching and, in addition, enables human investigators to understand manifestations of failure easily. Our model does not require any access to application source code, a specification of log messages, or deep application knowledge. We evaluate our proposal using fault-injection experiments against other proposals in the field. First, we show that the features needed for symptom definition can be extracted more efficiently than does related work. Second, we show that these features enable an accurate classification of recurrent faults using only standard machine learning techniques. This enables us to identify accurately up to 78% of the faults in our evaluation data set.",Yes,"논문은 로그 파일에서 반복적인 결함을 진단하기 위한 새로운 모델링 접근법과 의사결정 트리 분류기를 활용한 자동화된 결함 분류 방법을 제안하고 있으며, 이를 평가하기 위한 실험 결과도 포함하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문임을 보여준다."
Review and Prospect of Primary Frequency Regulation Evaluation for New Power System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10922461,"With the creation of new power systems in recent years, the system frequency security situation has significantly changed. One view is that replacing some thermal power plants with renewable sources like wind and solar is reducing system inertia and its frequency control capability. Conversely, increased variability on both supply and demand sides has led to frequent frequency safety incidents, challenging the safe and stable operation of power systems. Primary frequency regulation is crucial for enhancing the system's dynamic response to frequency disturbances, highlighting its importance to frequency security. Analyzing primary frequency regulation in the context of new power systems is essential. This study reviews literature on primary frequency regulation assessment from both unit and system perspectives. For units, the process can be evaluated using the frequency response curve, unit output power, and prime mover-governor model parameters. At the system level, methods like full-state time-domain simulations, linearized models, single-machine equivalent models, and AI techniques can assess primary frequency regulation capabilities. Lastly, this article forecasts future trends in evaluating primary frequency control in new power systems, considering the strengths and weaknesses of current approaches.",No,"본 논문은 기존 연구들을 종합적으로 검토하고 향후 전망을 제시하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 정리와 분석에 중점을 두고 있습니다."
Application of Machine Learning Techniques in Identification of Fungal Species: A Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10699143,"Identification of fungal species is an important task in the field of mycology that requires in-depth knowledge of the morphology, habitat and microscopic characteristics of fungi. However, manual identification processes are often time-consuming, require specialized skills, and can be prone to errors. In recent years, Machine Learning (ML) techniques have attracted interest as an alternative to support automatic fungal identification processes. The aim of this literature review is to investigate the application of ML techniques in the identification of fungal species, evaluate their effectiveness and reliability, and identify trends and challenges in their use. This research uses a literature review approach to collect and analyze related research that has been carried out in the application of ML in identifying fungal types. Research data was obtained from scientific journal databases, conferences and other trusted sources. This literature review reveals that ML techniques have been successfully applied in various contexts to identify fungal species. Classification, image processing and pattern recognition methods have been used to develop ML models capable of recognizing the morphological and microscopic characteristics of fungi with a satisfactory level of accuracy. Despite challenges such as limited data, variations in fungal morphology, and complex classification problems, the application of ML techniques promises to be an effective tool in supporting the identification of fungal types. By continuing to develop more sophisticated models and overcome existing challenges, ML has the potential to change the paradigm in the field of mycology and increase the efficiency and accuracy of fungal identification in the future.",No,"본 논문은 머신러닝 기법을 이용한 균류 종 식별에 관한 기존 연구들을 종합하여 분석한 문헌 리뷰이다. 따라서 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고, 기존 연구들의 동향과 한계를 평가하는 데 중점을 두고 있다."
Review of visual AI applications in smart surveillance for crime detection and prevention: a survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10660211,"The application of Visual Artificial Intelligence (AI) in smart surveillance systems has signifi-cantly evolved, particularly in the domain of crime detection and prevention. This survey aims to com-prehensively review and analyze the implementation of visual AI technologies within a smart surveillance framework, focusing on their role in enhancing security measures for crime detection. Most frameworks found in the literature rely on the use of Convolutional Neural Network for image classification, however, areas of improvements are still open particularly the limited availability of datasets, high computational costs, and effective deployment of these types of applications. The survey investigates various facets, in-cluding but not limited to, object detection, facial recognition, anomaly detection, behavior analysis, and scene understanding within the context of surveillance applications. Furthermore, this survey aims to of-fer insights into the current state-of-the-art methodologies, recent innovations, and prospects in the realm of Visual AI-powered smart surveillance, providing a foundation for further research and development in this evolving field.",No,"본 논문은 시각 인공지능을 활용한 스마트 감시 시스템에 대한 종합적인 리뷰 및 분석을 제공하는 서베이 논문으로, 기존 연구들을 정리하고 현황을 소개하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
A New Sentiment Analysis based Application for Analyzing Reviews of Web Series and Movies of Different Genres,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9058137,"This research paper proposes an application of sentiment analysis that works on the principle of machine learning. The proposed application provides a comparative analysis of web series and movies of different genres of a particular time period on the basis of sentiments of the viewers. Data is fetched from twitter through API keys and twitter access tokens. The movies and web series from the year 2017 to 2019 of four different genres were taken and sentiment analysis was performed on each web series and movie, which gives result in the form of positive reviews and negative reviews. The famous hashtag for each movie and web series are determined. The total number of tweet counts is 3000. A Table of each genre was formed that contained the name of movie and web series, percentage of positive sentiments of corresponding web series or movie and percentage of negative sentiments of corresponding movie or web series. The graphical representation of each genre was done to analyze the results graphically. The combined analysis was performed after calculating the average percentage reviews of a positive and negative sentiment of all the movies and web series of each genre. The graphical representation of the combined analysis is done to analyze the final results. Through the proposed application results were analyzed concluding that whether movies or web series of a particular genre in the year 2017-19 were more liked by the viewers.",Yes,"본 논문은 머신러닝 기반 감성 분석 애플리케이션을 제안하고, 트위터 데이터를 수집하여 다양한 장르의 영화와 웹 시리즈에 대한 감성 분석을 수행하는 독창적인 연구 내용을 포함하고 있다. 데이터 수집, 분석 방법, 결과 도출 및 시각화 과정을 직접 수행하여 연구 기여가 명확하다."
Robust Query Optimization in the Era of Machine Learning: State-of-the-Art and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597930,"Query optimizers are an essential component of database management systems (DBMSs) as they search for an execution plan that is expected to be optimal for a given query. However, they commonly use parameter estimates that are often inaccurate and make assumptions that may not hold in practice. Consequently, the optimizer may select sub-optimal execution plans at runtime, when these estimates and assumptions are not valid, which may result in poor query performance. Therefore, query optimizers do not adequately support the robustness of the database system. In this tutorial, we explore the notion of robustness in the context of query optimization, as well as how it is evaluated or even further supported. Firstly, we provide a comprehensive definition for the notion of robustness in this context that accounts for risks associated with execution plans and inaccurate parameter estimates as well as the limitations of the cost models. Next, we review the approaches proposed in the literature to address the issue of robustness, including techniques that rely on query re-optimization, discovering parameters, quantifying robustness, as well as recent techniques that employ machine learning. We focus on comparing traditional cost-model-based methods with modern ML-based techniques in terms of their ability to tackle the challenge of robustness in query optimization. Finally, we discuss the limitations and gaps in the current literature and provide some recommendations for future research directions.",No,"본 논문은 기존 연구들을 종합적으로 리뷰하고, 현재의 연구 동향과 미래 방향을 제시하는 튜토리얼 성격의 문헌 조사 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않고, 기존 연구의 정리와 비교에 중점을 두고 있습니다."
A Comparative Study on Supervised and Unsupervised Learning Approaches for Multilingual Text Categorization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1692037,"Recently users of internationally distributed information networks need tools and methods that enable them to discover, retrieve and categorize relevant information, in whatever language and form it may have been stored. This drives a convergence of numerous interests from diverse research communities focusing on the issues related to multilingual text categorization. In this work we compare and evaluate the performance of the leading supervised and unsupervised approaches for multilingual text categorization by using various performance measures and standard document corpora. For simplicity, we selected support vector machines (SVM) and latent semantic indexing (LSI) techniques as representatives of supervised and unsupervised methods for multilingual text categorization, respectively. The preliminary results show that our platform models including both supervised and unsupervised learning methods have the potentials for multilingual text categorization",Yes,논문 초록에서 SVM과 LSI 기법을 사용하여 다국어 텍스트 분류를 위한 지도학습 및 비지도학습 방법을 비교 평가하는 연구를 수행한 점이 명확히 드러납니다. 이는 기존 방법들을 단순히 소개하는 것이 아니라 직접 실험과 평가를 통해 성능을 분석한 독창적인 연구 내용임을 의미합니다.
An Empirical Evaluation of Machine Learning Algorithms for Identifying Software Requirements on Stack Overflow: Initial Results,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040720,"Context: The recent developments made during the last decade or two in requirements engineering (RE) methods have seen a rise in using different machine-learning (ML) algorithms to solve some complex RE problems. One such problem is identifying and classifying software requirements on Stack Overflow (SO). The suitability of ML-based techniques to this tackle problem has shown convincing results, much better than those generated by some traditional natural language processing (NLP) techniques. Nevertheless, a comprehensive and systematic comprehension of these ML based techniques is still deficient. Objective: To identify and classify the type of ML algorithms used for identifying software requirements on SO. Method: This article reports systematic literature review (SLR) gathering evidence published up to August, 2019. Results: This study identified 1073 published papers related to RE and SO. Only 12 primary papers were selected. The data extraction process revealed that; 1) Latent Dirichlet Allocation (LDA) topic modeling is the most widely used ML algorithm in the selected studies, and 2) Precision and recall are the most commonly used evaluation method to measure the performance of these ML algorithms. Conclusion: The SLR finds that while ML algorithms have great potential in the identification of RE on SO, they face some open issues that will ultimately affect their performance and practical application. The SLR calls for the collaboration between RE and ML researchers, to tackle the open issues facing the development of real-world ML systems.",No,본 논문은 머신러닝 알고리즘을 이용한 소프트웨어 요구사항 식별에 관한 기존 연구들을 체계적으로 문헌 검토(SLR)한 결과를 보고하는 리뷰 논문입니다. 직접적인 독창적 연구나 실험 결과를 제시하기보다는 기존 연구들을 종합하여 분석하는 데 초점이 맞춰져 있으므로 연구 논문에 해당하지 않습니다.
Reinforcement learning with space carving for plant scanning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350600,"Optimal plant reconstruction is an essential element in automating our future agriculture. Computerized inspection of proper growth, nutrition, or pest infestation has become mandatory in fully autonomous in-door or micro-farm settings, shifting from fixed to moving camera systems. In industrial environments, plant scanning must work efficiently with a limited number of significant images to become economically viable. We present an adaptive learning algorithm for agricultural plant inspection robots, in particular, a specific type of reinforcement learning that we developed for our micro-farming platform created within the EU project ROMI. We suggest a new approach to 3D plant reconstruction by integrating the space carving technique with categorical Deep Q-Networks. Space carving leverages images captured from various positions to create a binary voxel grid, representing the occupied and unoccupied spaces of the scanned object. The proposed method incorporates partial 3D reconstructions of plants obtained through space carving, which get compared to a ground truth model to calculate the reward and guide scanning policies. We explain the algorithmic details and the 3D reconstruction technique in design, implementation, and evaluation. Experimental results confirm our approach’s effectiveness in improving the 3D plant reconstruction process, highlighting its potential for further applications in agriculture and related fields.",Yes,"논문은 강화학습과 공간 조각(space carving) 기법을 결합한 새로운 3D 식물 재구성 알고리즘을 제안하고, 설계, 구현, 평가를 포함한 실험적 결과를 제시하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문에 해당한다."
Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10820328,"Early detection of Cerebral Palsy (CP) is crucial for effective intervention and monitoring. This paper tests the reliability and applicability of Explainable AI (XAI) methods using a deep learning method that predicts CP by analyzing skeletal data extracted from video recordings of infant movements. Specifically, we use XAI evaluation metrics — namely faithfulness and stability — to quantitatively assess the reliability of Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (GradCAM) in this specific medical application. We utilize a unique dataset of infant movements and apply skeleton data perturbations without distorting the original dynamics of the infant movements. Our CP prediction model utilizes an ensemble approach, so we evaluate the XAI metrics performances for both the overall ensemble and the individual models. Our findings indicate that both XAI methods effectively identify key body points influencing CP predictions and that the explanations are robust against minor data perturbations. Grad-CAM significantly outperforms CAM in the Relative Input Stability velocity (RISv) metric, which measures stability in terms of velocity. In contrast, CAM performs better in the Relative Input Stability bone (RISb) metric, which relates to bone stability, and the Relative Representation Stability (RRS) metric, which assesses internal representation robustness. Individual models within the ensemble show varied results, and neither CAM nor Grad-CAM consistently outperform the other, with the ensemble approach providing a representation of outcomes from its constituent models. Both CAM and Grad-CAM also perform significantly better than random attribution, supporting the robustness of these XAI methods. Our work demonstrates that XAI methods can offer reliable and stable explanations for CP prediction models. Future studies should further investigate how the explanations can enhance our understanding of specific movement patterns characterizing healthy and pathological development.",Yes,"본 논문은 뇌성마비 조기 진단을 위한 딥러닝 모델과 설명 가능한 AI(XAI) 기법의 신뢰성과 적용 가능성을 평가하는 독창적인 연구를 수행하고 있습니다. 새로운 데이터셋을 활용하고, XAI 평가 지표를 통해 모델과 설명 기법의 성능을 정량적으로 분석하는 직접적인 연구 기여가 포함되어 있습니다."
An Amalgamated Testability Measure Derived from Machine Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483429,"A testability measure provides test-related information about signal nodes of a circuit. Operations like test generation and test point insertion are exponentially complex in terms of the circuit size. Therefore, to be useful testability measure computation is kept linear, which makes the measures like controllabilities and observabilities, approximate. Well-known testability measures like SCOAP (Sandia controllability/observability analysis program) or COP (controllability and observability program) have played important roles in algorithms for test generation, test point insertion, and other test-related functions. Even the quantities such as distances of a node to primary input and output have been used as simple measures. Years of experience have shown that no single measure works for all situations – in test generation SCOAP may work best for one fault, while COP or distance do better for other faults in the same circuit. This study amalgamates all three measures mentioned here using the principal component analysis (PCA), an unsupervised machine learning procedure. This amalgamated measure, when used by a test generation program produced unexpected benefits. First, the measure reduced the test generation program backtracks for hard-to-detect faults below those by any single testability measure; the number of backtracks is a direct indicator of computing effort. Second, the backtracks reduced to 0 for several faults. This study tries to prove the efficacy of amalgamation by running categorical experiments, namely, testability analysis accuracy, ATPG improvements, and faults classification. In the continuing investigation, we plan to combine more testability measures into PCA. We will also investigate applications like test-point insertion and other test related functions in the future.",Yes,"논문은 기존의 여러 테스트 가능성 측정 방법들을 PCA를 이용해 통합하는 독창적인 연구를 수행하고 있으며, 이를 통해 테스트 생성 프로그램의 성능 향상이라는 구체적인 실험 결과를 제시하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
ChartWalk: Navigating large collections of text notes in electronic health records for clinical chart review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904479,"Before seeing a patient for the first time, healthcare workers will typically conduct a comprehensive clinical chart review of the patient's electronic health record (EHR). Within the diverse documentation pieces included there, text notes are among the most important and thoroughly perused segments for this task; and yet they are among the least supported medium in terms of content navigation and overview. In this work, we delve deeper into the task of clinical chart review from a data visualization perspective and propose a hybrid graphics+text approach via ChartWalk, an interactive tool to support the review of text notes in EHRs. We report on our iterative design process grounded in input provided by a diverse range of healthcare professionals, with steps including: (a) initial requirements distilled from interviews and the literature, (b) an interim evaluation to validate design decisions, and (c) a task-based qualitative evaluation of our final design. We contribute lessons learned to better support the design of tools not only for clinical chart reviews but also other healthcare-related tasks around medical text analysis.",Yes,"본 논문은 전자 건강 기록 내 텍스트 노트의 임상 차트 리뷰를 지원하는 인터랙티브 도구인 ChartWalk를 제안하고, 이를 위한 설계 과정과 평가를 포함한 독창적인 연구 내용을 다루고 있다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단된다."
Cybersecurity in SCADA Systems with Advanced AI and ML Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10741936,"SCADA systems are crucial for monitoring and controlling key infrastructures such as power grids, water supply, transportation networks, and industrial processes. With the increasing interconnectivity and complexity of these systems, they are exposed to numerous cybersecurity challenges. This paper provides a comprehensive literature review focusing on the vulnerabilities in SCADA systems and explores measures to mitigate these risks. Key vulnerabilities include the lack of built-in security mechanisms, open access networks, proprietary protocols, reliance on legacy systems and insecure network architectures. Various cybersecurity strategies such as adopting security standards, utilizing encryption, regular updates, network segmentation, intrusion detection systems (IDS) and security information and event management (SIEM) are discussed to enhance SCADA security. Additionally, the paper highlights the growing significance of Artificial Intelligence (AI) and Machine Learning (ML) in enhancing the security and robustness of SCADA systems.",No,"초록에서 본 논문은 SCADA 시스템의 취약점과 보안 대책에 대한 포괄적인 문헌 리뷰를 제공하는 것으로 보이며, 직접적인 독창적 연구 결과나 실험적 기여가 명시되어 있지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당한다고 판단됩니다."
A Comprehensive Survey of Intrusion Detection System Using Machine Learning and Deep Learning Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10717043,"The necessity for Computer networks' use is expanding quickly, which raises problems with preserving network secrecy, availability, and integrity. Intrusion can be defined as an intentional breach of security rules within a secured network. Such intrusions can be identified by an Intrusion Detection system which searches for any malicious actions and recognized dangers within a secured network. Intrusion detection systems patrol the traffic passing through computer systems and give out notifications when they do. Recently, an immense upsurge in cyber-attack cases on computer networks has imposed the need for an effective Intrusion detection system than ever before. Nowadays, network administrators are utilizing multivarious kinds of Intrusion Detection Systems (IDS), in order to monitor network traffic for malicious and unauthorized activities. This review paper focuses on various research works that has developed an approach for evaluating or identifying IDS using many kinds of Machine Learning (ML) and Deep Learning (DL) techniques.",No,초록에서 해당 논문은 다양한 머신러닝 및 딥러닝 기법을 이용한 침입 탐지 시스템에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과를 제시하는 연구 논문이 아니라 기존 연구를 정리한 조사 논문에 해당합니다.
Comparative Analysis of Dimensionality Reduction Techniques in Machine Learning Models for Liver Disease Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10714735,"Liver disease is a significant global health concern that necessitates accurate and timely diagnosis. This review paper explores the application of machine learning techniques, specifically focusing on the impact of dimensionality reduction methods, in liver disease prediction. This study evaluates both linear and non-linear dimensionality reduction techniques to assess their performance, accuracy, and computational efficiency in the context of liver disease prediction. Through a comprehensive review of existing research and experimental results, this study reveals that while non-linear methods offer advantages in data visualization, linear methods like PCA generally outperform them in terms of model accuracy and computational efficiency. This review provides valuable insights for researchers and healthcare professionals seeking to optimize machine learning models for liver disease diagnosis. By understanding the strengths and limitations of different dimensionality reduction techniques, researchers can select the most appropriate methods to improve the accuracy and efficiency of liver disease prediction models.",No,초록에서 해당 논문은 기존 연구와 실험 결과를 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 기여를 포함한 연구 논문으로 보기 어렵습니다.
Reinforcement Learning for Autonomous Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895165,"Reinforcement Learning (RL) has emerged as a transformative technology for autonomous vehicles, enabling sophisticated decision-making systems that enhance driving safety, efficiency, & adaptability. This paper explores the application of RL algorithms in the context of autonomous vehicle control, focusing on the development of intelligent agents capable of learning optimal driving policies through interaction with complex environments. The study reviews various RL techniques, including Deep Q-Learning, Policy Gradient methods, & Actor-Critic frameworks, evaluating their effectiveness in addressing key challenges such as real-time decision making, dynamic environment adaptation, & multi-agent interactions. Emphasis is placed on the design of reward functions, exploration strategies, & simulation environments, which are crucial for training robust & reliable autonomous driving systems. Case studies demonstrate the application of RL in scenarios such as lane-keeping, adaptive cruise control, & collision avoidance. The paper also discusses advancements in simulation platforms & hardware acceleration that facilitate scalable RL experiments. By integrating theoretical insights with practical implementations, this work provides a comprehensive overview of current developments in RL for autonomous vehicles & identifies future research directions aimed at overcoming limitations & achieving safer, more efficient.",No,"초록 내용은 기존 강화학습 기법들의 적용과 사례 연구, 그리고 관련 기술 동향을 종합적으로 리뷰하는 데 중점을 두고 있어, 독창적인 연구 결과나 새로운 실험적 기여가 명확히 드러나지 않습니다. 따라서 본 논문은 연구 논문이라기보다는 리뷰 논문에 더 가깝다고 판단됩니다."
AI and Climate Protection: Research Gaps and Needs to Align Machine Learning with Greenhouse Gas Reductions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10805301,"Machine learning (ML) promises to revolutionize our socio-economic landscape, yet its impacts on greenhouse gas (GHG) emissions and strategies to harness ML for climate protection are not well understood. This discussion paper reviews key research on ML's GHG effects, highlighting significant research gaps and needs for a climate-oriented ML transformation. The results show that research on GHG emissions caused during model development, training, and operation is progressing. However, there is no comprehensive overview of effective measures to reduce them along the entire ML software and hardware life cycle. (Industrial) research on the GHG effects of ML applications focuses mainly on GHG reduction potentials while neglecting the possibility that ML applications also increase emissions. Thus, research in at least three key areas is needed to align ML with GHG reductions. First, robust methods to assess and report the GHG impacts of ML models and applications are required to systematically compare them and identify best practices. Second, comprehensive GHG assessments at every effect level are essential to identify measures to increase the GHG efficiency of ML models and exploit their climate protection potential. Third, analysing ML business models is crucial to propose measures that incentivize ML providers and users to reduce GHG emissions. Addressing these issues is essential for mindfully steering ML toward GHG reductions. Otherwise, there is a risk that the GHG footprint of ML will skyrocket, that ML applications will primarily accelerate GHG-intensive activities, and that an opportunity for decoupling (economic) growth and GHG emissions will be missed.",No,"이 논문은 머신러닝과 온실가스 감축 간의 연구 격차와 필요성을 논의하는 리뷰 및 논의 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 향후 연구 방향을 제안하는 성격입니다."
"An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933776,"Model accuracy is the traditional metric employed in machine learning (ML) applications. However, privacy, fairness, and robustness guarantees are crucial as ML algorithms increasingly pervade our lives and play central roles in socially important systems. These four desiderata constitute the pillars of Trustworthy ML (TML) and may mutually inhibit or reinforce each other. It is necessary to understand and clearly delineate the trade-offs among these desiderata in the presence of adversarial attacks. However, threat models for the desiderata are different and the defenses introduced for each leads to further trade-offs in a multilateral adversarial setting (i.e., a setting attacking several pillars simultaneously). The first half of the paper reviews the state of the art in TML research, articulates known multilateral trade-offs, and identifies open problems and challenges in the presence of an adversary that may take advantage of such multilateral trade-offs. The fundamental shortcomings of statistical association-based TML are discussed, to motivate the use of causal methods to achieve TML. The second half of the paper, in turn, advocates the use of causal modeling in TML. Evidence is collected from across the literature that causal ML is well-suited to provide a unified approach to TML. Causal discovery and causal representation learning are introduced as essential stages of causal modeling, and a new threat model for causal ML is introduced to quantify the vulnerabilities introduced through the use of causal methods. The paper concludes with pointers to possible next steps in the development of a causal TML pipeline.",No,"논문 초록은 기존 연구들을 리뷰하고, 문제점과 도전과제를 제시하며, 인과적 방법론을 제안하는 개념적이고 종합적인 논의에 초점을 맞추고 있습니다. 직접적인 실험 결과나 새로운 알고리즘 개발 등 독창적인 연구 내용이 포함되었다고 명확히 언급되지 않아 연구 논문으로 보기 어렵습니다."
Integration of robust 3D modeling software into the design curriculum,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841663,"Engineering technology design curriculum is compartmentalized, and the students are not ready for contribution to the design team. This allegation is partly true, as there is no effective means of student integration of design concept into an indusrially sound application, using an industrially robust three dimensional design and manufacturing package. This paper studies the effectiveness of an Engineering Technology Program-Design concentration, via the traditional emphasis without an industrially capable integrated CAD/CAM software. The program effectiveness is evaluated by outcome based program evaluation and review technique. Next the program is studied along a different stream with the integration of a capable and robust 3D CAD/CAM/analysis software package. The same criteria of program effectiveness is used to compare the achievement and performance of the program. This is an evaluative study of efforts to integrate the design curriculum with a comprehensive unifying design software. Twenty final year students in a capstone Machine and Tool Design course used the robust design software. At the culmination of the course they were asked about their design skill experience and the effectiveness of the course. The survey revealed that student learning and professional design development was enhanced with the use of an effective CAD/CAM integration tool that enabled students to synthesize design problems which were challenging and practical. The result of this research is crucial to the ABET-2000 outcome based evaluation criteria and its implementation.",Yes,"본 논문은 3D CAD/CAM 소프트웨어를 설계 교육과정에 통합하는 효과를 평가하기 위해 학생들의 성과와 설계 능력 향상을 조사한 연구로, 직접적인 실험과 평가를 통해 독창적인 연구 결과를 제시하고 있다. 따라서 연구 논문에 해당한다."
The Impact of Artificial Intelligence on Language Translation: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10438431,"In the context of a more linked and globalized society, the significance of proficient cross-cultural communication has been increasing to a position of utmost importance. Language functions as a crucial medium that establishes connections among people, corporations, and countries, demanding the implementation of precise and effective translation systems. This comprehensive review paper aims to contribute to the evolving landscape of AI-driven language translation by critically examining the existing literature, identifying key debates, and uncovering areas of innovation and limitations. The primary objective is to provide a nuanced understanding of the current state of AI-driven language translation, emphasizing the advancements, challenges, and ethical considerations. In this review, ongoing debates surrounding AI-driven language translations were actively involved. By evaluating different viewpoints and methodologies, insights into unresolved questions that contribute to a broader discourse in the field were provided. The future trajectory of this study involves the incorporation of cross-lingual dialect adaptability and the advancement of Artificial Intelligence translation systems, with a focus on prioritizing inclusion and cultural understanding.",No,"본 논문은 기존 문헌을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 포함하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 분석과 논의에 중점을 두고 있다."
Advancing Solar Energy: Machine Learning Approaches for Predicting Photovoltaic Power Output,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10604373,"This research is primarily concentrated on predicting the output of photovoitaic power, an essential field in the study of renewable energy. The paper comprehensively reviews various forecasting methodologies, transitioning from conventional physical and statistical methods to advanced machine learning (ML) techniques. A significant shift has been observed from traditional point forecasting to machine learning-based forecasting in solar power. This transition offers a broader and more detailed perspective for power system operators. The core of this research lies in applying and comparing three distinct Machine Learning algorithms for forecasting photovoltaic power output. The primary aim is to evaluate each method's accuracy and to identify the algorithm with the lowest prediction error. This comparative analysis is crucial for determining the most effective machine learning forecasting method, significantly contributing to the more reliable and efficient integration of renewable energy into power systems.",Yes,논문은 기존 연구들을 리뷰하는 동시에 세 가지 머신러닝 알고리즘을 적용하고 비교하는 독창적인 연구를 수행하고 있다. 이는 단순한 리뷰가 아니라 새로운 실험과 분석을 포함한 직접적인 연구 기여로 판단된다.
Ensemble Image Explainable AI (XAI) Algorithm for Severe Community-Acquired Pneumonia and COVID-19 Respiratory Infections,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721585,"Since the onset of the COVID-19 pandemic in 2019, many clinical prognostic scoring tools have been proposed or developed to aid clinicians in the disposition and severity assessment of pneumonia. However, there is limited work that focuses on explaining techniques that are best suited for clinicians in their decision making. In this article, we present a new image explainability method named ensemble AI explainability (XAI), which is based on the SHAP and Grad-CAM++ methods. It provides a visual explanation for a deep learning prognostic model that predicts the mortality risk of community-acquired pneumonia and COVID-19 respiratory infected patients. In addition, we surveyed the existing literature and compiled prevailing quantitative and qualitative metrics to systematically review the efficacy of ensemble XAI, and to make comparisons with several state-of-the-art explainability methods (LIME, SHAP, saliency map, Grad-CAM, Grad-CAM++). Our quantitative experimental results have shown that ensemble XAI has a comparable absence impact (decision impact: 0.72, confident impact: 0.24). Our qualitative experiment, in which a panel of three radiologists were involved to evaluate the degree of concordance and trust in the algorithms, has showed that ensemble XAI has localization effectiveness (mean set accordance precision: 0.52, mean set accordance recall: 0.57, mean set FF1: 0.50, mean set IOU: 0.36) and is the most trusted method by the panel of radiologists (mean vote: 70.2%). Finally, the deep learning interpretation dashboard used for the radiologist panel voting will be made available to the community. Our code is available at https://github.com/IHIS-HealthInsights/Interpretation-Methods-Voting-dashboard.",Yes,"본 논문은 기존의 설명 가능 인공지능(XAI) 기법들을 결합한 새로운 앙상블 이미지 설명 방법을 제안하고, 이를 통해 폐렴 및 COVID-19 환자의 사망 위험 예측 모델을 시각적으로 해석하는 독창적인 연구 내용을 포함하고 있습니다. 또한 정량적, 정성적 평가를 수행하고 전문가 패널의 신뢰도 평가를 통해 방법의 유효성을 검증한 점에서 연구 논문으로 판단됩니다."
Green AI Quotient: Assessing Greenness of AI-based software and the way forward,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298412,"As the world takes cognizance of AI's growing role in greenhouse gas(GHG) and carbon emissions, the focus of AI research & development is shifting towards inclusion of energy efficiency as another core metric. Sustainability, a core agenda for most organizations, is also being viewed as a core non-functional requirement in software engineering. A similar effort is being undertaken to extend sustainability principles to AI-based systems with focus on energy efficient training and inference techniques. But an important question arises, does there even exist any metrics or methods which can quantify adoption of “green” practices in the life cycle of AI-based systems? There is a huge gap which exists between the growing research corpus related to sustainable practices in AI research and its adoption at an industry scale. The goal of this work is to introduce a methodology and novel metric for assessing “greenness” of any AI-based system and its development process, based on energy efficient AI research and practices. The novel metric, termed as Green AI Quotient, would be a key step towards AI practitioner's Green AI journey. Empirical validation of our approach suggest that Green AI Quotient is able to encourage adoption and raise awareness regarding sustainable practices in AI lifecycle.",Yes,"논문은 AI 기반 시스템의 에너지 효율성과 지속 가능성을 평가하는 새로운 지표인 Green AI Quotient를 제안하고, 이를 실증적으로 검증하는 연구 내용을 포함하고 있다. 이는 독창적인 연구 방법론과 실험적 검증을 통해 직접 기여하는 연구 논문으로 판단된다."
Monocular Depth Estimation Based on Deep Learning:A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327548,"Monocular depth estimation relied on RGB images is an important ill posed problem in ithe system of computer vision. Recently, people use the method of deep learning to discuss this problem Most of the existing monocular depth estimation algorithms relied on convolution neural network. Depth estimation based on 2D images has important applications in image segmentation, 3D object detection, robot navigation, object tracking and autonomous driving. This paper gives a brief overview of this problem, reviews, evaluates and discusses the monocular depth estimation algorithms relied on deep learning, and looks forward to the direction of further research in the face of some challenges.",No,"본 논문은 단일 시점 깊이 추정에 관한 기존 연구들을 종합적으로 검토하고 평가하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하고 있지 않습니다. 따라서 연구 논문보다는 연구 동향을 정리한 리뷰 논문에 해당합니다."
Product Categorization by Title Using Deep Neural Networks as Feature Extractor,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207093,"Natural Language Processing (NLP) has been receiving increasing attention in the past few years. In part, this is related to the huge flow of data being made available everyday on the internet, which increased the need for automatic tools capable of analyzing and extracting relevant information, especially from the text. In this context, text classification became one of the most studied tasks on the NLP domain. The objective is to assign predefined categories or labels to text or sentences. Important applications include sentence classification, sentiment analysis, spam detection, among many others. This work proposes an automatic system for product categorization using only their titles. The proposed system employs a state-of-the-art deep neural network as a tool to extract features from the titles to be used as input in different machine learning models. The system is evaluated in the large-scale Mercado Libre dataset, which has the common characteristics of real-world problems such as imbalanced classes, unreliable labels, besides having a large number of samples: 20,000,000 in total. The results showed that the proposed system was able to correctly categorize the products with a balanced accuracy of 86.57% on the local test split of the Mercado Libre dataset. It also surpassed the fourth place on the public rank of the MeLi Data Challenge with 91.19% of balanced accuracy, which represents less than 1% of the difference to the winner.",Yes,"논문은 딥 뉴럴 네트워크를 활용한 제품 제목 기반 자동 분류 시스템을 제안하고, 대규모 실제 데이터셋에서 평가한 결과를 제시하고 있다. 이는 기존 연구를 바탕으로 한 독창적인 방법론과 실험 결과를 포함한 연구 논문으로 판단된다."
Development of magnetic-sensor-based hand gesture recognition system for sign language,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167092,"Sign language recognition is essential for the automatic translation of sign languages to enable communication for hearing-impaired people. This work proposes a system based on multiple magnetic sensors for recognizing hand gestures related to sign language alphabets. In particular, a magnetic detection system consisting of six magnetic sensor nodes measures the orientation of fingers and palms. A deep learning classification algorithm processes the measured orientation data. Experimental tests validate the proposed system and classification method. The results show that the proposed method provides close to 100% classification accuracy for 26 sign language alphabets under laboratory conditions. Thus, the feasibility of the proposed gesture recognition system for automatic translation of sign language alphabets is proved.",Yes,"본 논문은 자기 센서를 이용한 손 제스처 인식 시스템을 개발하고, 딥러닝 분류 알고리즘을 적용하여 26개 수화 알파벳을 인식하는 실험적 검증을 수행하였다. 이는 독창적인 연구 내용과 실험 결과를 포함한 직접적인 연구 기여로 판단된다."
Leveraging PHM in Conjunction with Intelligent Scheduling to Improve Manufacturing Resilience,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172698,"The scheduling of a manufacturing facility is a complex endeavor even when the equipment resources are always considered available or at least available 24/7 except for planned maintenance. However, under real-world conditions, the added complexity of unplanned downtime can significantly increase the difficulty of meeting deadlines. More reliable and efficient operations can be achieved by predicting problems and then rescheduling operations to minimize or avoid the problems' adverse effects. Stottler Henke Associates Incorporated (SHAI) has been working with the US Air Force on best practices for leveraging diagnostics, prognostics, and health management in conjunction with intelligent scheduling to improve manufacturing system resilience. The goal, then, is for diagnostic systems to identify impending faults quickly and automatically, providing the information needed to the intelligent scheduling system in order to minimize or completely mitigate the issues. Prognostic systems can estimate impending failures or rates of performance degradation; the intelligent scheduling system uses these diagnoses and predictions, along with the manufacturing deadlines and priorities, to develop mitigation strategies to minimize or avoid disruptions. The strategies include scheduling offline operations optimally to minimize the effects of the machines being offline; rescheduling operations due to machines being offline, reconfiguring systems to change their capacity and performance profiles, and/or reduce the usage of critical equipment to lengthen their remaining useful life. In the most uncomplicated cases, the schedule can be adjusted using simple strategies such as reassigning tasks from the faulted equipment to other equipment with similar capabilities. However, in many cases, more global analysis of an adjustment of the schedule is necessary to satisfy the facility's deadlines and other manufacturing goals. A system for resilience needs to be able to model and simulate the manufacturing system. That is, scenarios need to run to evaluate the manufacturing system's response to various types of problem scenarios and analyze the effectiveness of responses. This evaluation capability can be used to compare resilience strategies that specify optimal policies for employing diagnostic and prognostic capabilities and for responding to current and projected faults via rescheduling and reconfiguration. This will provide insight into the most critical pieces of equipment as related to unplanned downtime. That is, when operating under normal conditions, the most constrained pieces of equipment, may be different from the equipment that has the most significant adverse effect if it was unavailable. Intuitively this may be difficult to understand, take the example that every piece manufactured goes through one of two of the same machines that are running at full speed, but there is a 3rd machine that performs a specialized operation on a subset of very high-value parts. In this case, the two machines are the constraint on overall throughput, but it may not be evident that the 3rd machine, if down, is the constraint on profit. Being able to run scenarios will surface actual cause and effect relationships. This paper will expand on these ideas and lessons learned from real-world application of these ideas.",Yes,"논문 초록은 제조 시스템의 복원력을 향상시키기 위해 진단, 예측 및 지능형 스케줄링을 결합하는 구체적인 방법론과 시스템 설계, 시뮬레이션 및 평가 전략을 다루고 있어 독창적인 연구 내용과 실질적인 기여를 포함하고 있음을 보여준다. 또한 실제 적용 사례와 교훈을 바탕으로 한 분석을 제시하고 있어 연구 논문으로 판단된다."
Supervised named entity recognition in Assamese language,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019728,"In each and every natural language nouns play a very important role. A subcategory of noun is proper noun. They represent the names of person, location, organization etc. The task of recognizing the proper nouns in a text and categorizing them into some classes such as person, location, organization and other is called Named Entity Recognition. This is a very essential step of many natural language processing applications that makes the process of information extraction easier. Named Entity Recognition (NER) in most of the Indian languages has been performed using rule-based, supervised and unsupervised approaches. In this work our target language is Assamese, the language spoken by most of the people in North-Eastern part of India and particularly in Assam. In Assamese language, Named Entity Recognition has been performed using the rule based and suffix stripping based approaches. Supervised learning technique is more useful and can be easily adapted to new domains compared to rule based approaches. This paper reports the first work in Assamese NER using a machine learning technique. In this paper Assamese Named Entity Recognition is performed using Naïve Bayes classifier. Since feature extraction plays the most important role in getting better performance in any machine learning technique, in this work our aim is to put forward a description of a few important features related to Assamese NER and performance measure of the system using these features.",Yes,"이 논문은 Assamese 언어의 Named Entity Recognition을 위해 Naïve Bayes 분류기를 사용한 최초의 기계 학습 기반 연구를 보고하고 있으며, 특징 추출과 성능 측정에 대한 구체적인 내용을 포함하고 있다. 이는 독창적인 연구 기여를 포함한 연구 논문으로 판단된다."
Biomechanical Risk Evaluation Through Machine Learning Algorithms Fed with Features Extracted From sEMG of Neck Extensors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796263,"Work-related musculoskeletal disorders (WRMDs) affect millions of workers worldwide, posing substantial economic burdens on industries and healthcare systems. Prolonged exposure, repetitive tasks, awkward postures and intensive efforts are keys factors contributing to the development of WRMDs. Several quantitative or semi-quantitative methodologies are employed to evaluate the biomechanical risk and to prevent WRMDs in the occupational ergonomics field. However, these methods are still time-consuming and operator-dependent. Recently, the application of wearable sensors combined with artificial intelligence is providing remarkable results in terms of biomechanical risk assessment in the occupational ergonomics field. Therefore, in the present work, we examined the potential of Machine Learning (ML) models to differentiate between biomechanical risk categories as defined by the Revised NIOSH Lifting Equation (RNLE). The ML models were trained using time-domain and frequency-domain features extracted from surface electromyographic (sEMG) signals obtained from the neck extensor muscles of four healthy subjects during weight-lifting tasks. The study findings indicated that the Support Vector Machine algorithm performed the best, achieving an accuracy of 83.6% and an area under the receiver operating characteristic curve of 89.9%. However, the study was limited by its small sample size and the restricted age range of the volunteers. Future research involving a larger and more diverse population in terms of age and number of subjects could further validate the effectiveness of the proposed methodology.",Yes,"본 논문은 sEMG 신호에서 추출한 특징을 이용해 머신러닝 모델을 훈련시키고, 이를 통해 생체역학적 위험도를 평가하는 독창적인 연구를 수행하였다. 이는 기존 방법의 한계를 극복하고자 한 새로운 접근법을 제시한 연구 논문으로 판단된다."
Fast and Lightweight UAV-based Road Image Enhancement Under Multiple Low-Visibility Conditions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150374,"The amalgamation of Unmanned Aerial Vehicle (UAV) based systems with models built on Artificial Intelligence (AI) and Computer Vision approaches have enabled several applications in urban planning and smart cities, such as remote health monitoring of roads and infrastructure. However, most of such existing models are trained and evaluated for clear lighting conditions, and they do not perform well under low visibility. This work proposes a fast and lightweight approach for deployment on UAV-based systems that can (i) detect the low-visibility condition in a road image captured by a UAV, and (ii) alleviate it and enhance the quality of the road image. The proposed approach achieves state-of-the-art results and thus establishes itself as an essential precursor to downstream Computer Vision tasks related to remote monitoring of roads, such as identification of different distress conditions.",Yes,"논문은 UAV 기반 시스템에서 저조도 및 저시정 조건 하의 도로 이미지 품질 향상을 위한 새로운 방법을 제안하고 있으며, 이는 기존 연구와 차별화된 독창적인 연구 내용이다. 또한, 제안된 접근법의 성능 평가를 통해 실질적인 기여를 하고 있음을 알 수 있다."
A Combined Method for Object Detection under Rain Conditions Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799899,"The process of object detection utilizing deep learning is one of the most important deep learning applications and computer vision techniques, where one can learn image features in normal weather conditions and different rain conditions. Therefore, a deep convolutional neural network (DCNN) has become more important for object detection. Rain is a common and maj or factor in degrading image quality and decreasing object detection reliability. The main aim of this work is to remove rain streaks to get high reliability in detection process and decrease the error rate, in normal conditions and different rain conditions (light, medium and heavy). Firstly, the quality of the images is improved and removed rain streaks by de-raining algorithm that use the Deep Detail Network (DDN) method. Then the way deep learning is the main object detector, through use the YOLO to detect objects and determine its type. YOLOv3 and tiny-YOLOv3 have been determine from the literature review as the most suitable and efficient algorithms for detecting objects in real time after improving the image quality. The performance of these algorithms has been calculated and compared with each other. To evaluate the effectiveness of the devised approach (De-raining+YOLOv3), Fl-score, Recall, and Precision were computed. Using the proposed method combined from DDN with YOLOv3 technique (De-raining+YOLOv3), the mean of Fl-score of 95.02%, Recall of 97.22%, and a Precision of 92.92% were attained. Our presented approach is more resilient and accurate in object detection under rain conditions according to the findings of the results of the experiments. It is considered the best way for object detection under rain conditions with high reliability.",Yes,"본 논문은 비오는 조건에서 객체 검출의 신뢰성을 높이기 위해 딥러닝 기반의 비 제거 알고리즘과 YOLOv3 객체 검출기를 결합한 새로운 방법을 제안하고, 실험을 통해 성능을 평가한 연구이다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문에 해당한다."
Feature Selection and Design of Intrusion Detection System Based on k-Means and Triangle Area Support Vector Machine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431864,"Nowadays, challenged by malicious use of network and intentional attacks on personal computer system, intrusion detection system has become an indispensible and infrastructural mechanism for securing critical resource and information. Most current intrusion detection systems focus on hybrid supervised and unsupervised machine learning technologies. The related work has demonstrated that they can get superior performance than applying single machine learning algorithm in detection model. Besides, with the scrutiny of related works, feature selecting and representing techniques are also essential in pursuit of high efficiency and effectiveness. Performance of specified attack type detection should also be improved and evaluated. In this paper, we incorporate information gain (IG) method for selecting more discriminative features and triangle area based support vector machine (TASVM) by combining k-means clustering algorithm and SVM classifier to detect attacks. Our system achieves accuracy of 99.83%, detection rate of 99.88% and false alarm rate of 2.99% on the 10% of KDD CUP 1999 evaluation data set. We also achieve a better detection performance for specific attack types concerning precision and recall.",Yes,"논문은 정보 이득(IG) 기반 특징 선택과 k-평균 클러스터링과 SVM을 결합한 새로운 침입 탐지 시스템 설계를 제안하며, 실험을 통해 성능을 평가하고 있다. 이는 독창적인 연구 방법과 실험 결과를 포함한 연구 논문에 해당한다."
Experimental validation of learning accomplishment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=632677,"This paper reports on educational assessment: measures to validate that a subject has been learned. Outcomes described are from actual UCLA Computer Science courses, but the approach is independent of subject matter. There is a bibliography describing application of the methods presented here to other subjects and school levels. That bibliography summarizes an extensive literature including assessment in distance learning and elementary school situations. The text here outlines ideas and derivations, and the references enable deeper understanding, but a reader can use these procedures without either. The paper describes a way to apply them and to display students' learning. The paper contributes new ways to indicate student achievement and distinguish individuals with subject mastery from others tested. This is by figures shown here that enable teachers and students to understand and apply this form of testing. The measures use probability and are based on concepts of information. The method is generally applicable. It is related to earlier work on a computerized learning system called Plate that handled a wide variety of subjects at many educational levels. This paper describes ways to use an unconventional assessment approach to rapidly determine concepts not yet absorbed. New methods presented here are those the author developed in classes he taught. Ideas and tools in this paper could empower others to expand and enrich their teaching and the learning processes it is to assist.",Yes,"논문은 새로운 평가 방법과 도구를 제안하고 이를 실제 수업에서 개발 및 적용한 내용을 포함하고 있어 독창적인 연구 기여가 명확합니다. 또한, 확률과 정보 개념을 활용한 새로운 학습 성취도 측정 방식을 제시하여 교육 평가 분야에 직접적인 연구적 기여를 하고 있습니다."
Research Advanced in Object Detection Based on Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10071507,"Object detection has always been a hot topic in the computer vision community. The task is easy for humans, while it is difficult for computers to learn the informational concepts to locate object positions, especially for the sense that multiple objects and cluttered backgrounds are mixed together. Traditional object detection methods are based on manual feature extraction. Coupled with the development in deep learning, both the detection accuracy and speed had made a breakthrough. This paper provides a review of deep learning-based object detection architectures. First, a simple summary on deep learning and its applications area are introduced. Then, this paper pay attention to the basic framework of object detection and its performance index. Then, it classify the object detection algorithms into three categories according to their structure: one-stage, two-stage and point-based. And this paper evaluates their performance as well as their practicality. Finally, several current problems and solutions are provided as suggestions for advanced research on object detection and its applications.",No,"본 논문은 딥러닝 기반 객체 검출 기술에 대한 리뷰 논문으로, 기존 연구들을 정리하고 분류하며 평가하는 내용을 담고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 기여를 포함하지 않아 연구 논문에 해당하지 않습니다."
Comparative Analysis of Different Neural Network Models for Speaker Gender Recognition by Voice,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421302,"This study examines the exploration of gender identification using voice data investigating how different deep learning models perform in this field. The models analyzed include the Artificial Neural Network (ANN) 2D Convolutional Neural Network (2D-CNN) Feedforward Neural Network (FNN) and Long Short-Term Memory (LSTM). The voice dataset used in the research consists of 20 feature columns and a label column. It undergoes steps, such, as label encoding and feature scaling. The models are carefully evaluated using important performance metrics like loss and accuracy percentages. Notably the findings highlight the strengths of each model; the ANN and FNN models demonstrate accuracy rates while the 2D-CNN model excels at capturing spatial relationships and the LSTM model focuses on temporal dependencies. The main emphasis of this work is a thorough review of the literature on speech detection and voice analysis using deep learning techniques as well as standard approaches. Beyond academia this research has implications for voice-controlled systems and security measures. The strong performance of ANN and FNN models shows promise for real world integration with applications in voice-based authentication and virtual assistants. These findings provide insights for researchers and professionals interested in developing gender recognition systems by emphasizing the importance of considering model architectures along, with their unique capabilities when working with voice data. Overall, this research adds to our understanding of the strengths and limitations of learning models when it comes to analyzing voices, for gender recognition. It provides insights for research and practical applications, in this field.",Yes,"본 논문은 다양한 신경망 모델을 사용하여 음성 데이터를 통한 성별 인식 성능을 직접 평가하고 비교하는 독창적인 연구 내용을 포함하고 있다. 또한, 데이터 전처리, 모델 평가 지표 활용 등 구체적인 연구 방법론과 실험 결과를 제시하여 연구 논문으로서의 요건을 충족한다."
Stack Algorithm Implementation in Robot-Based Mixed Case Palletizing System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000343,"This paper documents the design procedure of a mixed case palletizing system using the ABB robot variant, IRB660. With the evolution of the e-commerce industry and rising volumes of stock-flow in warehouses and trading facilities, there has been a requirement for the automation of some of the most important operations. An automated palletizing system is a potential solution for this problem, to create optimized pallets with greater packing density and pallet support compared with the manual practices in operation. By completing this project, the authors will provide an entirely robotized palletizing system capable of responding to any customized packing order in a quick and efficient manner. The system is designed to handle 1000 or more cases per hour making it ideal for any warehouse or logistic facility dealing with a high number of stock-keeping units. Starting with a literature review and market research on the available systems, three different bin packing algorithms were stated: the Guillotine, MAXRECTS, and Skyline algorithms. Further, a new metric ‘percolation’ was introduced to measure and compare the packing quality of a pallet. The main section of this paper consists of a study of the design solution the authors have implemented for the AI stacking algorithm while introducing the palletizing UI used in the system. A data-driven 3D bin packing approach was considered for the stacking process which makes use of even-height super boxes to form dense and supported pallets. This method was chosen for its flexibility and reliable packing density of 80% made within a short time frame. Moving to the UI, a sample design of a graphical user interface (GUI) was first made in Figma while focusing on the different functionalities of the palletizing system before implementing the UI as a Windows Forms application. A 2D image of the ongoing pallet is displayed with several other customizing features such as selecting the stacking pattern and choosing different box/ pallet dimensions. Finally, a workcell was devised in RobotStudio® to simulate the operation and testing facilities of the authors' design solution. In close, this report satisfies all the design requirements with detailed descriptions and analyses of the team's approach.",Yes,"논문은 ABB 로봇을 이용한 혼합 케이스 팔레타이징 시스템의 설계 절차와 AI 스태킹 알고리즘 구현, 새로운 평가 지표 도입 등 독창적인 연구 내용을 포함하고 있습니다. 또한, 3D 빈 패킹 접근법과 GUI 설계 및 시뮬레이션을 통해 직접적인 연구 기여를 하고 있음을 보여줍니다."
An Experimental Evaluation of a Cloud-Based Virtual Computer Laboratory Using Openstack,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8113328,"In previous work, we proposed a solution to facilitate access to computer science related courses and learning materials using cloud computing and mobile technologies. The solution was positively evaluated by the participants, but most of them indicated that it lacks support for laboratory activities. As it is well known that many of computer science subjects (e.g. Computer Networks, Information Security, Systems Administration, etc.) require a suitable and flexible environment where students can access a set of computers and network devices to successfully complete their hands-on activities. To achieve this criteria, we created a cloud-based virtual laboratory based on OpenStack cloud platform to facilitate access to virtual machine both locally and remotely. Cloud-based virtual labs bring a lot of advantages, such as increased manageability, scalability, high availability and flexibility, to name a few. This arrangement has been tested in a case-study exercise with a group of students as part of Computer Networks and System Administration courses at Kabul Polytechnic University in Afghanistan. To measure success, we introduced a level test to be completed by participants prior and after the experiment. As a result, the learners achieved an average of 17.1 % higher scores in the post level test after completing the practical exercises. Lastly, we distributed a questionnaire after the experiment and students provided positive feedback on the effectiveness and usefulness of the proposed solution.",Yes,본 논문은 OpenStack 기반의 클라우드 가상 실험실을 설계하고 실제 학생들을 대상으로 실험을 수행하여 학습 성과를 측정하는 등 독창적인 연구 내용을 포함하고 있다. 또한 실험 결과와 설문조사 데이터를 통해 제안한 시스템의 효과를 검증하였으므로 연구 논문에 해당한다.
Deep Learning within the Web Application Security Scope – Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159847,"Over the last few years, several breakthroughs in deep learning have contributed to the development of new models. One of many areas they are applied to is the web application security scope. Web applications are still one of the biggest information and business security threats. Requests sent to the Web application are divided into normal and malicious. Malicious requests contain a payload that exploits a discovered vulnerability. Detection of Web attacks can be reduced to natural language processing classification problem. Lately, pre-trained models on Transformer neural networks showed promising results in the detection of Web attacks. In development of models, the preprocessing step of data preparation is crucial. After preparation of good datasets and application of powerful models it is very important to evaluate and compare performance of algorithms. The goal of this paper is to conduct an overview of the deep learning methods used for Web attack detection. The research is conducted by querying scientific databases, analyzing relevant articles within the security scope, and summarizing the proposed state-of-the-art approaches. Findings of reviewed papers were summarized based on implementation details and used performance metrics. Also, open problems will be emphasized, as well as challenges and possibly new opportunities for the future research.",No,"본 논문은 웹 애플리케이션 보안 분야에서 딥러닝 기법을 사용한 기존 연구들을 종합적으로 검토하는 문헌 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않고, 기존 연구들을 요약하고 분석하는 데 중점을 두고 있다."
Lightweight EfficientNetB3 Model Based on Depthwise Separable Convolutions for Enhancing Classification of Leukemia White Blood Cell Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10100945,"Acute lymphoblastic leukemia (ALL) is a type of leukemia cancer that arises due to the excessive growth of immature white blood cells (WBCs) in the bone marrow. The ALL rate for children and adults is nearly 80% and 40%, respectively. It affects the production of immature cells, leading to an abnormality of neurological cells and potential fatality. Therefore, a timely and accurate cancer diagnosis is important for effective treatment to improve survival rates. Since the image of acute lymphoblastic leukemia cells (cancer cells) under the microscope is complicated to recognize the difference between ALL cancer cells and normal cells. In order to reduce the severity of this disease, it is necessary to classify immature cells at an early stage. In recent years, different classification models have been introduced based on machine learning (ML) and deep learning (DL) algorithms, but they need to be improved to avoid issues related to poor generalization and slow convergence. This work enhances the diagnosis of ALL with a computer-aided system that yields accurate results by using DL techniques. This research study proposes a lightweight DL-assisted robust model based on EfficientNet-B3 using depthwise separable convolutions for classifying acute lymphoblastic leukemia and normal cells in the white blood cell images dataset. The proposed lightweight EfficientNet-B3 uses less trainable parameters to enhance the performance and efficiency of the leukemia classification. Furthermore, two publicly available datasets are considered to evaluate the effectiveness and generalization of the proposed lightweight EfficientNet-B3. In addition, different measures are employed, such as accuracy, precision, recall, and f1-score, to evaluate the effectiveness of the proposed and baseline classifiers. In addition, a detailed analysis is given to evaluate and compare the performance and efficiency of the proposed with existing pre-trained and ensemble DL classifiers. Experimental results show that the proposed model for image classification achieves better performance and outperforms the existing benchmark DL and other ensemble classifiers. Moreover, our finding suggests that the proposed lightweight EfficientNet-B3 model is reliable and generalized to facilitate clinical research and practitioners for leukemia detection.",Yes,"본 논문은 EfficientNet-B3 모델을 경량화하고 깊이별 분리 합성곱을 적용하여 백혈병 세포 이미지 분류 성능을 향상시키는 독창적인 딥러닝 모델을 제안하고 있습니다. 또한, 공개 데이터셋을 활용한 실험과 기존 모델과의 성능 비교를 통해 연구 기여를 명확히 하고 있어 연구 논문에 해당합니다."
Blockchain and Machine Learning for Advanced Pattern Recognition in Biometric Security Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10828862,"Blockchain and machine learning allow biometric security systems to recognize complex patterns. This study analyzes how two unique technologies may improve fingerprint system safety and reliability. To demonstrate blockchain and machine learning in biometrics. The main reason for this study is that fingerprint protection is becoming more important in today’s digital world. There needs to be strong name verification right away because there is more sensitive data and privacy issues. Traditional biometric systems have come a long way, but there are still problems with data security, privacy, and being able to grow. This study suggests important worries about how blockchain technology and machine learning might be able to improve pattern recognition, make security measures stricter, make data unchangeable, and reach the level of scalability needed for widespread use. Upon careful study of related approaches and works, biometric security could be improved by combining the decentralized and unchangeable record of blockchain with the pattern recognition abilities of machine learning. Ten important ways are looked at, and each one has its own pros and cons. A trio of CNNs, SVMs, and RNNs are used in the method we suggest. The suggested blockchain and machine learning fusion is shown to be better in a detailed analysis. The study ends with compared tables that show the parameters and performance evaluations of 10 similar systems.",Yes,"논문 초록에서 제안된 방법론과 기술적 접근(블록체인과 머신러닝의 융합, CNN, SVM, RNN 사용 등)을 통해 생체인식 보안 시스템의 성능 향상을 직접 연구하고 있음을 알 수 있습니다. 또한, 10개의 유사 시스템과의 성능 비교 분석을 포함하여 독창적인 연구 기여가 포함되어 있음을 보여줍니다."
Fast Pedestrian Detection for Real-World Crowded Scenarios on Embedded GPU,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535550,"The behavior of individuals in crowds in public places has gained enormously in importance last year, for example through distancing requirements. However, automatically detecting pedestrians in real-world uncooperative scenarios remains a very challenging task. Especially crowded areas in surveillance footage are not only challenging for automatic vision systems, but also for human operators. Furthermore, complex detection models do not scale easily and are not traditionally designed for on-device processing in resource-constrained smart cameras, which become more and more popular due to technical and privacy issues at large events. In this work, we propose a new Fast Pedestrian Detector (FPD) based on RetinaNet which is a fast and efficient architecture for embedded platforms. The proposed FPD provides near real-time and real-time detection of hundreds of pedestrians on embedded platforms, outperforming popular YOLO-based approaches traditionally tuned for speed. Furthermore, by evaluating our approach on several different Jetson platforms in terms of speed and energy profiles, we highlight the challenges related to the deployment of a deep learning based pedestrian detector on embedded platforms for smart surveillance cameras.",Yes,"본 논문은 기존 방법과 차별화된 새로운 Fast Pedestrian Detector(FPD)를 제안하고, 이를 임베디드 GPU 플랫폼에서 실시간으로 동작하도록 최적화한 독창적인 연구 내용을 포함하고 있다. 또한 다양한 Jetson 플랫폼에서 성능 평가를 수행하여 실제 적용 가능성을 검증한 점에서 연구 논문으로 판단된다."
FitMe: A Fitness Application for Accurate Pose Estimation Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478168,"The advancements in deep learning have brought about crucial transformations in computer vision over the past two decades. Deep convolutional networks have found many applications in building fine-tuned models for implementation in vision-related tasks. Knowledge learned by deep learning models over enormous generic datasets can be transferred to be employed for much more specific tasks. In this work, are implementing the approach to provide health benefits to people. In the present work, we develop an application which help them in performing exercises without the help of a trainer and get instant feedback about the postures. We aim to make fitness accessible to all by removing barriers such as external hardware requirements and cost-based subscriptions. In this paper, we dive deep into the technical details about the application and the exact methodologies applied for building the same. Furthermore, results are evaluated after running the application over multiple scenarios and a comparative analysis is performed.",Yes,"논문은 딥러닝을 활용한 정확한 자세 추정 기술을 적용한 피트니스 애플리케이션 개발에 대해 구체적인 방법론과 기술적 세부사항을 다루고 있으며, 다양한 시나리오에서의 평가 및 비교 분석 결과를 포함하고 있다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문으로 판단된다."
Automated Detection of Racial Microaggressions using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9308569,"Microaggressions describe subtle often offensive comments or actions made by one individual to another. Typically, such comments or actions are made subconsciously with the offender potentially unaware of the impacts on the recipient. Currently, machine learning methods for racial microaggression detection are sparse with no, one, comparable approach to the one we propose further on. Automated detection in this work describes the method of finding microaggressions through the use of machine-learning algorithms. Efforts have been made for the detection of hate speech and harassment; providing us with a rather humble place to begin, as we explore further, such methods are proven ineffective. We propose a step forward in solving this problem with the demonstration of an automated racial microaggression detection method. Whilst racial hatespeech detection method provides us with an idea as to where we can start, we find further on, that microaggressions and hatespeech use very different features to portray their sentiment. This work aims to provide a technical review which explores the understanding of the automated racial microaggression detection, outlining the definitions of microaggressions currently described in the literature, with the presentation and assessment (through precision, callback and F-measure) of a promising approach in regards to racial microaggression detection. We also intend to analyse a case study in which we detect the presence of racial microaggressions within new reports with the intention of laying a foundation for the potential security-related applications of this work. Our further works begin to discuss this notion in a higher level of detail.",Yes,"논문 초록에서 제안된 방법은 기계학습 알고리즘을 활용한 인종적 미세공격 자동 탐지라는 독창적인 연구 내용을 포함하고 있으며, 성능 평가 지표(정밀도, 재현율, F-측정치)를 통해 접근법을 검증하고 있다. 이는 기존 연구와 차별화된 새로운 기여를 하는 연구 논문임을 나타낸다."
Optimizing Convolutional Neural Network Hyperparameters for MNIST Image Classification: Insights and Implications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912098,"The recognition of handwritten digits has been among the most enduring fundamental problems explored in the field of machine learning and computer vision. The objective of this work is to design a state-of-art Convolutional Neural Network (CNN) for the image classification on the MNIST dataset to resolve issues related to data pre-processing and parameter tuning effectively. The work starts with standardizing the image data and then using one-hot encoding for the models that will improve the generalization and representation. The CNN structure followed in this work includes the convolutional layer, batch normalization, pooling layers, and fully connected layers with an efficient integration of features. To measure such performance indicators as accuracy, the given dataset is divided for training and validation. The results are also preprocessed to control for overfitting and convergence using back propagation with early stop to stop training when significant accuracy improvement is not observed and learning rate decay to encourage the model to search for better weights in the final stages of training. The model’s performance is tested and evaluated by using different epoch intervals and learning rates, and it is shown how critical it is to select the correct default hyperparameters for achieving efficiency. This paper also presents the approach for the practical application of the preprocessing of images and the systematic assessment in enhancing CNN-based handwritten digit recognition.",Yes,"논문은 MNIST 데이터셋을 대상으로 CNN의 하이퍼파라미터 최적화와 전처리 방법을 제안하고, 다양한 실험을 통해 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Towards an intelligent system to manage IDS for IoT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766759,"Nowadays, the security of information system has become more and more important in our lives. Indeed, the appearance of 5G see 6G and technological progress which has given rise to the democratization of connected objects, thus increasing the related risks and making the task of information system security administrator more and harder. To remedy this, the researchers focused on several systems including IDS which is an Intrusion Detection System used in host and network security. However, this system generates a large number of alarms which must be managed by a security administrator, something which is not easy to do, but is necessary to guarantee an optimal level of security. In this work, we will present a system that helps the security administrator to properly detect and manage IDS alerts. This system is based on detecting attacks, collecting alerts generated by different IDS in a network of objects, analyzing these alerts and taking appropriate actions. We propose automation of said tasks based on artificial intelligence algorithms, especially Deep Learning. Our choice is directed towards the algorithm of the Artificial Neural Network (ANN) according to several criteria namely the performance and the speed of detection which is our major concern while combining it with the algorithm of Spider Monkey Optimization (SMO) for a good optimization of the entries. Our system aims to strengthen the second line of defense and make it more efficient and intelligent by equipping it with three intelligent engines namely, a detection engine, an analysis engine and an action engine. To illustrate the applicability of the proposed approaches, we begun to test the performance of detection by using different measures for example error of detection, training time and accuracy rate which have been obtained by testing with NSL-KDD dataset.",Yes,"본 논문은 IDS 경고를 자동으로 탐지, 분석, 대응하는 지능형 시스템을 제안하고, 인공신경망과 최적화 알고리즘을 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Extensive Analysis of Machine Learning Techniques in the Field of Heart Disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389903,"CAD (Coronary Artery Diseases) and CHF (Chronic Heart Failure) are two main forms of heart disease that lead to heart attacks. This disease is increasing the number of casualties worldwide. The heart is an organ that circulates filtered blood to each part of the body. The normal working of the heart is affected due to various conditions like Obesity, Hypertension, lifestyle etc. The large volume of heart diseases is avertable, but they continue to increase for the reason that protective measures are insufficient. To cure these life-threatening diseases, we need a more reliable and accurate system. Machine learning procedures have been employed on numerous therapeutic datasets to industrialise the investigation of medical data. Recently, many researchers have started applying a number of machine learning techniques to assist the medical community and experts in the detection of heart-related issues.Classification technique like Support Vector Machine,Artificial Neural Network and Random Forest are examples of data mining and machine learning approaches that are used to forecast heart disorders. This can help clinician to decide on the diagnosis and forecast of cardiac illness. The prime intention behind writing this research paper is to use machine learning procedures to forecast a patient's heart condition.",No,초록에서 제시된 내용은 기존의 여러 머신러닝 기법들을 심장 질환 예측에 적용한 연구들을 종합적으로 분석하는 리뷰 성격에 가깝습니다. 독창적인 연구 결과나 새로운 기법 개발에 대한 직접적인 언급이 없어 연구 논문으로 보기 어렵습니다.
Transfer Learning on Trial: A Case Study to Apply Existing Models to Heterogeneous Datasets,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145140,"Nowadays, transfer learning is getting more and more popular in both industry and academia. It enables people to benefit from current advanced AI technologies, which used to be only accessible to professional teams with the most powerful talents, software and hardware resources. It has been proved that transfer learning is the best available option to apply learned patterns for one problem to a different but related problem. But rare research has been done to evaluate the performance of employing an existing model to a less related problem. In this paper, we apply the pre-trained model in the computer vision field, VGG, to a radar dataset, Ionosphere, which is heterogeneous to the above vision data, and carry out extensive experiments. The results show that the classification accuracy is much lower than that in the early research work, and the application of transfer learning should depend on certain situations.",Yes,"본 논문은 기존에 학습된 모델(VGG)을 이질적인 데이터셋(레이더 데이터)에 적용하여 성능을 평가하는 실험을 수행하였으며, 그 결과를 분석하여 전이학습의 적용 가능성에 대해 논의하고 있다. 이는 단순 리뷰나 이론적 고찰이 아닌 직접적인 실험과 분석을 포함한 독창적인 연구 내용으로 판단된다."
Quantum Computing for Climate Resilience and Sustainability Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10821197,"The escalating impacts of climate change and the increasing demand for sustainable development and natural resource management necessitate innovative technological solutions. Quantum computing (QC) has emerged as a promising tool with the potential to revolutionize these critical areas. This review explores the application of quantum machine learning and optimization techniques for climate change prediction and enhancing sustainable development. Traditional computational methods often fall short in handling the scale and complexity of climate models and natural resource management. Quantum advancements, however, offer significant improvements in computational efficiency and problem-solving capabilities. By synthesizing the latest research and developments, this paper highlights how QC and quantum machine learning can optimize multi-infrastructure systems towards climate neutrality. The paper also evaluates the performance of current quantum algorithms and hardware in practical applications and presents realistic cases, i.e., waste-to-energy in anaerobic digestion, disaster prevention in flooding prediction, and new material development for carbon capture. The integration of these quantum technologies promises to drive significant advancements in achieving climate resilience and sustainable development.",No,본 논문은 최신 연구와 개발 동향을 종합하여 양자 컴퓨팅의 기후 변화 및 지속 가능성 문제 적용 가능성을 탐구하는 리뷰 논문으로 보입니다. 직접적인 실험 결과나 독창적인 연구 기여보다는 기존 연구를 평가하고 사례를 제시하는 데 중점을 두고 있습니다.
"A Survey on Recent Advancements in Autonomous Driving Using Deep Reinforcement Learning: Applications, Challenges, and Solutions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10682977,"Autonomous driving (AD) endows vehicles with the capability to drive partly or entirely without human intervention. AD agents generate driving policies based on online perception results, which are crucial to the realization of safe, efficient, and comfortable driving behaviors, particularly in high-dimensional and stochastic traffic scenarios. Currently, deep reinforcement learning (DRL) techniques to derive and validate AD policies have witnessed vast research efforts and have shown rapid development in recent years. However, a comprehensive interpretation and evaluation of their strengths and limitations concerning the full-stack AD tasks remain uncharted. This paper presents a survey of this body of work, which is conducted at three levels. First, it analyzes the multi-level AD task characteristics and delves deeply into the current DRL methodologies primarily employed in AD. Second, a taxonomy of the literature studies is constructed from the system perspective, identifying six modes of DRL model integration into an AD architecture that span the entire spectrum of AD policy processes, from perception understanding and decision-making to motion control, as well as verification and validation. Each literature review comprehensively encompasses the main elements of designing such a system, including modeling partially observable environments, state and action spaces, reward structuring, and the design and training methodologies of neural network models. Finally, an in-depth foresight is conducted on how the eight critical issues of AD application development are addressed by the DRL models tailored for real-world AD challenges.",No,본 논문은 최근 자율주행 분야에서 딥 강화학습을 활용한 연구들을 종합적으로 정리하고 평가하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구들의 분석과 분류에 중점을 두고 있습니다.
Concepts of Engineering Education Innovation and Design Thinking: Implementing the CDIO-approach Themes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578801,"The CDIO Framework within engineering education is based on the idea that graduates should be capable of conceiving, designing, implementing, and operating (i.e., CDIO) complex systems within a team-based learning environment. As an approach to engineering education, it aims to create an active, experiential learning setting where product and process are considered fundamental to the curriculum. Modern pedagogical strategies and innovative teaching methods allow for the implementation of the CDIO approach, where students are supported to develop deep knowledge, manage the process of designing and exploiting new items and systems, and evaluate the impact of the scientific-technological process on society. This article will highlight aspects of this approach in the development of a practical quantum electronics session. In particular, it will focus on the developed principles of an innovative practice-targeted programme that can support the development of students' engineering thinking through the linkage of education and professional activities. As graduates, the students will need to be able to create innovative engineering systems as well as integrate their understanding of natural and technological sciences to generate novel concepts. They will also need to be proficient in professional ethics and have knowledge of business and entrepreneurship fundamentals. As such, this indicates the need for a strategically-focused curriculum that supports the development of a range of transversal skills required for industry. In this paper, the CDIO model is detailed as a means of transforming engineering education on a large scale. In order to develop the fundamental framework of creative awareness sessions for quantum technologies and their applications, this study attempted to further explore and provide more specific illustrations of the concepts relating to CIDO and to consider how they may be applied within an engineering context. The current work will present and evaluate the principles of an innovative teaching technique aimed at providing students with the opportunities to understand pioneering technologies for future applications.",Yes,"본 논문은 CDIO 접근법을 활용한 공학 교육 혁신과 설계 사고에 관한 구체적인 교육 프로그램 개발과 적용 사례를 다루고 있으며, 특히 양자 전자공학 세션의 실질적 개발과 혁신적 교수법 원칙을 제시하고 평가하는 등 독창적인 연구 내용과 실천적 기여를 포함하고 있다. 따라서 연구 논문에 해당한다."
α-Satellite: An AI-Driven System and Benchmark Datasets for Dynamic COVID-19 Risk Assessment in the United States,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141399,"The fast evolving and deadly outbreak of coronavirus disease (COVID-19) has posed grand challenges to human society. To slow the spread of virus infections and better respond for community mitigation, by advancing capabilities of artificial intelligence (AI) and leveraging the large-scale and up-to-date data generated from heterogeneous sources (e.g., disease related data, demographic, mobility and social media data), in this work, we propose and develop an AI-driven system (named α-Satellite), as an initial offering, to provide dynamic COVID-19 risk assessment in the United States. More specifically, given a point of interest (POI), the system will automatically provide risk indices associated with it in a hierarchical manner (e.g., state, county, POI) to enable people to select appropriate actions for protection while minimizing disruptions to daily life. To comprehensively evaluate our system for dynamic COVID-19 risk assessment, we first conduct a set of empirical studies; and then we validate it based on a real-world dataset consisting of 5,060 annotated POIs, which achieves the area of under curve (AUC) of 0.9202. As of June 18, 2020, α-Satellite has had 56,980 users. Based on the feedback from its large-scale users, we perform further analysis and have three key findings: i) people from more severe regions (i.e., with larger numbers of COVID-19 cases) have stronger interests using our system to assist with actionable information; ii) users are more concerned about their nearby areas in terms of COVID-19 risks; iii) the user feedback about their perceptions towards COVID-19 risks of their query POIs indicate the challenge of public concerns about the safety versus its negative effects on society and the economy. Our system and generated datasets have been made publicly accessible via our website.",Yes,"본 논문은 AI 기반 시스템 α-Satellite를 개발하고, 이를 통해 동적 COVID-19 위험 평가를 수행하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 실험적 평가와 실제 데이터셋 검증을 통해 시스템의 성능을 입증하였으며, 사용자 피드백 분석을 통해 추가적인 연구 기여를 제시하고 있습니다."
Discriminating Healthy and IUGR fetuses through Machine Learning models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926874,"The purpose of this study is to develop and understand whether Machine Learning models can classify Cardiotocographic (CTG) recordings of healthy fetuses or Intra Uterine Growth Restricted (IUGR) fetuses, highlighting how a large amount of data can have unexpected effects. We started from other findings in the literature to see what Machine Learning model remained consistent even with a large amount of data. The CTG records used in this study were collected at the Department of Obstetrics of the Federico II University Hospital in Naples, Italy, from 2013 to 2021. From this dataset, we chose 1548 IUGR fetuses and 1548 healthy fetuses to train our models. Each recording contained several parameters, ranging from features calculated on the entire CTG tracing, features calculated every 3 and 1 minute of recording and features related to the pregnant woman, such as age and week of gestation. We trained our machine-learning models on this dataset, checking the results obtained before and after adjusting the hyperparameters, noting that among the best models was Random Forest, which has already been present in other studies, and that the Multilayer Perceptron and the AdaBoost classifier were overall the best performing. This work can surely form a basis for future works in the fetal heart rate classification thus leading to real clinical applications.",Yes,"이 논문은 기계 학습 모델을 사용하여 건강한 태아와 IUGR 태아를 분류하는 독창적인 연구를 수행하였으며, 실제 데이터를 수집하고 모델을 훈련 및 평가한 내용을 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Comparative Analysis of BERT-base Transformers and Deep Learning Sentiment Prediction Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10047651,"The state-of-the-art Bidirectional Encoder Representations from Transformers (BERT) and Deep Learning (DL) models are used for Natural Language Processing (NLP) applications. Social media marketing and customers positive sentiments play major role for many online businesses.It is a crucial task for companies to predict customers sentiment based on context from online reviews. Predicting accurate sentiment is a time-consuming and challenging task due to high volume of unstructured customers review dataset. There are many previous experimental results reveals the performance and inaccuracy issues on large scale customer reviews datasets. This paper presents the comparative analysis of experimental research work on BERT, Hybrid fastText-BILSTM, and fastText Trigram models overcome more accurate sentiment prediction challenges. We propose fine-tuned BERT and Hybrid fastText-BILSTM models for large customer review datasets. This comparative analysis results show that the proposed fine-tuned BERT model performs better compare to other DL models in terms of accuracy and other performance measures.",Yes,"본 논문은 BERT와 딥러닝 모델을 활용하여 감성 예측 문제에 대해 직접 실험을 수행하고, 제안된 모델의 성능을 비교 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 연구 논문에 해당한다."
Federated Learning in IoT: A Survey from a Resource-Constrained Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10874004,"The IoT ecosystem is able to leverage vast amounts of data for intelligent decision-making. Federated Learning (FL), a decentralized machine learning technique, is widely used to collect and train machine learning models from a variety of distributed data sources. Both IoT and FL systems can be complimentary and used together. However, the resource-constrained nature of IoT devices prevents the widescale deployment FL in the real world. This research paper presents a comprehensive survey of the challenges and solutions associated with implementing Federated Learning (FL) in resource-constrained Internet of Things (IoT) environments, viewed from 2 levels, client and server. We focus on solutions regarding limited client resources, presence of heterogeneous client data, server capacity, and high communication costs, and assess their effectiveness in various scenarios. Furthermore, we categorize the solutions based on the location of their application, i.e., the IoT client, and the FL server. In addition to a comprehensive review of existing research and potential future directions, this paper also presents new evaluation metrics that would allow researchers to evaluate their solutions on resource-constrained IoT devices.",No,"본 논문은 Federated Learning과 IoT의 결합에 관한 기존 연구들을 종합적으로 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 분류와 평가 지표 제안에 중점을 두고 있습니다. 따라서 새로운 연구 내용을 직접 제시하는 연구 논문으로 보기 어렵습니다."
[Research Paper] On the Use of Machine Learning Techniques Towards the Design of Cloud Based Automatic Code Clone Validation Tools,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530729,"A code clone is a pair of code fragments, within or between software systems that are similar. Since code clones often negatively impact the maintainability of a software system, a great many numbers of code clone detection techniques and tools have been proposed and studied over the last decade. To detect all possible similar source code patterns in general, the clone detection tools work on syntax level (such as texts, tokens, AST and so on) while lacking user-specific preferences. This often means the reported clones must be manually validated prior to any analysis in order to filter out the true positive clones from task or user-specific considerations. This manual clone validation effort is very time-consuming and often error-prone, in particular for large-scale clone detection. In this paper, we propose a machine learning based approach for automating the validation process. In an experiment with clones detected by several clone detectors in several different software systems, we found our approach has an accuracy of up to 87.4% when compared against the manual validation by multiple expert judges. The proposed method shows promising results in several comparative studies with the existing related approaches for automatic code clone validation. We also present our experimental results in terms of different code clone detection tools, machine learning algorithms and open source software systems.",Yes,"본 논문은 코드 클론 검증 자동화를 위한 머신러닝 기반 접근법을 제안하고, 여러 실험을 통해 그 정확도를 평가하는 독창적인 연구 내용을 포함하고 있습니다. 기존 방법과의 비교 연구 및 다양한 도구와 알고리즘을 적용한 실험 결과를 제시하여 직접적인 연구 기여가 명확합니다."
"Email Spam: A Comprehensive Review of Optimize Detection Methods, Challenges, and Open Research Problems",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10693429,"Nowadays, emails are used across almost every field, spanning from business to education. Broadly, emails can be categorized as either ham or spam. Email spam, also known as junk emails or unwanted emails, can harm users by wasting time and computing resources, along with stealing valuable information. The volume of spam emails is rising rapidly day by day. Detecting and filtering spam presents significant and complex challenges for email systems. Traditional identification techniques like blocklists, real-time blackhole listing, and content-based methods have limitations. These limitations have led to the advancement of more sophisticated machine learning (ML) and deep learning (DL) methods for enhanced spam detection accuracy. In recent years, considerable attention has focused on the potential of ML and DL methods to improve email spam detection. A comprehensive literature review is therefore imperative for developing an updated, evidence-based understanding of contemporary research on employing these methods against this persistent problem. The review aims to systematically identify various ML and DL methods applied for spam detection, evaluate their effectiveness, and highlight promising future research directions considering gaps. By combining and analyzing findings across studies, it will obtain the strengths and weaknesses of existing methods. This review seeks to advance knowledge on reliable and efficient integration of state-of-the-art ML and DL into identifying email spam.",No,"초록에서 해당 논문은 기존 연구들을 종합적으로 검토하는 리뷰 논문임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구의 정리와 평가에 중점을 둔 논문으로 판단된다."
Understanding Memories of the Past in the Context of Different Complex Neural Network Architectures,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342588,"Deep learning (primarily using backpropagation) and neuroevolution are the preeminent methods of optimizing artificial neural networks. However, they often create black boxes that are as hard to understand as the natural brains they seek to mimic. Previous work has identified an information-theoretic tool, referred to as R, which allows us to quantify and identify mental representations in artificial cognitive systems. The use of such measures has allowed us to make previous black boxes more transparent. Here we extend R to not only identify where complex computational systems store memory about their environment but also to differentiate between different time points in the past. We show how this extended measure can identify the location of memory related to past experiences in neural networks optimized by deep learning as well as a genetic algorithm.",Yes,논문 초록은 기존 연구에서 사용된 정보이론적 도구 R을 확장하여 신경망 내 기억의 위치와 시간적 차이를 식별하는 새로운 방법을 제안하고 있다. 이는 복잡한 신경망 아키텍처에서 기억을 이해하는 데 직접적인 기여를 하는 독창적인 연구 내용임을 보여준다.
Optimized Random Forest Classifier for Drone Pilot Identification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9181004,"Random forest is a powerful machine learning scheme which finds applications in real-time systems such as unmanned aerial vehicles. In such applications not only the classification performance is relevant but also several non-functional requirements including the classification time, the memory usage and the power consumption. This paper proposes a new approach to improve the real-time behavior of a random forest classifier. This is accomplished by reducing the number of evaluated nodes and branches as well as by reducing the branch length in the underlying binary decision trees with numerical split values. A hardware architecture is presented for the improved tree-based classification method. A proof-of-concept implementation on an FPGA platform and some preliminary results show the advantage of this approach compared to related work.",Yes,"논문은 기존 랜덤 포레스트 분류기의 실시간 성능을 개선하기 위한 새로운 접근법을 제안하고, 하드웨어 아키텍처 설계 및 FPGA 구현을 통해 그 효과를 입증하는 독창적인 연구 내용을 포함하고 있습니다. 이는 직접적인 연구 기여로 판단됩니다."
Normal/Cataract Detection in Fundus Image Using Individual and Fused ResNet Features,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894554,"The visual sensory information collected by the eyes is crucial for accurate perception and decision-making in the brain. Any ocular ailment will disrupt this process, perhaps resulting in mild to severe vision-related complications. Ocular ailments are primarily attributable to disease or senescence. Age-related eye illness is a prevalent concern that necessitates prompt detection and intervention. Cataract is a prevalent age-related ocular condition that results in mild to severe visual impairment and necessitates a small surgical intervention for correction. The image-guided identification of cataracts is a clinical practice, and this research intends to present a Deep Learning (DL) method to categorize Retinal Fundus Images (RFI) as normal or cataract. The proposed scheme comprises several phases: (i) image acquisition and resizing to 224×224 pixels, (ii) feature extraction utilizing a DL- model, (iii) optimal model selection, feature reduction with 50% dropout, and concatenation of serial features, and (iv) classification accompanied by 3-fold cross-validation to validate performance. This study evaluates the efficacy of the suggested DL-tool utilizing both traditional and fused-features. The experimental results of this work demonstrate that the fused-features technique achieves > 98% accuracy when applied to SM-based categorization.",Yes,"논문 초록에서 제안된 딥러닝 기반의 새로운 방법론과 그 성능 평가가 포함되어 있어 독창적인 연구 내용을 담고 있음을 알 수 있습니다. 또한, 실험 결과를 통해 제안 기법의 유효성을 검증한 점에서 연구 논문에 해당합니다."
A Hybrid Model-Based Approach on Prognostics for Railway HVAC,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907016,"Prognostics and health management (PHM) of systems usually depends on appropriate prior knowledge and sufficient condition monitoring (CM) data on critical components’ degradation process to appropriately estimate the remaining useful life (RUL). A failure of complex or critical systems such as heating, ventilation, and air conditioning (HVAC) systems installed in a passenger train carriage may adversely affect people or the environment. Critical systems must meet restrictive regulations and standards, and this usually results in an early replacement of components. Therefore, the CM datasets lack data on advanced stages of degradation, and this has a significant impact on developing robust diagnostics and prognostics processes; therefore, it is difficult to find PHM implemented in HVAC systems. This paper proposes a methodology for implementing a hybrid model-based approach (HyMA) to overcome the limited representativeness of the training dataset for developing a prognostic model. The proposed methodology is evaluated building an HyMA which fuses information from a physics-based model with a deep learning algorithm to implement a prognostics process for a complex and critical system. The physics-based model of the HVAC system is used to generate run-to-failure data. This model is built and validated using information and data on the real asset; the failures are modelled according to expert knowledge and an experimental test to evaluate the behaviour of the HVAC system while working, with the air filter at different levels of degradation. In addition to using the sensors located in the real system, we model virtual sensors to observe parameters related to system components’ health. The run-to-failure datasets generated are normalized and directly used as inputs to a deep convolutional neural network (CNN) for RUL estimation. The effectiveness of the proposed methodology and approach is evaluated on datasets containing the air filter’s run-to-failure data. The experimental results show remarkable accuracy in the RUL estimation, thereby suggesting the proposed HyMA and methodology offer a promising approach for PHM.",Yes,"본 논문은 철도 HVAC 시스템의 예측 유지보수를 위해 물리 기반 모델과 딥러닝을 결합한 하이브리드 모델을 제안하고, 실제 데이터와 실험을 통해 모델을 구축·검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Self-Driving Car: Simulation of Highly Automated Vehicle Technology using Convolution Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068691,"Driver behaviour is a significant factor in the smooth driving of vehicles on the roads. 94 % of crashes and road accidents are prone to drivers' rash driving behaviour. To address issues related to road accidents and crashing of vehicles on the road, Highly Automated Vehicle (HAV) Technologies have been proposed. Self-Driving Cars are part of Highly Automated Tech-nologies having promising benefits ranging from Greater Road Safety, Greater Independence, Saving money, More Productivity, Reduced Congestion and Green House Gains. The current study focuses on the deployment of self-driving automobiles based on the Deep Learning paradigm. The automobile has been simulated on the Udacity simulator for convenience and safety. On the Udacity platform, a technique for training and simulating an unmanned vehicle model using a convolutional neural network has been developed. The data used to train the model is captured in the simulator and fed as input into the Deep CNN. Following data collection, Deep CNN is trained to have Safety Navigation by passing Steering, Throttle, Brake and Speed as Control Inputs. The use of three cameras considerably improves the precision of the navigation job. To manage the car, the steering wheel amount will be modified such that it runs in the centre of the lane. We evaluated the model using UDACITY's simulation system. The proposed model has been evaluated considering the-No of epochs vs loss calculation, as performance metrics, and was found that the proposed model has shown superiority with the existing works.",Yes,"본 논문은 자율주행 자동차 시뮬레이션을 위해 합성곱 신경망(CNN)을 활용한 모델을 개발하고, 이를 Udacity 시뮬레이터에서 학습 및 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Automatic Pulmonary Lobe Segmentation Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759468,"Pulmonary lobe segmentation is an important task for pulmonary disease related Computer Aided Diagnosis systems (CADs). Classical methods for lobe segmentation rely on successful detection of fissures and other anatomical information such as the location of blood vessels and airways. With the success of deep learning in recent years, Deep Convolutional Neural Network (DCNN) has been widely applied to analyze medical images like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI), which, however, requires a large number of ground truth annotations. In this work, we release our manually labeled 50 CT scans which are randomly chosen from the LUNA16 dataset and explore the use of deep learning on this task. We propose pre-processing CT image by cropping region that is covered by the convex hull of the lungs in order to mitigate the influence of noise from outside the lungs. Moreover, we use a hybrid loss function with dice loss to tackle extreme class imbalance issue and focal loss to force model to focus on voxels that are hard to be discriminated. To validate the robustness and performance of our proposed framework trained with a small number of training examples, we further tested our model on CT scans from an independent dataset. Experimental results show the robustness of the proposed approach, which consistently improves performance across different datasets by a maximum of 5.87% as compared to a baseline model. The annotations are public available https://github.com/deep-voxel/automatic_pulmonary_lobe_segmentation_using_deep_learning/ and are for non-commercial use only.",Yes,"본 논문은 폐엽 분할을 위한 딥러닝 기반의 새로운 방법론을 제안하고, 소규모 데이터셋에서의 성능 향상과 일반화 가능성을 실험적으로 검증하는 독창적인 연구 내용을 포함하고 있다. 또한, 새로운 손실 함수 설계와 전처리 기법을 도입하여 기존 방법 대비 성능 개선을 입증하였다."
Tailoring Content with Keyword-Based Recommendation Engine Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10915535,"It is the information age thus; it is very important to understand how to personalize content in accordance with what appeals to a user. This work is conceptualized to be labeled ""Tailoring Content with Keyword-Based Recommendation Engine Using Machine Learning"". This work provides the applicative system catering for the experience of users by proposing personalization in terms of content. The core of the work is keyword-based recommendation engine chosen by the user to recommend posts as well as other contents believed to be of the user’s interest. A recommendation system uses various types of algorithms related to machine learning and a combination of data analysis technologies while considering the interaction of the user with posts, created, liked, or participated in. From this, it will then understand keen interests by a user and make recommendations based on those grounds. This will keep users interesting about the relevant content and help develop skills in areas they are interested in. Some scope includes designing a recommendation engine, developing it into an application, and measuring performance. This would be in the ability of doing so, with accuracies at all high degrees, when recommending this content, scalability, and also privacy for the users. It will be used to prove how machine learning techniques can be applied towards developing personalized user experience and, as a by-product, contribute to this field known as content recommendation systems.",Yes,"논문 초록에서 키워드 기반 추천 엔진 설계, 개발, 성능 측정 등 구체적인 시스템 구현과 머신러닝 알고리즘 적용에 대해 서술하고 있어 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문으로 판단됩니다. 또한, 사용자 맞춤형 추천 시스템 개발을 통한 새로운 방법론 제시가 명확히 드러납니다."
XTSC-Bench: Quantitative Benchmarking for Explainers on Time Series Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459958,"Despite the growing body of work on explainable machine learning in time series classification (TSC), it remains unclear how to evaluate different explainability methods. Resorting to qualitative assessment and user studies to evaluate explainers for TSC is difficult since humans have difficulties understanding the underlying information contained in time series data. Therefore, a systematic review and quantitative comparison of explanation methods to confirm their correctness becomes crucial. While steps to standardized evaluations were taken for tabular, image, and textual data, benchmarking explainability methods on time series is challenging due to a) traditional metrics not being directly applicable, b) implementation and adaption of traditional metrics for time series in the literature vary, and c) varying baseline implementations. This paper proposes XTSC-Bench, a benchmarking tool providing standardized datasets, models, and metrics for evaluating explanation methods on TSC. We analyze 3 perturbation-, 6 gradient- and 2 example-based explanation methods to TSC showing that improvements in the explainers' robustness and reliability are necessary, especially for multivariate data.",Yes,"본 논문은 시계열 분류에 대한 설명 방법들을 정량적으로 평가하기 위한 벤치마크 도구를 제안하고, 여러 설명 방법들을 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Decentralized Federated Learning: A Comprehensive Survey and a New Blockchain-based Data Evaluation Scheme,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922390,"Blockchain and Deep Learning (DL) are two of the most revolutionary concepts in the field of Computer Science. Both have made astounding leaps in research and application areas such as Finance, Healthcare, Internet of Things, and many more. Federated Learning (FL) is a type of distributed Deep Learning framework, in which the model is trained locally on each device and the trained gradients are sent to a central server which aggregates them and creates a global model. This helps ensure the data privacy of the user as the data never leaves the local device. However, this dependency on the central server can lead to various issues such as lack of transparency and communication bottleneck. Making this process decentralized can help address these issues. In this review, a detailed survey on using blockchain in federated learning is presented. This review also focuses on how can we use blockchain to make federated learning more transparent and decentralized to protect the privacy of the user. We also discuss the major strengths and drawbacks of each approach and further present a few ideas of our own, regarding some of these challenges and ways on how can these be improved. A new scheme to evaluate data using miners as well as methods to reduce storage overhead in decentralized federated learning are discussed in this paper.",Yes,"논문은 기존 연구들을 종합적으로 리뷰하는 동시에, 블록체인 기반의 새로운 데이터 평가 방식을 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 단순한 서베이 논문이 아니라 직접 기여하는 연구 논문으로 판단된다."
"Cybersecurity in the Oil and Gas Sector: Vulnerabilities, Solutions, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10778682,"The integral role of the oil and gas (O&G) industry in the global economy and digital ecosystem exposes it to escalating cyber threats that target critical operational technology and information systems. This literature review comprehensively examines the sector’s cybersecurity complexities, drawing from academic articles, industry reports, and case studies. Critical vulnerabilities exposing the industry to cyber-attacks are identified, and existing cybersecurity measures implemented by companies to combat these threats are evaluated. The review categorizes industry challenges into technological, organizational, and regulatory segments. Integrating legacy systems with modern technologies like cloud computing and IoT presents challenges. Organizationally, the focus is on cybersecurity culture, including employee training and policy enforcement. Regulatory challenges stem from the compliance burdens of national and international cybersecurity standards. The review discusses cybersecurity solutions, emphasizing innovative technologies like artificial intelligence for efficient risk detection and response. Strategic measures, such as incident response plans and executive leadership, are highlighted in fostering cybersecurity awareness. Future challenges driven by technological advancements and evolving cyber threat tactics are explored, with recommendations for ongoing research areas to strengthen the industry’s cyber defenses and resilience. This review serves as a critical resource for stakeholders, policymakers, and researchers, underscoring the urgent need for proactive cybersecurity strategies to safeguard the (O&G) industry’s assets and infrastructure.",No,"초록에서 해당 논문은 기존 문헌, 산업 보고서, 사례 연구를 종합하여 섹터의 사이버보안 현황과 문제점을 정리한 문헌 리뷰임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험, 데이터 분석 등의 기여가 포함된 연구 논문으로 보기 어렵습니다."
An experimental study of different machine and deep learning techniques for classification of encrypted network traffic,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378257,"There is a continuous evolution in the technology industry with different types of devices being produced and connected to the internet. Multiple types of applications run on the different devices, thereby generating a complex and huge amount of traffic (i.e., Big Data) on the internet. This has made it difficult and challenging for different Internet Service Providers (ISPs) to maintain their service quality and keep their networks and services secure. It is important for service providers to have the ability to classify large and complex network traffic to help promote a better Quality of Service (QoS) and manage networks. In this paper, we utilize, apply and evaluate different machine and deep learning techniques for classification of encrypted network traffic to help in managing networks, and thereby, help in improving quality and security of network. A comparison between the different algorithms used is presented. The experiment results show that ANN+XGB, CNN+XGB, and CapsNet+XGB performed better than the LSTM+XGB and Ensemble model in the classification of encrypted network traffic with accuracies of 96%, 96%, 96%, 93%, and 95% respectively using a total of 23 statistical features. More statistical features were considered compared to other existing related works to improve the process of the classification and different hidden patterns associated with the statistical features. The results show the effectiveness of the machine and deep learning algorithms for the classififcation of encrypted network traffic into different categories.",Yes,"본 논문은 암호화된 네트워크 트래픽 분류를 위해 다양한 머신러닝 및 딥러닝 기법을 적용하고 평가하는 실험 연구를 수행하였으며, 여러 알고리즘의 성능 비교와 새로운 통계적 특징 사용을 통해 분류 정확도를 향상시킨 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Robust Brain Magnetic Resonance Image Segmentation for Hydrocephalus Patients: Hard and Soft Attention,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098541,"Brain magnetic resonance (MR) segmentation for hydrocephalus patients is considered as a challenging work. Encoding the variation of the brain anatomical structures from different individuals cannot be easily achieved. The task becomes even more difficult especially when the image data from hydrocephalus patients are considered, which often have large deformations and differ significantly from the normal subjects. Here, we propose a novel strategy with hard and soft attention modules to solve the segmentation problems for hydrocephalus MR images. Our main contributions are three-fold: 1) the hard-attention module generates coarse segmentation map using multi-atlas-based method and the Vox-elMorph tool, which guides subsequent segmentation process and improves its robustness; 2) the soft-attention module incorporates position attention to capture precise context information, which further improves the segmentation accuracy; 3) we validate our method by segmenting insula, thalamus and many other regions-of-interests (ROIs) that are critical to quantify brain MR images of hydrocephalus patients in real clinical scenario. The proposed method achieves much improved robustness and accuracy when segmenting all 17 consciousness-related ROIs with high variations for different subjects. To the best of our knowledge, this is the first work to employ deep learning for solving the brain segmentation problems of hydrocephalus patients.",Yes,"본 논문은 하드 및 소프트 어텐션 모듈을 활용한 새로운 뇌 MRI 분할 방법을 제안하며, 실제 임상 시나리오에서 다양한 뇌 영역을 정확하게 분할하는 성능을 검증하였다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문에 해당한다."
"Development of a biomimetic non-invasive radial pulse sensor: Design, calibration, and applications",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090329,"The research work aims at developing a real time non-invasive metabolism and blood circulation surveillance system for monitoring human's health condition by sensing the various bio-signals on human body. Our goal is to use the developed system to study the functions and characters of organs and tissues that highly relate with the metabolism and blood circulation system, also it is expected to help modeling the entire circulation system. At Phase I of the research, in this paper we focus on developing a new low-cost, portable, high accuracy, non-invasive radial pulse sensor. Inspired by touch capability and related biomechanical advantage of human fingertip, the mechanical design of the sensor mimics the physiological structure of human fingertip. The designed biomimetic sensor is well calibrated using a high-accuracy force sensor and the model is accurately identified by the system identification method. The calibrated sensor is then applied to diagnose the arterial stiffness by measuring the augmentation index (AI) which is the important biomarker of vascular aging. Preliminary results demonstrate the sensor performance that it is capable of non-invasively, accurately and reliably measuring radial pulse signals at real time, as well can be used to quantitatively determine the vessel aging.",Yes,"본 논문은 새로운 비침습적 맥박 센서의 설계, 보정, 그리고 적용에 관한 독창적인 연구 내용을 포함하고 있으며, 센서의 성능 평가와 실제 진단 적용 결과를 제시하고 있다. 이는 직접적인 연구 기여를 나타내는 연구 논문에 해당한다."
Artificial Intelligence and Physics-Based Anomaly Detection in the Smart Grid: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10858740,"The integration of advanced communication systems and distributed resources has transformed power systems, enhancing control and efficiency in the Smart Grid. However, this increased complexity introduces new vulnerabilities, heightening risks of cyber-attacks, equipment failures, and other anomalies. Anomaly detection methods increasingly rely on Machine Learning techniques, that represent a game-changer tool for data analysis. The aim of this survey is to review anomaly detection techniques in the Smart Grid, focusing on methods that combine Artificial Intelligence and physics-based modeling. This work systematically examines the current state of research, evaluating the investigated use cases, the algorithms, the performances and the validation of the papers, identifying key gaps, and offering insights for advancing in this research field.",No,본 논문은 스마트 그리드에서 인공지능과 물리 기반 모델링을 결합한 이상 탐지 기법에 대한 기존 연구들을 체계적으로 검토하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들을 요약하고 분석하는 데 중점을 두고 있습니다.
Blockchain Based Decentralized Technology For Internet Naming Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10461759,"Domain Name Service (DNS) has become an essential element of the Internet in current era. The traditional DNS extends its authority down to the Top Level Domains (TLDs) and individual names before returning to the Root Server. To avoid the cons related to centralized authority scheme of the traditional DNS, recently many techniques based on the decentralized Internet with support of Blockchain (BC) are emerging. Hence, currently a variety of protocols and mechanisms are focusing on implementation of the Decentralized Naming Services which in turn supports concept of decentralized internet services. These Decentralized Naming Services are totally based on Blockchain architectures and provides various pros like resistance to single point of failure, resistance to censorship and permanent proof of ownership. Blockchain Domain Names act as Digital Identity in Decentralized Internet. Security is major concern in any DNS system and the similar challenges exists in the Blockchain DNS system too. Further, there is a lot of diversity being observed in decentralized internet space as a Domain name in a Blockchain DNS can refer to a website/portal or a wallet or a Non-Fungible Token (NFT) or simply be parked. Also, currently, there is rampant growth in development of decentralized applications with no standards implemented - neither across Blockchains nor regions. Hence, there is a need to detect security exploits carried out using malicious domains in Blockchain DNS system and implement mitigation measures accordingly. In this research work, we study and analyze malicious domain names in decentralized BC system. We explored Ethereum transaction dataset for detecting malicious transactions using Machine Learning (ML) and Deep Learning (DL) algorithms with good accuracy of 99% and 95% using Decision Tree and 1 Dimensional Convolutional Neural Network (CNN) respectively. We also applied ML techniques on Ethereum wallet dark list dataset for detection of domain names that were vii used for phishing attacks. We received 96% accuracy after handling data imbalance by Random Forest model. Model predicted nature of domain based on 17 features that were extracted from Ethereum wallet dark list dataset like number of consonants, vowels, digits, symbols and their ratios to length of domain.",Yes,"본 논문은 블록체인 기반 분산 인터넷 명명 시스템에서 악성 도메인 탐지 및 대응을 위해 머신러닝과 딥러닝 기법을 적용한 구체적인 연구를 수행하고 있으며, 실험 데이터와 모델 정확도 결과를 제시하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 연구 논문에 해당한다."
DC Series Arc Fault Detection Using Machine Learning in Photovoltaic Systems: Recent Developments and Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9287192,"DC arc faults, especially series arc faults, are becoming more common in photovoltaic (PV) systems. Without timely detection and interruption, such dangerous events can cause catastrophic fires, posing severe threat to human safety and properties. This paper presents a review on DC series arc fault detection using machine learning (ML) in PV systems. Technical details of applied ML methods, including conventional ML and deep learning (DL), in recent published paper are summarized and discussed. In addition, several popular ML methods are evaluated and compared using the same experimental datasets collected in laboratory to examine their effectiveness in DC series arc fault detection. Finally, practical challenges are identified, potential solutions are provided, and future research directions are recommended.",No,"본 논문은 기존 연구들을 종합하여 DC 직렬 아크 결함 탐지에 사용된 머신러닝 기법들을 리뷰하고 비교하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않고, 기존 연구의 정리와 평가에 중점을 두고 있습니다."
Identifying Student Profiles Within Online Judge Systems Using Explainable Artificial Intelligence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024788,"Online Judge (OJ) systems are typically considered within programming-related courses as they yield fast and objective assessments of the code developed by the students. Such an evaluation generally provides a single decision based on a rubric, most commonly whether the submission successfully accomplished the assignment. Nevertheless, since in an educational context such information may be deemed insufficient, it would be beneficial for both the student and the instructor to receive additional feedback about the overall development of the task. This work aims to tackle this limitation by considering the further exploitation of the information gathered by the OJ and automatically inferring feedback for both the student and the instructor. More precisely, we consider the use of learning-based schemes—particularly, multi-instance learning (MIL) and classical machine learning formulations—to model student behavior. Besides, explainable artificial intelligence (XAI) is contemplated to provide human-understandable feedback. The proposal has been evaluated considering a case of study comprising 2500 submissions from roughly 90 different students from a programming-related course in a computer science degree. The results obtained validate the proposal: The model is capable of significantly predicting the user outcome (either passing or failing the assignment) solely based on the behavioral pattern inferred by the submissions provided to the OJ. Moreover, the proposal is able to identify prone-to-fail student groups and profiles as well as other relevant information, which eventually serves as feedback to both the student and the instructor.",Yes,"본 논문은 온라인 저지 시스템에서 수집된 데이터를 활용하여 학생 행동을 모델링하고, 기계 학습 및 설명 가능한 인공지능 기법을 적용하여 학생 프로필을 식별하는 독창적인 연구를 수행하였다. 또한, 실제 학생 제출물 데이터를 기반으로 모델을 평가하고 유의미한 결과를 도출하여 연구 기여를 명확히 하고 있다."
Comparative Evaluation of Model Based Deep Learning Receivers in Coded MIMO Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757517,"Deep learning (DL) methods have shown potential in tackling the performance-complexity trade-off in multiple-input multiple-output (MIMO) detection. Unlike most studies that evaluate state-of-the-art (SoA) DL receivers in uncoded scenarios, our paper focuses on realistic coded MIMO systems. After a comprehensive literature review, three representative SoA model-based DL receivers viz: DetNet, OAMPNet2, and DUIDD (MMSE-PIC and LoCo-PIC) were selected and comprehensively evaluated. Our findings indicate that DL receivers such as DetNet and OAMPNet2, which base their classical designs on the principle of symbol denoising, fail to sustain their superior performances from uncoded systems to coded systems due to inaccurate residual noise statistics. In contrast, DUIDD, specifically designed for coded systems, achieves effective interference cancellation, resulting in improved coded bit error rates across i.i.d. Gaussian channels, suggesting promising avenues for future research. However, LoCo-PIC, which simplifies MMSE-PIC with a linear solution, suffers performance degradation in correlated urban microcell channels, highlighting the importance of considering the non-linear correlation impacts during detection. Additionally, this simplification leads to further degradation in out-of-distribution channel scenarios, emphasizing the need to address these impacts in realistic wireless systems with varying channel conditions.",Yes,"본 논문은 기존 연구들을 종합적으로 검토한 후, 세 가지 대표적인 딥러닝 기반 MIMO 수신기 모델을 선정하여 실제 부호화된 MIMO 시스템에서 성능을 비교 평가하는 독창적인 실험 및 분석을 수행하고 있다. 이는 단순 문헌 리뷰가 아닌, 새로운 실험 결과와 분석을 통해 연구에 직접 기여하는 내용임을 보여준다."
Assessing the Efficacy of Machine Learning and Deep Learning in the Field of Cyber Security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487227,"The use of machine learning has become widespread across various fields because of its superior performance compared to conventional rule-based algorithms. As a result, these models have also been integrated into cyber security systems, Machine learning is being utilized to aid or possibly even supplant the role of human security analysts. However, it's important to evaluate the effectiveness of machine learning in cyber security with careful consideration, especially if complete automation of detection and analysis is being considered. This study provides an in-depth research focuses on machine learning techniques applied in intrusion, malware, and spam detection that are tailored towards security professionals. The primary objective of our study is to evaluate the degree of advancement or maturity of these techniques of ML-based cybersecurity solutions and to identify any limitations that could impede their effectiveness as detection mechanisms. To achieve this, we conducted a thorough literature review and performed experiments on enterprise systems and network traffic in real-world settings. Our goal is to gain understanding of the capabilities and limitations of ML solutions and provide actionable insights for their improvement.",Yes,논문은 기존 문헌 검토뿐만 아니라 실제 기업 시스템과 네트워크 트래픽을 대상으로 한 실험을 수행하여 머신러닝 기반 사이버 보안 기술의 효과를 평가하고 개선점을 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문임을 나타낸다.
Face Mask Detection and Alert System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10593943,"The COVID-19 pandemic has underscored the importance of preventive measures, with the use of face masks being a crucial aspect of controlling the virus's spread. Thus, in the response to the global imperative to curb the spread of infectious diseases, particularly in the context of the COVID-19 pandemic, this research paper delves into the realm of face mask detection and alert systems. With the ubiquity of technology, the development of automated solutions for ensuring adherence to mask-wearing protocols has become most necessary. This paper provides a thorough examination of existing methodologies for face mask detection, ranging from image& video-based approaches to sophisticated deep learning models. Additionally, explores the integration of alert systems into these detection frameworks, emphasizing real- time processing considerations and user interface design. The review incorporates case studies, ethical considerations, and privacy implications, offering a comprehensive overview of the current state of the art. By elucidating the challenges, successes, and future directions, this research paper contributes to the discourse on leveraging technology for public health measures and sets the stage for continued advancements in face mask detection systems and also provides a comprehensive review of the state-of-the-art in face mask detection technologies and explores the design.",No,"초록 내용이 기존 연구 방법론의 검토와 종합적인 리뷰에 초점을 맞추고 있으며, 독창적인 연구 결과나 새로운 실험적 기여에 대한 언급이 없습니다. 따라서 이 논문은 연구 논문이라기보다는 리뷰 논문에 더 가깝다고 판단됩니다."
An Artificial Intelligence Computer System for Analysis of Social-Infrastructure Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7264719,"Development of big data analysis technologies has been highly competitive among companies and organizations. An artificial intelligence (AI) computer system, based on the basic idea of automating big-data analysis, called ""H"" is proposed and evaluated in this paper. It automates big-data analysis by creating hundreds of thousands of candidate features related to an outcome. It was applied for analyzing work-log data in a logistics company. The evaluation results confirm the effect of the system on the productivity of the company, namely, a 5.23% improvement in productivity.",Yes,"논문은 인공지능 컴퓨터 시스템 ""H""를 제안하고 평가하는 내용을 포함하고 있으며, 이는 빅데이터 분석 자동화를 위한 독창적인 연구 기여로 보인다. 또한 실제 물류 회사의 작업 로그 데이터를 분석하여 생산성 향상 효과를 검증한 점에서 연구 논문으로 판단된다."
Deep Learning Approach for Automated Skin Disease Diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10808369,"Skin diseases has been a significant challenge in healthcare, necessitating accurate and timely diagnosis for effective treatment. This paper presents research on the use of Convolutional Neural Networks (CNNs) for the categorization of skin illnesses. CNNs are a viable tool for automated skin disease detection since they successfully leverage the deep learning capabilities in image-based tasks. The research begins with a detailed review of existing methodologies and datasets in the domain of skin disease classification. Subsequently, we propose a novel CNN architecture tailored to address the unique challenges posed by dermatological images, such as variability in lesion size, color, and texture. The proposed model is trained and evaluated on a diverse and extensive dataset, ensuring its robustness and generalization capabilities across various skin conditions. The study delves into the interpretability of the CNN model, providing insights into the features learned during the training process. This analysis contributes to the understanding of how deep learning models make decisions in the context of dermatological diagnosis, enhancing the trustworthiness and acceptance of such automated systems in clinical settings. Furthermore, the paper investigates the impact of data augmentation techniques on model performance, addressing the scarcity of labeled dermatological datasets. By increasing the training data, we explore strategies to improve the model’s ability to generalize to unseen cases, thereby enhancing its practical utility. Experimental results are presented and compared with state-of-the-art approaches, proving the viability of the suggested CNN architecture.",Yes,"본 논문은 기존 연구 검토 후 새로운 CNN 아키텍처를 제안하고, 이를 다양한 데이터셋으로 학습 및 평가하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 또한 데이터 증강 기법의 영향 분석과 해석 가능성 연구를 통해 실질적인 기여를 하고 있으므로 연구 논문에 해당한다."
Advancing Intrusion Detection Precision Through Analysis of Diverse Classification Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10698858,"This research commences a thorough examination of the possibilities for leveraging Machine Learning (ML) algorithms to support Intrusion Detection Systems (IDS). We make use of the Kyoto dataset, a standard for intrusion detection studies that includes a wide variety of network traffic patterns related to both benign and malevolent activity. We carefully review a range of machine learning (ML) methods, such as decision trees, random forests, logistic regression, K-Nearest Neighbors (KNN), and Support Vector Machines (SVM) with different kernel functions. The distinct advantages and disadvantages of each algorithm in identifying network anomalies are clarified by this thorough examination. In addition to examining the detection capabilities, we delve into the performance metrics of these algorithms, including accuracy, precision, recall, and F1-score, providing a comprehensive assessment of their effectiveness. We also investigate the computational efficiency of these models by analyzing their training times and the impact of different data splits on their performance. Specifically, we evaluate batch sizes using 80:20,70:30, and 60:40 training-to-test ratios to understand their influence on the training dynamics and the overall efficacy of the IDS. Furthermore, we explore the resilience of these algorithms against various forms of intrusions, such as data alteration attempts, unauthorized access attempts, and denial-ofservice (DoS) attacks. By investigating these state-of-the-art developments and promoting a broader comprehension of IDS approaches, this research ultimately contributes to the strengthening of cybersecurity defenses over time. This guarantees the confidentiality, integrity, and availability of their vital data assets while enabling enterprises to adjust and stay resilient against the constantly shifting threat landscape within complex IT infrastructures. The source code of our paper is available at the following linkhttps://github.com/Snehitha-Narasani/IDS-using-ML-algorithms",Yes,"본 논문은 다양한 머신러닝 알고리즘을 활용하여 침입 탐지 시스템의 성능을 평가하고, 정확도, 정밀도, 재현율 등 여러 성능 지표와 계산 효율성까지 분석하는 독창적인 연구 내용을 포함하고 있다. 또한, 데이터셋을 활용한 실험과 다양한 조건에서의 성능 비교를 통해 직접적인 연구 기여를 하고 있음을 알 수 있다."
Machine Learning-based Services Provisioning for Intelligent Internet of Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9596012,"This paper is aimed to deliver a Machine Learning (ML) based intelligent system that is capable of intelligently issuing services in a pre-defined environment setup that simulates a simple real-life scenario of Internet of Vehicle (IoV). First, a detailed discussion about Vehicular Ad Hoc Networks (VANETs) and IoVs is introduced stating the significant differences between both of them and why IoVs outplay VANETs. A thorough literature review about the fundamental aspects of IoV is clearly addressed. Following the literature review, an environment setup is constructed backed up with an empirically generated dataset. This then paves the way to examine two different Machine Learning classifiers, namely Binary Logistic Regression and Shallow Neural Network for our ML based intelligent system. Both classifiers are discussed in terms of mechanism and mathematical formulation. Finally, an analysis of both classifiers’ performance along with the necessary statistical measures are presented and discussed in addition to a conclusive comparison between both classifiers.",Yes,"본 논문은 IoV 환경을 시뮬레이션하고, 실험적으로 생성된 데이터셋을 바탕으로 두 가지 머신러닝 분류기(Binary Logistic Regression, Shallow Neural Network)를 적용 및 성능 분석하는 독창적인 연구를 수행하고 있다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단된다."
Privacy and Security in Artificial Intelligence and Machine Learning Systems for Renewable Energy Big Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468941,"This paper explores the critical intersection of security and privacy in advanced artificial intelligence (AI) and machine learning (ML) with Internet of Things (IoT) systems and edge computing applied to big data in the renewable energy (RE) sector, where the generated data is grown exponentially, presenting unique challenges in data management, analysis, and security. This study discusses the complexities of anomaly detection (AD) in RE data, examining the evolving security threats and the need for real-time processing. Through a comprehensive literature review and the proposal of an innovative framework, we address the security and privacy challenges in AD for RE data, evaluate the effectiveness of current solutions, and propose robust strategies for enhancing security measures. The study underscores the need for continuous security protocols’ adaptation to evolving threats. It emphasizes the importance of regular audits and compliance with regulatory standards to maintain the resilience of RE systems against cyber threats.",Yes,"논문은 기존 연구를 종합하는 문헌 리뷰뿐만 아니라, 보안 및 프라이버시 문제를 해결하기 위한 혁신적인 프레임워크를 제안하고 있어 독창적인 연구 기여가 포함되어 있다. 따라서 단순 리뷰가 아닌 직접적인 연구 내용을 포함한 논문으로 판단된다."
Iotsdl: Internet Of Things Security For Deep Learning Techniques-A Research Perspectives,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128558,"The Internet of Things (IoT) is an important component of information technology. New trend technology is being developed during the Industnal 4.0 Revolution. IoT connects physical things from a variety of industries, including smart homes, wearable technology, vehicular ad hoc networks (VANETs), Healthcare and Smart Cities. The number of IoT applications has dramatically increased recently, and it is predicted that by 2030, there will be 131 billion linked devices, an increase of 13% each year on average. Security is a crucial issue in today’s IoT industry. In order to detect Internet of Things attacks and identify new types of intrusion to access a more secure network, using deep learning techniques in various models is a useful tool. This helps to overcome the challenges associated with securing IoT devices. The need for developing an attack-identification and classificationsystem for intrusion detection. We provide a study on the distinction between legitimate and malicious actions in order to distinguish abnormalities and intrusions, as well as Network traffic analysis to find new threats. This paper provides broad reviews of deep learning for Internet of Things Seurity. The Major contributions are helpful for researchers and academicians for further research in the direction of Internet of Things Security. This reviews studies by assessing their effectiveness using two kinds of fresh real-time traffic information. (i.e. Bot-IoT datasets and CSE-CIC-IDS2022). We evaluate ‘accuracy rate’, ‘ detection rate’ in many system.",No,"초록에서 본 논문은 IoT 보안과 딥러닝 기법에 대한 광범위한 리뷰를 제공하는 것으로 보이며, 직접적인 독창적 연구 결과나 새로운 실험적 기여보다는 기존 연구들을 평가하고 요약하는 데 중점을 두고 있습니다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
Development of a Remote Sensing System for Real-Time Detection of Military Threats,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10932929,"To strengthen the national defense system, advances in remote sensing technology are essential, especially to detect military threats that could threaten sovereignty. These threats can be physical or non-physical, such as cyber-attacks that can damage vital infrastructure. Therefore, effective border monitoring is critical to maintaining the security and stability of the country. Although remote sensing technology is developing rapidly, several issues hinder its implementation, such as inadequate infrastructure and lack of employee training. Previous research has not fully studied how various advanced technologies can be incorporated into Indonesia's border area defense system that is responsive and operates in real time. The purpose of this study is to see how well remote sensing systems detect military threats and evaluate the problems that arise when implementing them. In addition, this study aims to provide strategic suggestions that will help improve national defense. This research utilizes a literature review of relevant articles and analyzes information to gain a further understanding of advances in remote sensing technology and its applications in defense. Studies show that technologies such as radar, CCTV, and artificial intelligence (AI) can improve threat detection in border areas. AI-based radars enable faster identification and classification of threats, and cloud-based CCTV improves real-time monitoring. The findings from this study suggest that the development and utilization of advanced remote sensing technologies can improve Indonesia's defense system. By addressing infrastructure and personnel training issues, the country can more effectively safeguard its sovereignty and stability by improving responses to military and non-military threats.",No,"본 논문은 문헌 리뷰를 통해 기존 연구와 기술 동향을 분석하고 전략적 제안을 제공하는 내용으로, 직접적인 실험, 개발, 또는 새로운 기술 구현에 대한 독창적인 연구 결과를 포함하지 않습니다. 따라서 연구 논문이라기보다는 종합적 고찰에 가까운 리뷰 논문으로 판단됩니다."
Linearized Analysis of Noise and Resolution for DL-Based Image Generation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918072,"Deep-learning (DL) based CT image generation methods are often evaluated using RMSE and SSIM. By contrast, conventional model-based image reconstruction (MBIR) methods are often evaluated using image properties such as resolution, noise, bias. Calculating such image properties requires time consuming Monte Carlo (MC) simulations. For MBIR, linearized analysis using first order Taylor expansion has been developed to characterize noise and resolution without MC simulations. This inspired us to investigate if linearization can be applied to DL networks to enable efficient characterization of resolution and noise. We used FBPConvNet as an example DL network and performed extensive numerical evaluations, including both computer simulations and real CT data. Our results showed that network linearization works well under normal exposure settings. For such applications, linearization can characterize image noise and resolutions without running MC simulations. We provide with this work the computational tools to implement network linearization. The efficiency and ease of implementation of network linearization can hopefully popularize the physics-related image quality measures for DL applications. Our methodology is general; it allows flexible compositions of DL nonlinear modules and linear operators such as filtered-backprojection (FBP). For the latter, we develop a generic method for computing the covariance images that is needed for network linearization.",Yes,"논문은 딥러닝 기반 CT 영상 생성 방법에 대해 네트워크 선형화 기법을 적용하여 노이즈와 해상도를 효율적으로 분석하는 새로운 방법론을 제안하고, 이를 수치 평가와 실제 데이터에 적용한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
DEEPFAKE Image Synthesis for Data Augmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839427,"Field of medical imaging is scarce in terms of a dataset that is reliable and extensive enough to train distinct supervised deep learning models. One way to tackle this problem is to use a Generative Adversarial Network to synthesize DEEPFAKE images to augment the data. DEEPFAKE refers to the transfer of important features from the source image (or video) to the target image (or video), such that the target modality appears to animate the source almost close to reality. In the past decade, medical image processing has made significant advances using the latest state-of-art-methods of deep learning techniques. Supervised deep learning models produce super-human results with the help of huge amount of dataset in a variety of medical image processing and deep learning applications. DEEPFAKE images can be a useful in various applications like translating to different useful and sometimes malicious modalities, unbalanced datasets or increasing the amount of datasets. In this paper the data scarcity has been addressed by using Progressive Growing Generative Adversarial Networks (PGGAN). However, PGGAN consists of convolution layer that suffers from the training-related issues. PGGAN requires a large number of convolution layers in order to obtain high-resolution image training, which makes training a difficult task. In this work, a subjective self-attention layer has been added before 256 × 256 convolution layer for efficient feature learning and the use of spectral normalization in the discriminator and pixel normalization in the generator for training stabilization - the two tasks resulting into what is referred to as Enhanced-GAN. The performance of Enhanced-GAN is compared to PGGAN performance using the parameters of AM Score and Mode Score. In addition, the strength of Enhanced-GAN and PGGAN synthesized data is evaluated using the U-net supervised deep learning model for segmentation tasks. Dice Coefficient metrics show that U-net trained on Enhanced-GAN DEEPFAKE data optimized with real data performs better than PGGAN DEEPFAKE data with real data.",Yes,"본 논문은 기존 PGGAN 모델에 self-attention 레이어와 spectral normalization, pixel normalization을 도입하여 Enhanced-GAN을 제안하고, 이를 통해 DEEPFAKE 이미지 합성 성능을 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 모델의 성능을 다양한 지표와 실제 U-net 기반 분할 작업에 적용하여 평가한 점에서 직접적인 연구 기여가 명확하다."
Extracting Ancient Maya Structures from Aerial LiDAR Data using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10115095,"The advent of LiDAR technology has had a revolutionary impact on archaeological prospection by vastly enlarging the coverage of ancient landscapes and consequently the number of ancient surface features. However, manual analysis by experts requires a significant time and money investment. This paper describes a deep learning model developed to segment, i.e., label, the semantics of objects of interest as a means to augment or supplant manual labeling of LiDAR data. The U-Net deep learning model forms the backbone of the system which has shown success in providing accurate outputs on similar LiDAR data set. The trained U-Net model is integrated into an inference pipeline to transform expansive LiDAR datasets into labeled output images. Work focuses on the classification of two semantic types: (1) platforms and (2) annular structures whose attributes, e.g., location, shape, and distribution, play an important role in improving our understanding of ancient Maya civilizations. This article provides a deep learning-based system that efficiently extracted these structures. CNN-generated inferences were compared against expert-labeled features to measure algorithm performance. Results for a LiDAR survey of 479 sq. km. indicate that the CNN provides an IoU performance of 0.82 and 0.74 for annular structures and platforms respectively. The discussion further analyzes how IoU performance relates to the viability of this approach as an aid or substitute for manual labeling.",Yes,본 논문은 고대 마야 구조물을 추출하기 위해 딥러닝 모델(U-Net)을 개발하고 이를 LiDAR 데이터에 적용하여 직접적인 연구 결과와 성능 평가를 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함하는 연구 논문에 해당한다.
Investigation of Dynamic Eccentricity in Interior Permanent Magnet Synchronous Motor through Finite Element Method and Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10696743,"Interior permanent magnet synchronous motors (IPMSM) are strong competitors to other motors in commercial and traction applications due to their high efficiency and high speed. As non-uniform air gaps are difficult to identify at the beginning level, over 10% of faults occurring in PMSM are eccentricity-related. This work investigates the effect of dynamic eccentricity in an IPMSM through finite element analysis and a machine learning approach. In this work, an IPMSM of 550 W & 220 V finite element model in ANSYS Maxwell is considered. Eccentric damages ranging from 10% to 40% were included in the model and evaluated for further study. Motor characteristics such as stator current, and radial flux density are compared for both normal and eccentric conditions. Data on motor winding current and air gap flux (radial component) were acquired from the finite element model and faults were examined utilizing MATLAB machine learning tools. The ensemble bagged trees algorithm is employed to analyze the fault which provides an accuracy of 82.5% for stator current.",Yes,"본 논문은 IPMSM의 동적 편심 결함을 유한요소해석과 머신러닝 기법을 통해 직접 조사하고 있으며, 550W, 220V 모터 모델을 구축하여 다양한 편심 손상을 평가하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 실험 데이터와 머신러닝 분석을 통해 결함 진단 정확도를 제시하는 등 연구 기여가 명확하다."
UbeHealth: A Personalized Ubiquitous Cloud and Edge-Enabled Networked Healthcare System for Smart Cities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8382164,"Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients' data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications' communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.",Yes,"논문은 UbeHealth라는 새로운 헬스케어 프레임워크를 제안하고, 이를 구현한 개념 증명 시스템과 알고리즘을 상세히 설명하며, 세 가지 데이터셋을 사용해 평가한 연구 내용을 포함하고 있다. 이는 독창적인 연구 기여와 실험적 검증을 포함하는 연구 논문임을 나타낸다."
Learning Techniques for Depression Detection: A Comparative Studies,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083576,"Depression is a common mental problem that can fundamentally affect individuals' emotional wellness as well as their everyday lives. After COVID-19 other pandemics and subsequent social isolation this issue is more potent than ever. Numerous research works have been going on searching for methods that effectively recognize depression in order to detect depression. In this regard, a number of studies have been proposed. In this study, it examines a number of previous ones utilizing various Machine Learning (ML) and Artificial Intelligence (AI) methods for depression detection. In addition, various methods for determining an individual's mood and emotion are discussed. This study also discusses how facial expression, voice, gesture can be understood by chatbot and classified it as a depressed person or not. Addition to this, it reviews all the related research works and evaluates their methods to detect depression.",No,"초록에서 본 논문은 기존 연구들을 검토하고 비교하는 리뷰 성격의 논문으로 보이며, 직접적인 독창적 연구 결과나 새로운 실험, 방법론 제시가 포함되어 있지 않습니다. 따라서 연구 논문보다는 비교 연구 또는 문헌 리뷰에 해당합니다."
Exploring the Influence of Emotional States in Peer Interactions on Students’ Academic Performance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10367874,"Contribution: An AI model for speech emotion recognition (SER) in the educational domain to analyze the correlation between students’ emotions, discussed topics in teams, and academic performance.Background: Research suggests that positive emotions are associated with better academic performance. On the other hand, negative emotions have a detrimental impact on academic achievement. This highlights the importance of taking into account the emotional states of the students to promote a supportive learning environment and improve their motivation and engagement. This line of research allows the development of tools that allow educators to address students’ emotional needs and provide timely support and interventions. Intended Outcome: This work analyzes students’ conversations and their expressed emotions as they work on class activities in teams and investigates if their conversations are course-related or not by applying topic extraction to the conversations. Furthermore, a comprehensive analysis is conducted to identify the correlation between emotions expressed by students and the discussed topics with their performance in the course in terms of their grades. Application Design: The student’s performance is formatively evaluated, taking into account a combination of their scores in various components. The core of the developed model comprises a speech transcriber module, an emotion analysis module, and a topic extraction module. The outputs of all these modules are processed to identify the correlations. Findings: The findings show a strong positive correlation between the expressed emotions of “relief” and “satisfaction” with students’ grades and a strong negative correlation between “frustration” and grades. Data also shows a strong positive correlation between course-related topics discussed in teams and grades and a strong negative correlation between noncourse-related topics and grades.",Yes,"논문은 AI 모델 개발, 감정 인식, 주제 추출 및 이들 요소와 학업 성과 간의 상관관계 분석을 포함한 독창적인 연구 내용을 담고 있습니다. 또한, 실험적 데이터 분석과 결과 도출을 통해 학문적 기여를 하고 있으므로 연구 논문에 해당합니다."
Enhancing IC Fault Diagnosis with Ensemble Learning Models: A Random Forest Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10827897,"This paper presents a comprehensive study on the application of ensemble learning models, specifically the Random Forest algorithm, for the intelligent diagnosis of faults in integrated circuits (ICs). The significance of accurate and efficient fault detection in the semiconductor industry is underscored, highlighting the need for advanced diagnostic techniques. The study reviews the landscape of traditional and machine learning-based IC fault diagnosis methods, leading to a focused discussion on ensemble learning. The theoretical framework underpinning the Random Forest model is explored, elucidating its ensemble learning principles and algorithmic components. The methodology section details the data preprocessing, feature selection, and model configuration, culminating in a rigorous empirical evaluation of the model’s performance. Results demonstrate the model’s diagnostic accuracy and robustness, supported by a series of visualizations that illustrate the decision-making process and comparative effectiveness. The discussion interprets the findings within the IC fault diagnosis context, evaluating the model’s efficacy and suggesting avenues for improvement. The paper concludes with a summary of contributions and prospects for future research, emphasizing the potential of the Random Forest model in advancing fault diagnosis capabilities.",Yes,"논문은 Random Forest 기반의 앙상블 학습 모델을 IC 결함 진단에 적용한 구체적인 방법론과 실험 평가를 포함하고 있어, 독창적인 연구 내용과 직접적인 기여를 담고 있다. 따라서 연구 논문에 해당한다고 판단된다."
A Review of YOLO Models for Soccer-Based Object Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10756443,"The application of AI-driven computer vision techniques to sports footage for the purpose of automatic insight generation is a growing area of research and development. The ability to detect players of various teams along with other entities is a foundational component of such activities. This paper describes the machine learning-based detection of soccer players, balls, goalkeepers, referees, assistant referees, and other attendees from multiple teams which form the initial steps for soccer analytics. This study performs an empirical analysis of various versions of the computer vision model YOLO (You Only Look Once) using the open source soccer dataset named SoccerNet. The images contained within are trained on YOLO V3, V5, V8 and V9 with epoch configurations 1, 5, 10, 25, and 50 to facilitate a broad review of the capabilities of YOLO models. The results are evaluated using mAP50 and mAP50-95 (Mean Average Precision) metrics. YOLOv9-(9c, 9e) weights exhibited a similar level of performance to those of YOLOv8(8l, 8x), which both outperformed versions V3 and V5. However, YOLOv9 resulted in a higher true positive rate than YOLOv8, which suggests that a multi-metric analysis is pertinent for model ranking. The results of each model variant utilised are reported and visualized for reproducibility and to form a benchmark supporting future studies. The implementation was performed using the Python-based Ultralytics library.",Yes,"본 논문은 다양한 YOLO 모델을 사용하여 축구 영상에서 객체를 탐지하는 실험적 연구를 수행하고, 여러 모델의 성능을 비교 분석한 독창적인 연구 내용을 포함하고 있습니다. 따라서 단순 리뷰가 아닌 직접적인 실험과 결과 분석을 담은 연구 논문에 해당합니다."
The Use of Blockchain to Support Distributed AI Implementation in IoT Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372311,"This article presents a distributed and decentralized architecture for the implementation of distributed artificial intelligence (DAI) using hardware platforms provided by the Internet of Things (IoT). A trained DAI system has been implemented over IoT, where each IoT device acts as one or more of the neurons within the DAI layers. This is accomplished by the utilization of decentralized, self-managed blockchain technologies that allow trusted interactions and information to be exchanged between distributed neurons. The platform was built and customized to be used within the IoT system, and it is capable of handling DAI-related tasks. A new consensus mechanism based on Proof of Authority (PoA) and Proof of Work (PoW) has been designed and implemented, along with bespoke block and transaction formats. The proposed architecture was analyzed, implemented, and tested using a dedicated testbed with low-cost IoT devices. A quantitative measurement and performance evaluation of the system based on a real-world IoT application was conducted. The implemented DAI is found to have an accuracy of 92%–98%, with an energy cost of 0.12 joules (J) when utilizing a Raspberry Pi to run one neuron. The measured hash per joule (h/J) when using a Raspberry Pi for mining is 13.8 Kh/J compared to 54 Kh/J using an ESP32. The results showed that it is feasible to implement a DAI system utilizing the IoT hardware platform while maintaining the system’s accuracy. The integration of the blockchain has added an element of security and trust to the data and the interaction between system components.",Yes,"본 논문은 IoT 시스템에서 분산 인공지능 구현을 위한 블록체인 기반 아키텍처를 설계, 구현, 테스트하고 성능 평가를 수행한 독창적인 연구 내용을 포함하고 있습니다. 새로운 합의 메커니즘 설계 및 실제 하드웨어 플랫폼에서의 실험 결과를 제시하여 직접적인 연구 기여가 명확합니다."
An Unobtrusive Stress Recognition System for the Smart Office,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8856597,"This paper presents a novel approach to monitor office workers' behavioral patterns and heart rate variability. We integrated an EMFi sensor into a chair to measure the pressure changes caused by a user's body movements and heartbeat. Then, we employed machine learning methods to develop a classification model through which different work behaviors (body moving, typing, talking and browsing) could be recognized from the sensor data. Subsequently, we developed a BCG processing method to process the data recognized as `browsing' and further calculate heart rate variability. The results show that the developed model achieved classification accuracies of up to 91% and the HRV could be calculated effectively with an average error of 5.77ms. By combining these behavioral and physiological measures, the proposed approach portrays work-related stress in a more comprehensive manner and could contribute an unobtrusive early stress detection system for future smart offices.",Yes,"논문은 EMFi 센서를 활용한 새로운 스트레스 인식 시스템을 제안하고, 머신러닝 기반 분류 모델 개발 및 심박 변이도 계산 방법을 포함한 독창적인 연구 내용을 담고 있습니다. 또한, 실험 결과를 통해 모델의 정확도와 효과를 검증하여 직접적인 연구 기여를 하고 있음을 보여줍니다."
Text Filtering Insignificant Word (TFIW) Readability Framework Model to Explain NLP in Sentiment Analysis Using Reinforcement Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894597,Reading of a text in a given sentence have lot of parameters to consider for expressing an opinion on a given text to conclude nature of the text. The data which need to be formed in the form of data sets based on specific constrains. The Natural Language Processing (NLP) will always presume that Text readability made as the considerable measure for better round spread of any newspaper. The data related to these documents must be collected and store them in data sets. During this process of measuring text readability at the granular level of the text reading is a cumbersome task. This will always implicate a problem of machine readability of the text and explainability of the text taken from the documents mentioned above. In this context human readability will act an important role in predicting the text reading at granular level where the text-based sentiment analysis have been predicted through a machine learning model. The text reading has been evaluated in order to improve the readability through explainable system. The explainable systems will capture and explain the predictions of text reading. In this technical work an explainable system Text Filtering Insignificant Word (TFIW) is introduced where it improves the textual reading at the granule level based on sentiments by prediction the words which need to be filtered. This will improve the explainability of text and text reading and improve the fast text and explainability of the document.,Yes,논문 초록에서 제안된 Text Filtering Insignificant Word (TFIW) 모델은 텍스트 가독성 향상과 감성 분석을 위한 강화학습 기반의 설명 가능한 시스템을 개발하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구와 차별화된 새로운 방법론을 제시하는 연구 논문으로 판단된다.
Cathode Position Detection in a Transferred Arc Plasma Using Artificial Neural Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998184,"In a transferred arc plasma system, the position of the cathode is difficult to detect during the smelting process as it remains inside the cylindrical anode. Real-time and accurate cathode position detection leads to efficient smelting operation with optimal use of electrical energy. In this article, a machine learning technique is proposed to accurately detect the position of the cathode in a direct current (DC) transferred arc plasma system. The measured voltage signal sampled at 20 kHz is processed using a tunable Q-factor wavelet transform (TQWT) followed by statistical features extraction and a machine learning algorithm to provide accurate cathode position information. Two different machine learning algorithms are used in this work, namely, single hidden layer neural network (SHLNN) and single-layer extreme learning machine (SELM). The output of these machine learning algorithms provides accurate position information and is also compared to the traditional voltage-related position information. The experimental signal of a 30-kW DC plasma system and cathode position detection results is shown.",Yes,"본 논문은 전이 아크 플라즈마 시스템에서 음극 위치를 정확히 탐지하기 위한 기계 학습 기법을 제안하고, 실험 신호를 이용해 두 가지 신경망 알고리즘의 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
GitHub Bug Classification Using Pipeline Approach in Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10452560,"GitHub bug classification refers to the process of automatically categorizing bug reports or issues on GitHub into predefined categories based on their text content. This process helps to improve the efficiency of software development teams by reducing the time spent on manual issue triage. By using machine learning techniques such as Naive Bayes, Decision Trees, and Logistic Regression, a model can be trained on a dataset of past issues and then used to classify new issues as they are submitted. This study aims to build a machine-learning model for GitHub bug classification using a pipeline approach and evaluate its accuracy, precision, and recall performance.The study also includes a comprehensive literature review of bug tracking and classification techniques, existing systems and approaches, and evaluation metrics for performance measures. The research design and strategy, data collection and sources, and ethical considerations are also discussed in detail.The practice of automatically classifying bug reports or issues on GitHub into specified categories based on their text content is called GitHub bug categorization. By cutting down on the time needed for manual problem triage, this technique increases the effectiveness of software development teams.",Yes,"논문 초록에서 GitHub 버그 분류를 위한 머신러닝 모델을 구축하고, 파이프라인 접근법을 사용하여 성능 평가(정확도, 정밀도, 재현율)를 수행한다고 명시하고 있습니다. 이는 기존 연구를 종합하는 문헌 리뷰뿐만 아니라 직접적인 모델 개발과 평가를 포함하는 독창적인 연구 내용임을 보여줍니다."
Convolutional Neural Network in Combination with Multiple Machine Learning Models to Recognize the Spoken Digits,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10859457,"The classification of audio signals plays a crucial role in various applications including speech recognition, analysis of environmental audio, and music classification. Digit recognition, as a subset of audio classification, plays an essential role in domains comprising automated systems, telecommunications, transport systems, and security. A review of recent research papers in the field of audio classification, focusing on the use of Artificial Intelligence such as Neural Networks, and ML - Machine Learning, along with its hybrid models, has been conducted. The objective is to enhance training accuracy using the Mel Spectrogram in CNN - Convolutional Neural Networks and improve prediction accuracy with ML models. Various hybrid approaches, including CNN combined with Logistic Regression, K-Nearest Neighbor, Random Forest, Decision Tree, Naive Bayes, Support Vector Machine, Gaussian Mixture Model, and Linear Discriminant Analysis, have been evaluated. The proposed method, CNN combined with Logistic Regression, establishes the highest accuracy compared to the other models. Experimentation was conducted using the Audio MNIST dataset, which includes 10 distinct classes of digits. Numerical results reveal that the CNN + Logistic Regression accomplished the highest Accuracy value of about 99%, and CNN + K-Nearest Neighbor achieved 98%, making CNN + Logistic Regression the most effective model for digit recognition within the audio classification field.",Yes,"본 논문은 음성 숫자 인식을 위해 CNN과 여러 머신러닝 모델을 결합한 하이브리드 접근법을 제안하고, Audio MNIST 데이터셋을 사용한 실험을 통해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
A Deep Learning-Based Approach to Strawberry Grasping Using a Telescopic-Link Differential Drive Mobile Robot in ROS-Gazebo for Greenhouse Digital Twin Environments,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10807213,"The primary goal of this research is to develop a deep learning-powered robotic solution to address labor shortages and optimize harvesting processes in strawberry greenhouse farms. By incorporating this system into the development process, the aim is to provide continuous, 24/7 operational efficiency for strawberry harvesting in greenhouse environments. This study is grounded in a comprehensive literature review of simulated environments, such as ROS-Gazebo, deep learning detection models, and mobile robot platforms, with a focus on developing innovative robotic solutions for strawberry detection and grasping in a simulated digital twin greenhouse environment. The YOLOv9-GLEAN deep learning model, with super-resolution capabilities, is introduced to enhance strawberry detection accuracy by generating high-resolution image features. We developed a digital twin model of the SILAL strawberry greenhouse farm in Abu Dhabi, UAE, within the ROS-Gazebo environment, to validate our algorithm and test the MARTA (Mobile Autonomous Robot with Telescopic Arm) robot. The dataset used to improve model performance includes both real strawberry images from greenhouse farm and synthetic CAD-generated images. ROS-MoveIt was employed to implement visual servoing, allowing the robot to generate precise motion trajectories to approach and grasp identified strawberries, with visual feedback enhancing accuracy. Empirical results show that our proposed detection model outperforms other existing models, achieving a precision of 0.996 and a recall of 0.991. The model’s adaptability to varied datasets, including real and synthetic images, is notable, and it performs exceptionally well in the simulated digital twin model of the greenhouse farm. The model is uniquely trained on both real and synthetic strawberry images to ensure robust detection performance. It is compared to state-of-the-art models and deployed on a telescopic arm-based robotic platform, which is simpler to control than an articulated arm for strawberry harvesting and grasping tasks.",Yes,"본 논문은 딥러닝 기반의 새로운 딸기 인식 및 집게 로봇 시스템을 개발하고, 이를 디지털 트윈 환경에서 검증하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 실제 및 합성 데이터를 활용한 모델 학습과 성능 평가를 통해 직접적인 연구 기여를 하고 있음을 보여줍니다."
Design Amharic Text Sentiment Analysis Model Using Machine Learning Techniques. In Case of Restaurant Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10302239,"Sentiment analysis is a type of natural language processing for tracking the attitude of the public about a particular product, service or topic. It is also highly challenging as natural language processing research topic, and covers many novel sub-problems. Now business organizations and academics are putting forward their efforts to find the best system for sentiment analysis. The focus of this study was an Amharic unstructured restaurant review on the web. The objective of the paper was to design Amharic text sentiment analysis model using supervised machine learning techniques and evaluate the performance of classifiers. This paper explored the supervised machine learning classification approaches (naive Bayes, support vector machine and k-nearest neighbor) with different feature selection schemes to obtain a sentiment analysis model for domain specific restaurant review dataset at sentence level. The proposed model has the following components: Data preparation, preprocessing such as tokenization, normalization, filter stop words, feature extraction and selection to prepare feature vector, polarity classification. Performance analysis carried out on classifiers, based on n-grams proposed. From the results of the experimental studies, all algorithms are known to be highly effective classifiers, and are able to achieve good accuracy in this experiment. The experiments show that Term frequency (TF) and the TF -IDF scheme gives maximum accuracy 80.43 % and 79.49 % respectively for SVM in bigram features. Term frequency and term occurrence also give maximum accuracy 78.37 % and 78.00 % respectively for Naive Bayes classifier at bigram features. TF-IDF also give maximum accuracy 78.00% for KNN at 4-gram. The challenge was opinion holders sometimes give objective text to express their opinion, but the classifier did not identify those facts from opinions. These kinds of complexities of natural languages make sentiment mining systems more challenging and to resolving this challenge subjectivity and objectivity classification is needed.",Yes,"본 논문은 Amharic 텍스트 감성 분석 모델을 설계하고, 여러 머신러닝 기법을 적용하여 성능을 평가하는 실험적 연구를 수행하였다. 이는 독창적인 연구 내용과 실험 결과를 포함한 직접적인 연구 논문에 해당한다."
A Review on Intrusion Detection System using Machine Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397121,"Computer networks are exposed to cyber related attacks due to the common usage of internet, as the result of such, several intrusion detection systems (IDSs) were proposed by several researchers. Among key research issues in securing network is detecting intrusions. It helps to recognize unauthorized usage and attacks as a measure to ensure the secure the network’s security. Various approaches have been proposed to determine the most effective features and hence enhance the efficiency of intrusion detection systems, the methods include, machine learning-based (ML), Bayesian based algorithm, nature inspired meta-heuristic techniques, swarm smart algorithm, and Markov neural network. Over years, the various works being carried out were evaluated on different datasets. This paper presents a thorough review on various research articles that employed single, hybrid and ensemble classification algorithms. The results metrics, shortcomings and datasets used by the studied articles in the development of IDS were compared. A future direction for potential researches is also given.",No,초록에서 해당 논문은 기존 연구들을 종합적으로 검토하고 비교하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과를 제시하는 연구 논문이 아니라 기존 연구들의 분석과 정리를 목적으로 한 논문입니다.
Rapid Wildfire Hotspot Detection Using Self-Supervised Learning on Temporal Remote Sensing Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10641631,"Rapid detection and well-timed intervention are essential to mitigate the impacts of wildfires. Leveraging remote sensed data from satellite networks and advanced AI models to automatically detect hotspots (i.e., thermal anomalies caused by active fires) is an effective way to build wildfire monitoring systems. In this work, we propose a novel dataset containing time series of remotely sensed data related to European fire events and a Self-Supervised Learning (SSL)-based model able to analyse multi-temporal data and identify hotspots in potentially near real time. We train and evaluate the performance of our model using our dataset and Thraws, a dataset of thermal anomalies including several fire events, obtaining an F1 score of 63.58.",Yes,"논문은 유럽 화재 사건과 관련된 시계열 원격 감지 데이터를 포함하는 새로운 데이터셋을 제안하고, 이를 활용한 Self-Supervised Learning 기반 모델을 개발하여 화재 핫스팟을 실시간에 가깝게 탐지하는 연구를 수행하였다. 이는 독창적인 데이터셋 구축과 모델 개발을 포함한 직접적인 연구 기여로 판단된다."
Using Echocardiography and AI to Predict Cardiac Biological Age,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633303,"Biological age is a personalized measure of the health status of an organism, organ, or system, as opposed to simply accounting for chronological age. To date, there have been known attempts to create estimators of biological age based on various biomedical data. In this work, we focused on developing an approach for assessing heart biological age using echocardiographic data. The current study included echocardiographic data from more than 5,000 different cases. As a result, indicators such as EA (E/A ratio of maximum flow rates in the first and second phases), IVS (thickness of the interventricular septum), LVPW (thickness of the posterior left ventricular wall), LVCO (cardiac output), and RWT (relative wall thickness) showed the greatest predictive potential. Based on these parameters, we created and trained neural network models to determine heart biological age, with a Mean Absolute Error (MAE) of approximately 3.5 years, an R-squared (R2) value of around 0.87, and a Spearman's rank correlation coefficient (rho) greater than 0.9 in men. In women, the MAE was approximately 3.4 years, with an R2 value of around 0.89 and a rho greater than 0.9. In addition, we have applied an AI explanation algorithm to improve understanding of how the model performs an assessment. Furthermore, the EchoAGE model was tested on echo cardiographic data from patients with age-related diseases, patients with multimorbidity, children with progeria syndrome, and diachronic data series.",Yes,"논문은 5,000건 이상의 심초음파 데이터를 활용하여 심장 생물학적 나이를 예측하는 신경망 모델을 개발하고 평가하는 독창적인 연구를 수행하였으며, AI 설명 알고리즘 적용과 다양한 환자군에 대한 테스트도 포함하고 있어 직접적인 연구 기여가 명확하다."
Classification of OLTC defects based on AE signals measured by two different transducers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778140,"The subject of the article is related to diagnosis of on-load tap-changer (OLTC) based on acoustic emission (AE) signals measured by two different transducers. The main advantage of the AE method is the possibility of its application during normal operation without having to turn off the device under investigation. Based on chosen parameters of the AE signal various defects of the OLTC may be recognized. A number of signals, gathered from laboratory tests, in which four types of typical OLTC defects were simulated, was applied for classification studies with the use of artificial intelligence methods. In particular seventeen different supervised learning algorithms were investigated, while their effectiveness was compared by using common measures. Based on the performed studies the best algorithm for each of the two applied transducers was determined. Results of the works in form of the chosen algorithm may be applied in an expert system for diagnosis of OLTC devices when using different AE measuring sensors.",Yes,논문은 OLTC 결함 진단을 위해 두 가지 다른 트랜스듀서로 측정한 AE 신호를 기반으로 다양한 인공지능 분류 알고리즘을 적용하고 비교하는 실험적 연구를 수행하였다. 이는 직접적인 실험 데이터와 알고리즘 평가를 포함하는 독창적인 연구 내용으로 판단된다.
Unsupervised classification of online community input to advance transportation services,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301704,"The traditional approach taken by transit agencies to assess their performance is through regular rider surveys. The performance metrics include safety, timeliness, efficiency, and cleanliness. However, with the increased use of online social media by the public, including users of public transit, it has become possible to automatically summarize the riders' opinions on the services provided by transit agencies by statistical analysis of the words used in the social media messages. This work describes a system based on machine learning to summarize text messages regarding transportation in California on online social media platforms. This tool is intended to reveal factors important to transportation users that may not be evident to transit agencies and hence will not be collected by rider surveys. The system uses an unsupervised statistical topic modeling algorithm (latent Dirichlet allocation) to cluster public messages related to transportation on the Twitter social media platform into distinct “topics.” Sentiment analysis was then utilized to assign a polarity (positive, negative, or neutral) to each message and then the sentiment is aggregated by topic. The system is thus able to summarize the sentiment towards each automatically identified topic. The approach was applied to a set of 10,400 tweets containing words related to transportation; these messages were downloaded over a period of three weeks in 2016. The proposed system was evaluated by varying topic modeling algorithm parameters and studying the effect of parameters on the interpretability of results. It was found that the quality of topic identification depends on the size of the dataset, the number of topics that has to be specified to the topic modeling algorithm, and the positive/negative thresholds for the sentiment analysis algorithm.",Yes,이 논문은 온라인 소셜 미디어 데이터를 활용하여 교통 서비스에 대한 이용자 의견을 자동으로 분류하고 감성 분석하는 기계 학습 기반 시스템을 제안하고 평가하는 독창적인 연구 내용을 포함하고 있습니다. 기존 설문조사 방식과 달리 새로운 데이터 소스와 알고리즘을 적용하여 교통 서비스 평가 방법을 발전시키는 직접적인 연구 기여가 명확합니다.
An Efficient Hybrid Quantum Variational Classifier With Matrix Product State,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888238,"Matrix Product States have been extensively explored as a powerful tool for simulating quantum states in image classification task. However, most research has focused on classical simulations or computations involving high-dimensional unitaries, and significant challenges still exist in the practical preparation of Matrix Product States on quantum computers. This paper proposes a novel and practically feasible quantum variational algorithm based on Matrix Product States for image classification task. We design a hardware-efficient quantum circuit with several adjustable entangling operators to prepare the local tensors in Matrix Product States and integrate minimal residuals to ensure computational stability. We demonstrate that our algorithm can reduce the parameter complexity from growing exponentially with the system size to a linear scale. To validate the effectiveness of this quantum variational algorithm, we conducted experiments on the MNIST dataset, achieving accuracies of 99.95% and 95.96% for binary and ten-class classification tasks, which outperforms other related quantum algorithms. This work advances the practical application of quantum machine learning in resource-constrained environments of quantum computing.",Yes,"논문은 기존 연구의 한계를 극복하기 위한 새로운 양자 변분 알고리즘을 제안하고, 하드웨어 효율적인 양자 회로 설계 및 실험을 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Forecasting Students Academic Achievement Using Machine Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531560,"In today’s competitive world, investigating students learning performance is the major component for accessing any educational systems, Predicting the students’ performance in academics is a difficult task. In order to address this, researchers use the EDM (Educational Data Mining) technique to examine data from educational settings and enhance the educational system based on the collected data. This allows for a more accurate and comprehensive understanding of students' performance and helps them achieve better educational outcomes. Prediction of student performance in academics helps to analyze students community in better understandable and communicable with the wide range of socio demographic(age, gender, family, size, obesity, marital status of parents, occupation,),learning practices(school level), student related(stress and lifestyle) variables Numerous methods are employed to assess students' academic performance; however, the primary goal of this work is to build a prediction model through the application of machine learning techniques, such as logistic regression, K-Nearest Neighbor algorithm (KNN), and Support Vector Machine (SVM).Multiple approaches are used here to measure the performance of academics of students along with good prediction method based on accuracy. The predictive models offer valuable insights into the factors influencing academic success, allowing for the allocation of resources through social media and interventions more effectively. The prediction is a variable that helps to predict how many hours they spend on internet in some activities based on data collected. The result is a predictor variable that aids in forecasting academic achievement on the final exam of the semester (CGPA). The study identifies the lagged part of the studies and how it affect the education history in future. The results as the prediction have been increased which helps the institution and others in encouraging and help the students in different ways. The health-conscious lifestyle positively helps to improve the performance in academics. However, gender is not significant predictor to predict performance and classify their talents based on their skills helps to enhance their performance",Yes,"본 논문은 머신러닝 기법을 활용하여 학생들의 학업 성취도를 예측하는 모델을 구축하고, 다양한 변수들이 학업 성과에 미치는 영향을 분석하는 독창적인 연구 내용을 포함하고 있다. 이는 단순한 문헌 리뷰나 이론적 고찰이 아닌, 실제 데이터 분석과 예측 모델 개발을 통한 직접적인 연구 기여로 판단된다."
Machine Learning for Data Trust Evaluations in Blockchain-Enabled IoT Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10634433,"Recently, there has been a surge of interest surrounding the integration of blockchain with the Internet of Things (IoT), aiming to address IoT’s inherent issues like single points of failure and concerns related to data integrity. However, although blockchain provides decentralization and transparency, it does not guarantee the accuracy and reliability of IoT-generated data. Therefore, additional measures are needed to assess and verify the reliability of IoT data stored on blockchains. In this demonstration, we present a novel approach that employs support vector machine (SVM) models in edge servers and multiple machine learning (ML) models executed by validators for data trust evaluations in blockchain-enabled IoT systems. Our approach introduces a composite trust metric that combines past device reputation on the blockchain with real-time data assessment enabled by SVM models. This composite measure provides a dynamic method for determining the trustworthiness of data at the point of submission. The multiple different ML models used by validators work as a distributed ensemble, leading to improved classification accuracy. This novel approach helps to calculate reputation scores more accurately, increasing the system’s reliability. We illustrate the feasibility of our approach through a description of our prototype implementation.",Yes,"논문 초록에서 제안된 방법은 SVM과 여러 머신러닝 모델을 활용하여 블록체인 기반 IoT 시스템에서 데이터 신뢰도를 평가하는 새로운 접근법을 제시하고 있으며, 프로토타입 구현을 통해 그 타당성을 입증하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 나타낸다."
Coupling Machine Learning Models with Innovative Technology-based Screening Tool for Identifying Psychological Distress among Aboriginal Perinatal Mothers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340563,"Aboriginal perinatal mothers are at a significant risk of experiencing mental health problems, which can have profound negative impacts, despite their overall resilience. This work aimed to build prediction models for identifying high psychological distress among Aboriginal perinatal mothers by coupling machine learning models with an innovative and culturally-safe screening tool. The original dataset of 179 Aboriginal mothers with 337 variables was obtained from twelve perinatal health settings at Perth metropolitan and regional centers in Western Australia between July and September 2022, using a specifically designed web-based rubric for the perinatal mental health assessment. After data preprocessing and feature selection, 23 variables related to emotional manifestations, the problematic partner, worries about daily living, and the need for follow-up wraparound support were identified as significant predictors for the high risk of psychological distress measured by the Kessler 5 plus adaptation. The selected predictors were used to train prediction models, and most of the chosen machine learning models achieved satisfactory results, with Random Forest and Support Vector Machine yielding the highest AUC of over 0.95, accuracy over 0.86, and F1 score above 0.87. This study demonstrates the potential of using machine learning-based models in clinical decision-making to facilitate healthcare and social and emotional well-being for Aboriginal families.",Yes,"본 논문은 원시 데이터를 수집하고, 변수 선택 및 머신러닝 모델을 구축하여 심리적 고통을 예측하는 독창적인 연구를 수행하였으므로 직접 기여하는 연구 내용이 포함된 연구 논문에 해당한다. 또한, 새로운 문화적으로 안전한 평가 도구와 기계학습 기법을 결합한 점에서 독창성이 있다."
Generalization vs Personalization: A Trade-off for better Data Heterogeneity impact Mitigation in FL,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10901830,"Federated learning (FL) was introduced recently as a new machine learning (ML) paradigm. It is a distributed network of client nodes that train ML and deep learning (DL) models on their local data without sharing them to preserve data privacy (DP). However, these data are heterogeneous by nature as they are collected in different contexts using various sources such as IoT devices. Consequently, data heterogeneity (DH) in FL has brought new performance-related challenges. Few of these challenges have been addressed in the literature; moreover, context heterogeneity and balance rate were not explored at all. In this paper, we introduce an FL approach in which a trade-off between personalization and generalization is achieved to mitigate the impact of DH and obtain better performance. We focus on three DH challenges: context, non-independent and identically distributed (non-IID) data, and balance rate. For the implementation, fall detection (FD) data is used to demonstrate the potential of our approach in improving the FL system’s performance. FD is an important subject and is particularly prevalent for the safety of elderly people. Hence, we collected fall data from two sensors: accelerometer (ACC) and heart rate (HR), then, we used two ML models to evaluate our approach. We utilized XGBoost (XGB) for balanced and unbalanced clients and One-Class Support Vector Machine (OC-SVM) for one-label clients. Our approach achieved an average F1-score of 88%. A comparative study was also conducted with previous works on FD. Our results showed a performance improvement which exceeded 94.30% on average.",Yes,"논문은 연합학습(FL)에서 데이터 이질성 문제를 해결하기 위한 새로운 접근법을 제안하고, 이를 실제 낙상 감지 데이터에 적용하여 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 또한, 기존 연구에서 다루지 않은 맥락 이질성과 균형 비율 문제를 탐구하며 실험적 결과를 제시하고 있어 연구 논문에 해당한다."
Automation of Bug-Report Allocation to Developer using a Deep Learning Algorithm,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493515,"Software bug maintenance is an important aspect of all software projects. The assignment of bug reports is essential in order to resolve bugs efficiently. In the case of open-source software developments and large projects, where many developers are engaged on different aspects of software development, it can be difficult to assign bug removal tasks to an appropriate developer. An increase in reported bugs, coupled with an increase in the number of software developers, will complicate the bug triage process. In these situations, bug triaging might be slow and may increase the Bug Tossing Length (BTL). An automated system to triage bug reports could potentially reduce BTL, as manual assignment of bug reports is tiresome, costly, and very time-consuming. The assignment of bug reporting to an irrelevant developer who does not possess sufficient experience to deal with the bug will adversely impact BTL and customer satisfaction. Text-based classification methods have the potential to make a strong contribution to automating the bug triaging process. In this research, different types of Information Retrieval and Machine Learning algorithms are used to determine the appropriate developer/s to rectify the reported bugs. This study used deep learning algorithms, such as the Bidirectional Long Short-Term Memory Network, to automate the bug triaging process. Bug reports contain textual data related to the bug information. In this research, the pretrained GloVe model is employed for word-to-vector representation of bug reports’ textual information. In this framework, developers’ activities are monitored based on their working history. To test the proposed approach, three large datasets, Net-Beans, Eclipse, and Mozilla, are used. It was observed that the proposed technique produced better results in terms of accuracy, recall, precision and f-measure compared to traditional Machine Learning algorithms for bug report recommendation.",Yes,"본 논문은 딥러닝 알고리즘을 활용하여 버그 리포트 할당을 자동화하는 새로운 방법을 제안하고, 여러 데이터셋을 통해 성능을 평가한 연구 내용을 포함하고 있다. 이는 기존 방법과 비교하여 정확도 및 성능 지표를 개선한 독창적인 연구 기여로 판단된다."
"Review on Scheduling, Clustering, and Forecasting Strategies for Controlling Electric Vehicle Charging: Challenges and Recommendations",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825773,"The usage and adoption of electric vehicles (EVs) have increased rapidly in the 21st century due to the shifting of the global energy demand away from fossil fuels. The market penetration of EVs brings new challenges to the usual operations of the power system. Uncontrolled EV charging impacts the local distribution grid in terms of its voltage profile, power loss, grid unbalance, and reduction of transformer life, as well as harmonic distortion. Multiple research studies have addressed these problems by proposing various EV charging control methods. This manuscript comprehensively reviews EV control charging strategies using real-world data. This review classifies the EV control charging strategies into scheduling, clustering, and forecasting strategies. The models of EV control charging strategies are highlighted to compare and evaluate the techniques used in EV charging, enabling the identification of the advantages and disadvantages of the different methods applied. A summary of the methods and techniques for these EV charging strategies is presented based on machine learning and probabilities approaches. This research paper indicates many factors and challenges in the development of EV charging control in next-generation smart grid applications and provides potential recommendations. A report on the guidelines for future studies on this research topic is provided to enhance the comparability of the various results and findings. Accordingly, all the highlighted insights of this paper serve to further the increasing effort towards the development of advanced EV charging methods and demand-side management (DSM) for future smart grid applications.",No,초록에서 이 논문은 기존 연구들을 종합적으로 검토하고 분류하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하는 연구 논문이 아니라 기존 연구를 정리하고 평가하는 데 중점을 둔 논문입니다.
Towards A Machine Learning Framework for Suicide Ideation Detection in Twitter,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918782,"A worldwide rise of number of individuals had been observed to commit suicide at young age since 2019. With emergence of digital technologies, more suicides attempts were seen to have been pre-communicated through social media posts before the attempts. As such, much initiatives had been conducted to detect communication of distress on social media which imply risks of suicide attempts or any related forms of self-harm. Such initiatives aim to predict the possibility of suicide attempts for prevention measures. This work proposes a framework for machine learning of suicide ideation detection in social media which will contribute to the prediction of possibility of suicide attempt in a social media post. The framework includes data acquisition, data annotation, data pre-processing, feature extraction, classification and performance evaluation of the machine learning model. Acquisition of social media post samples from Twitter is proposed for training and testing datasets through representational state transfer application programming interface. Manual annotation is conducted on the training datasets with different formulated suicide risks. Data pre-processing is done through natural language toolkit and performance evaluation through valence aware dictionary for sentiment reasoning. This work will be further extended with a web-based application for implementation and evaluation of the trained machine learning model.",Yes,"본 논문은 자살 생각 탐지를 위한 기계 학습 프레임워크를 제안하며, 데이터 수집, 주석, 전처리, 특징 추출, 분류 및 성능 평가 등 구체적인 연구 과정을 포함하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문으로 판단된다."
Optimizing User Experience Through Machine Learning: A Review of Key Tools and Technologies,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10851181,"The application of machine learning (ML) to user experience (UX) design is the subject of this study, with a particular focus on privacy concerns, bias mitigation, and the development of best practices for ethical ML implementation. It evaluates several ML tools, including PyTorch, TensorFlow, Azure, BigQuery, alongside design platforms like Figma, can be assessed for their effectiveness in optimizing UX workflows, enhancing user satisfaction, and improving system adaptability. A comprehensive literature review identifies gaps in the current research, such as the need for more robust, user-centered frameworks and a lack of detailed analysis regarding the long-term impacts of ML-driven UX systems. The study emphasizes the critical importance of collaboration between ML experts and UX designers to create more efficient, inclusive, and user-friendly systems while maintaining high ethical standards in data use. Furthermore, it explores emerging technologies, such as augmented and virtual reality, that are expected to influence the future of ML-UX integration. These findings offer valuable guidance for both practitioners and researchers aiming to leverage ML's potential in UX design. Ultimately, this review contributes to the broader discourse by providing recommendations that address innovation, ethical challenges, and responsible implementation across diverse digital design contexts.",No,"초록에서 본 논문은 기존 연구들을 종합하여 평가하고, 현재 연구의 공백을 식별하는 문헌 리뷰(review)임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵습니다."
Extraction-Based Text Summarization and Sentiment Analysis of Online Reviews Using Hybrid Classification Method,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995164,"The field of sentiment mining and text summarization has evoked the interest of many scientists and researchers over the last few years, as the textual data has become useful for many real-world applications and challenges. Sentiment Analysis and Opinion Mining is the most popular field for analyzing and discovering insights from text data from various sources, such as Facebook, Twitter and Amazon, Zomato, etc. It involves a computational study of an individual’s behavior in terms of buying interest and then extracting his opinions on the business entity of the company. This entity can be viewed as an event, individual, blog post or product experience. Scholars in the fields of natural language processing, data mining, machine learning and others have tested a variety of methods for automating sentiment analysis. These reviews are increasing on a daily basis, as a result of which the summarization of the reviews plays a role where the text is summarized as needed, which provides useful information from a large number of reviews. It is very difficult for a human being to extract and interpret useful data from a very large file. In the text analysis, the value of sentences is decided on the basis of the linguistic characteristics of sentences. This paper provides a comprehensive review of current and past work on sentiment analysis and text description. In this research work, a new hybrid classification system is proposed based on coupling classification methods using arcing classifiers and their quality is evaluated within terms of accuracy. The Classifier Collection was constructed using Naïve Bayes (NB), Support Vector Machine (SVM) and Genetic Algorithm (GA). The proposed work consists of a comparative study of the efficacy of the ensemble technique for sentiment classification. The feasibility and benefits of the proposed approaches are demonstrated by a restaurant review that is widely used in the field of sentiment classification. A wide range of comparative studies is performed and, ultimately, some in-depth analysis is addressed and conclusions are drawn on the efficacy of the ensemble technique for sentiment classification.",Yes,"논문 초록에서 새로운 하이브리드 분류 시스템을 제안하고, 이를 Naïve Bayes, SVM, 유전 알고리즘을 결합한 앙상블 기법으로 구현하여 정확도를 평가하는 등 직접적인 연구 기여가 포함되어 있음을 알 수 있습니다. 또한, 제안된 방법의 효용성을 실험적으로 검증하고 비교 분석한 점에서 독창적인 연구 논문으로 판단됩니다."
Handwriting Styles: Benchmarks and Evaluation Metrics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554834,"Extracting styles of handwriting is a challenging problem, since the style themselves are not well defined. It is a key component to develop systems with more personalized experiences for humans. In this paper, we propose baseline benchmarks, in order to set anchors to estimate the relative quality of different handwriting style methods. This will be done using deep learning techniques, which have shown remarkable results in different machine learning tasks, learning classification, regression, and most relevant to our work, generating temporal sequences. We discuss the challenges associated with evaluating our methods, which is related to evaluation of generative models in general. We then propose evaluation metrics, which we find relevant to this problem, and we discuss how we evaluate the performance metrics. In this study, we use IRON-OFF dataset [1]. To the best of our knowledge, no existing benchmarks or evaluation metrics for this task exit yet, and this dataset has not been used before in the context of handwriting synthesis.",Yes,"본 논문은 손글씨 스타일 추출을 위한 새로운 벤치마크와 평가 지표를 제안하며, 딥러닝 기법을 활용한 독창적인 연구 내용을 포함하고 있다. 또한 기존에 사용되지 않았던 IRON-OFF 데이터셋을 손글씨 합성에 적용하는 등 직접적인 연구 기여가 명확하다."
A Learning-Based NLOS Mitigation Method for Single-Anchor SLAM,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9769439,"Location-awareness has playing an increasingly important role in wireless networks. Indoor radio frequency-based simultaneous localization and mapping (SLAM) can be enabled through ultra-wide bandwidth systems due to its ability to provide accurate channel information. In this paper, we provide a deep learning approach to improve the localization accuracy of users and reflection points based on a single-anchor ultra-wide bandwidth system in indoor complex environments. In the proposed approach, we propose a deep generative model to mitigate the distance and the angle-of-arrival estimation errors introduced by non-line-of-sight propagation. Instead of manually extracting features, we make full use of the amplitude and phase information in the channel impulse response to obtain more location-related information. Our work is validated by an indoor sufficient measurement campaign with self-built FCC-compliant ultra-wide bandwidth transceivers, and the results show that our approach outperforms conventional machine learning approaches in practical scenarios.",Yes,"본 논문은 단일 앵커 초광대역 시스템을 이용한 실내 SLAM에서 비가시선(NLOS) 전파로 인한 거리 및 도착각 추정 오차를 완화하기 위한 딥러닝 기반의 새로운 방법을 제안하고 있습니다. 또한, 자체 구축한 측정 장비를 활용한 실험을 통해 제안 기법의 우수성을 검증하여 독창적인 연구 기여를 포함하고 있음을 보여줍니다."
Tracking user trust and mental states during cyber-attacks: A survey of existing methods and future research directions on AI-enabled decision-making for the Royal Canadian Navy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555658,"The increased use of artificial intelligence to detect cyber-attacks against military organizations can put the operator at risk of having miscalibrated trust in the autonomous system. The present paper reviews techniques to capture the human experience of cyber-attacks to support advanced cybersecurity training strategies for the Royal Canadian Navy. Psychophysiological sensors can provide objective measures to help predict the changing mental states of cyber operators (e.g., predict cognitive load, trust, stress) of whether the operator is aware of ongoing human-noticeable aspects of cyber-attacks. Techniques for measuring the experience of cyber-attacks will help advance research on decision-making in automated systems.",No,"본 논문은 기존 기법들을 검토하고 향후 연구 방향을 제시하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 포함하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합 및 분석에 초점을 맞추고 있습니다."
Affective Lexicon for Intelligent Tutoring Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261000,"Affective Tutoring Systems (ATS) is a next-generation Intelligent Tutoring System (ITS) that can detect learners' affective states and provide affective interventions to encourage learners and improve their motivation to learn using artificial intelligence techniques. However, little research has been done on creating an educational affective lexicon in English that those systems can use for providing affective intervention. There needs to be more evidence as to which phrases learners may like to receive from an ATS. This study investigates popular congratulating and encouraging phrases to build a comprehensive affective educational lexicon in English for ATS. Firstly, we examined and collected phrases by reviewing existing learning support systems from the literature that provide affective feedback to learners for constructing an affective lexicon. Secondly, 84 students from various qualifications and backgrounds evaluated the collected phrases by choosing the most popular ones in the congratulating and encouraging dimensions. The lexicon currently consists of 43 encouraging and 32 congratulating phrases categorised by popularity. The findings from this study will augment the ATS to provide better affective tutoring support for students by choosing more constructive and popular affective phrases.",Yes,"본 논문은 기존 문헌 검토와 실험 참여자 평가를 통해 영어 교육용 정서 어휘집을 구축하는 구체적인 연구를 수행하였으며, 이는 ATS에 적용 가능한 독창적인 연구 결과를 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Classifying El Niño-Southern Oscillation Combining Network Science and Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042254,"Machine learning and complex network theory have emerged as crucial tools to extract meaningful information from big data, especially those related to complex systems. In this work, we aim to combine them to analyze El Niño Southern Oscillation (ENSO) phases. This non-linear phenomenon consists of anomalous (de)increase of temperature at the tropical Pacific Ocean, which has irregular occurrence and causes climatic variability worldwide. We construct temporal Climate Networks from the Surface Air Temperature time-series and calculate network metrics to characterize the warm and cold ENSO episodes. The metrics are used as topological features for classification. We employ ten classifiers and achieved 80% AUC ROC when predicting the intensity of Strong/ Weak El Niño and Strong/Weak La Niña for the next season. The complex network represents the relationship among different regions of the planet and machine learning creates models to classify the different classes of ENSO. This work opens new paths of research by integrating network science and machine learning to analyze complex data like global climate systems.",Yes,"본 논문은 ENSO 현상을 분석하기 위해 복잡계 네트워크 이론과 머신러닝을 결합하여 새로운 분류 모델을 개발하고 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 단순히 요약하거나 리뷰하는 것이 아니라, 직접적인 데이터 분석과 모델링을 수행한 연구 논문에 해당한다."
Power Quality Improvement in Grid Connected PV System using Fuzzy Logic Controller based Maximum Power Point Tracing(MPPT),https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837635,"The expansion of the spread of solar photovoltaic power generation systems is fraught with challenges related to the low efficiency and performance of these systems in light of the change in the surrounding weather conditions and the disturbances associated with the loads, which weakens the electrical energy productivity of these photovoltaic systems due to energy loss. To overcome and reduce these challenges as much as possible, it is considered the use of controllers that track the maximum power point of photovoltaic systems is the ideal solution to increase their efficiency and improve the quality of their electrical power, in order to obtain the maximum productive capacity from these systems, and this will be reflected in achieving an optimum benefit from the available solar energy and thus reducing the loss of unused energy in the absence of the control system.This paper presents a study, analysis and simulation of the actual behaviour of the grid-connected solar photovoltaic system model by means of the (MATLAB/Simulink_R2018a) program. The performance of Fuzzy Logic Controller is then evaluated to track the path of the maximum power point, and its work is based on tracking techniques that use the direct control method to control the operating cycle of the DC commutator, which in turn performs by feeding the inverter with the maximum output power from the solar photovoltaic system.A tracking technology was selected that relies on the method of artificial intelligence, which is the technique (fuzzy logic controller), and a fuzzy logic controller of the type (Mamdani) was used. The most suitable for tracking the maximum power point at the steady state of the Grid and under constant and changing weather conditions, as well as choosing the most appropriate controller when transient disturbances occur in the Grid under constant weather conditions only. The results proved the effectiveness of the fuzzy logic controller, and it had good performance in tracking the maximum power point in terms of achieving more stability, reducing error and settling time.",Yes,"본 논문은 태양광 발전 시스템의 최대 전력점 추적을 위한 퍼지 논리 제어기 설계 및 시뮬레이션을 통해 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, MATLAB/Simulink를 이용한 실제 동작 모델 분석과 제어기 성능 검증을 수행하여 직접적인 연구 기여를 하고 있음을 알 수 있다."
Explainable Unsupervised Machine Learning for Cyber-Physical Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536751,"Cyber-Physical Systems (CPSs) play a critical role in our modern infrastructure due to their capability to connect computing resources with physical systems. As such, topics such as reliability, performance, and security of CPSs continue to receive increased attention from the research community. CPSs produce massive amounts of data, creating opportunities to use predictive Machine Learning (ML) models for performance monitoring and optimization, preventive maintenance, and threat detection. However, the ‘‘black-box’’ nature of complex ML models is a drawback when used in safety-critical systems such as CPSs. While explainable ML has been an active research area in recent years, much of the work has been focused on supervised learning. As CPSs rapidly produce massive amounts of unlabeled data, relying on supervised learning alone is not sufficient for data-driven decision making in CPSs. Therefore, if we are to maximize the use of ML in CPSs, it is necessary to have explainable unsupervised ML models. In this paper, we outline how unsupervised explainable ML could be used within CPSs. We review the existing work in unsupervised ML, present initial desiderata of explainable unsupervised ML for CPS, and present a Self-Organizing Maps based explainable clustering methodology which generates global and local explanations. We evaluate the fidelity of the generated explanations using feature perturbation techniques. The results show that the proposed method identifies the most important features responsible for the decision-making process of Self-organizing Maps. Further, we demonstrated that explainable Self-Organizing Maps are a strong candidate for explainable unsupervised machine learning by comparing its model capabilities and limitations with current explainable unsupervised methods.",Yes,"본 논문은 사이버-물리 시스템에서 설명 가능한 비지도 학습 기법을 제안하고, Self-Organizing Maps 기반의 설명 가능한 클러스터링 방법론을 개발 및 평가하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법의 성능을 실험적으로 검증하여 직접적인 연구 기여를 하고 있음을 보여준다."
ECG Based Stress Detection Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807877,"Today, the endeavour of accomplishment and performance has increased the efficiency immensely, yet it comes with its own price. There has been a drastic increase in the diseases related to stress, especially in the past couple of decades. The plethora of diseases and disorders related to long-term effects of stress vary from muscle related disorders to nervous system related diseases. Stress can be defined as unrest in the normal homeostasis. Since this state of unrest is usually triggered by the sympathetic nervous system as a physiological response, stress can be captured by physiological signals. Though a variety of approaches such as the use of questionnaires, biochemical measures and physiological techniques are available to diagnose stress; physiological signals are the most reliable method. Therefore, we have analysed stress using Electrocardiogram which is a physiological signal to increase the accuracy rate by using machine learning algorithms. Here we propose a simple algorithm for the classification of ECG signal as stress or normal by the automatic detection of heart rate variability from R peaks through DWT method. Works includes ECG raw data extraction, wavelet de-noising, R peak detection and classification. Machine learning algorithm uses various parameters obtained from classification for finding the accuracy of the results. Short term ECG is needed for stress detection, which produces a reliable classification with high accuracy.",Yes,"논문 초록에서 ECG 신호를 이용한 스트레스 분류를 위해 새로운 알고리즘을 제안하고, 심박 변이도 자동 검출 및 기계 학습 분류를 수행하는 구체적인 연구 방법과 결과를 기술하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문임을 나타낸다."
Enhancing QoE of WebRTC-based Video Conferencing using Deep Reinforcement Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10258222,"The proportion of video traffic within the total internet traffic is steadily increasing and then video traffic already accounts for over half of the internet traffic. The increase in video traffic is due to the growing number of users for video-related services such as video streaming, live streaming, and video telephony. With the increasing users on video-related services, the importance of the quality of experience (QoE) for these services will become even more crucial in the future. Numerous studies to enhance the experience quality of video streaming have been conducted using adaptive bitrate (ABR) algorithms and artificial intelligence (AI). However, this work focuses on a more complex problem: improving the experience quality in multi-party, bi-directional communication scenarios such as video conferences. We propose a system that applies deep reinforcement learning (DRL) to the media server of a webRTC-based video conferencing system to allocate a bitrate’s video stream that suits the network conditions for users. The proposed method was implemented and evaluated, demonstrating great improvements. When the network conditions changed dynamically, the proposed approach achieved approximately 56.2% higher video bitrate compared to existing methods, resulting in a 24.7% enhancement in user experience quality.",Yes,"본 논문은 WebRTC 기반 화상회의에서 QoE 향상을 위해 딥 강화학습을 적용한 새로운 시스템을 제안하고, 이를 구현 및 평가하여 기존 방법 대비 성능 향상을 입증하였다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문에 해당한다."
Deep Learning and Multivariate Time Series for Cheat Detection in Video Games,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564219,"Online video games drive a multi-billion dollar industry dedicated to maintaining a competitive and enjoyable experience for players. Traditional cheat detection systems struggle when facing new exploits or sophisticated fraudsters. More advanced solutions based on machine learning are more adaptive but rely heavily on in-game data, which means that each game has to develop its own cheat detection system. In this work, we propose a novel approach to cheat detection that doesn't require in-game data. Firstly, we treat the multimodal interactions between the player and the platform as multivariate time series. We then use convolutional neural networks to classify these time series as corresponding to legitimate or fraudulent gameplay. Our models achieve an average accuracy of respectively 99.2% and 98.9% in triggerbot and aimbot (two widespread cheats), in an experiment to validate the system's ability to detect cheating in players never seen before. Because this approach is based solely on player behavior, it can be applied to any game or input method, and even various tasks related to modeling human activity.",Yes,"논문은 기존의 치트 탐지 시스템과 달리 게임 내 데이터 없이 플레이어와 플랫폼 간의 상호작용을 다변량 시계열로 처리하고, 이를 CNN을 통해 분류하는 새로운 방법을 제안하고 있습니다. 이는 독창적인 연구 방법과 실험 결과를 포함한 직접적인 연구 기여로 판단됩니다."
A Survey on Swarm Intelligence Algorithms for Optimizing Path Planning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10933006,"This survey provides an exhaustive review of advances in Swarm Intelligence algorithms applied to path planning from the year 2019 to 2024. Swarm Intelligence is a field of investigation that takes its inspiration from the behavior of social animals and has evolved to now incorporate machine learning and sensor fusion to solve pathfinding problems of growing difficulty. The application of SI in dynamic and uncertain environments is still problematic, since the efficiency and adaptability of algorithms change greatly under different conditions of operation. The paper discusses how research in diverse fields like mobile robots, autonomous vehicles, and logistics synthesizes and then evaluates the efficiency, adaptability, and performance of prominent SI algorithms. It highlights the integration of SI with modern technologies like machine learning and sensor fusion, which has demonstrated enlarged capabilities in the navigation of complex and dynamic environments. This paper identifies the research gaps and proposes certain directions for future research, hence acting as a vital resource in applying SI with a view to optimizing path planning in autonomous systems.",No,"이 논문은 2019년부터 2024년까지의 연구 동향을 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 요약하고 분석하는 데 중점을 두고 있습니다."
Multiple Disease Prediction System using Machine Learning and Streamlit,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10060903,"There are many existing machine learning models related to health care which mainly focuses on detecting only one disease. Therefore, this study has developed a system to forecast several diseases by using a single user interface. The proposed model can predict multiple diseases such as diabetes, heart disease, chronic kidney disease and cancer. If left untreated, these diseases pose a risk to humanity. As a result, many lives can be saved by early detection and diagnosis of these disorders. This research work attempts to implement various classification algorithms (K-Nearest Neighbor, Support Vector Machine, Decision Tree, Random Forest, and Logistic Regression, Gaussian naive bayes.) to perform disease prediction. The accuracy of each algorithm is validated and compared with each other to find the best one for prediction. Furthermore, multiple datasets (for each disease each dataset) are used to achieve utmost accuracy in the predicted results. The main goal is to create a web application capable of forecasting several diseases by using machine learning, including diabetes, heart disease, chronic kidney disease, and cancer.",Yes,"이 논문은 여러 질병을 예측하기 위해 다양한 머신러닝 분류 알고리즘을 적용하고, 각 알고리즘의 정확도를 비교 분석하는 독창적인 연구 내용을 포함하고 있다. 또한, 여러 데이터셋을 활용하여 예측 정확도를 높이는 방법을 제시하며, 이를 바탕으로 웹 애플리케이션을 구현하는 구체적인 연구 기여가 있다."
A Comprehensive Review of the Application of Machine Learning in Fabrication and Implementation of Photovoltaic Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192283,"Solar energy is a promising source of renewable energy, but its low efficiency, instability, and high manufacturing costs remain a big challenge. Recently, machine learning (ML) techniques have gained attention in the photovoltaic (PV) sector because of advances in computer power, tools, and data creation. The existing ML techniques used for fabrication and the different operational procedures of the PV sector have shown very impressive results with a higher degree of accuracy and precision. While previous studies have discussed ML techniques for PV fabrication or operational procedures, there is a lack of end-to-end research that covers the entire process from fabrication to implementation. In this paper, we present a comprehensive review of the application of ML in the field of solar energy, focusing on the development of new materials, enhancement of solar cell efficiency, implementation, and integration with the system, including fault detection, sizing, control, forecasting, management, and site adaptation. We evaluated more than 100 research articles, a significant proportion of which were published in the past three years. In our study investigating ML implementation in solar cell fabrication, we discovered that the Random Forest (RF), Linear Regression (LR), XGBoost, and Artificial Neural Network (ANN) algorithms are the most commonly employed techniques. Our findings demonstrate that XGBoost exhibits superior performance in optoelectronic prediction, while RF, LR, and ANN algorithms are better suited for predicting electrical parameters. Moreover, our analysis indicates recent ML research in this field explicitly emphasizes perovskite solar cells (PSCs). This work also discusses the challenges, directions, insights, and potential applications of ML for future PV system analysis.",No,"본 논문은 머신러닝의 태양광 시스템 제작 및 구현 분야 적용에 대한 종합적인 리뷰를 제공하는 논문으로, 기존 연구들을 평가하고 요약하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적인 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다."
Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707620,"Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and to support the selection of relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine with approximately 74% recall and 15% precision. The use of such algorithms with conservative thresholds to minimize the risk of missing papers can already significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE.",Yes,"본 논문은 자동화 도구 개발과 머신러닝 알고리즘 적용을 통한 체계적 문헌고찰 업데이트 지원 방법을 제안하고, 이를 평가한 결과를 제시하고 있다. 이는 독창적인 연구 방법과 실험 결과를 포함한 직접적인 연구 기여로 판단된다."
Maritime Human Drone Teaming For Search and Rescue Operations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531426,"Drowning detection is a critical safety measure aimed at identifying and preventing drowning incidents in various environments, such as swimming pools, beaches, rivers, and other bodies of water. Drowning is a leading cause of accidental death worldwide, and early detection is essential for saving lives and ensuring water-related activities remain safe. The goal of drowning detection systems is to promptly identify when a person is in distress or submerged in the water, triggering an immediate response to prevent potential drowning incidents. In this proposed work, a monitoring system using UAV (drones) to detect a drowning person is proposed. The system includes an operator, camera gear, Raspberry Pi3, and rescue UAV. The real-time image is assisted by the drowning detection algorithm, and judges whether there are targets to be rescued, so the performance of detection algorithm used in this process is very important. The drowning activity detection is identified using deep learning algorithm. After comparison with other models/methods, the deep learning model chosen for drowning detection is Convolutional Neural Networks (CNN). Conv2dLstm is the convolution layer used for developing the model, since the proposed work deals with 3D input data instead of 1D vector. Deep learning model is successfully built using CNN architecture which has an accuracy of 0.9341. The processing time taken for the model to detect the drowning activity is 5 seconds where the rescue operation can be done immediately to save the person. Real time implementation of the proposed system has been achieved using Raspberry Pi 3. For the rescue operation, human-drone communication is established. The controller is mounted on a carrier device and is tested in real time environment. The drowning detection system has successfully identified a person drowning in a swimming pool.",Yes,"본 논문은 드론과 딥러닝 기반의 익사 감지 시스템을 제안하고, CNN 모델을 개발하여 실시간으로 익사 활동을 탐지하는 독창적인 연구 내용을 포함하고 있다. 또한, Raspberry Pi 3를 이용한 실시간 구현과 인간-드론 통신 테스트 등 실험적 검증도 수행하여 연구 논문으로 판단된다."
Enhancing Autism Spectrum Disorder Recognition in EEG Data through Filtering-Driven CNN Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444503,"Wearable Electroencephalogram (EEG) devices hold immense potential for optimizing health monitoring processes. This paper introduces an innovative approach that seamlessly integrates deep learning techniques into EEG-based recognition, bridging the gap between traditional feature-based methods and Convolutional Neural Networks (CNNs). The methodology revolves around crafting CNNs to emulate the information extraction principles of feature-based techniques, resulting in a streamlined yet robust architecture tailored for EEG-based classification.To validate this approach, two comprehensive experiments were conducted, involving EEG data collection and subsequent evaluation within health monitoring scenarios. Specifically, the focus was on the diagnosis of Autism Spectrum Disorder (ASD), necessitating precise identification and characterization of EEG signals. Deep learning, embedded within a neural network framework, emerges as a powerful tool for the identification and classification of ASD.The proposed deep CNN structure employs separated temporal and spatial filters, selecting raw EEG signals from electrode pairs over the motor cortex region as hybrid samples without preprocessing or artificial feature extraction operations. This 5-layer CNN is augmented by a 4-layer max pooling mechanism to reduce dimensionality, and a fully-connected (FC) layer for classification. Dropout and batch normalization techniques are incorporated to mitigate overfitting risks.In the experimental phase, 20m EEG data from 5, 10, 20, and 50 subjects sourced from the Physionet database, utilizing a 14 Channel Emotiv EPOC EEG headset, were employed. States of focus, unfocused, and drowsed were considered. Results indicate a remarkable global averaged accuracy of 92.31% in group-level classification, an outstanding area under the receiver operating characteristic (ROC) curve at 0.981, and, notably, the electrode pair FC3-FC4 achieved the highest accuracy of 99.61% on a 10-subject dataset.This work contributes to the evolving landscape of EEG-based health monitoring, showcasing the potential of a deep CNN architecture in accurately classifying states related to ASD, and thereby advancing the precision and efficacy of neurodevelopmental disorder diagnoses.",Yes,"본 논문은 EEG 데이터를 활용한 자폐 스펙트럼 장애(ASD) 인식에 있어 새로운 CNN 기반 방법론을 제안하고, 실험을 통해 그 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Exploring Pre-Processing Approaches for Deep Learning-based Fingerprint Spoof Detection Mechanisms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777133,"Fingerprint-based human authentication being most widely deployed systems are also exposed to many security threats. Among all, spoofing attacks is widely attempted that involves circumventing the sensor module of the system by presenting a fake replica of the original trait of a genuine user. A counter measuring mechanism is deployed in these systems that intelligently measure the vitality characteristics of a presented fingerprint image. These sub-modules are integrated besides sensing device of the system and popularly known as anti-spoofing methods or fingerprint spoof detection (FSD) mechanisms. The evaluation of data-driven enabled computer vision paradigm has facilitated researchers to design intelligent FSD. Pre-processing data to train these FSD models is required to improve the overall quality of the input images. This paper presents a focused review of pre-processing approaches used in deep learning-based FSD models. The proposed research study has clearly observed that image pre-processing is a vital step to significantly improve the overall accuracy and efficiency of FSD approaches.",No,"본 논문은 딥러닝 기반 지문 스푸핑 탐지 메커니즘에서 사용되는 전처리 기법들에 대한 리뷰를 제공하는 연구로, 직접적인 실험이나 새로운 방법론 제안보다는 기존 연구들을 정리하고 평가하는 데 중점을 두고 있다. 따라서 독창적인 연구 결과를 포함한 연구 논문으로 보기 어렵다."
6GENABLERS: A Holistic Approach to Establish Pervasive Trust in 6G Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10478383,"The sixth generation of mobile networks endeavors to cater the needs of different verticals by intrinsically supporting the deployment and operation of applications delivered by third-party providers in a multi-domain scenario. However, this raises several management challenges due to the diversity of stakeholders proposing heterogeneous networking technologies, impacting application’s orchestration life-cycle meanwhile increasing the already complicated vulnerable surface area. Hence, these applications require a trusted resource acquisition and execution, transparency of automated management processes and resilience against security threats. In this position paper, we introduce the UNICO 6GENABLERS research project that will investigate (i) explainable AI techniques for the lifecycle management of ML models used for orchestration, (ii) trustworthy resource trading via a DLT-anchored marketplace and (iii) privacy and security preservation of 6G networks through a unified security management plane. This paper presents the foundation of the 6GENABLERS project, justifies its contribution by a review of similar efforts, and exposes its validation settings and methodology. Specifically, the outcomes of this project will be validated through a holographic communication use case to demonstrate its advancements in the efficiency, trustworthiness and security of a typical 6G platform.",No,"본 논문은 6GENABLERS 프로젝트의 개요와 연구 방향, 그리고 검증 방법론을 소개하는 포지션 페이퍼(position paper)로, 직접적인 실험 결과나 독창적인 연구 내용을 포함하기보다는 연구 계획과 기대 효과를 설명하는 데 중점을 두고 있습니다. 따라서 독창적인 연구 결과를 제시하는 연구 논문으로 보기 어렵습니다."
Language Identification Models for Short Medical Texts,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991579,"In the context of SARS-CoV-2 transmission prevention, the short texts on the social networks are full of abbreviations, technical medical terms, slang words that are outside the native languages or pejorative jargon. In other words, the language identification issues are not trivial. In fact, always this task remained a challenge for short texts originating from social media, which abounds in hard-to-understand sequences of letters. Moreover, when the whole world is facing a “medical crisis”, the situation must somehow be controlled in order for the panic to not degenerate even more. As a result, we need enhanced tools that can help us speed up the process of detection of linguistic aspects of the online content. This paper presents a new method intended to identify the language of a Twitter collection related to COVID-19 subject, based on AI algorithms. The aim of this work is to optimally determine the main language in which a text is written, with the constrains given by the diversity of the short text language found in tweets. Therefore, to find an optimal solution for language detection, we evaluated the impact of several language detection algorithms for short texts. Moreover, the results of this analysis were used for the implementation of a new language detection model, focused primarily on the detection of the Romanian language of the gathered tweets. The results suggest that a hybrid approach which combines classic techniques looks like a realistic direction of research.",Yes,"본 논문은 COVID-19 관련 트위터 데이터의 언어 식별을 위해 여러 알고리즘을 평가하고, 이를 바탕으로 루마니아어 탐지에 특화된 새로운 언어 식별 모델을 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
"Analysis and simulation of social behavior during the COVID-19 pandemic in Argentina, using intelligent agents",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782344,"The present work is an innovative educational strategy that uses a Final Integrative Work (FIW) as a method of evaluation of subjects of the Computer Engineering degree where students learn different subjects such as Artificial Intelligence and Databases, through real world problems related to COVID-19. The evaluation process through the FIW is based on several skills acquisition and by measuring the way in which students apply concepts of Databases and intelligent agents by means of numerical simulations that involves social behavior in times of the COVID-19 pandemic in the province of Tucumán, in the northwest of Argentina. The students carried out simulations of a multiagent system through the tool Netlogo, applying rules with a high impact factor for tackling a decision making problem. The results observed suggest that a paradigm shift in the degree evaluation processes is possible and necessary.",No,"본 논문은 COVID-19 관련 사회적 행동을 다룬 시뮬레이션 교육 전략과 학생 평가 방법에 관한 내용으로, 직접적인 독창적 연구 결과나 새로운 지식 창출보다는 교육적 접근과 평가 방법에 초점을 맞추고 있습니다. 따라서 연구 논문보다는 교육 방법론에 가까운 성격을 띕니다."
Multi-task Learning Based Ocular Disease Discrimination and FAZ Segmentation Utilizing OCTA Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631043,"In this paper, we proposed and validated a multi-task based deep learning method for simultaneously segmenting the foveal avascular zone (FAZ) and classifying three ocular disease related states (normal, diabetic, and myopia) utilizing optical coherence tomography angiography (OCTA) images. The essential motivation of this work is that reliable predictions on disease states may be made based on features extracted from a segmentation network, by sharing a same encoder between the classification network and the segmentation network. In this study, a cotraining network structure was designed for simultaneous ocular disease discrimination and FAZ segmentation. Specifically, we made use of a classification head following a segmentation network’s encoder, so that the classification branch used the feature information extracted in the segmentation branch to improve the classification results. The performance of our proposed network structure has been tested and validated on the FAZID dataset, with the best Dice and Jaccard being 0.9031±0.0772 and 0.8302 ±0.0990 for FAZ segmentation, and the best Accuracy and Kappa being 0.7533 and 0.6282 for classifying three ocular disease related states.Clinical Relevance— This work provides a useful tool for segmenting FAZ and discriminating three ocular disease related states utilizing OCTA images, which has a great clinical potential in ocular disease screening and biomarker delivering.",Yes,"본 논문은 OCTA 이미지를 활용하여 다중 작업 기반 딥러닝 모델을 설계하고, 이를 통해 FAZ 분할과 안구 질환 분류를 동시에 수행하는 독창적인 연구 방법을 제안하고 검증하였다. 또한, 제안된 네트워크 구조의 성능을 실험적으로 평가하여 직접적인 연구 기여를 포함하고 있음을 보여준다."
A Review of Moving Object Detection Techniques for Night Time,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541663,"Moving Object Detection in low light conditions is a challenging task due to limited visibility and the presence of noise and objects in the captured images, low contrast, low brightness, low appearance information, and low Signal to Noise Ratio. The majority of night surveillance models suffer from lack of adaptability for different scenes and separation between detection and tracking. This review paper provides a comprehensive survey of recent advancements in Moving Object Detection Techniques for night-time situations. It presents an overview of various approaches proposed in the literature, including traditional and deep learning-based methods, and discusses their strengths and limitations. Moreover, the paper highlights evaluation metrics commonly used in the literature, the datasets they have been evaluated upon, along with a comparative analysis of the performance metrics & various limitations of different methods. Finally, the paper identifies the key research challenges and future directions for Moving Object Detection in Night Time viz. dim-lit environments, such as the need for more robust and efficient algorithms that can handle diverse environmental conditions and real-time applications.",No,"본 논문은 기존 연구들을 종합적으로 정리하고 비교 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 문헌 조사 및 현황 분석에 초점을 맞춘 리뷰 논문에 해당합니다."
Uncertainty Quantification for Safe and Reliable Autonomous Vehicles: A Review of Methods and Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10879299,"In the past decade, deep learning has been widely applied across various fields. However, its applicability in open-world scenarios is often limited due to the lack of quantifying uncertainty in both data and models. In recent years, a multitude of uncertainty quantification (UQ) approaches for neural networks have emerged and found applications in safety-critical domains such as autonomous vehicles and medical analysis. This paper aims to review the latest advancements in UQ methods and investigate their application specifically in the field of computer vision and autonomous vehicles. Initially, we identify several key qualifications, namely practicability, robustness, accuracy, scalability, and efficiency (referred to as PRASE), and employ them as evaluation criteria throughout this study. By considering these criteria as uniform measurements, we meticulously evaluate and compare the performance of different types of UQ methods, including Bayesian methods, ensemble methods, and single deterministic methods. Furthermore, we delve into the discussion of their application in diverse tasks within the autonomous vehicle domain, such as semantic segmentation, object detection, depth estimation, and end-to-end control. Through comprehensive analysis and comparison, we identify a range of challenges and propose future research directions in this field. Our findings shed light on the importance of addressing uncertainty quantification in deep learning models and provide insights into enhancing the reliability and performance of autonomous vehicles in real-world scenarios.",No,"본 논문은 최신 불확실성 정량화 방법들을 종합적으로 검토하고 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들을 정리하고 분석하는 데 중점을 두고 있습니다."
A Review of Unsupervised Anomaly Detection Techniques for Health Insurance Fraud,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10729915,"This paper surveys the latest unsupervised anomaly detection methodologies applied to health insurance fraud, covering studies from 2017 to 2024. Our review includes a variety of machine-learning approaches, evaluating their effectiveness in handling complex, high-dimensional, and imbalanced healthcare datasets. Techniques such as Isolation Forest, Bayesian hierarchical models, and deep autoencoders demonstrate superior performance compared to traditional methods. Despite significant advancements, gaps remain with regard to transfer learning, interpretability and explainability of models, and the development of real-time, incremental learning algorithms. Future research should focus on these areas to enhance fraud detection accuracy and trust. Our work aims to provide a valuable resource for researchers and practitioners, supporting the development of more robust and adaptive fraud detection systems to protect healthcare integrity and reduce financial losses.",No,"본 논문은 기존 연구들을 종합하여 최신 기법들을 리뷰하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 방법론 제시는 포함되어 있지 않습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Recent Advances of Generative Adversarial Networks in Computer Vision,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576508,"The appearance of generative adversarial networks (GAN) provides a new approach and framework for computer vision. Compared with traditional machine learning algorithms, GAN works via adversarial training concept and is more powerful in both feature learning and representation. GAN also exhibits some problems, such as non-convergence, model collapse, and uncontrollability due to high degree of freedom. How to improve the theory of GAN and apply it to computer-vision-related tasks have now attracted much research efforts. In this paper, recently proposed GAN models and their applications in computer vision are systematically reviewed. In particular, we firstly survey the history and development of generative algorithms, the mechanism of GAN, its fundamental network structures, and theoretical analysis of the original GAN. Classical GAN algorithms are then compared comprehensively in terms of the mechanism, visual results of generated samples, and Frechet Inception Distance. These networks are further evaluated from network construction, performance, and applicability aspects by extensive experiments conducted over public datasets. After that, several typical applications of GAN in computer vision, including high-quality samples generation, style transfer, and image translation, are examined. Finally, some existing problems of GAN are summarized and discussed and potential future research topics are forecasted.",No,"본 논문은 최근 GAN 모델과 그 응용에 대한 체계적인 리뷰를 제공하는 논문으로, 기존 연구들을 종합하고 비교 분석하는 내용에 초점이 맞춰져 있습니다. 따라서 직접적인 독창적인 연구 결과나 새로운 기여를 포함한 연구 논문으로 보기 어렵습니다."
PSARE: A RL-Based Online Participant Selection Scheme Incorporating Area Coverage Ratio and Degree in Mobile Crowdsensing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797859,"Mobile crowdsensing (MCS) is a cost-effective paradigm for gathering real-time and location-related urban sensing data. To complete MCS tasks, MCS platform needs to exploit the trajectory of participants (vehicles or individuals, etc.) for effectively choosing participants. On one hand, the existing works usually assume that platform has possessed the abundant historical movement trajectory for participant selection, or can accurately predict the movement of participant before selection, but this assumption is impractical for many MCS applications, for some candidates have just arrived without sufficient mobility profiles, so-called trajectory from-scratch, or cold-trajectory issue. On the other hand, most of works only considers the coverage ratio of the sensing area, while some hotspots should be sensed frequently, so-called coverage degree of hotspots. To solve the issue, this paper proposes a reinforcement learning (RL) based, i.e., an improved Q-learning based online participant selection scheme to incorporate both coverage ratio and degree, PSARE. First, to solve the explosion of state-value table in traditional tabular Q-learning, an improved two-level Q-learning method is proposed to select participants in online way so as to achieve high long-term return. Specifically, in each selection round, PSARE dynamically compresses all the real participants (RPs) into several virtual participants (VPs) using the available historical trajectories of RPs, and the VP-based state-value table is constructed and constantly updated (i.e., the first level). Then, after selecting the VP through looking up the table, PARSE chooses the RP with the largest expected reward in this VP using epsilon-greedy way to balance the effect of exploration and exploitation (i.e., the second level). Moreover, the reward function is designed to measure the MCS coverage quality, including both coverage degree of hotspots and coverage ratio of target area. Thorough experiments on real-world mobility data set demonstrate that PSARE outperforms than other RL based online participant selection schemes (including deep Q-learning network) and traditional offline selection methods.",Yes,"본 논문은 모바일 크라우드센싱에서 참가자 선택 문제를 해결하기 위해 강화학습 기반의 새로운 온라인 참가자 선택 기법(PSARE)을 제안하고, 이를 위해 개선된 Q-러닝 알고리즘과 보상 함수 설계 등 독창적인 연구 내용을 포함하고 있다. 또한 실제 데이터셋을 활용한 실험을 통해 제안 기법의 우수성을 입증하고 있어 직접 기여하는 연구 논문에 해당한다."
Child Safety and Protection in the Online Gaming Ecosystem,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933399,"Online gaming no longer has limited access, as it has become available to a high percentage of children in recent years. Consequently, children are exposed to multifaceted threats, such as cyberbullying, grooming, and sexting. Although the online gaming industry is taking concerted measures to create a safe environment for children to play and interact with, such efforts remain inadequate and fragmented. There is a vital need to develop laws and policies to regulate and build minimum standards for the industry to safeguard and protect children online on the one hand, while promoting innovations in the gaming industry to preempt such threats. Many tools have been adapted to control threats against children in the form of content filtering and parental controls, thereby restricting contact with children to protect them from child predators. Different approaches utilizing machine learning (ML) techniques to detect child predatory behavior have been designed to provide potential detection and protection in this context. In this paper, we survey online threats to children in the gaming environment and present the limitations of existing solutions that address these threats. We also aimed to present the challenges that ML techniques face in protecting children against predatory behavior by presenting a systematic review of the available techniques in the literature. Therefore, this analysis provides not only recommendations to stakeholders to develop policies and practices that safeguard children when gaming, but also to the gaming industry to continue providing appropriate measures for a safe and entertaining gaming environment.",No,"본 논문은 온라인 게임 환경에서 아동 보호와 관련된 기존 위협과 해결책을 조사하고, 머신러닝 기법의 한계와 도전과제를 체계적으로 검토하는 서베이 논문으로 보입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 연구의 종합적 분석과 정책 제안에 중점을 두고 있습니다."
"Approaches, Challenges, and Applications for Deep Visual Odometry: Toward Complicated and Emerging Areas",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262957,"Visual odometry (VO) is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep-learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep-learning-based VO (Deep VO). Therefore, this article aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications, including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, and pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.",No,"본 논문은 딥러닝 기반 비주얼 오도메트리(Deep VO)의 최근 연구 동향과 응용 분야를 종합적으로 리뷰하는 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 문헌을 분석하고 평가하는 데 중점을 두고 있다. 따라서 새로운 연구 기여보다는 현황 정리 및 향후 연구 방향 제시에 초점이 맞춰져 있다."
Machine Learning-Based Joint Vital Signs and Occupancy Detection With IR-UWB Sensor,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054563,"This article proposes the machine learning (ML)-based joint vital signs (VSs) and occupancy detection (OD) with an impulse radio ultra-wideband (IR-UWB) sensor. Works that have been done on VS or OD development using an IR-UWB are related to how VS works. In the related experiments performed, the OD and state of individuals were not sufficiently verified, and the methods were computationally complex. Issues related to the use of ML for joint VS and OD (VSOD) have also not been studied in the literature. Extensive experimental scenarios involving the application of an ML-based classifier for human OD and VS classification, which we extended toward three sub-scenarios, were evaluated. We formulated a solution for VS estimation, which was aligned, so that each network input sequence received signal corresponding to respective VS over different scenarios. The performance of the proposal was evaluated with other competing ML-based classification algorithms. Compared with other techniques, our proposed deep neural network (DNN)-based classifier achieved the best results, and it also offers benefits over other algorithms, such as not needing to extract features from the data.",Yes,"본 논문은 IR-UWB 센서를 활용한 생체 신호 및 점유 감지에 대해 머신러닝 기반의 새로운 분류기 설계와 성능 평가를 포함한 독창적인 연구 내용을 다루고 있습니다. 또한, 기존 연구의 한계점을 보완하고 다양한 실험 시나리오를 통해 제안 기법을 검증한 점에서 연구 논문으로 판단됩니다."
Performance Evaluation of Transmission Mode Selection in D2D communication,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432648,"Device to Device (D2D) Communication is expected to be a one of the major contributing factors of the realisation of 5G and Beyond Mobile communication networks as it promises improvements in energy efficiency, spectral efficiency, overall system capacity, and higher data rates with the use of the same frequencies for different D2D transmissions in short communication distances within the Cell. However, in order to achieve optimum results, it is important, among others, to select wisely the Transmission Mode of the D2D Device. Towards this end, our previous work proposed an intelligent Transmission mode selection approach in a framework that is utilizing Artificial Intelligence (AI) BDIx agents to collectively satisfy the D2D challenges in a Distributed Artificial Intelligent (DAI) manner and act autonomously and independently. In this paper, as a first step, a literature review focused on related Transmission mode approaches is performed. Then, our investigated Transmission mode selection approach is further explained with formulas, evaluated based on different threshold values and investigated how these can affect the overall spectral efficiency and power usage of the network in order to achieve the maximum performance. The investigated thresholds on utilized values (i.e., D2D Device Weighted Data Rate (WDR), D2D Device Battery Power Level) and metrics (i.e., WDR) are also further analyzed and formulated. In addition, the effect the transmission power of the D2D links has on the total spectral efficiency and total power consumption of the network, is also examined. The evaluation results revealed some interesting findings that can contribute in other approaches that utilized similar or same thresholds. Also, the results obtained demonstrate that with the right tuning of the thresholds and transmission power, one can achieve a significant improvement in the network power usage and total spectral efficiency.",Yes,"논문 초록에서 제안된 전송 모드 선택 접근법을 수식으로 설명하고, 다양한 임계값에 따른 성능 평가를 수행하여 스펙트럼 효율과 전력 사용량에 미치는 영향을 분석한 점으로 보아, 독창적인 연구 내용과 실험적 평가가 포함된 연구 논문임을 알 수 있습니다."
Semantic Segmentation of Histopathological Images with Fully and Dilated Convolutional Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632856,"Nowadays, the segmentation of different components in medical images is a major subject of study, and parallel to this, numerous image segmentation methods are still being developed. This study aimed to assess image segmentation methodologies utilizing deep learning models, due to the success of deep learning models in image processing applications. Firstly, starting from the introduction, a literature review on semantic segmentation and medical image segmentation is introduced in this study. In addition, pre-processing steps and techniques, models used, evaluation criteria, and the reasons for their preference are also explained. In the methods section, SegNet, U-Net, and DeepLabV3+ model architectures are introduced, and the architectures of these models are visualized at a basic level. The application results section includes all evaluation results with the metrics used in measuring accuracy. The comparison of the evaluation results and the evaluations on these results are included in the results and discussion section. In addition to these, visualized prediction results are also presented under the application results section.",No,"본 논문 초록은 기존의 딥러닝 모델(SegNet, U-Net, DeepLabV3+)을 활용한 의료 영상 분할 방법을 소개하고 평가하는 내용에 집중되어 있으며, 독창적인 연구 방법이나 새로운 모델 제안에 대한 직접적인 언급이 없습니다. 따라서 기존 연구의 리뷰 및 성능 비교에 가까운 논문으로 판단됩니다."
ResCovNet: A Deep Learning-Based Architecture For COVID-19 Detection From Chest CT Scan Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293887,"Automatic disease detection using machine learning-based techniques from X-ray and computed tomography (CT) can play a major role in the frontline to assist medical professionals during the current outbreak of COVID-19. Fast diagnosis of the disease is the key to reduce the uncontrollable spread of this life-threatening disease, where machine learning-based applications can contribute greatly by predicting the situation of patients so that professionals can decide accordingly. The major drawbacks of detecting COVID-19 are its similarities with different types of pneumonia, and the absence of properly labeled data. Considering the ResNet152V2 as a backbone network, an efficient architecture, namely ResCovNet is proposed to detect COVID-19 accurately from chest CT scan images by separating it from three types of pneumonia and normal cases. Otsu's thresholding is applied in the pre-processing step to strengthen the features for the classification network. With the use of proposed architecture, a very satisfactory classification accuracy of 88.1% is achieved to separate COVID-19 from all other four classes. Evaluating the performance of this study by 3-fold cross-validation, and comparison with related works prove that this adroit algorithm provides an effective way to be implemented as a diagnostic tool in the COVID-19 screening.",Yes,"본 논문은 ResNet152V2를 기반으로 한 새로운 딥러닝 아키텍처 ResCovNet을 제안하여 COVID-19를 CT 영상에서 정확히 분류하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 전처리 기법과 3-폴드 교차 검증을 통해 성능을 평가한 점에서 직접적인 연구 기여가 명확합니다."
Evaluation of Multi-layer Perceptron Neural Networks in Predicting Ankle Dorsiflexion in Healthy Adults using Movement-related Cortical Potentials for BCI-Neurofeedback Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896584,"Brain computer interface (BCI) systems were initially developed to replace lost function; however, they are being increasingly utilized in rehabilitation to restore motor functioning after brain injury. In such BCI-mediated neurofeedback training (BCI-NFT), the brain-state associated with movement attempt or intention is used to activate an external device which assists the movement while providing sensory feedback to enhance neuroplasticity. A critical element in the success of BCI-NFT is accurate timing of the feedback within the active period of the brain state. The overarching goal of this work was to develop a reliable deep learning model that can predict motion before its onset, and thereby deliver the sensory stimuli in a timely manner for BCI-NFT applications. To this end, the main objective of the current study was to design and evaluate a Multi-layer Perceptron Neural Network (MLP-NN). Movement-related cortical potentials (MRCP) during planning and execution of ankle dorsiflexion was used to train the model to classify dorsiflexion planning vs. rest. The accuracy and reliability of the model was evaluated offline using data from eight healthy individuals (age: 26.3 ± 7.6 years). First, we evaluated three different epoching strategies for defining our 2 classes, to identify the one which best discriminated rest from dorsiflexion. The best model accuracy for predicting ankle dorsiflexion from EEG before movement execution was 84.7%. Second, the effect of various spatial filters on the model accuracy was evaluated, demonstrating that the spatial filtering had minimal effect on model accuracy and reliability.",Yes,본 논문은 Multi-layer Perceptron Neural Network를 설계하고 평가하여 뇌 신호를 기반으로 발목 배굴 운동을 예측하는 독창적인 딥러닝 모델을 개발하는 연구를 수행하였다. 이는 직접적인 실험과 데이터 분석을 통해 새로운 연구 결과를 제시하는 연구 논문에 해당한다.
Artificial Dataset Generation for Modeling and Simulation of Shared Electric Automated and Connected Mobility Systems with Autonomous Repositioning: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10361760,"The introduction of Shared, Electric, Automated, and Connected Mobility (SEACM) systems will completely transform the transportation sector in the near future. However, these systems present additional challenges that require comprehensive performance analysis and effective deployment techniques. This paper will investigate the importance of creating artificial datasets for training Machine Learning models (ML), with a focus on generating datasets specific to SEACM systems. In contrast, to generate artificial datasets for the SEACM system, many characteristics should be considered such as user behavior, vehicle dynamics, charging infrastructure, and environmental conditions. As a result, using these synthetic datasets, decision-makers can correctly simulate several scenarios and evaluate the performance of these complex systems. This review paper also shows how artificial datasets may be used to train ML models to simulate SEACM with autonomous repositioning. This allows researchers to assess how well ML-based algorithms can be used to optimize vehicle routing, charging infrastructure usage, demand forecasting, and other important operational elements. In addition, we highlight the significant impact that the generation of artificial datasets has on performance evaluation and deployment approaches. Academics can develop efficient deployment strategies that maximize the effectiveness and sustainability of shared automated and connected electric mobility systems by training ML models on these datasets to better understand the strengths and limitations of various algorithms, identify potential areas for improvement, and more. Taken as a whole, this research highlights the importance of generating artificial datasets in the context of shared automated and connected electric mobility systems. It illustrates the methods used to generate these datasets, their importance for modeling and simulation, and their implications for performance evaluations and deployment strategies. To more rapidly build and optimize machine learning (ML) models for shared mobility systems, researchers and practitioners can exploit the potential of artificial datasets. This will ultimately lead to better transportation experiences and sustainable urban mobility.",No,"본 논문은 SEACM 시스템을 위한 인공 데이터셋 생성 방법과 그 활용에 대해 종합적으로 검토하는 서베이 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 기존 연구들을 정리하고 분석한 리뷰 논문에 해당합니다."
Comparing the Design Quality and Efficiency between Design Intelligence and Intermediate Designers,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903240,"Design intelligence is to adapt and introduce artificial intelligence methods to design tasks. SmartPaint is a typical design intelligence system to generate paintings, and its improved version further considers the causal relation between objects in a scene and is trained with a larger dataset with over one million artwork samples. In this paper, we compare the improved SmartPaint with 12 intermediate designers on a controlled painting task. The intermediate designers have been working in the design and related industries for over two years. The comparison is about design quality and efficiency, evaluated by 20 volunteers. Empirically we find over 50% of the paintings generated by SmartPaint are more preferred than those drawn by human designers along creativity, artistic quality and color scheme. Besides, SmartPaint takes only less than 1% of the time spended by human designers.",Yes,"본 논문은 SmartPaint라는 인공지능 기반 디자인 시스템과 인간 디자이너 간의 디자인 품질과 효율성을 비교하는 실험 연구를 수행하고 있으며, 이를 통해 새로운 지식과 실증적 결과를 제시하고 있다. 따라서 독창적인 연구 내용이 포함된 연구 논문으로 판단된다."
DES-KNORA Model: Developing an Intelligent System for Automatic Power Mode Adjustment in Electric Bicycles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10725483,"Electric bicycles (e-bikes) have gained considerable popularity due to their environmentally friendly nature and suitability as a mode of transportation. However, they face challenges related to manual switches for power mode changes. A potential solution to this issue could be the application of machine learning to improve their performance and safety. Previous research has explored the use of machine learning for gear shifting in bicycles, but there remains a gap in the specific application of ML for automating power mode changes in electric bikes. In order to address this gap, the study proposes the design and evaluation of an intelligent system based on machine learning that can automatically adjust electric bike power modes for users. The research team utilized a Hero Lectro C7+ electric bike and designed and tested a novel ensemble model, the dynamic ensemble selection approach model (DES-KNORA), against several machine learning techniques to predict the need for power mode changes. The DES-KNORA ensemble model fit the measured data the best, with an F1-score of 0.9995 and an accuracy of 99.95%. The study’s results show that the suggested method works in real life, making the ride smoother and less annoying for riders.",Yes,"본 논문은 전기자전거의 자동 파워 모드 조절을 위한 기계학습 기반의 새로운 앙상블 모델(DES-KNORA)을 설계하고 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 실험을 통해 모델의 성능을 검증하여 실제 적용 가능성을 입증하였으므로 연구 논문에 해당한다."
SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9842816,"Mobile devices such as drones and autonomous vehicles increasingly rely on object detection (OD) through deep neural networks (DNNs) to perform critical tasks such as navigation, target-tracking and surveillance, just to name a few. Due to their high complexity, the execution of these DNNs requires excessive time and energy. Low-complexity object tracking (OT) is thus used along with OD, where the latter is periodically applied to generate ""fresh"" references for tracking. However, the frames processed with OD incur large delays, which does not comply with real-time applications requirements. Offloading OD to edge servers can mitigate this issue, but existing work focuses on the optimization of the offloading process in systems where the wireless channel has a very large capacity. Herein, we consider systems with constrained and erratic channel capacity, and establish parallel OT (at the mobile device) and OD (at the edge server) processes that are resilient to large OD latency. We propose Katch-Up, a novel tracking mechanism that improves the system resilience to excessive OD delay. We show that this technique greatly improves the quality of the reference available to tracking, and boosts performance up to 33%. However, while Katch-Up significantly improves performance, it also increases the computing load of the mobile device. Hence, we design SmartDet, a low-complexity controller based on deep reinforcement learning (DRL) that learns to achieve the right trade-off between resource utilization and OD performance. SmartDet takes as input highly-heterogeneous context-related information related to the current video content and the current network conditions to optimize frequency and type of OD offloading, as well as Katch-Up utilization. We extensively evaluate SmartDet on a real-world testbed composed by a JetSon Nano as mobile device and a GTX 980 Ti as edge server, connected through a Wi-Fi link, to collect several network-related traces, as well as energy measurements. We consider a state-of-the-art video dataset (ILSVRC 2015 - VID) and state-of-the-art OD models (EfficientDet 0, 2 and 4). Experimental results show that SmartDet achieves an optimal balance between tracking performance – mean Average Recall (mAR) and resource usage. With respect to a baseline with full Katch-Up usage and maximum channel usage, we still increase mAR by 4% while using 50% less of the channel and 30% power resources associated with Katch-Up. With respect to a fixed strategy using minimal resources, we increase mAR by 20% while using Katch-Up on 1/3 of the frames.",Yes,"논문은 모바일 객체 탐지를 위한 엣지 태스크 오프로딩의 동적 제어를 위한 새로운 추적 메커니즘(Katch-Up)과 이를 최적화하는 딥 강화학습 기반 컨트롤러(SmartDet)를 제안하며, 실제 실험과 평가를 통해 성능 향상을 입증하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함하는 연구 논문에 해당한다."
Machine Learning Enabled FBG Optical Sensor Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10868970,"This review paper develops the current understanding of machine learning applications in the Structural health monitoring industry using Fiber Bragg Grating (FBG) sensors. We describe how this synergy transforms strain measuring, temperature sensing, and structural health monitoring based on analysis of papers and case studies. In addition to aspects involving these sensors, the discussion of how integration can achieve accuracy, real-time capabilities, and predictive analytics also raises issues with data management complexity, the efficiency of the algorithms to be used, and the efficiency of integrating this into current systems. This research paper also presents a comprehensive investigation into using FBG optical sensor technology and machine learning models for the early detection and management of diabetes mellitus. Thus, as we suggest possible solutions and directions of further research in the form of conclusions, it seems to us that a growing trend will make in aviation operations safer than ever and more efficient.",No,초록에서 해당 논문은 기존 연구와 사례들을 종합하여 머신러닝과 FBG 센서의 응용을 리뷰하는 논문임을 명확히 밝히고 있습니다. 독창적인 실험 결과나 새로운 연구 방법론에 대한 직접적인 기여가 언급되어 있지 않아 연구 논문으로 보기 어렵습니다.
Mental Task Classification using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150533,"Brain-computer interface (BCI) applications are an exciting area of research that has the potential to drastically alter the way we engage with technology. By translating brainwaves into an operational signal, it offers its users an additional means of controlling their environment. There are various techniques available to record brain activity including EEG (Electroencephalogram), fNIRS (functional near-infrared spectroscopy), and DTI (diffusion tensor imaging). Because of the low cost of acquisition devices and the lack of any known clinical risks associated with EEG, it has been widely used in the construction of brain-computer interface systems. In this work, an EEG data is recorded from 5 subjects, in three different sessions following motor imagery, motor movement, and resting state protocols. Using this dataset, we have trained different machine learning models to perform binary class classification of mental tasks into task and non-task related ones. For taskrelated data, we have used Motor Imagery (MI) data while the non-task related involves the Resting state (eyes open) recording of data. We have evaluated the performances of four machine learning algorithms: Decision Tree, KNN, Ensembles, and SVM, out of which SVM performed the best with the training and test accuracy of 87.9 and 72.7% respectively.",Yes,"본 논문은 EEG 데이터를 수집하고, 이를 바탕으로 여러 머신러닝 모델을 훈련시켜 정신 과제 분류를 수행하는 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 알고리즘의 성능을 평가한 실험 결과를 제시하고 있어 연구 논문에 해당한다."
Academic Excellence in E Learning Based Virtual Assistance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9788100,"In today's fast-growing virtual world the use of web systems and e-learning sites are providing a future workforce and becoming the common successful learning method in the wider academic contexts especially after the COVID19 pandemic situation. The usage of web systems for educational needs in academic institutes is essential and provides new opportunities for education and development. It facilitates innovation, collaboration, teaching, and learning to improve learning support. In this present demand seeking valid sources and user feedback for information sharing with an academic perspective is challenging. For this purpose, there is a need for a strategy to be built for the effective sharing of knowledge under quality check parameters. This paper discusses and evaluates the quality of e-learning sites and ranks the most common quality characteristics under ISO/IEC-9126 extended quality modeling approach. In the literature review, modern web technology features for e-learning sites are presented. In this present demand 3.0 and above generation web technology where mind control interfaces by machines are implemented, ubiquitous in nature. In ‘academic success’ we have given several recommendations for technological use in an e-learning environment with online platforms. It is checked that quality characteristics are extended to check the sustainability of e-learning sites.",No,"본 논문은 e-러닝 사이트의 품질 특성을 평가하고 기존 ISO/IEC-9126 모델을 확장하여 품질을 분석하는 문헌 리뷰 및 평가 연구에 초점을 맞추고 있습니다. 직접적인 실험, 데이터 수집, 또는 독창적인 연구 결과를 제시하기보다는 기존 기술과 문헌을 종합하여 논의하는 내용으로 보입니다."
Effect of robot's title in human-robot interaction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057515,"The collaboration between humans and intelligent agents keeps increasing as the technology advances. In this sense, each can rely on the other's strengths to achieve the best overall performance. But as intelligent agents play an increasingly escalating role as a partner rather than mere tools, it is imperative that we understand how humans are influenced by such agents. This is so as to achieve a certain level of persuasiveness to ensure human compliance with intelligent agents in times of need, and also prevent over-reliance on such agents, which may make humans susceptible to manipulation. Reviewing the literature from human-computer interaction, persuasion, and decision-making processes, this study examined the effect of labelling source expertise (i.e., title) on the human decision-making process, when there is not an agreement between human and robot partners. Specifically, we conducted an experiment (n = 88) to investigate how perceived expertise of a robot resulted in the attractiveness, intelligence and credibility of the robot, and how these perceptions influence the level of cooperation and compliance in a desert survival game. The results showed that participants evaluated the robot as more credible when labelled as expert, compared to when labelled as novice. The results of a mediation analysis also showed that perceived credibility successfully mediated the effect of title on cooperation and compliance when there was a conflict in the ranking order of items between each participant and the robot. Implications for future development of artificial intelligence and future research directions are discussed.",Yes,본 논문은 로봇의 직함(전문가 vs 초보자)이 인간-로봇 상호작용에서 인간의 의사결정과 협력에 미치는 영향을 실험을 통해 직접 조사한 연구이다. 이는 기존 문헌 검토뿐만 아니라 실험적 데이터를 바탕으로 한 독창적인 연구 내용이 포함되어 있어 연구 논문에 해당한다.
Machine Learning Driven Analysis of Mental Health Indicators in Social Media Posts,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10689856,"Mental stress has been a major issue among all age groups today. This is more like a stigma in the society where people don't even want to talk about it openly leading to severe health issues. Nowadays, people are deeply engaged with social media platforms for connection and self-expression of their emotions. They express all these through posts, photos, and comments. Thus, social media platforms can help in predicting the mental state and well-being of an individual. Prediction of a person's mental health can help to undertake some preventive measures. Various researchers have worked around this topic but there is no evidence of utilizing the most active web of people for obtaining the most accurate results. Machine Learning (ML) algorithms are used in this research study to identify whether a person is mentally stressed or not based on the social media data obtained from the user's post or by using a text input of around hundred words from users related to their day-to-day activities. This research study has used a decision tree, random forest, and Bidirectional Encoder Representations from Transformers (referred as BERT onwards) model classifier to train models. This study has used a dataset of Reddit (a social media platform) posts text data for training the models. The primary objective is to compare the efficacy of various machine learning models in predicting mental health based on social media data.",Yes,"이 논문은 소셜 미디어 데이터를 활용하여 머신러닝 모델을 훈련시키고, 정신 건강 상태를 예측하는 독창적인 연구를 수행하고 있다. 다양한 알고리즘을 비교 분석하는 실험적 연구 내용이 포함되어 있어 연구 논문에 해당한다."
Systematic Analysis of Effective Segmentation and Classification for Land Use Land Cover in Hyperspectral Image using Deep Learning Methods: A Review of the State of the Art : Reviewing Deep Learning Techniques for Land Use and Cover in Hyperspectral Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475229,"This review paper delves into the intricate realm of segmenting and classifying Hyperspectral Images (HSI), complex visuals spanning numerous electromagnetic spectrum bands. HSI, garnering significant research attention, holds pivotal roles in geospatial applications, notably Land Use/Land Cover (LULC) mapping, demanding precise object identification. Addressing challenges posed by imbalanced data and limited labelled examples, the survey scrutinizes how researchers navigate HSI segmentation and classification. Briefly touching upon the foundational background of these techniques, the study navigates through diverse processing methods—thresholding, clustering, watershed, Deep Learning (DL) and others. Systematically exploring literature trends, DL advancements, attention mechanisms, data types, achieved accuracies and existing weaknesses, it strives to guide future research. Critically evaluating current knowledge, the paper illuminates’ gaps in HSI segmentation and classification, culminating in discussions on pertinent issues and prospective projects in this domain. Ultimately, this work aims to propel advancements by addressing knowledge lacunae and charting potential pathways for upcoming research initiatives.",No,"논문 초록에서 본 연구는 기존 문헌과 기술들을 체계적으로 검토하고 분석하는 리뷰 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구 동향과 한계점, 향후 연구 방향을 제시하는 데 중점을 두고 있습니다."
Early Prediction of Diabetes Mellitus Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197832,"Diabetes mellitus is one of the noxious disease which causes abnormalities of blood glucose due to the resistance of producing insulin hormone in the body. It affects various organs in the body such as the kidney, nerves, and eyes if it is not an early diagnosis. With the advancement in technological growth, people attract to personalized healthcare. Machine learning is a very growing field in the predictive analysis and often used in healthcare applications where the prediction of diseases and their symptoms is identified in an early stage. The main objective of this work is to build a model for early prediction of diabetes by using machine learning classification algorithms under consideration of significant features related to diabetes. The proposed model gives the closest results comparing to clinical outcomes and also helps in the personalized diagnosis of patients. There are four machine learning algorithms these are Linear Discriminant Analysis (LDA), K-nearest neighbor (KNN), Support Vector Machine (SVM), and Random Forest (RF) are used in the predictive analysis of early-stage diabetes. Pima Indian Diabetes Database (PIDD) is used for experimental analysis which is taken from the UCI machine learning repository from the University of California, Irvine. The performance measures of these classification algorithms are done on various statistical measures such as sensitivity (recall), precision, specificity, F-score, and accuracy. Accuracy is the measurements of classifying correctly and incorrectly instances. The experimental results show that Random Forest (RF) gives the maximum accuracy of 87.66 % and outperformed in other classification algorithms.",Yes,"본 논문은 당뇨병의 조기 예측을 위해 여러 머신러닝 알고리즘을 적용하고, Pima Indian Diabetes Database를 사용하여 실험적 분석을 수행한 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 성능 지표를 통해 알고리즘의 예측 성능을 평가한 점에서 연구 논문으로 판단된다."
Is the Future Cold or Tall? Design Space Exploration of Cryogenic and 3D Embedded Cache Memory,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158184,"Memory latency, density, and power efficiency are key bottlenecks in a variety of computing systems, and the need for efficient and dense memory solutions is exacerbated by the continued importance of data-intensive applications such as machine learning, graph processing, and scientific computing. A myriad of emerging technologies and approaches aim to address the limitations of current systems. For example, 3D integration can enable highly dense memory structures, and multiple alternative device technologies such as STT and PCM have emerged as compelling solutions to improve memory system density and efficiency. Additionally, cryogenic operation of computing systems (i.e., ultra-low temperature cooling) is becoming a compelling solution as thermal hotspots have become a primary roadblock to conventional transistor scaling. This work probes, evaluates, and compares the potential capabilities of 3D integration, embedded non-volatile memories (eNVMs), and cryogenic operation towards improving future memory systems by presenting the first design space exploration of cryogenic operation and 3D integration applied towards the largest on-chip memory structure, the last level cache, as well as presenting and providing open-source tools for future, related design studies. This work specffically evaluates the applicationlevel benefits or limitations of such proposals by leveraging a cross-computing-stack simulation approach. Our studies reveal that the most compelling solution varies depending on the expected memory traffic patterns and workloads of interest, which in turn exposes several opportunities for future optimization and customization. For example, due to potentially high costs of cooling to cryogenic operation, we find that SRAM or 3T-eDRAM operating at 77K is sub-optimal compared to room-temperature SRAM and eNVM solutions, but exhibits advantages for relatively low-traffic workloads.",Yes,"논문은 3D 통합, 임베디드 비휘발성 메모리, 극저온 동작 등 다양한 기술을 적용하여 차세대 캐시 메모리 설계 공간을 탐색하고, 시뮬레이션을 통해 성능과 효율성을 평가하는 독창적인 연구를 수행하고 있다. 또한, 새로운 설계 도구를 제공하며 실제 워크로드에 대한 적용 가능성을 분석하는 등 직접적인 연구 기여가 포함되어 있다."
Deep Learning Models for Time Series Forecasting: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10583885,"Time series forecasting involves justifying assertions scientifically regarding potential states or predicting future trends of an event based on historical data recorded at various time intervals. The field of time series forecasting, supported by diverse deep learning models, has made significant advancements, rendering it a prominent research area. The broad spectra of available time series datasets serve as valuable resources for conducting extensive studies in time series analysis with varied objectives. However, the complexity and scale of time series data present challenges in constructing reliable prediction models. In this paper, our objectives are to introduce and review methodologies for modeling time series data, outline the commonly used time series forecasting datasets and different evaluation metrics. We delve into the essential architectures for trending an input dataset and offer a comprehensive assessment of the recently developed deep learning prediction models. In general, different models likely serve different design goals. We boldly examine the performance of these models under the same time series input dataset with an identical hardware computing system. The measured performance may reflect the design flexibility among all the ranked models. And through our experiments, the SCINet model performs the best in accuracy with the ETT energy input dataset. The results we obtain could give a glimpse in understanding the model design and performance relationship. Upon concluding the paper, we shall provide further discussion on future deep learning research directions in the realm of time series forecasting.",No,"본 논문은 다양한 딥러닝 모델을 소개하고 비교 평가하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 모델 제안보다는 기존 연구들을 종합하고 분석하는 데 중점을 두고 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
"Objectives, Methods, and Challenges of Applying Intelligent Assessment in Education: A systematic review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685783,"With the development of artificial intelligence technology, traditional assessment has been gradually replaced by intelligent assessment. This study systematically reviews research on the use of intelligent assessment in education between 2015 and 2024. 51 papers were finally included and analyzed for the objectives, methods, and challenges of adopting intelligent assessment in educational activities. The results show that the goals of intelligent assessment in education can be grouped into three categories: 1) assessment of students’ competence and knowledge levels; 2) assessment of students’ personality and mental health; and 3) assessment of teaching and learning processes. The studies reviewed used a range of assessment methods to achieve their objectives. These encompassed process-based, student competency-based, and dynamism-based approaches, as well as social media text-based, audio, and video-based assessment methods. Additionally, they employed student assessment-based and teacher performance assessment-based methods. These methods involved designing algorithms, application systems, or scale assessment tools to effectively measure and evaluate various aspects of learning and teaching. The challenges were present in three areas: data collection and processing, adaptation of assessment models, and accuracy and interpretability of assessment results.",No,"본 논문은 2015년부터 2024년까지 발표된 51편의 연구를 체계적으로 검토한 리뷰 논문으로, 직접적인 독창적 연구 결과보다는 기존 연구들의 목표, 방법, 도전 과제를 종합하여 분석하는 데 중점을 두고 있다. 따라서 새로운 실험이나 연구 결과를 제시하는 연구 논문으로 보기 어렵다."
Review and Analysis of Patients’ Body Language From an Artificial Intelligence Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155429,"Body language is a nonverbal communication process consisting of movements, postures, gestures, and expressions of the body or body parts. Body language expresses human feelings, thoughts, and intentions. It also reveals physical and psychological health conditions: abnormal activities inform peoples’ health conditions, facial expressions indicate their emotional states and abnormal body actions convey specific diseases’ external signs and symptoms. We can observe the importance of studying the body language of people with health conditions through many reports in literature written by healthcare (medical) and artificial intelligence researchers. This paper comprehensively reviews artificial intelligence-based articles that have studied patients’ body language. We also conduct different descriptive and exploratory examinations of the findings using data analysis techniques, which provide more authentic domain knowledge of abnormal activities, abnormal body actions, and more precise analysis of methodologies used in machine learning tasks for studying these abnormalities. The paper’s results are essential for developing intelligent automated systems that accurately evaluate patients’ physical and psychological conditions, precisely identify external signs and symptoms of diseases, and adequately monitor patients’ health conditions.",No,"본 논문은 인공지능 관점에서 환자의 몸짓 언어를 연구한 기존 문헌들을 종합적으로 검토하고 분석하는 리뷰 논문입니다. 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구들을 요약하고 평가하는 데 중점을 두고 있어, 독창적인 연구 내용이 포함된 연구 논문으로 보기 어렵습니다."
Systematic Exploration of GAN-Generated Image Recognition Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10828629,"Nowadays, GANs are becoming more and more skilled at producing realistic imagery. Furthermore, as GAN technologies continue to progress, reliable and transparent detection techniques must be created in order to protect digital content integrity and uphold public confidence. Hence, it is of utmost importance to enhance the recognition ability of deep learning models to discriminate between actual and GAN-generated images. The widespread use of GAN images raises ethical concerns regarding deep fake’s proliferation and cyber security risks. The methods for identifying images produced by Generative Adversarial Networks (GANs) are thoroughly examined in this research accompanied by their practical applications. Using GAN-generated images as a backdrop, the paper reviews current image recognition methods. Key factors for successful GAN image recognition are highlighted in this review by addressing the significance and challenges of identifying GAN-generated images, which also evaluates the advantages and disadvantages of various techniques. The article also delves into the societal and ethical implications by outlining potential future directions and areas for investigation in the rapidly developing fields of image recognition and GAN technology.",No,"초록에서 본 논문은 GAN 이미지 인식 기술에 대한 체계적인 검토 및 기존 연구들의 요약과 평가를 주로 다루고 있으며, 직접적인 독창적 연구 결과나 새로운 기법 제안이 포함되어 있지 않은 리뷰 논문으로 보입니다. 따라서 연구 논문으로 보기 어렵습니다."
Empowering Robo-Advisors: Data-Driven Mutual Fund and Stock Market Price Prediction with Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10718557,"In the contemporary financial landscape, the integration of deep learning techniques has revolutionized the capabilities of robo-advisors in providing data-driven insights for mutual fund and stock market price prediction. This study explores the application of deep learning methods, specifically Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, in empowering robo-advisors to offer personalized investment recommendations and enhance decision-making processes for investors. Leveraging vast amounts of historical financial data, including market trends, asset prices, and economic indicators, LSTM and GRU networks are adept at capturing complex temporal patterns and dependencies within sequential data, enabling more accurate predictions of future stock prices and market trends. By employing these deep learning techniques, robo-advisors can analyze market dynamics in real-time, adapt investment strategies to changing conditions, and provide tailored recommendations aligned with individual investor preferences and risk profiles. Through a comprehensive review of existing literature and empirical studies, this paper evaluates the performance and effectiveness of LSTM and GRU networks in mutual fund and stock market price prediction tasks. The findings suggest that LSTM and GRU networks offer significant advantages over traditional forecasting methods, such as autoregressive models and technical analysis, by effectively capturing long-term dependencies and nonlinear relationships within financial time series data. Moreover, the integration of additional data sources, such as news sentiment analysis and social media trends, further enhances the predictive accuracy and robustness of the models. Overall, the application of deep learning techniques in empowering robo-advisors holds immense potential for revolutionizing investment management practices, democratizing access to financial markets, and empowering investors with actionable insights for informed decision-making. The proposed method is implemented in Python and has an accuracy of about 99.12%.",Yes,"논문은 LSTM과 GRU 네트워크를 활용한 주가 및 펀드 가격 예측에 대한 구체적인 딥러닝 기법 적용과 성능 평가를 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한, 제안된 방법을 구현하고 정확도 수치를 제시하는 등 직접적인 연구 기여가 명확하다."
Improvement of Min-Entropy Evaluation Based on Pruning and Quantized Deep Neural Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032186,"In the field of information security, the unpredictability of random numbers plays determinant role according to the security of cryptographic systems. However, limited by the capability of pattern recognition and data mining, statistical-based methods for random number security assessment can only detect whether there are obvious statistical flaws in random sequences. In recent years, some machine learning-based techniques such as deep neural networks and prediction-based methods applied to random number security have exhibited superior performance. Concurrently, the proposed deep learning models bring out issues of large number of parameters, high storage space occupation and complex computation. In this paper, for the challenge of random number security analysis: building high-performance predictive models, we propose an effective analysis method based on pruning and quantized deep neural network. Firstly, we train a temporal pattern attention-based long short-term memory (TPA-LSTM) model with complex structure and good prediction performance. Secondly, through pruning and quantization operations, the complexity and storage space occupation of the TPA-LSTM model were reduced. Finally, we retrain the network to find the best model and evaluate the effectiveness of this method using various simulated data sets with known min-entropy values. By comparing with related work, the TPA-LSTM model provides more accurate estimates: the relative error is less than 0.43%. In addition, the model weight parameters are reduced by more than 98% and quantized to 2 bits (compression over 175x) without accuracy loss.",Yes,"본 논문은 기존의 딥러닝 모델을 기반으로 프루닝과 양자화 기법을 적용하여 모델의 효율성을 개선하고, 이를 통해 미니엔트로피 평가 정확도를 높이는 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 데이터셋을 이용한 실험과 성능 비교를 통해 새로운 방법론의 유효성을 입증하고 있어 연구 논문에 해당한다."
Key Challenges and Limitations of the OSINT Framework in the Context of Cybersecurity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10212168,"In today's world, Open-Source Intelligence (OSINT) has gained popularity as a method of gathering data, both for cyber-attacks and detecting cyber threats. It involves using publicly available information from the internet to identify and evaluate potential cybersecurity risks. Despite its usefulness, the OSINT framework is not without its challenges and limitations. The objective of this research study is to examine the key challenges and limitations of the OSINT framework specifically in the context of data gathering for cybersecurity purposes. This study relies on a comprehensive review of relevant literature on OSINT and its applications in cybersecurity, as well as case studies. Some of the primary challenges and limitations of the OSINT framework that have been identified include issues with data quality, data quantity, data integration, analysis and interpretation, privacy, and ethical considerations. To tackle these challenges, this research study proposes a range of potential solutions, such as the development of more advanced analytical tools and techniques, the integration of machine learning and artificial intelligence algorithms, and the implementation of responsible and ethical data collection and analysis practices. Overall, this research provides valuable insights into the challenges and limitations that need to be addressed when using OSINT as a method of data gathering for cybersecurity purposes.",No,"본 논문은 OSINT 프레임워크의 한계와 문제점을 문헌 검토와 사례 연구를 통해 분석하고 있으며, 직접적인 실험이나 새로운 방법론 개발과 같은 독창적인 연구 기여보다는 기존 연구의 종합적 고찰에 초점을 맞추고 있습니다. 따라서 연구 논문보다는 리뷰 또는 개념적 분석에 가까운 성격을 띕니다."
A study of the Dream Net model robustness across continual learning scenarios,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10031093,"Continual learning is one of the major challenges of deep learning. For decades, many studies have proposed efficient models overcoming catastrophic forgetting when learning new data. However, as they were focused on providing the best reduce-forgetting performance, studies have moved away from real-life applications where algorithms need to adapt to changing environments and perform, no matter the type of data arrival. Therefore, there is a growing need to define new scenarios to assess the robustness of existing methods with those challenges in mind. The issue of data availability during training is another essential point in the development of solid continual learning algorithms. Depending on the streaming formulation, the model needs in the more extreme scenarios to be able to adapt to new data as soon as it arrives and without the possibility to review it afterwards. In this study, we propose a review of existing continual learning scenarios and their associated terms. Those existing terms and definitions are synthesized in an atlas in order to provide a better overview. Based on two of the main categories defined in the atlas, “Class-IL.” and “Domain-IL”, we define eight different scenarios with data streams of varying complexity that allow to test the models robustness in changing data arrival scenarios. We choose to evaluate Dream Net - Data Free, a privacy-preserving continual learning algorithm, in each proposed scenario and demonstrate that this model is robust enough to succeed in every proposed scenario, regardless of how the data is presented. We also show that it is competitive with other continual learning literature algorithms that are not privacy preserving which is a clear advantage for real-life human-centered applications.",Yes,"본 논문은 기존의 연속 학습 시나리오를 검토하고 새로운 평가 시나리오를 정의하며, Dream Net 모델을 다양한 상황에서 평가하는 실험적 연구를 포함하고 있다. 이는 독창적인 연구 내용과 실험 결과를 제시하는 연구 논문에 해당한다."
Adaptive control measures and thermal comfort : Input parameters for IoT devices,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211786,"Thermal comfort can be defined as the ‘condition of mind that expresses satisfaction with the thermal environment.’, and adaptive control measures comprise an adjustment to indoor environmental conditions based on occupant preferences, behavior, and feedback. The use of Artificial Intelligence (AI) and Internet of Things (IoT) in thermal comfort and adaptive control measures has the potential to significantly improve energy efficiency and occupant comfort in buildings. The adoption of the Working-from-home modality by several institutions and companies during and after the 2020 pandemic, accelerated the development of even more advanced and effective solutions in this area. This study, in its first phase, seeks to identify input parameters related to adaptive control measures adopted by the same occupants in two work environments (office building and residences) to promote personal thermal comfort contributing to the development of individual devices and IoT applications related to thermal comfort monitoring.",Yes,"본 논문은 적응 제어 조치와 개인 열쾌적성 향상을 위한 입력 변수들을 식별하는 연구를 수행하고 있으며, 이는 IoT 장치 개발에 기여하는 독창적인 연구 내용으로 보인다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단된다."
Detection And Tracking of Multiple Pedestrians Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10307543,"Recent advancements in deep learning and computer vision have benefitted security systems. Recent object tracking algorithms, in example, have incorporated deep learning in a variety of methods to enhance tracking performance. Object tracking and security systems continue to provide a number of difficulties. This work will first identify the several challenges related to the object tracking for an autonomous system and further the object tracker system will be evaluated in a simulated environment for multiple objects like pedestrian, moving or stationary object. This research paper is all about designing the tracking algorithm for multiple pedestrians. This research employed YOLO V3 with a Deep Sort architecture. In order to get heightened accuracy different kind of scenario is used for preparing the test data. As a result, precision is 93 %, Recall is 98% and accuracy is 96%.",Yes,"논문 초록에서 YOLO V3와 Deep Sort 아키텍처를 활용하여 다중 보행자 추적 알고리즘을 설계하고, 다양한 시나리오를 통해 성능을 평가한 연구임을 명확히 밝히고 있습니다. 이는 기존 방법을 적용하고 실험을 통해 성능을 검증한 독창적인 연구 내용으로 판단됩니다."
Augmenting Dementia Cognitive Assessment With Instruction-Less Eye-Tracking Tests,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124654,"Eye-tracking technology is an innovative tool that holds promise for enhancing dementia screening. In this work, we introduce a novel way of extracting salient features directly from the raw eye-tracking data of a mixed sample of dementia patients during a novel instruction-less cognitive test. Our approach is based on self-supervised representation learning where, by training initially a deep neural network to solve a pretext task using well-defined available labels (e.g. recognising distinct cognitive activities in healthy individuals), the network encodes high-level semantic information which is useful for solving other problems of interest (e.g. dementia classification). Inspired by previous work in explainable AI, we use the Layer-wise Relevance Propagation (LRP) technique to describe our network's decisions in differentiating between the distinct cognitive activities. The extent to which eye-tracking features of dementia patients deviate from healthy behaviour is then explored, followed by a comparison between self-supervised and handcrafted representations on discriminating between participants with and without dementia. Our findings not only reveal novel self-supervised learning features that are more sensitive than handcrafted features in detecting performance differences between participants with and without dementia across a variety of tasks, but also validate that instruction-less eye-tracking tests can detect oculomotor biomarkers of dementia-related cognitive dysfunction. This work highlights the contribution of self-supervised representation learning techniques in biomedical applications where the small number of patients, the non-homogenous presentations of the disease and the complexity of the setting can be a challenge using state-of-the-art feature extraction methods.",Yes,"논문은 치매 환자의 눈 추적 데이터를 활용한 새로운 인지 평가 방법과 자기지도 학습 기반의 특징 추출 기법을 제안하며, 이를 통해 치매 분류 성능을 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한, 실험 결과와 해석 기법을 통해 연구의 유효성을 검증하고 있어 연구 논문에 해당한다."
Dynamic Graph Representation Learning With Neural Networks: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473053,"In recent years, Dynamic Graph (DG) representations have been increasingly used for modeling dynamic systems due to their ability to integrate both topological and temporal information in a compact representation. Dynamic graphs efficiently handle applications such as social network prediction, recommender systems, traffic forecasting, or electroencephalography analysis, which cannot be addressed using standard numerical representations. As a direct consequence, dynamic graph learning has emerged as a new machine learning problem, combining challenges from both sequential/temporal data processing and static graph learning. In this research area, the Dynamic Graph Neural Network (DGNN) has become the state-of-the-art approach and a plethora of models have been proposed in the very recent years. This paper aims to provide a review of the problems and models related to dynamic graph learning. The various dynamic graph supervised learning settings are analyzed and discussed. We identify the similarities and differences between existing models concerning the way time information is modeled. Finally, we provide guidelines for DGNN design and optimization, and review public datasets for evaluating model performance on various tasks, along with the corresponding publications.",No,"이 논문은 동적 그래프 표현 학습 분야의 기존 연구들을 종합적으로 리뷰하고 분석하는 서베이 논문입니다. 따라서 직접적인 독창적 연구 결과나 새로운 모델 제안이 포함되어 있지 않고, 기존 연구들을 정리하는 데 중점을 두고 있습니다."
Early Prediction of Neonatal Sepsis From Synthetic Clinical Data Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341082,"Neonatal sepsis is one of the most serious complications in neonatal intensive care units. Due to the often immature immune system, sepsis-related comorbidities are the major contributors to increased neonatal mortality. The rapid progression of the disease makes early treatment critical for patient survival. However, early diagnosis of sepsis remains difficult due to its non-specific symptoms. In recent years, Machine Learning-based techniques have been used in various medical applications to predict diseases using clinical data. In this work, we optimized and evaluated four prediction models with different architectural concepts. Two public datasets containing clinical data from adults and neonates were used for training. The adult data were collected to pre-train the models. Since neonatal data with sepsis diagnosis are very limited, we propose an augmentation method to generate synthetic clinical data. For the final evaluation, the real data of neonatal patients were defined as a test set. An AUROC of 0.91 and an AUPRC of 0.38 were obtained. These results are promising for early prediction of neonatal sepsis using artificial data for augmentation.Clinical relevance— This work demonstrates the potential of Machine Learning-based prediction models for the detection of sepsis to improve the early diagnosis of life-threatening conditions in neonatal intensive care units.",Yes,"본 논문은 신생아 패혈증 조기 예측을 위해 기계학습 모델을 최적화하고 평가하는 독창적인 연구를 수행하였으며, 제한된 신생아 데이터를 보완하기 위한 합성 데이터 생성 방법도 제안하였다. 이는 기존 연구를 바탕으로 새로운 방법론과 실험 결과를 제시하는 연구 논문에 해당한다."
Sentiment Analysis for Informal Malay Text in Social Commerce,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574436,"Sentiment Analysis (SA) is opinion mining which often defines as the study of emotions, opinions, or feedback that relates to the usage of computational linguistics, text analytics, and natural language processing. With the rise of social media posts, it is becoming more challenging to evaluate brief, casual, and non-structured texts to optimize consumer feedback and spot patterns. Meanwhile, social commerce involves social media for social interaction in assisting customers and merchants to do business transactions. From a social media perspective, the informal Malay Text is less explored by the researchers. Thus, it will directly yield difficulties in conducting and preparing the SA processes. Cross-Industry Standard Process for Data Mining (CRISP-DM) was adapted as a reference model for the methodology of this work with machine learning approaches in classifying the informal Malay textual data based on sentiment. The dataset was extracted from the Facebook platform of Pos Laju Malaysia pages. The comparison of the classification technique performances was analyzed in identifying the most accurate classifier for SA, within three different machine learning classifiers was experimented by using 1200 instances from an informal Malay textual dataset. The results of Decision Tree (J48), Support Vector Machine (SVM), and Naïve Bayes (NB) were analyzed and discussed. The result of the highest accuracy of Ten-Fold Cross-Validation is 69.7% and meanwhile, for the Percentage Split method, the highest accuracy result is 70.9%. It shows that Support Vector Machine (SVM) is the best classifier compared to other classifiers of text classification based on sentiment.",Yes,"이 논문은 비공식 말레이어 텍스트에 대한 감성 분석을 위해 기계 학습 기법을 적용하고, 여러 분류기 성능을 비교하는 독창적인 연구를 수행하였다. 데이터셋 구축, 방법론 적용, 실험 및 결과 분석이 포함되어 있어 직접적인 연구 기여가 있다고 판단된다."
Modeling Flow Boiling Utilizing Machine Learning Vision Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10709607,"Flow boiling is a very efficient configuration for meeting the high heat dissipation demands of thermal management systems. However, the lack of a clear understanding of physics affecting two-phase flow has limited its wide implementation across thermal systems. Recently, novel machine learning vision tools have been developed to capture physical feature information during subcooled flow boiling in a rectangular channel with single-sided heating. These features include local and averaged, as well as transient and steady-state statistical data on void fraction, vapor-liquid voids, interfacial behaviors, and liquidsolid wall wetting front areas. In this study, this data is used to model performance parameters in flow boiling. The statistical information relating to void fraction, bubble behaviors, interfacial waviness, and wetting fronts is analyzed and correlated with heat transfer coefficients, and critical heat flux. The data is used in combination with traditional control-volume-based theoretical modeling techniques to capture the relationship between the parameter of interest and the input parameters. The data on channel void fraction and wetting front areas are used to improve theoretical models predicting heat transfer coefficient. The data on interfacial behaviors and wetting front areas is to improve mechanistic model predicting critical heat flux. With this work, a new approach to utilizing machine vision data is proposed and validated.",Yes,"본 논문은 머신러닝 비전 데이터를 활용하여 유동 끓음 현상의 성능 파라미터를 모델링하고, 이를 기존 이론 모델과 결합하여 예측 정확도를 향상시키는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Differentially Private Multi-Site Treatment Effect Estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516620,"Patient privacy is a major barrier to healthcare AI. For confidentiality reasons, most patient data remains in silo in separate hospitals, preventing the design of data-driven healthcare AI systems that need large volumes of patient data to make effective decisions. A solution to this is collective learning across multiple sites through federated learning with differential privacy. However, literature in this space typically focuses on differentially private statistical estimation and machine learning, which is different from the causal inference-related problems that arise in healthcare. In this work, we take a fresh look at federated learning with a focus on causal inference; specifically, we look at estimating the average treatment effect (ATE), an important task in causal inference for healthcare applications, and provide a federated analytics approach to enable ATE estimation across multiple sites along with differential privacy (DP) guarantees at each site. The main challenge comes from site heterogeneity—different sites have different sample sizes and privacy budgets. We address this through a class of per-site estimation algorithms that reports the ATE estimate and its variance as a quality measure, and an aggregation algorithm on the server side that minimizes the overall variance of the final ATE estimate. Our experiments on real and synthetic data show that our method reliably aggregates private statistics across sites and provides better privacy-utility tradeoff under site heterogeneity than baselines.",Yes,"본 논문은 다중 사이트에서 차등 개인정보 보호를 적용한 평균 치료 효과(ATE) 추정 방법을 제안하며, 사이트 이질성 문제를 해결하는 새로운 알고리즘을 개발하고 실험을 통해 성능을 검증하고 있다. 이는 기존 연구와 차별화된 독창적인 연구 내용을 포함한 연구 논문으로 판단된다."
A Study on Machine Learning Techniques based Software Reliability Assessment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985530,"Software Reliability (SR) is a substantial aspect of software quality and one of the most important variables to consider when evaluating the quality of a software product. Software managers have a difficult task in selecting the optimum SR model for a real software development project. Throughout the software life cycle, it is concerned with the creation and maintenance of dependable software systems. Software measurement, software modeling, and software enhancement are 3 steps of a three-step method for boosting software dependability. Each of these stages is critical to establishing a solid software system. To control software quality, it is critical to improving accuracy in predicting dependability. A variety of measures for assessing software dependability have been suggested in the literature. Machine learning (ML) methods have been demonstrated to be effective in assessing many aspects of software dependability. To capture the many aspects of a software system, numerous machine learning approaches have been developed in this work. In this review paper the results of the various algorithms are compared. ML classifiers are compared with each other based on accuracy, precision, and recall parameters to determine the best classifier. Machine learning methods have been used in comparative analysis to estimate the defect level of the software instances. Comparison was placed among SVM, NB, DT, RF, and ANN classifiers. Where ANN shows the best results among other classifiers it has highest accuracy of 65.5%.",No,"본 논문은 다양한 머신러닝 기법을 비교 분석하는 리뷰 논문으로 보이며, 직접적인 실험이나 새로운 알고리즘 개발 등 독창적인 연구 결과를 제시하지 않고 기존 연구들을 종합하여 평가한 내용에 집중하고 있습니다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 보기 어렵습니다."
Comparative Analysis of Deep Convolution Neural Networks on Medical Image-Based COVID-19 Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670901,"The COVID-19 pandemic has spurred a dire need for efficient and accurate diagnostic tools to combat the spread of the virus. Medical imaging, particularly chest X-rays and computed tomography (CT) scans, has emerged as a vital component in the diagnosis and monitoring of COVID-19-related lung abnormalities. An effective method is presented in this paper for identifying COVID-19 and distinguishing it from normal X-rays and viral pneumonia cases using Deep Convolutional Neural Networks (CNN). An assessment of three CNN models (EfcientNetB0, VGG16, and InceptionV3) is performed using transfer learning methods. The most common deep learning algorithms used in this study are convolutional neural networks (CNNs). Performance measures (accuracy, recall, specificity, precision, and F1 scores) and deep learning approaches are used in this work. With an overall accuracy of 96.27% and a sensitivity of 95.63% for COVID-19, the findings demonstrate that the suggested technique generated a high-quality model. Effective detection and screening procedures may be facilitated by implementing computer vision design, as demonstrated by the study.",Yes,"본 논문은 COVID-19 진단을 위해 세 가지 딥러닝 CNN 모델을 비교 분석하고, 전이 학습을 활용하여 성능 평가를 수행하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
A Comprehensive Experiment to Enhance Multidisciplinary Engineering Ability via UAVs Visual Navigation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273977,"This Research to Practice WIP presents a UAVs visual navigation based comprehensive experiment to enhance multidisciplinary engineering ability in Aerospace engineering education. In traditional courses, aerospace-related disciplines are independently distributed in different courses, and there is rarely a hands-on platform which includes signal processing, control theory, and artificial intelligence into Aerospace engineering. Facing this problem, this paper designs a multidisciplinary comprehensive experiment, aiming to provide a hand-on platform and flexible project-based program to students of aerospace engineering professions. First of all, in order to let the students understand actual aerospace problems, a multidisciplinary simulation platform containing UAVs and remote objects scenarios is constructed for them to explore in the experiments. Second, the content of the experiment is designed into three stages including data acquisition and processing, conceptual design and simulation, in-flight validation, during which the multidisciplinary engineering ability runs through the whole process of the activities. Finally, Project Oriented Design Based Learning is also introduced here to combine engineering design education with innovation and creativity. Through the project demonstration and presentation at the end of the experiment, the multidisciplinary engineering ability of each student can be effectively evaluated. The UVN comprehensive experiment enables students to work on real-world aerospace engineering problems through a hardware-software integration framework, which may greatly stimulate their curiosity and interest in autonomously learning. It also provides students unprecedented opportunities to immerse themselves in projects that cross disciplinary boundaries, improve their professional ability and enhance their exploration competence in aerospace areas.",No,"본 논문은 UAV 시각 내비게이션을 활용한 교육용 종합 실험 설계와 교육 방법론에 관한 내용으로, 직접적인 독창적 연구 결과나 새로운 기술 개발보다는 교육 프로그램 구성과 학생 능력 향상에 초점을 맞추고 있습니다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 보기 어렵습니다."
Solar Radiation Prediction Using Machine Learning Techniques: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8891934,"Solar radiation estimation determines how much energy the sun provides to a particular region. This radiation is the primary energy source of conversion in photovoltaic plants and solar thermal power plants. The incident radiation is not constant and depends on climatic data, which results in an intermittency in its behavior and changes in the production of electrical energy are observed. This justifies the development of a tool for predicting and estimating incident radiation in order to foresee changes in the performance of photovoltaic generation systems. This paper presents an analysis and review of the literature published in the Science Direct and IEEE databases since 1990, from the point of view of techniques application for the estimation of the primary solar resource. These techniques are classified according to the nature of the model from numerical and analytical approaches to Machine Learning. These approaches use different databases, inputs and mathematical relationships to establish dependencies among solar radiation, longitude, latitude and climatic parameters. In this paper, the selection criteria and behavior of the models are identified from the linearity treatment of the database to be used, the number of entries, the deviation between the value that is predicted and the test portion of the set of data that evaluate their behavior and provide decision tools for their use. Many authors apply Machine Learning to estimation both with unitary predictors and with hybrid models that profit from their potential.",No,"본 논문은 1990년 이후 발표된 문헌들을 분석하고 분류하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 기존 연구들을 종합하여 평가하는 개관적 성격의 논문에 해당합니다."
Performance Evaluation of Clinical Models on Sequential Clinical Text for AI Powered Medical Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10649316,"Clinical terminologies are used by the experts to evaluate or conclude specific results based on the symptoms of the patients. The clinical terminologies used by the experts can be termed as aspects, aspect extraction is the major sub-task in any sentimental analysis. For the extraction of aspect from a given sentence, the existing natural language processing methods mainly focuses on the results generated by a pre-trained model. These pre-trained models work very well for normal English, however when it comes to clinical terminologies there exists a deficiency in generating the specific results. To address the issues related to clinical terminology. There exist multiple clinical models such as Clinical Big bird, Clinician Bert, Clinical Longformer which are transformer-based learning model which perform better against these clinical data. These models perform better on large dataset. In this paper, we would like to evaluate the performance of these Bio models with sequential clinical text and compare the performance of these models and evaluate context and coherence. The best results of these models will be presented to artificial bee colony for further optimization and integrate with an Artificial Intelligence based applications.",Yes,"본 논문은 기존 임상 텍스트 처리 모델들의 성능을 평가하고, 최적화 기법을 적용하여 AI 기반 의료 응용에 통합하는 연구를 수행하고 있다. 이는 단순 리뷰나 개념적 논의가 아닌, 직접적인 성능 평가와 최적화 방법을 제안하는 독창적인 연구 내용에 해당한다."
Pseudo Labeling Methods for Semi-Supervised Semantic Segmentation: A Review and Future Perspectives,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771804,"Semantic segmentation is a fundamental task in computer vision and finds extensive applications in scene understanding, medical image analysis, and remote sensing. With the advent of deep learning, significant advancements have been made in segmentation tasks. However, deep learning models require a substantial amount of labeled data for training, and accurately annotating datasets is labor-intensive and costly. Recently, numerous studies have explored the semantic segmentation task through the lens of semi-supervised learning, with the pseudo-labeling (PL) method emerging as a straightforward and widely applicable approach. This paper provides a comprehensive review and analysis of various PL methods and their applications in semi-supervised semantic segmentation (SSSS) from multiple angles. Initially, it captures the essence of individual model self-training and the collaborative training of multiple models from a model-centric viewpoint. Next, it explores strategies for refining or dismissing unreliable methods. Then, it categorizes techniques for addressing noisy PL data and inspects improvements in PL methods from the perspective of data augmentation. It further provides insights into optimization strategies. Furthermore, it examines PL methods from an application-oriented standpoint, such as in medical image segmentation and remote sensing image segmentation. Lastly, this paper evaluates the performance of cutting-edge methods on public datasets and concludes by discussing the challenges and potential directions for future research.",No,"본 논문은 기존의 pseudo labeling 방법들을 종합적으로 리뷰하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험적 기여를 포함하지 않습니다. 따라서 연구 논문보다는 연구 동향을 정리한 개관 논문에 해당합니다."
Feature importance calculation and protein quality assessment on the decoy discrimination problem,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965862,"The function of each protein in the body is determined by its 3D structure, which can be predicted by computational methods. These methods generate an exceptional quantity of candidate models (decoys). similarity and machine learning methods are used to assess their quality. When measuring the distance from the decoy to its native structure (RMSD, TM-Score, Z-Score), similarity methods may be applied. On the other hand, machine learning methods use a subset of structural and physicochemical features to assess the quality of these candidate models. In the preprocessing step, a subset of these features is selected by hand to be used in the machine learning process. The model proposed in this work considers different sets of features simultaneously, and automatically selects, via an evolutionary model, the optimal subset to be used in the machine learning method. The proposed model also provides the relative feature importance related to the quality of the decoy model. The new model, named Score Wrapper to Feature Importance Calculation (SWtoFIC), also calculates the quality of the decoy model. These characteristics make this model an important tool to assess the decoy quality and to better understand the influence of different types of features in the decoy quality determination.",Yes,"논문은 단순한 리뷰나 이론적 고찰이 아니라, 진화 모델을 이용해 최적의 특징 집합을 자동으로 선택하고 단백질 구조 후보 모델의 품질을 평가하는 새로운 기계 학습 모델(SWtoFIC)을 제안하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문에 해당한다."
Real-time fuzzy logic speed tracking controller for a DC motor using Arduino Due,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069560,"Designing and developing AI controllers on separately dedicated chips have many advantages. This paper reviews the development of a real-time fuzzy logic controller for speed control of a dc motor using Arduino Due board. The proposed fuzzy logic controller is based on Mamdani approach and has been tested on the aforementioned high performance microcontroller board and using MATLAB. During the real-time operation the dc motor behavior and the fuzzy controller's response were plotted and the data were stored in MATLAB without interrupting the fuzzy logic controller. Based on these observed information, the system settling time and the rise time reduction were calculated for each input wave patent trajectories while increasing the wave frequency. It was noted that the system overshoot is negligible. Utilizing the aforementioned parameters the Arduino Due board performance was analyzed with the fuzzy logic speed control approaches of dc motors made by past researchers as mentioned above. The system response shows a satisfactory performance for this particular dc motor application when the input signal (desired output signal) frequency is less than 2 Hz, but further research is needed when identifying the optimum performance of the Arduino Due board for different fuzzy logic algorithms while increasing the desired input signal frequency.",Yes,"본 논문은 Arduino Due 보드를 이용한 실시간 퍼지 논리 속도 제어기를 설계하고 테스트한 독창적인 연구 내용을 포함하고 있습니다. 또한, 시스템 성능을 실험적으로 분석하고 기존 연구와 비교하는 등 직접적인 연구 기여가 명확히 드러납니다."
Evaluating ML Algorithm precision over indoor thermal comfort through comparison of popular metrics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10187456,"Predicting the temperature in indoor spaces is essential for upholding cozy living and working conditions as well as for assuring the security and preservation of delicate tools and materials. This parameter's forecast beforehand can aid in reducing hazards, enhancing operations, and improving energy efficiency. In this paper, data retrieved from sensors, positioned within a particular building, has been simulated through three machine learning ML models. Machine Learnings (ML) become a potent tool for temperature prediction based on historical data, weather patterns, and other related elements in recent years. We evaluated the performance of three machine learning (ML) models using a train and test dataset. This paper focuses on identifying the best model selection comparing the three chosen models.",Yes,"논문은 센서 데이터를 활용해 세 가지 머신러닝 모델을 시뮬레이션하고, 이들의 성능을 비교 평가하는 연구를 수행하고 있다. 이는 직접적인 실험과 분석을 포함한 독창적인 연구 내용으로 판단된다."
Analysis of Movie Recommendation Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308144,"This research paper presents a comprehensive literature review of various movie recommendation systems, including techniques such as sentiment analysis of comments, movie trailer data, facial expressions, browsing history, and view percentage. The paper also compares various recommenders based on deep learning and swarm algorithms. Furthermore, this paper provides a novel approach for movie recommender system that uses a hybrid model combining content-based and collaborative filtering approaches. Specifically, the proposed approach involves creating a matrix factorization-based model and providing certain custom-designed features, such as averages of all movies and users, top similar movies, and top similar users, to produce the final model. The proposed approach is evaluated on the MovieLens dataset and achieves a mean absolute percentage error (MAPE) of 19.868316 and a root mean squared error (RMSE) of 0.672788. These outcomes illustrate the potency of the suggested strategy and its potential to enhance movie recommendation systems.",Yes,"논문은 기존 연구들을 리뷰하는 동시에, 콘텐츠 기반 및 협업 필터링을 결합한 하이브리드 모델이라는 독창적인 접근법을 제안하고 이를 MovieLens 데이터셋으로 평가한 결과를 제시하고 있다. 이는 직접 기여하는 연구 내용이 포함된 연구 논문임을 의미한다."
Neural Currency Guard using Generative Adversarial Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537784,"Counterfeit currency detection remains an ongoing challenge worldwide, as counterfeiters continuously enhance their techniques. This paper provides a comprehensive review of recent advancements in counterfeit currency detection systems, with a specific focus on innovative approaches using Generative Adversarial Networks (GANs). We implement GANs to generate realistic synthetic currency images for training robust counterfeit detectors. By thoroughly analyzing current research, we gain valuable perspectives into GAN-based methodologies for producing diversified fake currency data. Our study also examines the utilization of image processing, machine learning, and spectroscopic techniques in existing counterfeit recognition systems. Additionally, we detail a practical implementation of GANs for generating counterfeit currency images in Indian context, as well as evaluate its effectiveness. This research aims to deliver vital insights into cutting-edge counterfeit currency detection, presenting fresh perspectives on harnessing GANs. It also intends to aid future research by highlighting potential areas for improvement.",Yes,"논문 초록에서 GAN을 활용한 위조 화폐 이미지 생성 및 이를 통한 위조 탐지기 훈련이라는 구체적인 구현과 평가가 언급되어 있어, 단순 리뷰가 아닌 직접적인 연구 및 실험 결과를 포함한 독창적인 연구임을 알 수 있습니다. 따라서 연구 논문에 해당합니다."
User interest acquisition by adding home and work related contexts on mobile big data analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562072,"User interest acquisition facilitates customized service by figuring out user preferences in various areas, such as recommendation system and intelligence city. Mobile Internet enriches traditional user behaviors (such as who (user), when (time) and what (content)) by introducing where (mobility) into the analysis of user interest acquisition. However, user mobility is highly predictable, and user interests are constrained in a small scope. In the era of mobile big data, although several association rules and Bayesian model based approaches have been proposed to identify user interests, the impact of home and work related contexts in users' daily lives on user interest has not been fully investigated. In fact, home and work locations are anchors in user mobility and provide abundant behavior contexts to know a person. So this article proposes a framework using home and work related contexts to identify user interests. The proposed framework consists of home-work related contexts awareness based on greedy strategy, dimensionality reduction based on principle components analysis, and modeling based on various state-of-the-art machine learning algorithms. Then the proposed framework is validated on a real dataset covering 6,800 residents with more than 3.2 million records in 23 days. Results show that the proposed framework is effective, and the precision can reach more than 82% with only 7 principle components.",Yes,"논문은 홈과 직장 관련 맥락을 활용한 사용자 관심사 획득을 위한 새로운 프레임워크를 제안하고, 이를 실제 데이터셋에 적용하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
A Deep Learning Approach for Semantic Similarity Prediction Between Question Pairs Using Siamese Network and Word Embedding Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10585920,"Question-answering platforms, such as Quora, Red-dit, and StackOverflow, have become immensely popular in the virtual community. Among these platforms, Quora stands out as a widely utilized and resourceful repository with over 300 million monthly visitors. However, the prevalence of similar questions with paraphrased content poses challenges for users in finding relevant answers. This paper proposes a deep learning approach for predicting the semantic similarity of question pairs to address the issue of duplicate questions. The proposed model employs a Siamese network-based Long Short-Term Memory (LSTM) architecture, utilizing word embedding vectors from various algorithms such as Glo Ve, Word2Vec, and FastText. Previous research on duplicate question identification is reviewed, with our approach overcoming drawbacks such as low-level semantic connections and text type incompatibility. After pre-processing the Siamese LSTM network compares the embeddings using L1 distance to determine question pair similarity. The model is trained and evaluated on the Quora Question Pair Classification dataset, and outperformed other state-of-the-art methods with an accuracy of 84.77% with the FastText word embedding.",Yes,"본 논문은 시암 네트워크 기반 LSTM 아키텍처와 다양한 단어 임베딩 기법을 활용하여 질문 쌍의 의미적 유사성을 예측하는 새로운 딥러닝 모델을 제안하고, 이를 기존 방법들과 비교 평가한 연구 내용을 포함하고 있다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 판단된다."
Binarized Neural Network for Edge Intelligence of Sensor-Based Human Activity Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529002,"A wide diversity of sensors has been applied in human activity recognition. These sensors generate enormous amounts of data during human activity monitoring. Server-based computing and cloud computing require to upload all sensor data to servers/clouds for data processing and analysis. The long-distance data traveling between sensors and servers increases the costs of bandwidth and latency. However, human activity recognition has a high demand for real-time processing. Recently, edge computing is surging to solve this problem by moving computation and data storage closer to the sensors, rather than relying on a central server/cloud. Most human activity recognition is conducted by artificial intelligence, which requires intensive computation and high power consumption. Edge servers are usually designed for low power, low cost, and low computation. They do not support computation-intensive deep learning algorithms or result in high latency. Fortunately, the development of binarized neural networks enables edge intelligence, which supports AI running at the network edge for real-time applications. In this paper, we implement a binarized neural network (BinaryDilatedDenseNet) to enable low-latency and low-memory human activity recognition at the network edge. We applied the BinaryDilatedDenseNet on three sensor-based human activity recognition datasets and evaluated it with four metrics. In comparison, the BinaryDilatedDenseNet outperforms the related work and other three binarized neural networks in overall and saves 10× memory and 4.5×–8× inference time compared to the FPDilatedDenseNet(the full-precision version of the BinaryDilatedDenseNet).",Yes,"본 논문은 센서 기반 인간 활동 인식을 위한 이진화 신경망(BinaryDilatedDenseNet)을 구현하고, 이를 세 가지 데이터셋에 적용하여 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 연구 및 다른 이진화 신경망과 비교하여 메모리 절약과 추론 시간 단축 효과를 입증하였으므로 연구 논문에 해당한다."
Noise and performance analysis on fundus images with CNN and transformer models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10455148,"Fundus imaging is a valuable diagnostic tool in ophthalmology, providing clinicians with detailed visualizations of the retina and aiding in the detection and monitoring of various eye diseases, including age-related macular degeneration (AMD), glaucoma, diabetic retinopathy (DR), and cataract. However, the quality of fundus images can be significantly affected by noise, mainly additive white Gaussian noise (AWGN), which is inherent in many imaging systems. The presence of noise in real-world data poses significant challenges for computer vision tasks. In the field of medical image classification, a wrong diagnoisis has heavy consequences. Understanding the impact of AWGN on fundus images is crucial for developing practical denoising algorithms and improving diagnostic accuracy. This work presents an analysis of AWGN noise in fundus images aims to characterize its effects on image quality and assess its impact on diagnostic tasks. The work also analyzes the performance of six models (3 each) of two popular deep learning architectures, Convolutional Neural Networks (CNN) and Vision Transformers (ViT) in the presence of AWGN. AWGN is first introduced to the clean image datasets to conduct the analysis. The CNN and ViT models are trained on the noisy datasets to evaluate the performance of the image classification task. The work also involves six denoising algorithms and a popular image enhancement algorithm- Contrast Limited Adaptive Histogram Equalization (CLAHE).",Yes,"논문은 AWGN 노이즈가 안저 이미지에 미치는 영향을 분석하고, CNN 및 ViT 모델의 성능을 평가하는 실험적 연구를 포함하고 있다. 또한, 노이즈 제거 알고리즘과 이미지 향상 기법을 적용하여 진단 정확도 개선을 시도하는 등 독창적인 연구 기여가 포함되어 있다."
From Past to Present: A Historical Review of Recurrent Neural Networks in Bitcoin Price Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10895054,"This comprehensive review synthesizes insights from many research papers on Bitcoin, encompassing aspects like its foundational peer-to-peer system and the application of advanced machine learning for price prediction. Examining methodologies such as Bayesian neural networks and LSTM models, the paper identifies trends, challenges, and advancements in understanding Bitcoin prices. Additionally, it intersects with Indonesia’s monetary system, employing diverse predictive mod- eling techniques. The study evaluates recurrent neural networks and LSTM algorithms for forecasting, delves into Bitcoin’s architectural assessment using Systems Modeling Language, and compares machine learning models. Spanning two decades of econophysics, it explores milestones, challenges, and addresses Bitcoin and Ethereum’s role as safe-havens during the global COVID 19 pandemic.",No,논문 초록에서 다루는 내용은 여러 연구 논문을 종합하여 비트코인 가격 예측에 관한 과거와 현재의 연구 동향을 리뷰하는 데 초점이 맞춰져 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 실험적 기여보다는 기존 연구의 종합 및 분석에 해당합니다.
Meta Mimetic: Few-Shot Classification of Mobile-App Encrypted Traffic via Multimodal Meta-Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555634,"Despite its proven effectiveness in classifying encrypted network traffic, deep learning requires large amounts of labeled data to feed typical data-hungry training processes. Few-shot learning provides means to overcome these limitations, supporting classification tasks related to traffic with few labeled data available. Its extensive investigation in other domains notwithstanding (e.g., computer vision), it has been only preliminarily adopted for classifying encrypted traffic.In this work, we design and evaluate Meta Mimetic a novel multimodal few-shot learning solution for classifying mobile-app encrypted traffic. The proposal is based on the meta-learning paradigm and introduces enhancements via the adoption of a multimodal feature extractor trained via a novel ad-hoc meta-learning procedure. Since Meta Mimetic is orthogonal to the specific few-shot learning approach, in our experimentation, we adapt it to a number of different meta-learning approaches (namely MatchingNet, ProtoNet, RelationNet, MetaOptNet, foMAML, and ANIL). We provide an empirical assessment of these approaches, considering the Mirage-2019 dataset as a test bench. Results show that Meta Mimetic represents the best trade-off in terms of performance and complexity in mobile-app traffic classification (up to 91% F1-score) when compared to state-of-the-art solutions. The in-depth analysis of the performance of its components allows us to shed light on the multimodal internal mechanisms and further improve classification performance. Finally, we demonstrate the robustness of our proposal (only ≈ 2% F1-score drop) against the next variations introduced by the TLS 1.3 encryption that may impair the information exploitable by payload-based traffic classifiers.",Yes,"논문은 모바일 앱 암호화 트래픽 분류를 위한 새로운 멀티모달 메타 학습 기반의 few-shot 학습 방법인 Meta Mimetic을 설계하고 평가하는 독창적인 연구 내용을 포함하고 있습니다. 또한 다양한 메타 학습 기법에 적용하여 실험적 성능 평가를 수행하고, 기존 방법 대비 우수한 성능을 입증하는 실험 결과를 제시하고 있습니다."
Learning semantics in content based image retrieval,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1296909,"Content-based image retrieval (CBIR) is an attempt to remove the bottleneck of visual semantic understanding needed in automated indexing in visual information retrieval. However, the myth about the power of visual-feature-based indexing was quickly diminished as such features are far from representing semantic visual contents and producing meaningful indexes. One solution is to apply relevance feedback to refine queries or similarity measures in the search process and apply machine learning techniques to learn semantic annotations. In this paper, we address the key issues involved in relevance feedback of CBIR systems and review solutions to these issues. Based on these discussions, we present a relevance feedback and semantic learning framework for CBIR. We hope the ideas presented in this paper serve as a catalyst to more research efforts in this direction.",No,"초록에서 본 논문은 기존 연구들을 리뷰하고, 관련 문제들을 논의하며, 제안된 프레임워크를 소개하는 개념적 논문으로 보입니다. 직접적인 실험 결과나 독창적인 연구 기여보다는 연구 방향 제시와 문제점 분석에 중점을 두고 있습니다."
Obstacles and Opportunities Provided using the Impact of Fifth-Generation Networks on Cybersecurity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10846411,"Communication, data access, and our interactions with the digital world are all going to undergo a sea change as a result of the introduction of fifth-generation (5G) networks. Through its promise of faster speed, decreased latency, and wider connection, fifth-generation wireless (5G) technology is positioned to enable a wide range of applications, ranging from improved mobile broadband to the Internet of Things (IoT). When it comes to cybersecurity, however, this technological leap also brings with it a new set of challenges and opportunities. As the landscape of cyber threats continues to grow, it is necessary to reevaluate the security measures that are currently in place. This is because 5G networks are becoming the backbone of vital infrastructure and enable the proliferation of connected devices. In this study, the influence of 5G networks on cybersecurity is investigated, with a particular emphasis placed on the challenges and opportunities that are brought about by this revolutionary technology. An examination of the architectural and operational changes that 5G brings about, such as network slicing, software-defined networking (SDN), and network function virtualisation (NFV), as well as the consequences these changes have for security, is the first step in this exploration. We then proceed to conduct an analysis of the various vulnerabilities that 5G networks may reveal, which includes the possibility of additional attack surfaces, the risk of assaults on supply chains, and the difficulties associated with securing a diversified ecosystem of Internet of Things devices. In addition to this, we explore the prospects that 5G networks bring for the optimisation of cybersecurity. The capacity to implement more advanced security procedures, the possibility of real-time threat detection and response, and the facilitation of secure, high-speed data transfer for security applications are some of the benefits this technology offers. When it comes to improving security outcomes, we also take into consideration the role that emerging technologies like artificial intelligence (AI) and machine learning play in conjunction with 5G. Reviewing the most recent literature, industry reports, and expert evaluations allows us to provide a thorough knowledge of the cybersecurity landscape around 5G. We also perform a comparison examination of the security aspects of 5G with those of earlier generations in order to emphasise both the gains that have been made and the new problems that have been introduced. Finally, the findings of this study highlight the fact that 5G technology is a double-edged sword: while it does open up new frontiers in terms of connection and creativity, it also necessitates a proactive and adaptable approach to cybersecurity. In the era of 5G, stakeholders have the ability to strengthen the digital ecosystem against new threats and safeguard the security and privacy of users by recognising and tackling the hurdles that are posed by 5G and by capitalising on the opportunities that are presented by 5G.",No,"초록 내용은 5G 네트워크가 사이버보안에 미치는 영향에 대한 문헌 검토와 기존 연구, 산업 보고서, 전문가 평가를 종합한 분석에 초점이 맞춰져 있습니다. 독창적인 실험 결과나 새로운 연구 방법론, 직접적인 연구 기여가 명확히 제시되어 있지 않아 연구 논문으로 보기 어렵습니다."
Enhancing Electricity Consumption Forecasting with Artificial Intelligence on Small Datasets: A Comparative Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10821571,"Accurate electricity consumption forecasting is essential for efficient resource allocation and grid management, particularly with limited data. The present paper investigates how artificial intelligence techniques can enhance forecasting in small datasets and addresses data quality concerns in the educational sector. It reviews challenges in traditional forecasting, explores artificial intelligence methods such as support vector machines, autoregressive integrated moving average, and long short-term memory networks, and evaluates their performance using real-world data from four educational buildings belonging to the Technical University of Cluj-Napoca. By combining theoretical insights with empirical results, the present study advances artificial intelligence-driven forecasting for electricity consumption in small datasets, offering insights for future research and industry applications in energy management and policy formulation.",Yes,"본 논문은 인공지능 기법을 활용하여 소규모 데이터셋에서 전력 소비 예측을 향상시키는 방법을 실험적으로 평가하고, 실제 데이터를 사용한 비교 연구를 수행하고 있다. 이는 기존 연구를 검토하는 데 그치지 않고, 직접적인 실험과 분석을 통해 독창적인 연구 기여를 포함하고 있음을 보여준다."
Machine Learning Based Battery Aging Management Strategy for Electric Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533026,"Electric vehicles are becoming the front runners for both urban and rural mobility challenges on account of less pollution, rising fuel cost, better performance and environmental impacts. However, the aging and replacement cost of battery packs, resulting from dynamic and nonlinear behavior of the battery degradation is still an unresolved problem in electric automobile applications. The goal of this work is to introduce a novel machine learning based battery degradation control strategy to avert the rapid capacity loss of battery packs bearing in mind of vehicle performance. Battery currents and depth of discharge are chosen as the battery aging control parameters and performance validation is achieved by doing simulation on degrading battery pack in the electric vehicle model using various charging and discharging profiles. The proposed closed-loop control strategy is developed by evaluating different regression models using generated dataset, based on work related to data-driven power management strategy and controlling battery current limit values. From the comparison, it is observed that Gaussian process regression shows better precision over other regression models. The simulation outputs prove the ability of the proposed strategy to extend the electric vehicle battery life by 2.03% over 200,000km. This work can be extended by using deep learning based models and more charge and discharge profiles.",Yes,논문 초록에서 제안된 기계 학습 기반 배터리 노화 관리 전략은 새로운 제어 방법을 개발하고 시뮬레이션을 통해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 바탕으로 한 데이터 기반 모델 평가 및 배터리 수명 연장 효과를 입증한 실질적인 연구 기여로 판단된다.
Implementation of a Customized Named Entity Recognition (NER) Model in Document Categorization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841691,"In institutions, one of the problems that arise is related to accumulating a large amount of documentation about their processes and other important information; a recurring task to manage these documents occurs when classifying them. In addition to this, in current times, with the exploitation of applications using Artificial Intelligence (AI) and Natural Language Processing (NLP), they are allowing to provide solutions at a technological level. In this work, we propose a methodology for the development of a customized Named Entity Recognition (NER) model and implement it, for the task of classifying documents and determining and verifying if the classification performed corresponds to a manual classification. To evaluate the performance of the classifier, an automatic classification of 1049 documents from the corpus for knowledge management of an institution was used, using a customized NER model. The results allow us to determine that the presented model achieves 95% positive classifications; the methodology is developed with the purpose of being replicated and scaled, according to the different needs of organizations.",Yes,"논문은 맞춤형 NER 모델을 개발하고 이를 문서 분류 작업에 적용하여 성능 평가를 수행하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 제안된 방법론과 실험 결과를 제시하여 직접적인 연구 기여가 있음을 보여줍니다."
Rendering 3D City for Smart City Digital Twin,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9821113,"The field of 3D city modelling has attracted a growing interest for representing the city digital twin, providing interactive visualizations of building infrastructures integrated with a wide range of data typically produced in a Smart City environment. This paper presents a method for producing a 3D city model with photorealistic rooftop textures extracted from aerial images, as well as the integration of the 3D city model into an open-source Smart City framework. The proposed solution provides a smart visualization of 3D city entities integrated with a large variety of Smart City data (coming, for instance, from IoT Devices which generate time-series data, heatmaps, geometries and shapes related to traffic flows, bus routes, cycling paths etc.). The proposed method for rooftop detection and alignment follows a deep learning approach based on U-Net architecture, and it has been validated against a manually created ground-truth of 50 buildings scattered uniformly on the covered area. The solution is implemented in the open-source Snap4City Smart City platform. DEMO: The solution will be demonstrated and the readers and conference attendees will be capable to interact and work with the tools by using the links provided.",Yes,"본 논문은 U-Net 기반의 딥러닝 방법을 활용한 옥상 검출 및 정렬 기법을 제안하고, 이를 스마트 시티 데이터와 통합한 3D 도시 모델링 솔루션을 개발하여 직접적인 연구 기여를 하고 있다. 또한, 제안된 방법을 실제 데이터셋으로 검증하고 오픈소스 플랫폼에 구현한 점에서 독창적인 연구 내용이 포함되어 있다."
Undergraduate In-class Research Experience for Computer Architecture Students,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799265,"This study presents a hands-on research experience for undergraduate senior-level computer architecture course students. The students have investigated scientific research process, literature review approaches, technical writing as well as blind-review principles, and conducted hands-on research on three different computer systems, namely, a supercomputer, an office desktop, and an autonomous vehicle artificial intelligence computer systems, for a budget-constrained final computer configuration of an office desktop computer.The final student team outcomes, relevant feedback, and the corresponding surveys, evaluated by the project administrators, strongly imply the success of the project for an effective research component inclusion in an undergraduate course.",No,"본 논문은 학부생을 대상으로 한 수업 내 연구 경험 제공과 그 효과에 대한 평가를 다루고 있으며, 직접적인 독창적 연구 결과나 새로운 과학적 발견을 제시하지 않습니다. 따라서 연구 논문보다는 교육 방법론 및 교육 효과 분석에 초점을 맞춘 보고서로 판단됩니다."
Statistical Minimax Lower Bounds for Transfer Learning in Linear Binary Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9834760,"Modern machine learning models require a large amount of labeled data for training to perform well. A recently emerging paradigm for reducing the reliance of large model training on massive labeled data is to take advantage of abundantly available labeled data from a related source task to boost the performance of the model in a desired target task where there may not be a lot of data available. This approach, which is called transfer learning, has been applied successfully in many application domains. However, despite the fact that many transfer learning algorithms have been developed, the fundamental understanding of ""when"" and ""to what extent"" transfer learning can reduce sample complexity is still limited. In this work, we take a step towards foundational understanding of transfer learning by focusing on binary classification with linear models and Gaussian features and develop statistical minimax lower bounds in terms of the number of source and target samples and an appropriate notion of similarity between source and target tasks. To derive this bound, we reduce the transfer learning problem to hypothesis testing via constructing a packing set of source and target parameters by exploiting Gilbert-Varshamov bound, which in turn leads to a lower bound on sample complexity. We also evaluate our theoretical results by experiments on real data sets.",Yes,"논문은 전이 학습의 샘플 복잡도에 대한 통계적 최소 최대 하한을 새롭게 제시하며, 이를 위해 가설 검정 문제로 환원하는 독창적인 이론적 기여를 포함하고 있다. 또한 실제 데이터 실험을 통해 이론적 결과를 검증하는 등 직접적인 연구 내용을 담고 있다."
XR for Augmented Utilitarianism,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8942355,"Steady progresses in the AI field create enriching possibilities for society while simultaneously posing new complex challenges of ethical, legal and safety-relevant nature. In order to achieve an efficient human-centered governance of artificial intelligent systems, it has been proposed to harness augmented utilitarianism (AU), a novel non-normative ethical framework grounded in science which can be assisted e.g. by Extended Reality (XR) technologies. While AU provides a scaffold to encode human ethical and legal conceptions in a machine-readable form, the filling in of these conceptions requires a transdisciplinary amalgamation of scientific insights and preconditions from manifold research areas. In this short paper, we present a compact review on how XR technologies could leverage the underlying transdisciplinary AI governance approach utilizing the AU framework. Towards that end, we outline pertinent needs for XR in two hereto related contexts: as experiential testbed for AU-relevant moral psychology studies and as proactive AI Safety measure and enhancing policy-by-simulation method preceding the deployment of AU-based ethical goal functions.",No,"본 논문은 XR 기술이 증강 공리주의(Augmented Utilitarianism) 프레임워크를 지원하는 방식을 개괄적으로 검토하는 리뷰 성격의 논문으로, 직접적인 실험 결과나 독창적인 연구 데이터를 제시하지 않는다. 따라서 새로운 연구 결과를 포함한 연구 논문으로 보기 어렵다."
Mental Effort Estimation by Passive BCI: A Cross-Subject Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9630613,"Despite the technological advancements, the employment of passive brain computer interface (BCI) out of the laboratory context is still challenging. This is largely due to methodological reasons. On the one hand, machine learning methods have shown their potential in maximizing performance for user mental states classification. On the other hand, the issues related to the necessary and frequent calibration of algorithms and to the temporal resolution of the measurement (i.e. how long it takes to have a reliable state measure) are still unsolved. This work explores the performances of a passive BCI system for mental effort monitoring consisting of three frontal electroencephalographic (EEG) channels. In particular, three calibration approaches have been tested: an intra-subject approach, a cross-subject approach, and a free-calibration procedure based on the simple average of theta activity over the three employed channels. A Random Forest model has been employed in the first two cases. The results obtained during multi-tasking have shown that the cross-subject approach allows the classification of low and high mental effort with an AUC higher than 0.9, with a related time resolution of 45 seconds. Moreover, these performances are not significantly different from the intra-subject approach although they are significantly higher than the calibration-free approach. In conclusion, these results suggest that a light (three EEG channels) passive BCI system based on a Random Forest algorithm and cross-subject calibration could be a simple and reliable tool for out-of-the-lab employment.",Yes,"본 논문은 수동형 BCI 시스템의 정신적 노력 모니터링을 위한 세 가지 보정 방법을 비교하고, 랜덤 포레스트 모델을 적용하여 성능을 평가하는 독창적인 연구를 수행하였다. 실험 결과와 분석을 통해 새로운 방법론적 기여를 제시하고 있으므로 연구 논문에 해당한다."
Systematic Comprehension for Developer Reply in Mobile System Forum,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668016,"Review-based software development has become increasingly prevalent in recent years. Existing efforts aiming at either informative evaluation or sentiment analysis are mainly from the perspective of the reviewers, while neglecting the attitude and behavior of the developers. Such efforts inevitably suffer from recommendation bias in practice, and thus benefit little for the improvement of user reviews.In this paper, we attempt to bridge the gap between user review and developer reply, and conduct a systematic study for review reply in development forums, especially in Chinese mobile system forums. To this end, we concentrate on three research questions: 1) should a targeted review be replied; 2) how long time it should be replied; 3) does traditional review analysis help to pursue a reply for certain review? To answer such questions, given certain review datasets, we perform a systematical study including the following three stages: 1) a binary classification for reply behavior prediction, 2) a regression for prediction of reply time, 3) a systematic factor study for the relationship between traditional review analysis and reply performance. To enhance the accuracy of prediction and analysis, we proposed a CNN-based weak-supervision analysis framework, which exploits manifold techniques from NLP and deep learning. We validate our approach via extensive comparison experiments. The results show that our analysis framework is effective. More importantly, we have uncovered several interesting findings, which provide valuable guidance for further review improvement and recommendation.",Yes,"본 논문은 개발자 답변 예측과 답변 시간 예측을 위한 이진 분류 및 회귀 모델을 제안하고, CNN 기반의 약지도 학습 프레임워크를 활용하여 실험적으로 검증하는 등 독창적인 연구 방법과 결과를 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Fine-Grained Sentiment Analysis for Enhanced Financial Distress Prediction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485942,"Sentiment analysis aims to identify the sentiment polarity of specific aspects within given sentences or comments, and aspect-based sentiment analysis is considered a fundamental task in sentiment analysis. With practical applications in areas such as product reviews, food delivery evaluations, and public opinion monitoring, sentiment analysis plays a crucial role. This paper focuses on the application of fine-grained sentiment analysis in financial distress prediction (FDP) to enhance early warnings of the management status of companies. In previous studies, there has been a narrow emphasis on using document-level sentiment analysis to extract overall sentiment from text, overlooking the semantic nuances conveyed by sentiments. Therefore, this paper aims to extract fine-grained sentiments from the Management Discussion & Analysis (MD&A) of Chinese listed companies. The proposed model is based on a two-step framework, consisting of an unsupervised aspect-level financial sentiment extraction phase and a model validation phase. Specifically, the former is built on a deep learning model with an attention mechanism, conducting unsupervised aspect extraction, aspect identification, and aspect-level sentiment classification in a sequential manner to obtain fine-grained sentiments. The latter is responsible for evaluating the effectiveness of the newly acquired features on benchmark machine learning models, including SVM, DT, LR, CNN, and DNN. Experimental results reveal that MD&A predominantly covers eight types of aspects, including ownership, business scope, development, capital, sales, management, prizes, and probability. Additionally, it has been observed that fine-grained sentiment features can enhance the performance of FDP. This study represents a significant innovation in existing literature, being the first to introduce aspect-level financial sentiment analysis into the realm of FDP.",Yes,"본 논문은 금융 기업의 재무 고난 예측을 위해 세부적인 감성 분석 모델을 제안하고, 딥러닝 기반의 새로운 방법론을 개발하여 실험적으로 성능을 검증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Performance enhancement of cooperative learning algorithms by improved decision making for context based application,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877588,"Cooperation in learning (CL) can be understood in a multiagent system. In this the agents are capable of learning from both their own experiments and other agents' knowledge and expertise. Implementation of CL is a complicated task in the real world. In distributed systems several agents cooperate to achieve a common goal or accomplish a shared task. In particular, if there are different people or organizations with different goals and information, then a multiagent system (MAS) is needed to handle their interactions. In this paper, various issues related with cooperative machine learning are studied and implemented. A new set of improved cooperative learning algorithms is proposed in the paper. Expertness measuring criteria which were used in earlier work is further enhanced in proposed method. Six methods for measuring the agents' expertness are used i.e. Normal (Nrm), Absolute (Abs), Positive (P), Negative (N), Certainty (Cer) and Entropy (Ent). The novelty of this approach lies in the implementation of Weighted Strategy Sharing with expertness measuring criteria by means of Q-learning, Sarsa learning, Q(λ) and Sarsa(λ) learning algorithms. The paper shows implementation results and performance comparison of all these algorithms.",Yes,"논문 초록에서 새로운 협력 학습 알고리즘을 제안하고, 기존의 전문가 측정 기준을 개선한 방법을 구현 및 성능 비교한 연구임을 명확히 밝히고 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문임을 의미합니다."
Subjective and Objective Testing in Support of the JPEG Pleno Point Cloud Compression Activity,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9922803,"Point clouds have many applications in today’s society ranging from entertainment to autonomous driving. With these new applications comes the need to compress the growing volume of point cloud data in a manner that is both suitable for human visualization and machine processing applications. The JPEG Pleno Point Cloud activity has been working toward a learning-based coding standard for point clouds, offering a single-stream, compact compressed domain representation, supporting advanced flexible data access functionalities targeting both in-teractive human visualization, and effective performance for 3D processing and machine-related computer vision tasks. As part of this activity, the JPEG Committee has been performing a number of exploration studies to evaluate existing coding standards as well set up baseline anchors and examine objective metrics against which new learning-based solutions may be compared. This article provides an overview of the JPEG Pleno Point Cloud activity and discusses challenges and solutions to the problem of evaluating and comparing cloud coding solutions. Experimental results will be presented demonstrating methodologies used by the JPEG Committee for point cloud compression assessment as well as outlining the performance of current state of the art compression standards on point clouds as well as the sensitivity of the objective metrics used for this activity to various adjustable parameters.",Yes,"논문은 JPEG Pleno Point Cloud 압축 활동에 대한 실험적 연구 결과와 평가 방법론을 제시하며, 기존 코딩 표준과 새로운 학습 기반 솔루션을 비교하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Understanding Security Issues based on App Comment Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914435,"Mobile Applications (App) security issues occur in sync with the progress of information technology. User comments serve as a valuable source of information for evaluating a mobile app, for both new users and developers. However, previous works rarely rely on existing comments to provide a detailed summarization of the app's security issues. We present a novel comment summarization framework, SBR-Miner(Security Bug Related-Comment Miner). It first extracts the security-related sentences from the comment sentences using a classification model, then extracts the aspect, opinion, misbehavior of the sentences using a deep learning model, and assigns security issues categories to the triads, and finally visualizes the comment summaries using a radar plot. Our evaluation of the manually labeled dataset shows that SBR-Miner can provide a clearer and more comprehensive aspect than the state-of-the-art, with an average F1 score of 0.92, higher than AR-Miner (0.80) and SUR-Miner (0.81). 92% of developers agree that the SBR-Miner summaries are beneficial, according to feedback from relevant practitioners.",Yes,"본 논문은 기존 연구와 차별화된 새로운 댓글 요약 프레임워크(SBR-Miner)를 제안하고, 분류 모델과 딥러닝 모델을 활용하여 보안 관련 댓글을 분석하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 수동으로 라벨링된 데이터셋을 이용한 평가와 실무자 피드백을 통해 성능을 검증한 점에서 연구 논문으로 판단됩니다."
Development of Novel Big Data Analytics Framework for Smart Clothing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162027,"Recent advances in micro electro-mechanical systems (MEMS) have produced wide variety of wearable sensors. Owing to their low cost, small size and interfacability, those MEMS based devices have become increasingly commonplace and part of daily life for many people. Large amount of data from heart and breath rates to electrocardiograph (ECG) signals, which contain a wealth of health-related information, can be measured. Hence, there is a timely need for novel interrogation and analysis methods for extracting health related features from such a Big Data. In this paper, the prospects from smart clothing such as wearable devices in generating Big Data are critically analyzed with a focus on applications related to healthcare, sports and fashion. The work also covers state-of-the-art data analytics methods and frameworks for health monitoring purposes. Subsequently, a novel data analytics framework that can provide accurate decision in both normal and emergency health situations is proposed. The proposed novel framework identifies and discusses sources of Big Data from the human body, data collection, communication, data storage, data analytics and decision making using artificial intelligence (AI) algorithms. The paper concludes by identifying challenges facing the integration of Big Data analytics with smart clothing. Recommendation for further development opportunities and directions for future work are also suggested.",Yes,"논문 초록에서 제안된 ""novel data analytics framework""는 기존 연구와 차별화된 독창적인 연구 내용을 포함하고 있으며, 스마트 의류에서 생성되는 빅데이터를 분석하는 새로운 방법론을 개발하고 있다. 따라서 본 논문은 직접 기여하는 연구 논문에 해당한다."
Melanoma Risk Prediction with respect to Modifiable Lifestyle Factors by Meta-Analysis Aided Machine Learning Technique,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9231223,"Melanoma is a highly prevalent dermatological disease and a life-threatening form of skin cancer. Lifestyle factors have been observed to influence melanoma risk. The relationship between increased BMI and melanoma incidence has been investigated using epidemiological, in our previous studies. In order to study relationships between melanoma and other modifiable lifestyle factors such as alcohol consumption, smoking, sunscreen application, and use of tanning devices, a meta-analysis aided by machine learning technique was carried out. PubMed database was searched carefully to sort out literature pertaining to the associations of these lifestyle factors with melanoma risk. Meta-analyses were carried out using a software called Review Manager 5.3. It gave risk ratios and 95% confidence intervals as results. Analysis of these data revealed weak positive relationship between alcohol consumption (OR=1.46; 95% CI=1.32−1.62) and use of tanning devices (OR=1.36; 95% CI=1.20−1.53). Negative associations were found between smoking (OR=0.78; 95% CI=0.67−0.92) and sunscreen application (OR=0.5; 95% CI=0.44−0.57). These results were matched, and association trends were confirmed with the rule induction results obtained by applying Naïve Bayes model to the data of each lifestyle factor. All the data from the five factors were pooled in together to create a master datasheet, and machine learning was performed on it to generate a predictive model for melanoma risk. The results were validated through a test split (ratio 0.7:0.3) and cross-validation as well. The accuracy was observed to be 70.23% and 70.35% + 0.79%. The functioning of the model was tested on an unlabeled dataset.",Yes,본 논문은 메타분석과 머신러닝 기법을 활용하여 흑색종 위험 예측 모델을 개발하고 검증하는 독창적인 연구 내용을 포함하고 있습니다. 기존 문헌을 종합 분석한 후 새로운 예측 모델을 생성하고 성능을 평가한 점에서 연구 논문에 해당합니다.
GuiltyTargets: Prioritization of Novel Therapeutic Targets With Network Representation Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121705,"The majority of clinical trials fail due to low efficacy of investigated drugs, often resulting from a poor choice of target protein. Existing computational approaches aim to support target selection either via genetic evidence or by putting potential targets into the context of a disease specific network reconstruction. The purpose of this work was to investigate whether network representation learning techniques could be used to allow for a machine learning based prioritization of putative targets. We propose a novel target prioritization approach, GuiltyTargets, which relies on attributed network representation learning of a genome-wide protein-protein interaction network annotated with disease-specific differential gene expression and uses positive-unlabeled (PU) machine learning for candidate ranking. We evaluated our approach on 12 datasets from six diseases of different type (cancer, metabolic, neurodegenerative) within a 10 times repeated 5-fold stratified cross-validation and achieved AUROC values between 0.92 - 0.97, significantly outperforming previous approaches that relied on manually engineered topological features. Moreover, we showed that GuiltyTargets allows for target repositioning across related disease areas. An application of GuiltyTargets to Alzheimer’s disease resulted in a number of highly ranked candidates that are currently discussed as targets in the literature. Interestingly, one (COMT) is also the target of an approved drug (Tolcapone) for Parkinson’s disease, highlighting the potential for target repositioning with our method. The GuiltyTargets Python package is available on PyPI and all code used for analysis can be found under the MIT License at https://github.com/GuiltyTargets. Attributed network representation learning techniques provide an interesting approach to effectively leverage the existing knowledge about the molecular mechanisms in different diseases. In this work, the combination with positiveunlabeled learning for target prioritization demonstrated a clear superiority compared to classical feature engineering approaches. Our work highlights the potential of attributed network representation learning for target prioritization. Given the overarching relevance of networks in computational biology we believe that attributed network representation learning techniques could have a broader impact in the future.",Yes,"본 논문은 네트워크 표현 학습과 양성-비라벨 학습을 결합한 새로운 타겟 우선순위 결정 방법인 GuiltyTargets를 제안하고, 다양한 데이터셋에서 기존 방법보다 우수한 성능을 입증하는 독창적인 연구 내용을 포함하고 있다. 또한, 실제 질병 사례에 적용하여 타겟 재배치 가능성을 보여주는 등 직접적인 연구 기여가 명확하다."
Analytical Review on Smart Plant Disease Detection and Prevention System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616362,"Intelligent plant disease detection and prevention systems have become an increasingly important component of today’s agricultural practices because they provide a way that is both more effective and more ecologically friendly for the management of plant diseases. This research comprises a review of the literature on smart plant disease detection and prevention systems, which encompass a wide range of sensors, algorithms, crops, and illnesses. The investigations took place in the last five years. According to the research, a proposed system that integrates various types of sensors and data sources, machine learning algorithms, decision support and management, user interface and accessibility, as well as privacy and security measures, has the potential to provide a comprehensive and efficient solution for detecting and preventing plant diseases in agricultural settings. A system like this might improve disease detection accuracy and speed, as well as reduce the use of hazardous chemicals and support the creation of more sustainable and healthier crops. Future research may focus on the creation of more advanced and integrated systems capable of dealing with the limits and limitations posed by existing systems and providing higher value to agricultural professionals and farmers.",No,초록에서 해당 논문은 기존 문헌을 종합하여 스마트 식물 질병 탐지 및 예방 시스템에 대해 리뷰하는 연구임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여가 포함된 연구 논문으로 보기 어렵습니다.
"Discovering the Ganoderma Boninense Detection Methods Using Machine Learning: A Review of Manual, Laboratory, and Remote Approaches",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490674,"Ganoderma disease is a kind of infection that actuates oil palm death. Early detection of Ganoderma disease is the most recommended strategy for proper treatment and disease control plan to be taken promptly. In this paper, the detection methods for Ganoderma disease were reviewed and categorized accordingly. It was found that the combination of remote sensors and machine learning techniques could identify the disease up to four severity levels, including the early stage of infection. It also significantly reduced the labor and time costs compared to the traditional visual inspection and lab-based approaches. In terms of machine learning, support vector machine (SVM) using the idea of finding a hyperplane was suggested as the best classifier in several studies. Despite only one research was done on ANN and no research evaluating CNN and GAN in Ganoderma disease detection; ANN, CNN and GAN were recognized as the potential machine learning techniques that could enhance the detection system.",No,본 논문은 기존 연구들을 종합하여 Ganoderma 병해 탐지 방법들을 리뷰하고 분류한 문헌 고찰(review) 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 기존 연구를 요약하는 내용에 해당합니다.
A Systematic Survey of Happiness from an Analytical Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10074361,"The paper is a survey paper that talks about studies around happiness. We have surveyed papers about the scales of measuring happiness, in which the scales are proposed, demonstrated and examined. Happiness is affected by various factors, which can be called indicators of happiness. Some of the papers we reviewed validate the significance of such indicators with applications. The indicators include inflation, unemployment, health, loneliness, and sports. Modern technology helps researchers estimate and forecast happiness and effectively find the relation between factors affecting happiness. Researchers use different methodologies to study happiness. The data used in the papers were retrieved from surveys and existing Happiness Report, designed surveys appropriate for the study. Models were proposed for forecasting happiness using Machine Learning and Neural Networks. From the reviews, we identify research gaps in the area for future work. This paper gives an overview of the studies around the area of happiness from an analytical approach.",No,"본 논문은 기존 연구들을 체계적으로 조사하고 요약한 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험, 모델 제안이 포함되어 있지 않습니다. 따라서 연구 논문보다는 문헌 리뷰에 해당합니다."
The Risk Classification of Ergonomic Musculoskeletal Disorders in Work-related Repetitive Manual Handling Operations with Deep Learning Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302701,"The injury resulted from the repetitive and load-bearing works is the most frequent work-related musculoskeletal disorders (WMSD) or cumulative trauma disorders (CTD). It comes from the overload of repetitive load-bearing actions, which resulting in fatigue, inflammation, even injuries of musculoskeletal system. According to the annular report of Labor Insurance Bureau in Taiwan, WMSD is up to 85-88% payment. Thus, the aim of this study is to evaluate the risk of WMSD during work by using the simple, quick, and correct methods by using the deep learning algorithms. In the proposed research method, after collection the videos of hand repeated movements, the ergonomic injuries are evaluated by using the 2D human pose estimation method, which is based on the Key Indicator Method - Manual Handling Operations (KIM-MHO). Then, a model of predefined classifications through deep learning approaches for manual handling operating tasks is built. The analysis results show that the classification accuracy is more than 80%, compared with the doctor's judgment. The goal of this study is to get the accuracy up to 90%, so as to achieve fast and accurate assistance for deciding the risk of ergonomics, and immediately give proper feedback.",Yes,"본 논문은 딥러닝 알고리즘을 활용하여 작업 관련 근골격계 질환의 위험도를 평가하는 새로운 모델을 제안하고, 2D 인간 자세 추정 기법과 KIM-MHO 지표를 적용하여 분류 정확도를 검증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Lung Cancer Classification Based on Ensembling EfficientNet Using Histopathology Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10913233,"Lung cancer is a leading cause of cancer-related deaths, and accurate, early diagnosis is critical for effective treatment. Histopathological analysis is a standard diagnostic approach but requires significant expertise and time. This study aims to improve lung cancer classification through an ensemble of EfficientNetV2 models (B0-B3) applied to histopathological images. EfficientNetV2 was chosen for its scalability and strong performance in image classification tasks. Data augmentation was used to enhance robustness, simulating variability in histopathological slides, while transfer learning from ImageNet pre-trained models enabled faster convergence with limited data. The models were trained on the LC25000 dataset, containing augmented images, and evaluated individually and in ensemble configurations. Grad-CAM provided interpretability, generating heatmaps that highlight model focus, aiding in understanding decision-making. Results showed that individual EfficientNetV2 models achieved near-perfect accuracy, with the ensemble approach further improving performance. Ensemble models, particularly those using hard voting, achieved up to 100% accuracy, precision, and recall, underscoring the effectiveness of combined predictions. However, the high accuracy may be partially due to the dataset's limited unique images, as repeated patterns in augmented data might inflate performance. Future work will test the ensemble on larger, more diverse datasets to validate generalizability. These findings demonstrate the potential of EfficientNetV2 ensembles in lung cancer diagnostics, paving the way for reliable, interpretable AI-based pathology tools in clinical settings.",Yes,"본 논문은 EfficientNetV2 모델 앙상블을 활용한 폐암 분류라는 구체적이고 독창적인 연구 방법을 제시하고, 실험을 통해 성능을 평가한 연구 논문입니다. 데이터 증강, 전이 학습, 해석 가능성 기법 적용 등 직접적인 연구 기여가 포함되어 있습니다."
Will Fault Localization Work for These Failures? An Automated Approach to Predict Effectiveness of Fault Localization Tools,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676902,"Debugging is a crucial yet expensive activity to improve the reliability of software systems. To reduce debugging cost, various fault localization tools have been proposed. A spectrum-based fault localization tool often outputs an ordered list of program elements sorted based on their likelihood to be the root cause of a set of failures (i.e., their suspiciousness scores). Despite the many studies on fault localization, unfortunately, however, for many bugs, the root causes are often low in the ordered list. This potentially causes developers to distrust fault localization tools. Recently, Parnin and Orso highlight in their user study that many debuggers do not find fault localization useful if they do not find the root cause early in the list. To alleviate the above issue, we build an oracle that could predict whether the output of a fault localization tool can be trusted or not. If the output is not likely to be trusted, developers do not need to spend time going through the list of most suspicious program elements one by one. Rather, other conventional means of debugging could be performed. To construct the oracle, we extract the values of a number of features that are potentially related to the effectiveness of fault localization. Building upon advances in machine learning, we process these feature values to learn a discriminative model that is able to predict the effectiveness of a fault localization tool output. In this preliminary work, we consider an output of a fault localization tool to be effective if the root cause appears in the top 10 most suspicious program elements. We have experimented our proposed oracle on 200 faulty programs from Space, NanoXML, XML-Security, and the 7 programs in Siemens test suite. Our experiments demonstrate that we could predict the effectiveness of fault localization tool with a precision, recall, and F-measure (harmonic mean of precision and recall) of 54.36%, 95.29%, and 69.23%. The numbers indicate that many ineffective fault localization instances are identified correctly, while only very few effective ones are identified wrongly.",Yes,"본 논문은 결함 위치 도구의 효과성을 예측하는 오라클을 구축하고, 머신러닝 기법을 활용하여 예측 모델을 학습하는 독창적인 연구 내용을 포함하고 있다. 또한, 다양한 프로그램을 대상으로 실험을 수행하여 예측 성능을 평가한 점에서 직접적인 연구 기여가 있다고 판단된다."
An Occlusion Compensation Learning Framework for Improving the Rendering Quality of Light Field,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241065,"Occlusions are common phenomena in light field rendering (LFR) technology applications. The 3-D spatial structures of some features may be missing or incorrect when capturing some samples due to occlusion discontinuities. Most prior works on LFR, however, have neglected occlusions from other objects in 3-D scenes that do not participate in the capturing and rendering of the light field. To improve rendering quality, this report proposes an occlusion probability learning framework (OPLF) based on a deep Boltzmann machine (DBM) to compensate for the occluded information. In the OPLF, an occlusion probability density model is applied to calculate the visibility scores, which are modeled as hidden variables. Additionally, the probability of occlusion is related to the visibility, the camera configuration (i.e., position and direction), and the relationship between the occlusion object and occluded object. Furthermore, a deep probability model based on the OPLF is used for learning the occlusion relationship between the camera and object in multiple layers. The proposed OPLF can optimize the LFR quality. Finally, to verify the claimed performance, we also compare the OPLF with the most advanced occlusion theory and light field reconstruction algorithms. The experimental results show that the proposed OPLF outperforms other known occlusion quantization schemes.",Yes,"본 논문은 빛장 렌더링 품질 향상을 위한 새로운 학습 프레임워크(OPLF)를 제안하고, 이를 딥 볼츠만 머신 기반의 확률 모델로 구현하여 직접적인 연구 기여를 하고 있다. 또한, 기존 방법들과의 성능 비교 실험을 통해 제안 기법의 우수성을 검증하였으므로 독창적인 연구 내용이 포함된 연구 논문으로 판단된다."
"Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805162,"Wireless sensor networks (WSNs) monitor dynamic environments that change rapidly over time. This dynamic behavior is either caused by external factors or initiated by the system designers themselves. To adapt to such conditions, sensor networks often adopt machine learning techniques to eliminate the need for unnecessary redesign. Machine learning also inspires many practical solutions that maximize resource utilization and prolong the lifespan of the network. In this paper, we present an extensive literature review over the period 2002–2013 of machine learning methods that were used to address common issues in WSNs. The advantages and disadvantages of each proposed algorithm are evaluated against the corresponding problem. We also provide a comparative guide to aid WSN designers in developing suitable machine learning solutions for their specific application challenges.",No,"초록에서 이 논문은 2002년부터 2013년까지 발표된 기존 연구들을 종합적으로 검토하는 문헌 리뷰임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 새로운 알고리즘 제안이 아닌, 기존 연구들의 정리와 비교 분석에 중점을 둔 논문으로 판단됩니다."
A Comparative Machine Learning Approaches for Patient Flow Forecasting in an Emergency Department during the COVID-19,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938025,"The Corona Virus Disease 2019 (COVID-19) has impacted numerous areas of the health system. In fact, it made the world work remotely during several months and created an assorted uncertainty for medical service recipients. Thus, anticipating novel everyday patient income in relation to the COVID-19 has become pivotal for clinical, political, and different authorities who handle on a daily basis, COVID-19 related planned operations. Current machine learning draws near, in an attempt to get dynamic results. This work intends to demonstrate the wayan Emergency Department (ED) is able to use machine-learning approaches during the daily patient flow forecasting for better management in an emergency department. Thus, it is essential to test five different supervised machine-learning approaches by evaluating their coefficient of determination (R2) to figure the everyday patient flow income for better management.",Yes,논문 초록에서 다섯 가지 감독 학습 기법을 테스트하고 환자 흐름 예측을 위한 모델 성능을 평가하는 등 직접적인 연구 방법과 실험을 수행한 내용이 포함되어 있어 독창적인 연구 결과를 제시하는 연구 논문으로 판단됩니다.
A Hybrid Deep Random Neural Network for Cyberattack Detection in the Industrial Internet of Things,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399085,"The Industrial Internet of Things (IIoT) refers to the use of traditional Internet of Things (IoT) concepts in industrial sectors and applications. IIoT has several applications in smart homes, smart cities, smart grids, connected cars, and supply chain management. However, these systems are being more frequently targeted by cybercriminals. Deep learning and big data analytics have great potential in designing and developing robust security mechanisms for IIoT networks. In this paper, a novel hybrid deep random neural network (HDRaNN) for cyberattack detection in the IIoT is presented. The HDRaNN combines a deep random neural network and a multilayer perceptron with dropout regularization. The proposed technique is evaluated using two IIoT security-related datasets: (i) DS2OS and (ii) UNSW-NB15. The performance of the proposed scheme is analyzed through a number of performance metrics such as accuracy, precision, recall, F1 score, log loss, Region of Convergence (ROC), and Area Under the Curve (AUC). The HDRaNN classified 16 different types of cyberattacks using with higher accuracy of 98% and 99% for DS2OS and UNSW-NB15, respectively. To measure the effectiveness of the proposed scheme, the performance metrics are also compared with several state-of-the-art attack detection algorithms. The findings of HDRaNN proved its superior performance over other DL-based schemes. The deployment perspective of the proposed work is also highlighted in this work.",Yes,"본 논문은 IIoT 사이버 공격 탐지를 위한 새로운 하이브리드 딥 러닝 모델(HDRaNN)을 제안하고, 두 개의 데이터셋을 사용해 성능을 평가하며 기존 기법과 비교 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Remaining Useful Life Prediction of Turbofan Engines with Fuzzy Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10814888,"Since the beginning of the 21st century, predictive maintenance (PdM) has gained increasing prominence in the industry, providing the capability to predict the Remaining Useful Life (RUL) of equipment and mitigate accidents and financial losses. In this context, Machine Learning (ML) models are widely employed. This study proposes the prediction of Turbofan engine RUL through a machine learning model trained with historical data from this equipment. Two distinct models were evaluated: the Fuzzy System and the Neuro-Fuzzy System. To enable training, it was necessary to select the best features using a genetic algorithm, aiming to reduce complexity and enhance the performance of the Fuzzy System. The model with the best performance was the Fuzzy System with 5 selected features and 11 Fuzzy sets. Despite not yielding the lowest RMSE metric compared to related works, this interpretable model demonstrated reasonable performance in comparison to the model by Babu et al. [8]. This suggests that, for the dataset in question, the Fuzzy system is recommended to ensure better interpretability, while neural networks used in previous studies are better suited for precise predictions.",Yes,"본 논문은 터보팬 엔진의 잔여 수명 예측을 위해 퍼지 시스템과 신경 퍼지 시스템을 적용하고, 유전 알고리즘을 활용한 특징 선택을 통해 모델 성능을 향상시키는 독창적인 연구를 수행하였다. 이는 기존 연구와 비교 평가를 포함한 직접적인 기여가 있는 연구 논문으로 판단된다."
Time Machine: Generative Real-Time Model for Failure (and Lead Time) Prediction in HPC Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10202658,"High Performance Computing (HPC) systems generate a large amount of unstructured/alphanumeric log messages that capture the health state of their components. Due to their design complexity, HPC systems often undergo failures that halt applications (e.g., weather prediction, aerodynamics simulation) execution. However, existing failure prediction methods, which typically seek to extract some information theoretic features, fail to scale both in terms of accuracy and prediction speed, limiting their adoption in real-time production systems. In this paper, differently from existing work and inspired by current transformer-based neural networks which have revolutionized the sequential learning in the natural language processing (NLP) tasks, we propose a novel scalable log-based, self-supervised model (i.e., no need for manual labels), called Time Machine 11A Time Machine allows us to travel into the future to observe the health state of HPC system and report back. Here, we travel into the log extension to report an upcoming failure., that predicts (i) forthcoming log events (ii) the upcoming failure and its location and (iii) the expected lead time to failure. Time Machine is designed by combining two stacks of transformer-decoders, each employing the self-attention mechanism. The first stack addresses the failure location by predicting the sequence of log events and then identifying if a failure event is part of that sequence. The lead time to predicted failure is addressed by the second stack. We evaluate Time Machine on four real-world HPC log datasets and compare it against three state-of-the-art failure prediction approaches. Results show that Time Machine significantly outperforms the related works on Bleu, Rouge, MCC, and F1-score in predicting forthcoming events, failure location, failure lead-time, with higher prediction speed.",Yes,"본 논문은 HPC 시스템의 고장 예측을 위해 새로운 transformer 기반의 자기지도 학습 모델(Time Machine)을 제안하고, 이를 실제 데이터셋에 적용하여 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Exploring machine learning techniques on Yeast dataset classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235819,"Generating information from different datasets using machine learning technologies is one of the crucial and pivotal jobs in the area of data mining. In the recent years, application of these data mining is seen in healthcare, academics and different organizations. A massive application is identified in the area of healthcare. An inseparable part of healthcare organizations is pharmacological industry-i.e. industry related to producing, discovering drugs, their composition and their reaction to human body. Yeast is considered one of the major components in producing drugs for human, for its similarity in cell structure with human cell. In this proposed work, a thorough study of application of machine learning algorithms along with different validation and ensemble techniques have been explored on Yeast dataset. The models are evaluated with mean accuracy. Several other performance measures f-score, Recall, RMSE etc. also has been considered to decide the best model for the said dataset. Results show that ensemble method bagged decision tree works best with a mean accuracy of 61%.The performance of Bagged extra tree is also best with respect to other performance metrics like f1-score, precision etc.",Yes,"논문 초록에서 Yeast 데이터셋 분류를 위해 다양한 머신러닝 알고리즘과 검증, 앙상블 기법을 적용하고 성능 평가를 수행한 연구임을 명확히 밝히고 있습니다. 이는 직접적인 실험과 성능 비교를 포함한 독창적인 연구 내용으로 판단됩니다."
Predictive Traffic Control and Differentiation on Smart Grid Neighborhood Area Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274318,"Smart Grid (SG) networks include an associated data network for the transmission and reception of control data related to the electric power supply service. A subset of this data network is the SG Neighborhood Area Network (SG NAN), whose objective is to interconnect the subscribers’ homes with the supplier control center. The data flows transmitted through these SG NANs belong to different applications, giving rise to the need for different quality of service requirements. Additionally, other subscriber appliances could use this network to communicate over the Internet. To avoid network congestion, as well as to differentiate the quality of service (QoS) received by the different data flows, a congestion control mechanism with traffic differentiation capabilities is required. The main contribution of this work is the proposal of a new congestion control mechanism based on machine learning techniques to try to guarantee the different QoS requirements to the different data flows. A main problem when applying machine learning techniques is the need for datasets to be used in the training steps. In this sense, a second contribution of this article is the proposal of a method to generate such datasets by means of simulation techniques. The proposed mechanism is then evaluated in the context of a wireless SG NAN. The nodes of this network are the subscriber’s smart meters, which in turn perform the function of concentrating the data traffic sent and received by the rest of the home appliances. Besides, different machine learning classification methods are taken into account. The evaluation carried out shows significant improvements in terms of network throughput, transit time, and quality of service differentiation. Finally, the computational cost of the algorithms used in this proposal has also been evaluated, using real low-cost IoT hardware platforms.",Yes,"본 논문은 스마트 그리드 네트워크에서 트래픽 제어 및 품질 차별화를 위한 새로운 혼잡 제어 메커니즘을 머신러닝 기법을 활용해 제안하고, 시뮬레이션을 통한 데이터셋 생성 방법과 실제 평가 결과를 포함하고 있다. 이는 독창적인 연구 내용과 실험적 검증을 포함한 연구 논문에 해당한다."
Impact of Switching Variability of 65nm CMOS Integrated Hafnium Dioxide-based ReRAM Devices on Distinct Level Operations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9312855,"Limitations related to the von Neumann bottleneck have resulted in novel circuits and architectures, including designs that utilize Resistive Random Access Memory (ReRAM) as nonvolatile memory (NVM) devices. ReRAM implemented with hafnium oxide (HfO2) is a strong candidate for such applications. The non-volatility of these devices and their amenability to compute in memory functionality makes them ideal for neuromorphic applications, deep learning, and mathematical accelerator circuits (e.g. Vector Matrix Multiplication - VMM). However, these devices suffer from stochastic switching variability that currently limits their usage and performance. To realize the full potential of these devices, reliability analysis is required. In this work, a reliability study was performed using previously developed a 65 nm CMOS/Memristor process on a 300 mm wafer platform. To address the influence of switching compliance current on the variability of Low Resistance State (LRS) and High Resistance State (HRS), a total of 23 different compliance current values were implemented. The effects of temperature on device performance was also measured.",Yes,"논문은 65nm CMOS 기반의 HfO2 ReRAM 소자의 스위칭 변동성에 대한 신뢰성 분석을 수행한 연구로, 다양한 전류 조건과 온도 영향을 실험적으로 평가하여 직접적인 연구 결과를 제시하고 있다. 이는 독창적인 실험과 분석을 포함한 연구 논문에 해당한다."
Speech Recognition Applications in Enhancing Safety for Women in Built Environment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482912,"This research delves into the potential of speech recognition technology to enhance the safety of women in urban environments. Given the increasing concerns about safety for women in cities, technology presents viable solutions. Among these innovations, speech recognition stands out as a powerful security tool. The study introduces speech recognition and explores its various applications in addressing women's safety issues while identifying vulnerabilities. Additionally, it evaluates how speech recognition can enhance security measures, encompassing personal safety devices, emergency response systems, and smart city integration through case studies. Ethical considerations related to privacy and user empowerment are also discussed. The paper suggests integrating speech technology with other tools, further research initiatives, and policies to foster safer environments. In today's technological landscape, speech recognition empowers women by providing discreet emergency access and real-time monitoring capabilities in public spaces. AI-driven voice recognition algorithms continually improve accuracy and personalization based on comprehensive data sets, thus enabling safer navigation within urban areas. This review offers insights for academia, practitioners, and policymakers, advocating for collaboration to foster secure public spaces for women.",No,"초록에서 본 논문은 기존 기술과 사례를 소개하고 평가하는 리뷰 성격의 연구로 보이며, 직접적인 독창적 연구 결과나 실험 데이터가 포함되어 있지 않습니다. 따라서 새로운 연구 기여보다는 기술의 적용 가능성과 정책 제안을 중심으로 한 논문으로 판단됩니다."
Gated Stacked Target-Related Autoencoder: A Novel Deep Feature Extraction and Layerwise Ensemble Method for Industrial Soft Sensor Application,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174659,"These days, data-driven soft sensors have been widely applied to estimate the difficult-to-measure quality variables in the industrial process. How to extract effective feature representations from complex process data is still the difficult and hot spot in the soft sensing application field. Deep learning (DL), which has made great progresses in many fields recently, has been used for process monitoring and quality prediction purposes for its outstanding nonlinear modeling and feature extraction abilities. In this work, deep stacked autoencoder (SAE) is introduced to construct a soft sensor model. Nevertheless, conventional SAE-based methods do not take information related to target values in the pretraining stage and just use the feature representations in the last hidden layer for final prediction. To this end, a novel gated stacked target-related autoencoder (GSTAE) is proposed for improving modeling performance in view of the above two issues. By adding prediction errors of target values into the loss function when executing a layerwise pretraining procedure, the target-related information is used to guide the feature learning process. Besides, gated neurons are utilized to control the information flow from different layers to the final output neuron that take full advantage of different levels of abstraction representations and quantify their contributions. Finally, the effectiveness and feasibility of the proposed approach are verified in two real industrial cases.",Yes,"본 논문은 기존의 딥러닝 기반 자동인코더 방법의 한계를 극복하기 위해 새로운 게이트 스택 타겟 관련 자동인코더(GSTAE)를 제안하고, 이를 통해 산업용 소프트 센서의 성능을 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법의 효과를 실제 산업 사례에 적용하여 검증한 점에서 연구 논문으로 판단된다."
TCP-Net: Test Case Prioritization using End-to-End Deep Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787970,"Regression testing is facing a bottleneck due to the growing number of test cases and the wide adoption of continuous integration (CI) in software projects, which increases the frequency of running software builds, making it challenging to run all the regression test cases. Machine learning (ML) techniques can be used to save time and hardware resources without compromising quality. In this work, we introduce a novel end-to-end, self-configurable, and incremental learning deep neural network (DNN) tool for test case prioritization (TCP-Net). TCP-Net is fed with source code-related features, test case metadata, test case coverage information, and test case failure history, to learn a high dimensional correlation between source files and test cases. We experimentally show that TCP-Net can be efficiently used for test case prioritization by evaluating it on three different real-life industrial software packages.",Yes,"논문은 테스트 케이스 우선순위 지정을 위해 새로운 엔드투엔드 딥 뉴럴 네트워크 모델(TCP-Net)을 제안하고, 이를 실제 산업용 소프트웨어 패키지에 적용하여 실험적으로 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문으로 판단된다."
Deep Learning for Screen-Shot Image Demoiréing: A Survey,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913967,"Image demoiréing is an important image processing technology in computer vision, used to remove the moiré from images and improve the image quality. In recent years, the image demoiréing technique based on the deep learning method has gained more attention and achieved good results, but it still has some limitations. This paper aims to provide a review and perspective on the recent advances in deep learning-based image demoiréing techniques. First, the definition and production principle of the image moiré pattern are given. Common datasets and image quality evalution methods in demoiréing studies are analyzed. Then two internationally famous competitions in image demoiréing are introduced. Second, the research status of the supervised demoiréing technique is summarized from four dimensions: sampling method, model network design, baseline model, and training learning strategy. Recent progress made by the mainstream model of unsupervised deep learning in the field of image demoiréing is summarized. The typical application of the image demoiréing technique in panel defect detection and digital radiography is analyzed. The performance and the image quality of the above mentioned models based on different data sets are evaluated in detail. Finally, this paper analyzes and forelocks the problems to be solved in the coming years.",No,"본 논문은 딥러닝 기반 이미지 데모아링 기술에 대한 최신 연구 동향과 기법을 종합적으로 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 리뷰하는 데 초점이 맞춰져 있습니다. 따라서 연구 논문보다는 리뷰 논문에 해당합니다."
Prioritization of Mobile IoT Data Transmission Based on Data Importance Extracted From Machine Learning Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759870,"Predicting real-time spatial information from data collected by the mobile Internet of Things (IoT) devices is one solution to the social problems related to road traffic. The mobile IoT devices for real-time spatial information prediction generate an extremely high volume of data, making it impossible to collect all of it through mobile networks. Although some previous works have reduced the volume of transmitted data, the prediction accuracy of real-time spatial information is still not ensured. Therefore, this paper proposes an IoT device control system that reduces the amount of transmitted data used as input for real-time prediction while maintaining the prediction accuracy. The main contribution of this paper is that the proposed system controls data transmission from the mobile IoT devices based on the importance of data extracted from the machine learning model used for the prediction. Feature selection has been widely used for extracting the importance of data from the machine learning model. Feature selection methods were also used to reduce communication overhead in distributed learning. Unlike the conventional usage of feature selection methods, the proposed system uses them to control the data transmission of the mobile IoT devices with priority. In this paper, the proposed system is evaluated with a real-world vehicle mobility dataset in two practical scenarios using the random forest model, which is an extensively used machine learning model. The evaluation results show that the proposed system reduces the amount of transmitted input data for real-time prediction while achieving the same level of prediction accuracy as benchmark methods.",Yes,"본 논문은 모바일 IoT 장치에서 수집된 데이터를 기반으로 실시간 공간 정보 예측의 정확도를 유지하면서 데이터 전송량을 줄이는 새로운 시스템을 제안하고 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 머신러닝 모델에서 추출한 데이터 중요도를 활용하여 데이터 전송 우선순위를 제어하는 방법을 실험적으로 검증한 점에서 연구 논문에 해당한다."
Investigating barriers and facilitators to wearable adherence in fine-grained eating detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917597,"Energy balance is one component of weight management, but passive objective measures of caloric intake are non-existent. Given the recent success of actigraphy as a passive objective measure of the physical activity construct that relieves participants of the burden of biased self-report, computer scientists and engineers are aiming to find a passive objective measure of caloric intake. Passive sensing food intake systems have failed to go beyond the lab and into behavioral research in part due to low adherence to wearing passive monitoring systems. While system accuracy and battery lifetime are sine qua non to a successfully deployed technology, they come second to adherence, since a system does nothing if it remains unused. This paper focuses on adherence as affected by: 1) perceived data privacy; 2) stigma of wearing devices; 3) comfort. These factors highlight new challenges surrounding participant informed consent and Institutional Review Board (IRB) risk assessment. The wearables examined include neck- and wrist-worn sensors, and video camera-based systems. Findings support the potential for adherence using wrist- and shoulder-based video cameras, and personalized style-conscious neck-worn sensors. The feasibility of detecting fine-grained eating gestures to validate the machine learning models is shown, improving the potential of translation of this technology.",Yes,"본 논문은 웨어러블 기기 착용 준수에 영향을 미치는 요인들을 조사하고, 기계 학습 모델 검증을 위한 세밀한 식사 동작 탐지 가능성을 제시하는 등 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구를 바탕으로 새로운 실험적 결과와 분석을 제공하는 연구 논문에 해당한다."
Programmable Switches for in-Networking Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9488840,"Deploying accurate machine learning algorithms into a high-throughput networking environment is a challenging task. On the one hand, machine learning has proved itself useful for traffic classification in many contexts (e.g., intrusion detection, application classification, and early heavy hitter identification). On the other hand, most of the work in the area is related to post-processing (i.e., training and testing are performed offline on previously collected samples) or to scenarios where the traffic has to leave the data plane to be classified (i.e., high latency). In this work, we tackle the problem of creating simple and reasonably accurate machine learning models that can be deployed into the data plane in a way that performance degradation is acceptable. To that purpose, we introduce a framework and discuss issues related to the translation of simple models, for handling individual packets or flows, into the P4 language. We validate our framework with an intrusion detection use case and by deploying a single decision tree into a Netronome SmartNIC (Agilio CX 2x10GbE). Our results show that high-accuracy is achievable (above 95%) with minor performance degradation, even for a large number of flows.",Yes,"논문은 네트워킹 환경에서 머신러닝 모델을 데이터 플레인에 직접 배포하는 새로운 프레임워크를 제안하고, 이를 실제 장비에 적용하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Automatic Voice Disorder Detection Using Self-Supervised Representations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041907,"Many speech features and models, including Deep Neural Networks (DNN), are used for classification tasks between healthy and pathological speech with the Saarbruecken Voice Database (SVD). However, accuracy values of 80.71% for phrases or 82.8% for vowels /aiu/ are the highest reported for audio samples in SVD when the evaluation includes the wide amount of pathologies in the database, instead of a selection of some pathologies. This paper targets this top performance in the state-of-the-art Automatic Voice Disorder Detection (AVDD) systems. In the framework of a DNN-based AVDD system we study the capability of Self-Supervised (SS) representation learning for describing discriminative cues between healthy and pathological speech. The system processes the SS temporal sequence of features with a single feed-forward layer and Class-Token (CT) Transformer for obtaining the classification between healthy and pathological speech. Furthermore, there is evaluated a suitable data extension of the training set with out-of-domain data is also evaluated to deal with the low availability of data for using DNN-based models in voice pathology detection. Experimental results using audio samples corresponding to phrases in the SVD dataset, including all pathologies available, show classification accuracy values until 93.36%. This means that the proposed AVDD system achieved accuracy improvements of 4.1% without the training data extension, and 15.62% after the training data extension compared to the baseline system. Beyond the novelty of using SS representations for AVDD, the fact of obtaining accuracies over 90% in these conditions and using the whole set of pathologies in the SVD is a milestone for voice disorder-related research. Furthermore, the study on the amount of in-domain data in the training set related to the system performance show guidance for the data preparation stage. Lessons learned in this work suggest guidelines for taking advantage of DNN, to boost the performance in developing automatic systems for diagnosis, treatment, and monitoring of voice pathologies.",Yes,"본 논문은 Self-Supervised 표현 학습과 DNN 기반 모델을 활용하여 음성 장애 자동 탐지 시스템의 성능을 향상시키는 독창적인 연구를 수행하였으며, 실험을 통해 기존 방법 대비 정확도 향상을 입증하였다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Advanced Traffic Violation Detection with Tesseract OCR and Computer Vision,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10616606,"The paper introduces an intelligent system that utilizes machine learning to effectively detect traffic infractions. It incorporates adaptive stop line detection, robust licence plate extraction, and real-time traffic light monitoring. The system’s machine learning component accurately recognizes traffic signal colors, facilitating better traffic flow management. Adaptive stop line detection adjusts to various traffic situations, enhancing system flexibility. Licence plates are accurately extracted using Optical Character Recognition (OCR), which is powered by image processing and Haar Cascade classifiers. The system’s effectiveness in identifying traffic violations, especially during red lights, is demonstrated through video data testing. Additionally, the integration of a MySQL database enables comprehensive logging and tracking of penalized licence plates, providing a detailed overview of traffic infractions and keeping track of all the violating vehicles. This technological integration promises significant improvements in traffic enforcement and road safety. The combination of adaptive stop line detection, real-time traffic light analysis, and licence plate identification makes this system an invaluable tool for traffic control authorities. By improving enforcement and monitoring, the system has the potential to revolutionize traffic management and enhance safety measures. This proposed work suggests that such an intelligent system can be a sophisticated asset in addressing traffic-related challenges and ensuring safer road conditions.",Yes,"논문은 머신러닝 기반의 교통 위반 감지 시스템을 제안하고, 적응형 정지선 검출, 번호판 추출, 실시간 신호등 모니터링 등 구체적인 기술적 방법과 실험 결과를 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한, 시스템의 효과성을 검증하는 테스트와 데이터베이스 통합 등 실질적인 기여가 명확히 드러난다."
A Performance Evaluation of Machine Learning-Based Streaming Spam Tweets Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400989,"The popularity of Twitter attracts more and more spammers. Spammers send unwanted tweets to Twitter users to promote websites or services, which are harmful to normal users. In order to stop spammers, researchers have proposed a number of mechanisms. The focus of recent works is on the application of machine learning techniques into Twitter spam detection. However, tweets are retrieved in a streaming way, and Twitter provides the Streaming API for developers and researchers to access public tweets in real time. There lacks a performance evaluation of existing machine learning-based streaming spam detection methods. In this paper, we bridged the gap by carrying out a performance evaluation, which was from three different aspects of data, feature, and model. A big ground-truth of over 600 million public tweets was created by using a commercial URL-based security tool. For real-time spam detection, we further extracted 12 lightweight features for tweet representation. Spam detection was then transformed to a binary classification problem in the feature space and can be solved by conventional machine learning algorithms. We evaluated the impact of different factors to the spam detection performance, which included spam to nonspam ratio, feature discretization, training data size, data sampling, time-related data, and machine learning algorithms. The results show the streaming spam tweet detection is still a big challenge and a robust detection technique should take into account the three aspects of data, feature, and model.",Yes,"이 논문은 머신러닝 기반의 스트리밍 스팸 트윗 탐지 방법들의 성능을 평가하기 위해 대규모 데이터셋을 구축하고, 다양한 특징과 모델을 실험하여 직접적인 성능 분석 결과를 제시하고 있다. 이는 기존 연구의 공백을 메우는 독창적인 연구 기여로 판단된다."
Sentiment Analysis from Depression-Related User-Generated Contents from Social Media,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467214,"In this paper, we try to detect the sentiment levels such as positive, negative and neutral sentiments from depression related posts and comments generated in social media platforms. Social media platforms such as Facebook, Twitter are not only used for communication or building networks among connections, but also are getting useful for supporting needy peoples who are on special need or care in terms of mental support. In Facebook, there are several depression support groups, which are very much effective to provide mental support to the victims. In this paper, we try to formalize the depression-related posts and comments into a concise lexicon database and detect the sentiment levels form each instance. We have segmented the total work into two parts: sentiment detection and applying machine learning algorithms to analyze the ability to detect sentiment from such special category of texts. We have utilized python textblob package to detect the sentiment levels and applied traditional machine learning algorithms such as Naïve Bayes (NB), Decision Tree (DT), Random Forest (RF), Support Vector Machine (SVM), Sequential Minimal Optimization (SMO), Logistic Regression (LR), Adaboost (AB), Bagging (Bg), Stacking (St) and Multilayer Perceptron (MP) on the linguistic features. We have determined the precision, recall, F-measure, accuracy, ROC values for each of the classifiers. Among the classifiers Random Forest has outperformed others showing 60.54% correctly classified instance. We believe such sentiment analysis on special category of texts may lead to further investigation in natural language understandings.",Yes,"본 논문은 우울증 관련 소셜 미디어 게시글의 감정 분석을 위해 독자적인 데이터 전처리와 여러 기계 학습 알고리즘을 적용하여 성능을 평가하는 연구를 수행하고 있다. 이는 기존 연구를 단순 요약한 것이 아니라, 직접적인 실험과 분석을 포함한 독창적인 연구 내용으로 판단된다."
Quantum Machine Learning in Disease Detection and Prediction: a survey of applications and future possibilities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197142,"Quantum machine learning (QML) in the field of disease detection and prediction use quantum computing techniques and algorithms to analyze and classify large datasets of medical information, by identifying subtle patterns and predict the occurrence or progression of diseases. It involves applying machine learning techniques to data from biological and medical research, such as-genomic and proteomic data, medical imaging, electronic health records, and clinical trial data, using quantum computing algorithms and architectures to perform these analyses more efficiently and accurately than classical computing methods. This approach has the potential to provide new insights into complex biological systems and facilitate the development of more effective treatments and personalized medicine. In this paper, a systematic review of the use of QML algorithms has been conducted, which focuses on the detection and prediction of diseases among patients. The current essence of the field along with the challenges and limitations of current works have also been discussed. After evaluating the implemented and proposed methods of data analysis, algorithm development, usefulness and efficiency of the system in various disease detection and prediction, a recommendation was made on the open research scopes in this field at the end of the paper.",No,본 논문은 양자 기계 학습(QML)의 질병 탐지 및 예측 분야에서의 응용과 미래 가능성에 대한 체계적인 리뷰를 제공하는 서베이 논문입니다. 직접적인 독창적 연구 결과나 새로운 알고리즘 개발보다는 기존 연구들을 종합하고 평가하는 데 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Predictive Modeling for Smart Traffic Systems: Harnessing IoT Data Insights,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10453841,"The paper discusses the development of an IoT-based smart traffic system that utilizes real-time data and machine learning algorithms to enhance traffic safety, reduce congestion, and improve emergency response times. The exponential growth of vehicles on roads has increased traffic congestion, accidents, and environmental pollution, necessitating a smarter traffic management system. The paper reviews several studies demonstrating the potential of IoT-based traffic management solutions in improving traffic flow, enhancing safety, and reducing environmental impact. Furthermore, creating a sustainable and efficient urban environment is possible by integrating IoT-based smart traffic systems with other smart city solutions, such as smart energy and waste management. Chi-squared tests and regression models were employed to extensively analyze various variables in this study, including accident severity, weather conditions, and sunrise/sunset times. The findings rejected the null hypothesis, indicating strong associations among these factors. Furthermore, the residual frequency graph and regression analysis demonstrated the model's data fit and ability to capture predictor-response relationships. This research offers valuable insights into traffic accident causes and underscores the potential for real-time implementation to enhance traffic management and safety measures in smart cities.",Yes,"논문은 IoT 기반 스마트 교통 시스템 개발과 머신러닝 알고리즘 적용을 통한 교통 안전 및 혼잡 완화에 대한 직접적인 연구를 수행하고 있으며, 통계적 분석과 모델링 결과를 제시하여 독창적인 연구 기여를 포함하고 있다. 따라서 연구 논문에 해당한다."
Training AI to Recognize Objects of Interest to the Blind and Low Vision Community,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340454,"Recent object detection models show promising advances in their architecture and performance, expanding potential applications for the benefit of persons with blindness or low vision (pBLV). However, object detection models are usually trained on generic data rather than datasets that focus on the needs of pBLV. Hence, for applications that locate objects of interest to pBLV, object detection models need to be trained specifically for this purpose. Informed by prior interviews, questionnaires, and Microsoft’s ORBIT research, we identified thirty-five objects pertinent to pBLV. We employed this user-centric feedback to gather images of these objects from the Google Open Images V6 dataset. We subsequently trained a YOLOv5x model with this dataset to recognize these objects of interest. We demonstrate that the model can identify objects that previous generic models could not, such as those related to tasks of daily functioning – e.g., coffee mug, knife, fork, and glass. Crucially, we show that careful pruning of a dataset with severe class imbalances leads to a rapid, noticeable improvement in the overall performance of the model by two-fold, as measured using the mean average precision at the intersection over union thresholds from 0.5 to 0.95 (mAP50-95). Specifically, mAP50-95 improved from 0.14 to 0.36 on the seven least prevalent classes in the training dataset. Overall, we show that careful curation of training data can improve training speed and object detection outcomes. We show clear directions on effectively customizing training data to create models that focus on the desires and needs of pBLV.Clinical Relevance— This work demonstrated the benefits of developing assistive AI technology customized to individual users or the wider BLV community.",Yes,"논문은 시각장애인 및 저시력자를 위한 특정 객체 인식 모델을 개발하기 위해 사용자 중심 데이터를 수집하고, 이를 바탕으로 YOLOv5x 모델을 훈련시켜 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
On the effectiveness of machine and deep learning for cyber security,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8405026,"Machine learning is adopted in a wide range of domains where it shows its superiority over traditional rule-based algorithms. These methods are being integrated in cyber detection systems with the goal of supporting or even replacing the first level of security analysts. Although the complete automation of detection and analysis is an enticing goal, the efficacy of machine learning in cyber security must be evaluated with the due diligence. We present an analysis, addressed to security specialists, of machine learning techniques applied to the detection of intrusion, malware, and spam. The goal is twofold: to assess the current maturity of these solutions and to identify their main limitations that prevent an immediate adoption of machine learning cyber detection schemes. Our conclusions are based on an extensive review of the literature as well as on experiments performed on real enterprise systems and network traffic.",Yes,"논문은 기존 문헌 리뷰뿐만 아니라 실제 기업 시스템과 네트워크 트래픽을 대상으로 한 실험을 포함하고 있어, 단순한 리뷰 논문이 아니라 직접적인 연구 기여와 실험 결과를 제시하는 연구 논문으로 판단됩니다. 따라서 독창적인 연구 내용이 포함되어 있습니다."
"Case studies in computing education: Presentation, evaluation and assessment of four case study-based course design and delivery models",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044194,"Case studies have been used in different fields of university-level education already for decades. More recently, the advantages of using case studies have been realised by university-level computing educators, too. New approaches have been introduced in computing education - such as project-based learning, problem-based learning, situated learning or inquiry-based learning. Many of those approaches successfully use case studies. Despite the increasing popularity of this teaching methodology there seems to be a deep lack in any research papers or practice reports which would attempt to describe, evaluate and assess possible approaches or models in using case studies in computing education. This conference contribution reports on selected best practices of course design and delivery implemented in one of the core courses of the Bachelor of Science (Information Systems Management) degree program (BSc (ISM)) offered by the School of Information Systems (SIS) at the Singapore Management University (SMU). The paper presents, evaluates, compares and assesses four different course design and delivery models which are largely based on case studies and are extensively using this teaching methodology throughout the entire course lifecycle (starting with the course design process, delivery of face-to-face teaching sessions, student assessment process and post-mortem course review process).",Yes,"본 논문은 컴퓨팅 교육에서 사례 연구 기반의 네 가지 강의 설계 및 운영 모델을 제시하고 평가하는 내용을 포함하고 있어, 독창적인 연구 내용과 실증적 평가를 포함한 연구 논문으로 판단됩니다. 단순한 리뷰나 이론적 고찰이 아니라 실제 사례를 바탕으로 한 분석과 비교가 이루어졌기 때문입니다."
Adeptness Evaluation of Memory Based Classifiers for Credit Risk Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965029,"Banking industry is an important source of finance in any country. Credit Risk analysis is a critical and decisive task in banking sector. Loan sanction procedure can be followed based on the credit risk analysis of any customer. Automation of decision making in financial applications using best algorithms and classifiers is much useful. This work evaluates the adeptness of different Memory based classifiers on credit risk analysis. The German credit data have been taken for adeptness evaluation and is done using open source machine learning tool. The performances of different memory based classifier are analyzed and a practical guideline for selecting exceptional and well suited algorithm for credit analysis is presented. Apart from that, some discreet criteria for relating and evaluating the best classifiers are discussed.",Yes,"본 논문은 신용 위험 분석을 위해 다양한 메모리 기반 분류기의 성능을 평가하는 연구를 수행하고 있으며, 실제 데이터(독일 신용 데이터)를 사용하여 알고리즘의 적합성을 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Automate surgical tasks for a flexible Serpentine Manipulator via learning actuation space trajectory from demonstration,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487640,"Surgical robotic systems with miniaturized flexible Tendon-driven Serpentine Manipulators (TSM) have enjoyed increasing popularities among surgeons and researchers for their advantages of working in constrained and torturous human lumen such as oral cavity and upper GI tract. However, they suffer from sufficient nonlinearities and model uncertainties due to friction, tension varying, tendon slacking, etc. Model based control is insufficient to overcome such uncertainties and automate challenging surgical related tasks. The objective of this work is to automate certain clinical tasks to alleviate surgeon fatigue and promote task efficiency in kinematics free and sensor free circumstances. We present a data-driven approach based on Learning from Demonstration (LfD), which utilizes statistical machine learning models to encode system underlying dynamics and generalize smooth motor trajectories by direct actuation space learning. Motion segmentation is enabled with soft margin Support Vector Machine (soft-SVM) in complicated tasks. We also make attempts to retrieve task-specific properties by Locally Weighted Regression (LWR). We evaluated the approach on two surgical related tasks: compliant insertion and simplified Endoscopic Submucosal Dissection (ESD). The flexible TSM successfully reproduced both tasks and demonstrated superior trajectory performance. A video is available at: https://youtu.be/rLQo6xKtyMI.",Yes,"본 논문은 Learning from Demonstration 기법을 활용하여 유연한 Serpentine Manipulator의 수술 작업 자동화를 위한 데이터 기반 제어 방법을 제안하고, 실제 수술 관련 작업에 적용하여 성능을 평가한 연구 내용을 포함하고 있다. 이는 기존 연구와 차별화된 독창적인 연구 기여를 포함하는 연구 논문으로 판단된다."
Real-Time Position Falsification Attack Detection System for Internet of Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613271,"Ensuring secured and reliable dissemination of information for a mission-critical system such as the Internet of Vehicle (IoV) in real-time is of utmost importance. In this work, a False Location Detection System (FLDS) based on an optimized Ensemble Random Forest (Ens.RF) was proposed. The performance of the Ens.RF was compared with four other Machine Learning (ML) algorithms, using the Veremi dataset where five (5) different location falsification categories and one benign category were modeled. To validate the idea in this work, a performance comparison with recent work was presented. The result shows that the proposed Ens.RF outperformed other algorithms modeled in this work as well as related works with an accuracy of 99.92%",Yes,"본 논문은 최적화된 앙상블 랜덤 포레스트 기반의 위치 위조 탐지 시스템을 제안하고, 여러 머신러닝 알고리즘과의 성능 비교 및 검증을 수행하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Sentiments Detection for Amazon Product Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402414,"In this paper, we evaluated the sentiments in the present technological age over the reviews of online products, online products are used by the majority of people. They provide their feedback and then products are recommended for purchase and sale on that factor too. The large e-commerce platforms such as Flipkart, Myntra, Amazon, and many others enable their users to review the Products. To buy a commodity, the consumer will examine to have a better-quality understanding of the product and product work. The interpretation will be a really simple product polarized into positive, neutral, and negative Product checks. We may use machine learning methods to perform this experiment. Sentiment Analysis is research in which consumers are conscious of a product reaction. A Kaggle of amazon product reviews gathers the data collection used. We use various Logistic Regression, Naive Bayes, and Random Forest methodology for classifying feedback and achieving the best of precision. Among all the algorithms used we find that the Random forest machine-learning algorithm to be the most accurate.",Yes,"논문 초록에서 아마존 제품 리뷰 데이터를 활용하여 여러 머신러닝 알고리즘(Logistic Regression, Naive Bayes, Random Forest)을 적용하고, 그 중 가장 정확한 알고리즘을 도출하는 실험적 연구를 수행한 점이 명확히 드러납니다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문으로 판단됩니다."
Review of Using Various Deep Learning Techniques and Cycle-GANs with Transformer for Disease Detection and Classification in Plant Leaves,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10603311,"In the upcoming years, agricultural yields should rise by 50–60 % to assure food security and feed the world's rapidly expanding population. Considering the importance of agriculture, it is imperative to look at methods that can enhance planting practices, track agricultural conditions, identify plant illnesses, and streamline harvesting procedures. The significance of these fundamental elements has prompted in-depth investigation and the creation of numerous studies that offer solutions to these problems. To improve productivity, identifying and preventing plant diseases is crucial. For increased agricultural yields, cost savings, and decreased crop losses, early disease diagnosis is essential. Deep learning and other computer vision-based technologies can help diagnose diseases early. However, the commonality of illness traits in images and interference from the external environment pose hurdles to existing machine vision approaches. Most of the previous work on image classification used plant leaf images captured in controlled environment, having similar background, which produced good results on the same data-set but failed to generalise on the images captured in real conditions. To solve this problem real background images ere used to train the classifiers which increased the generalisation significantly on same data-set but still performed poor on cross data-set generalisation. Attempts are done to solve the problem of data having no background diversity (Background, lighting, camera setting, distance and angle of capture), by data augmentation using GANs, Specifically by using unpaired image to image translation to translate healthy images into diseased images. Earlier models were based on CycleGAN and could not increase the accuracy significantly as CycleGAN translated the latent features also with the disease region. No study within our knowledge has employed Swin transformers in the generator network to augment the CycleGAN model's ability to capture the distribution of the diseased dataset. Furthermore, the use of a transformer-based Image-to-Image translation model reveals issues related to the generation of artifacts, particularly in areas with excessively high or low pixel values. This phenomenon was noted in the present study when applying existing models to plant leaf data augmentation. Thus, by evaluating good fid, psnr, and ssim values using the Swin transformer cycle-GAN, good quality images are obtained and the background diversity issue is resolved. Therefore, classification accuracy is increased by using both actual and produced images to aid in the process. To offer farmers effective solutions for safeguarding their crops and future applications, this paper assesses various methods, delineating their advantages and disadvantages. It proposes the utilization of a hybrid deep learning architecture along with diverse strategies for the detection and classification of plant leaf diseases.",Yes,"본 논문은 기존 연구의 한계점을 지적하고 Swin transformer를 활용한 CycleGAN 모델을 제안하여 식물 잎 질병 데이터 증강 및 분류 정확도 향상에 기여하는 독창적인 연구 내용을 포함하고 있다. 또한, 새로운 하이브리드 딥러닝 아키텍처와 다양한 전략을 평가 및 제안하는 점에서 직접적인 연구 기여가 있다고 판단된다."
Practices for Engineering Trustworthy Machine Learning Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474373,"Following the recent surge in adoption of machine learning (ML), the negative impact that improper use of ML can have on users and society is now also widely recognised. To address this issue, policy makers and other stakeholders, such as the European Commission or NIST, have proposed high-level guidelines aiming to promote trustworthy ML (i.e., lawful, ethical and robust). However, these guidelines do not specify actions to be taken by those involved in building ML systems. In this paper, we argue that guidelines related to the development of trustworthy ML can be translated to operational practices, and should become part of the ML development life cycle. Towards this goal, we ran a multi-vocal literature review, and mined operational practices from white and grey literature. Moreover, we launched a global survey to measure practice adoption and the effects of these practices. In total, we identified 14 new practices, and used them to complement an existing catalogue of ML engineering practices. Initial analysis of the survey results reveals that so far, practice adoption for trustworthy ML is relatively low. In particular, practices related to assuring security of ML components have very low adoption. Other practices enjoy slightly larger adoption, such as providing explanations to users. Our extended practice catalogue can be used by ML development teams to bridge the gap between high-level guidelines and actual development of trustworthy ML systems; it is open for review and contributions.",Yes,"이 논문은 다중 문헌 리뷰와 글로벌 설문조사를 통해 14개의 새로운 신뢰성 있는 ML 개발 관행을 도출하고, 이를 기존 관행 목록에 추가하는 등 독창적인 연구 결과를 제시하고 있습니다. 따라서 단순한 리뷰나 정책 제안이 아니라 직접적인 연구 기여가 포함된 논문으로 판단됩니다."
A Comparative Analysis on the Existing Techniques of Wheat Spike Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9456284,"Wheat is one of the essential crops for humans. A fair amount of work is done related to wheat, such as determining the phonological stages of wheat plants, the reckoning number of wheat spikes, and detecting diseases like Fusarium head blight(FHB) in wheat plants, measuring the quality of wheat grains. Wheat head detection is a primary task among all the tasks. Object detection is nowadays affecting researchers, and several approaches are related to machine learning, image processing, computer vision, and deep learning are discussed in the literature. This paper provides a comparative analysis of variegated object detection methods used in different applications on wheat head detection and various challenges for wheat detection. With comparative analysis, we are comparing mostly used CNN-based object detection models.",No,"논문 초록에서 제시된 내용은 기존의 밀 이삭 검출 기법들을 비교 분석하는 리뷰 성격의 연구로 보입니다. 독창적인 연구 결과나 새로운 방법론 제시는 없으며, 주로 기존 연구들을 정리하고 비교하는 데 초점이 맞춰져 있습니다."
Validation of a simulation tool for ship traffic noise,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271489,"There is a rising concern about the environmental status of the seas of our planet. Such concern regards several kinds of anthropogenic activities and their impact on the marine ecosystems. Part of the adverse effects are recognised to be due to various forms of acoustic emissions generated by human activities related to the exploitation of sea resources and to shipping. The main regulatory bodies at international level, in the last decade, have started to look into the problem with the aim of monitoring the environmental status of marine waters, (in particular the acoustical aspects) identify the situations where the anthropogenic impact is most intrusive and study and promote future actions for its control and reduction. The present work reports about the development and validation of a simulation tool able to predict the noise field generated by a set of ships sailing in a specific area. The simulation is based on real traffic data derived from the Automatic Identification System (AIS) normally used to monitor shipping traffic and on specific data characterising the environment of the zone of interest. The tool is meant to provide the Regulator with a means to study the present situation (with on-line AIS data) and/or study the effectiveness of possible measures devoted to reduce the acoustical impact of shipping by design and/or operative requirements. Motivation, bases and characteristics of the developed procedure are described in the paper, where a first validation of the tool is presented on the basis of data regarding a sea area off Barcelona (Spain). Data include AIS records, environmental parameters and underwater sound levels surveyed by an hydrophone placed in the area. The work has been developed in the framework of the collaborative project AQUO (Achieve QUieter Oceans by shipping noise footprint reduction), funded by the European Commission within a call of the 7th Framework Programme dedicated to the assessment and mitigation of noise impacts of the maritime transport on the marine environment, coordinated topic “The Ocean of Tomorrow”.",Yes,"본 논문은 선박 소음 시뮬레이션 도구를 개발하고 실제 데이터를 이용해 검증하는 과정을 다루고 있어, 독창적인 연구 내용과 직접적인 기여가 포함되어 있다. 또한, 해양 환경 보호를 위한 새로운 방법론을 제시하고 있어 연구 논문에 해당한다."
How to Support ML End-User Programmers through a Conversational Agent,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548214,"Machine Learning (ML) is increasingly gaining significance for enduser programmer (EUP) applications. However, machine learning end-user programmers (ML-EUPs) without the right background face a daunting learning curve and a heightened risk of mistakes and flaws in their models. In this work, we designed a conversational agent named “Newton” as an expert to support ML-EUPs. Newton's design was shaped by a comprehensive review of existing literature, from which we identified six primary challenges faced by ML-EUPs and five strategies to assist them. To evaluate the efficacy of Newton's design, we conducted a Wizard of Oz within-subjects study with 12 ML-EUPs. Our findings indicate that Newton effectively assisted ML-EUPs, addressing the challenges highlighted in the literature. We also proposed six design guidelines for future conversational agents, which can help other EUP applications and software engineering activities.",Yes,"본 논문은 ML 비전문가 프로그래머를 지원하는 대화형 에이전트 ""Newton""을 설계하고, 이를 평가하기 위한 실험 연구를 수행하여 직접적인 연구 기여를 하고 있다. 또한, 연구 결과를 바탕으로 향후 설계 지침을 제안하는 등 독창적인 연구 내용을 포함하고 있다."
Celebrity: Creation of A Database Towards an Automatic Beauty Evaluation System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147452,"In recent days, facial beauty analysis has attracted various beauty-related fields like plastic surgery, the cosmetic industry, photo retouching applications, etc., The facial beauty analysis framework depends on an aesthetic scoring mechanism and needs high quality large-scale facial image database for facial beauty prediction. Beauty analysis can be extended to the ranking of beauty pageants in beauty pageant competitions. Most of the beauty scoring systems have focused on feature fusion and selection mechanisms through hand-crafted features with limited low-quality databases which are inadequate to analyze facial beauty and lead to infancy in the research of facial beauty. This work presents a machinebased automatic evaluation system for beauty by creating and compiling a database that includes beauty pageant images of winners and runners-up of beauty pageants competition using a deep learning algorithm. To evaluate the created database, pre-trained deep-learning models are used where deep-learned facial features are extracted for learning latent representations of beauty from facial images. From experiments performed on the created dataset, the classification accuracy achieved is 76.15%, using a pre-trained CNN model, ResNet 50.",Yes,"본 논문은 얼굴 미용 평가를 위한 자동 시스템 구축을 위해 새로운 데이터베이스를 생성하고, 딥러닝 모델을 활용하여 미용 점수 예측을 수행하는 독창적인 연구 내용을 포함하고 있다. 또한, 실험을 통해 분류 정확도를 제시하며 직접적인 연구 기여를 하고 있음을 보여준다."
Temperature Compensation Schemes for In-Memory Computing using Phase-Change Memory,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074003,"The explosive growth in data-centric artificial intelligence related applications necessitates exploration of non-von Neumann computing paradigms such as in-memory computing. The ability to perform certain computational tasks within the memory unit will reduce dramatically the time and energy that is spent into shuttling the data from the memory to the processing unit. However, the nanoscale resistive memory devices that are useful for these technologies suffer from non-ideal characteristics. In this work we deal with the computational precision loss due to the strong and inhomogeneous temperature dependence of resistive devices and in particular phase-change memory. We describe a temperature compensation method that applies to resistive crossbar arrays and its realization as a peripheral circuit. We derive array-level temperature compensation functions that are remarkably effective for projected phase-change memory devices. We simulate the system and experimentally validate its efficacy in the task of matrix-vector multiplications. The computational precision is found to be equivalent to an 8-bit multiplier at elevated temperatures.",Yes,"본 논문은 상변화 메모리 기반의 인메모리 컴퓨팅에서 온도 보상 방법을 제안하고, 이를 회로 수준에서 구현하며 시뮬레이션과 실험을 통해 검증한 연구 내용을 포함하고 있다. 이는 기존 연구와 차별화된 독창적인 기여를 하는 연구 논문으로 판단된다."
Machine learning-based multi-channel evaluation pooling strategy for image quality assessment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738088,"Multi-channel peculiarity is one of the most widely accepted human visual system (HVS) models for perceptual image quality assessment (IQA). Otherwise than extensive studies of channel decomposition and intra-channel distortion measure, relatively scant research effort has been devoted to develop efficient multichannel evaluation pooling strategies. In this paper, we review and address the limitations of the conventional pooling models based on HVS sensitivities-weighted average. Instead, we explore the utilization of machine learning for this pooling problem, since machine learning can establish an optimal and generalized mapping that models the highly complex relationship between the multi-channel distortion evaluations and the perceived image quality. Experiments based on available subjective IQA databases demonstrate the rationality, reliability and robustness of our proposed scheme.",Yes,"본 논문은 기존의 다중 채널 평가 풀링 전략의 한계를 검토하고, 기계 학습을 활용한 새로운 풀링 방식을 제안하여 이미지 품질 평가 문제에 대한 최적화된 모델을 개발하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법의 타당성과 신뢰성을 실험적으로 검증하여 직접적인 연구 기여를 하고 있음을 보여준다."
Brain Vessel Segmentation Using Deep Learning—A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919808,"This article provides a comprehensive review of deep learning-based blood vessel segmentation of the brain. Cerebrovascular disease develops when blood arteries in the brain are compromised, resulting in severe brain injuries such as ischemic stroke, brain hemorrhages, and many more. Early detection enables patients to obtain more effective treatment before becoming critically unwell. Due to the superior efficiency and accuracy compared to manual segmentation and other computer-assisted diagnosis procedures, deep learning algorithms have been extensively deployed in brain vascular segmentation. This study examined current articles on deep learning-based brain vascular segmentation, which examined the proposed methodologies, particularly the network architectures, and determined the model trend. We evaluated challenges and crucial factors associated with the application of deep learning to brain vascular segmentation, as well as future research prospects. This paper will assist researchers in developing more sophisticated and robust models in the future to develop deep learning solutions.",No,"본 논문은 뇌 혈관 분할에 관한 딥러닝 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 요약하고 분석하는 리뷰 논문에 해당합니다."
Subject-aware PET Denoising with Contrastive Adversarial Domain Generalization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10656150,"Recent advances in deep learning (DL) have greatly improved the performance of positron emission tomography (PET) denoising performance. However, DL model performance can vary a lot across subjects, due to the large variability of the count levels and spatial distributions. A generalizable DL model that mitigates the subject-wise variations is highly expected toward a reliable and trustworthy system for clinical application. In this work, we propose a contrastive adversarial learning framework for subject-wise domain generalization (DG). Specifically, we configure a contrastive discriminator in addition to the UNet-based denoising module to check the subject-related information in the bottleneck feature, while the denoising module is adversarially trained to enforce the extraction of subject-invariant features. The sampled low-count realizations from the list-mode data are used as anchor-positive pairs to be close to each other, while the other subjects are used as negative samples to be distributed far away. We evaluated on 9718 F-MK6240 tau PET studies, each having 20 noise realizations with 25% fractions of events. Training, validation, and testing were implemented using 1400, 120, and 420 pairs of 3D image volumes in a subject-independent manner. The proposed contrastive adversarial DG demonstrated superior denoising performance than conventional UNet without subject-wise DG and cross-entropy-based adversarial DG.",Yes,"본 논문은 PET 영상의 노이즈 제거를 위한 새로운 대조적 적대적 학습 프레임워크를 제안하고, 이를 대규모 데이터셋에 적용하여 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
ThermAI: Exploring Temperature Analysis Through Diverse Machine Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10593875,"Meteorological forecasting is crucial in multiple industries, including agriculture, aviation, and daily routines. The objective of this inquiry is to improve temperature predictions by examining and comparing several machine learning methods, such as linear regression, decision trees, and random forests. This work aims to fill the gap in assessing machine learning models for temperature forecasting on a broader scale by utilising the comprehensive Indian meteorological dataset, which covers a wide range of geographical regions. The research utilises a thorough technique that includes gathering data, selecting relevant features, choosing appropriate models, and evaluating the results using R-squared and Mean Square Error metrics. The findings demonstrate that the Random Forest model surpasses both multiple linear regression and decision trees in terms of performance, displaying superior accuracy and reduced prediction errors. This study enhances proactive weather management and decision-making processes by offering valuable insights and tools to stakeholders in various industries. The work is organised into distinct sections that encompass a literature review, methodology, results, and conclusions, providing a comprehensive viewpoint on developments in temperature forecasting.",Yes,"논문은 다양한 머신러닝 모델을 활용하여 온도 예측 성능을 비교 분석하는 독창적인 연구를 수행하고 있으며, 구체적인 데이터셋과 평가 지표를 사용해 모델 성능을 검증한 점에서 직접 기여하는 연구 논문에 해당한다. 또한, 기존 연구의 공백을 메우고 실질적인 예측 정확도 향상에 기여하는 내용을 포함하고 있다."
Design of an explainable machine learning challenge for video interviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966320,"This paper reviews and discusses research advances on “explainable machine learning” in computer vision. We focus on a particular area of the “Looking at People” (LAP) thematic domain: first impressions and personality analysis. Our aim is to make the computational intelligence and computer vision communities aware of the importance of developing explanatory mechanisms for computer-assisted decision making applications, such as automating recruitment. Judgments based on personality traits are being made routinely by human resource departments to evaluate the candidates' capacity of social insertion and their potential of career growth. However, inferring personality traits and, in general, the process by which we humans form a first impression of people, is highly subjective and may be biased. Previous studies have demonstrated that learning machines can learn to mimic human decisions. In this paper, we go one step further and formulate the problem of explaining the decisions of the models as a means of identifying what visual aspects are important, understanding how they relate to decisions suggested, and possibly gaining insight into undesirable negative biases. We design a new challenge on explainability of learning machines for first impressions analysis. We describe the setting, scenario, evaluation metrics and preliminary outcomes of the competition. To the best of our knowledge this is the first effort in terms of challenges for explainability in computer vision. In addition our challenge design comprises several other quantitative and qualitative elements of novelty, including a “coopetition” setting, which combines competition and collaboration.",Yes,"논문은 설명 가능한 머신러닝 모델의 설계와 평가를 위한 새로운 챌린지를 제안하며, 구체적인 설정, 시나리오, 평가 지표 및 예비 결과를 포함하고 있다. 이는 기존 연구를 리뷰하는 데 그치지 않고 독창적인 연구 기여와 실험적 결과를 포함한 연구 논문으로 판단된다."
An inference mechanism using Bayes-based classifiers in pregnancy care,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749475,"Significant advances on smart decision support systems (DSSs) development have influenced important results on pregnancy care. Nevertheless, even considering the efforts to reduce the number of women deaths due to problems related to pregnancy, this decrease presented less impact than other areas of human development. Hypertensive disorders in pregnancy, particularly pre-eclampsia and eclampsia, account for significant proportion of perinatal morbidity and maternal mortality. In this context, this paper proposes an inference model that uses data mining (DM) techniques capable for operating in a data set to extract patterns and assist in knowledge discovery. Identifying hypertensive crises that complicate pregnancy, it can impact in a meaningful reduction the incidence of sequelae and death of pregnant women. Comparison between two Bayesian classifiers is performed in this work to better classify the hypertensive disorders severity. Results showed that Naïve Bayes classifier had an excellent performance, presenting better precision and F-measure, compared to the other experimented classifiers. Even finding a good performance to predict hypertensive disorders, other Bayesian methods need to be evaluated, as well as other DM techniques such as those based on artificial intelligence (AI) and tree-based methods.",Yes,"논문은 임신 관련 고혈압 질환을 분류하기 위해 베이즈 분류기를 활용한 추론 모델을 제안하고, 두 가지 베이즈 분류기를 비교 평가하는 실험적 연구를 수행하였다. 이는 독창적인 데이터 마이닝 기법 적용과 성능 평가를 포함한 직접적인 연구 기여로 판단된다."
Cheetah: An Accurate Assessment Mechanism and a High-Throughput Acceleration Architecture Oriented Toward Resource Efficiency,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146930,"Convolutional neural network (CNN) is widely used in artificial intelligence for its excellent recognition accuracy. With its scale increasing rapidly and architecture becoming complicated, it is much difficult to implement CNN in hardware platform efficiently. Many FPGA-based CNN accelerators are proposed in previous work. However, when evaluating resource efficiency, their assessment methods are: 1) device related; 2) frequency related; or 3) they confuse resource efficiency with resource occupancy. There is an insistent demand for intuitive and fair assessment criteria. When implementing CNNs, they still have improvement room in computing resource efficiency, especially for layers with large feature size and few feature maps. In this work, we propose Rscore and Cscore , which compose a comprehensive and accurate resource efficiency assessment mechanism for evaluation and design guidance, respectively. Under the guidance, we introduce Cheetah, an FPGA-based high-throughput acceleration architecture. Its computing part can optimize the use of available resources in both time and space aspects, resulting in better throughput improvement. An auxiliary storage system and a pipeline stage compression method are designed for less storage overhead and shorter inference latency. We implement AlexNet and ResNet18 on KCU1500 at 230 and 240 MHz, respectively, with a throughput of 2411.01GOP/s and 2435.05GOP/s for 16-bit quantification. Cheetah achieves an excellent average Rscore of (0.9441, 0.9456) on different FPGA devices, while the others’ mainly distribute between 0.3 and 0.8. Finally, Cheetah has 6.78X speed improvement and 1.87X power-efficiency improvement than that of Nvidia Jetson TX2, which is the fastest, most power-efficient embedded AI computing device.",Yes,"본 논문은 CNN 가속기 설계와 평가 메커니즘(Rscore, Cscore)을 새롭게 제안하고, 이를 기반으로 FPGA 아키텍처인 Cheetah를 구현하여 성능을 실험적으로 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Predicting Object Trajectories from High-Speed Streaming Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345501,"Huge amounts of loosely structured and high velocity data are now being generated by ubiquitous mobile sensing devices, aerial sensory systems, cameras and radiofrequency identification readers, which are generating key knowledge into social media behaviors, intelligent transport patterns, military operational environments and space monitoring, safety systems etc. Machine learning models and data mining techniques can be employed to produce actionable intelligence, based on predictive and prescriptive analytics. However, more data is not leading to better predictions as the accuracy of the implicated learning models hugely varies in accordance to the complexity of the given space and related data. Especially in the case of open-ended data streams of massive scale, their efficiency is put to the challenge. In this work, we employ a variety of machine learning methods and apply them to geospatial time-series surveillance data, in an attempt to determine their capacity to learn a vessels behavioral pattern. We evaluate their effectiveness against metrics of accuracy, time and resource usage. The main concept of this study is to determine the most appropriate machine-learning model capable of learning a vessels behavior and performing predictions into a future point in time. Our aim is to document the prediction accuracy of a set of traditional forecasting models and then compare this to the prediction accuracy of streaming algorithms.",Yes,"논문 초록에서 다양한 머신러닝 기법을 적용하여 선박의 행동 패턴을 학습하고 예측하는 모델의 성능을 평가하는 연구를 수행한 점으로 보아, 독창적인 연구 내용과 실험 결과를 포함한 연구 논문으로 판단됩니다. 또한 기존 예측 모델과 스트리밍 알고리즘의 정확도를 비교하는 분석도 포함되어 있어 직접적인 연구 기여가 명확합니다."
"Deepfake Disasters: A Comprehensive Review of Technology, Ethical Concerns, Countermeasures, and Societal Implications",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10767452,"Deepfakes are fake videos or pictures created using AI software to make them appear real by superimposing someone's face. This paper investigates the ethical implications of deepfake technology and its effects on society, examining the potential impacts on privacy, trust, misinformation, cyberbullying, and the well-being of individuals and communities. A thorough review was conducted by carefully collecting and categorizing approximately 50 papers from renowned academic databases and journals to ensure their validity and relevance. Each paper's relevance, methodological quality, and field contribution were carefully assessed during the selection process. A strict screening process selected 32 papers for the in-depth review conducted in this study. The research relies on these papers, which are deemed the most significant and influential in the field. In addition, this study emphasizes the imperative need for ethical guidelines and the responsible application of deepfake technology to mitigate its negative effects. To address the challenges posed by deepfakes, it is essential to raise awareness, implement regulatory measures, and promote media literacy, according to the findings of this study. This study carefully gathers, sorts, and chooses papers to give a full and useful summary of Review on Deepfakes. Finally, the paper will propose a code of conduct for the responsible creation and application of deepfake technologies.",No,"본 논문은 약 50편의 기존 연구를 수집, 분류, 평가하여 심층 리뷰를 수행한 종합적 검토 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구의 정리와 윤리적 논의에 중점을 둔 리뷰 논문에 해당한다."
Deep U-Net Based Dental Caries Detection and Classification in Panoramic X-Ray Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10859665,"Dental caries, identified as a frequent oral health issue, demands timely detection and classification for effective treatment planning and prevention of further complications. This work focuses on developing a robust automated system to detect and classify dental caries using radiology images of panoramic X-ray. The proposed automated classification system aims to accurately identify the required region of interest and categorize regions of dental caries within images of X-ray using deep convolutional learning methods. The workflow begins with acquisition of raw input images of panoramic X-ray images, containing various stages and types of dental caries followed by finetuning them. Subsequently, a custom-designed CNN architecture (U-Net) is trained on the selected dataset using appropriate loss functions and optimization techniques. Testing image sets are used to evaluate the model, and performance is assessed by accuracy, sensitivity, specificity, and other relevant metrics. A maximum of 94% of diagnostic accuracy is reported with appropriate sensitivity and specificity using U-Net based segmented images. The effectiveness related to the developed system is measured in automated finding the regions of dental caries, by minimizing the errors of manual examination by dental professionals. This automated approach holds significant potential for improving diagnostic efficiency, enhancing treatment procedures, and ultimately contributing to better oral healthcare outcomes.",Yes,"논문 초록에서 제안된 시스템은 딥러닝 기반의 U-Net 아키텍처를 활용하여 치아 우식증을 자동으로 탐지하고 분류하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 모델 학습, 평가 및 성능 지표 보고가 명확히 기술되어 있어 직접적인 연구 기여가 있음을 알 수 있습니다."
Towards clinically relevant automatic assessment of upper-limb motor function impairment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455856,"This paper is to develop an automated assessment system of upper-limb motor function impairment for clinical environment. Although we had proposed the system in our previous work, there are some rooms to be improved. Using glove sensor was difficult due to stroke patient's hand contracture. Moreover, it was based on machine learning, and thus required huge effort to collect reference data to increase classification accuracy. To address those issues, three tests of Fugl-Meyer Assessment which were closely related the issues were chosen as target tests. Since Kinect v2 and force-sensing resister can provide hand-related information, the tests were automated without glove sensor. Fuzzy-logic classification table that is based on traditional FMA guidelines was implemented to rate the FMA score without machine learning. With a healthy subject, simple experiments were conducted to evaluate the proposed system with novel classification scheme. The results show a feasibility for more convenient automated assessment of upper-limb motor function impairment.",Yes,"본 논문은 기존 연구의 한계를 개선하여 새로운 자동 평가 시스템을 개발하고, 이를 평가하는 실험을 수행한 독창적인 연구 내용을 포함하고 있다. 또한, 기계학습 대신 퍼지 논리 기반 분류 방식을 도입하여 임상 환경에 적합한 평가 방법을 제안하고 있어 연구 논문에 해당한다."
A Deep Reinforcement Learning Approach for Improving Age of Information in Mission-Critical IoT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9692982,"The emerging mission-critical Internet of Things (IoT) play a vital role in remote healthcare, haptic interaction, and industrial automation, where timely delivery of status updates is crucial. The Age of Information (AoI) is an effective metric to capture and evaluate information freshness at the destination. A system design based solely on the optimization of the average AoI might not be adequate to capture the requirements of mission-critical applications, since averaging eliminates the effects of extreme events. In this paper, we introduce a Deep Reinforcement Learning (DRL)-based algorithm to improve AoI in mission-critical IoT applications. The objective is to minimize an AoI-based metric consisting of the weighted sum of the average AoI and the probability of exceeding an AoI threshold. We utilize the actor-critic method to train the algorithm to achieve optimized scheduling policy to solve the formulated problem. The performance of our proposed method is evaluated in a simulated setup and the results show a significant improvement in terms of the average AoI and the AoI violation probability compared to the related-work.",Yes,"본 논문은 미션 크리티컬 IoT에서 정보 신선도(AoI)를 개선하기 위한 딥 강화학습 기반 알고리즘을 제안하고, 이를 actor-critic 방법으로 학습시켜 최적 스케줄링 정책을 도출하는 독창적인 연구 내용을 포함하고 있다. 또한 시뮬레이션을 통해 제안 기법의 성능을 평가한 점에서 직접적인 연구 기여가 명확하다."
Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776343,"Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt ""auras"". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.",Yes,"본 논문은 뇌전증 발작 예측을 위해 EEG 신호에 딥러닝과 전이학습 전략을 적용한 독창적인 연구를 수행하였으며, 여러 데이터셋을 활용해 예측 성능을 향상시킨 결과를 제시하고 있다. 이는 직접적인 연구 기여와 새로운 방법론 적용을 포함하는 연구 논문에 해당한다."
Assessing Application Portfolios of IT Services through Maturity Levels of IT Governance,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874902,"Managing application portfolios of Information Technology (IT) services are not limited to information services. There should be guarantee and integration of synchronization and interoperability of information services. Availability of information services can create risks of IT investment and hinder the effectiveness of organizational performance. The aim of this research was to find out to what extent the use of application portfolios of IT services could support IT processes of Acquire and Implement (AI) and Monitor and Evaluate (ME) Domains. This survey research applied the combination method and a convergent triangulation model through a follow-up explanatory design. Quantitative analysis was performed after online questionnaires were filled out by respondents working at 65 credit unions. These respondents have used application portfolios of IT services in West Kalimantan, Indonesia. COBIT (Control Objectives for Information and Related Technology) 4.1 Framework was in use to measure maturity levels of IT governance. Maturity values of IT governance indicate that IT processes are at the scale interval of 2.51 to 3.50. So far, the use of application portfolios of IT services has been restricted to procedure standardization and documentation system. Also, the conduct has not involved appropriateness of obvious synchronization as well as consistency of procedures and IT service implementation.",Yes,"본 논문은 IT 서비스의 애플리케이션 포트폴리오가 IT 거버넌스 성숙도에 미치는 영향을 실증적으로 조사한 설문조사 연구로, 직접적인 데이터 수집과 분석을 통해 독창적인 연구 결과를 제시하고 있다. 따라서 연구 논문에 해당한다."
A Fault Diagnosis Model for Wind Turbine Blade Using a Deep Learning Method,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10353888,"A wind turbine plays a pivot role in the field of energy supply. A stable working state of the wind turbine is highlighted in power generation. For this reason, it is thus of great interest to develop a fault diagnosis method for status identification of wind turbine. In this work, the imbalance of turbine blade is investigated on the task of fault diagnosis. Vibration sensors are attached to the blade to detect vibration signals. A deep learning based method using gated convolution neural network and long short term memory unit, together with the attention mechanism, is established. The application of the proposed model is capable of capturing the most-related features that characterize the working states. The working performance of the proposed model is validated on the samples from various working conditions. The experimental results set solid evidence of a high accuracy.",Yes,"본 논문은 풍력 터빈 블레이드의 결함 진단을 위해 딥러닝 기반의 새로운 모델을 제안하고, 이를 실험적으로 검증한 연구 내용을 포함하고 있다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 판단된다."
EXTRA: An Experience-driven Control Framework for Distributed Stream Data Processing with a Variable Number of Threads,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521325,"In this paper, we present design, implementation and evaluation of a control framework, EXTRA (Experience-driven conTRol frAmework), for scheduling in general-purpose Distributed Stream Data Processing Systems (DSDPSs). Our design is novel due to the following reasons. First, EXTRA enables a DSDPS to dynamically change the number of threads on the fly according to system states and demands. Most existing methods, however, use a fixed number of threads to carry workload (for each processing unit of an application), which is specified by a user in advance and does not change during runtime. So our design introduces a whole new dimension for control in DSDPSs, which has a great potential to significantly improve system flexibility and efficiency, but makes the scheduling problem much harder. Second, EXTRA leverages an experience/data driven model-free approach for dynamic control using the emerging Deep Reinforcement Learning (DRL), which enables a DSDPS to learn the best way to control itself from its own experience just as a human learns a skill (such as driving and swimming) without any accurate and mathematically solvable model. We implemented it based on a widely-used DSDPS, Apache Storm, and evaluated its performance with three representative Stream Data Processing (SDP) applications: continuous queries, word count (stream version) and log stream processing. Particularly, we performed experiments under realistic settings (where multiple application instances are mixed up together), rather than a simplified setting (where experiments are conducted only on a single application instance) used in most related works. Extensive experimental results show: 1) Compared to Storm’s default scheduler and the state-of-the-art model-based method, EXTRA substantially reduces average end-to-end tuple processing time by 39.6% and 21.6% respectively on average. 2) EXTRA does lead to more flexible and efficient stream data processing by enabling the use of a variable number of threads. 3) EXTRA is robust in a highly dynamic environment with significant workload change.",Yes,"논문은 분산 스트림 데이터 처리 시스템에서 스레드 수를 동적으로 조절하는 새로운 제어 프레임워크(EXTRA)를 설계, 구현 및 평가한 독창적인 연구 내용을 포함하고 있습니다. 또한, 딥 강화학습을 활용한 경험 기반 제어 방법을 제안하고 실제 시스템에 적용하여 성능을 검증한 점에서 연구 논문으로 판단됩니다."
The Supermarket Model With Known and Predicted Service Times,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695323,"queues at random and join the one with the fewest customers. This model demonstrates the power of even small amounts of choice, as compared to simply joining a queue chosen uniformly at random, for load balancing systems. In this work we perform simulation-based studies to consider variations where service times for a customer are predicted, as might be done in modern settings using machine learning techniques or related mechanisms. Our primary takeaway is that using even seemingly weak predictions of service times can yield significant benefits over blind First In First Out queueing in this context. However, some care must be taken when using predicted service time information to both choose a queue and order elements for service within a queue; while in many cases using the information for both choosing and ordering is beneficial, in many of our simulation settings we find that simply using the number of jobs to choose a queue is better when using predicted service times to order jobs in a queue. In our simulations, we evaluate both synthetic and real-world workloads–in the latter, service times are predicted by machine learning. Our results provide practical guidance for the design of real-world systems; moreover, we leave many natural theoretical open questions for future work, validating their relevance to real-world situations.",Yes,"본 논문은 시뮬레이션 기반 연구를 통해 예측된 서비스 시간을 활용한 큐잉 모델의 성능을 분석하고, 머신러닝 기법을 적용한 실제 워크로드에 대한 평가를 수행하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 실무적 설계 지침과 이론적 문제 제기를 통해 직접적인 연구 기여를 하고 있음을 알 수 있다."
"Precision Biometrics Based on PPG Measured From an IoT Device With OPDs, Real-Time Quality Check Through PSD, DC Drift, and Deep Learning",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670010,"A high-accuracy biometric identification system based on photoplethysmography (PPG) is proposed in this study. Equipped with continuous quality assessment on PPG in real-time by calculated power spectral density (PSD) and large-area organic photodetectors (OPDs) in the PPG sensor offering low-noise PPG, the deep learning model built herein is able to acquire delicate PPG features varying clearly from subject to subject, and then achieves high accuracy for biometric applications. It is known that PPG is a technology capable of measuring blood volume changes by emitting optical power into skin, reaching blood vessels and collects the reflected optical power back and out of skin, suitable for ensuring live body biometrics while many other biometrics are unable to. The raw PPG measured by the PPG device is first preprocessed by a bandpass filter, and then those with low PSD of PPG versus noise or large direct current drifts are screened out in real time to ensure the signal quality of PPG prior to biometrics. This preprocessing step is crucial to disregard all the unqualified PPG that may lead to wrongful result of biometrics later. The biometrics is next conducted by a built deep-learning (DL) model of a convolutional neural network (CNN) and long short-term memory (LSTM) layers. The DL model is trained by the PPG data collected from 42 subjects. Experimental results show an accuracy of 99.64% for binary while 98.8% for multiclass classification, outperforming other related works using PPG.",Yes,"본 논문은 IoT 장치에서 측정한 PPG 신호를 이용한 생체인식 시스템을 제안하고, 실시간 신호 품질 평가 및 딥러닝 모델을 통한 분류 정확도 향상에 대한 실험적 결과를 포함하고 있어 독창적인 연구 내용을 직접적으로 다루고 있다. 따라서 연구 논문에 해당한다."
Performance Analysis of Regression and Artificial Neural Network Schemes for Dynamic Model Reduction of Power Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633912,"The performance of regression and artificial neural network schemes is evaluated for dynamic model reduction of power systems. The evaluation criterion is based on the goodness of fit in each reduced model with respect to the original model. Multiple linear regression, polynomial regression, and support vector are used as regression models while a Feedforward Artificial Neural Network with different activation functions is used for comparison with regression models. All simulations are based on a simplified Australian 14 Generator model. Datasets for training and test sets are obtained by measuring boundary bus properties and power flowing through tie lines. The simulation results show that the artificial neural network outperforms the regression models in making a reduced model of the power system, but only related to the system responses corresponding to the contingencies that were used for training. However, they perform poorly for unknown contingencies. Research work is being continued by the authors to create better models by combining classical models with machine learning techniques.",Yes,"본 논문은 전력 시스템의 동적 모델 축소를 위해 회귀 및 인공신경망 기법을 평가하는 독창적인 연구를 수행하고 있으며, 다양한 모델을 비교 분석한 시뮬레이션 결과를 제시하고 있다. 이는 기존 연구를 바탕으로 새로운 성능 평가와 모델링 기법을 제안하는 직접적인 연구 기여로 판단된다."
Comprehensive Review of Drones Collision Avoidance Schemes: Challenges and Open Issues,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10477874,"In the contemporary landscape, the escalating deployment of drones across diverse industries has ushered in a consequential concern, including ensuring the security of drone operations. This concern extends to a spectrum of challenges, encompassing collisions with stationary and mobile obstacles and encounters with other drones. Moreover, the inherent limitations of drones, namely constraints on energy consumption, data storage capacity, and processing power, present formidable obstacles in developing collision avoidance algorithms. This review paper explores the challenges of ensuring safe drone operations, focusing on collision avoidance. We explore collision avoidance methods for UAVs from various perspectives, categorizing them into four main groups: obstacle detection and avoidance, collision avoidance algorithms, drone swarm, and path optimization. Additionally, our analysis delves into machine learning techniques, discusses metrics and simulation tools to validate collision avoidance systems, and delineates local and global algorithmic perspectives. Our evaluation reveals significant challenges in current drone collision prevention algorithms. Despite advancements, critical UAV network and communication challenges are often overlooked, prompting a reliance on simulation-based research due to cost and safety concerns. Challenges encompass precise detection of small and moving obstacles, minimizing path deviations at minimal cost, high machine learning and automation expenses, prohibitive costs of real testbeds, limited environmental comprehension, and security apprehensions. By addressing these key areas, future research can advance the field of drone collision avoidance and pave the way for safer and more efficient UAV operations.",No,"본 논문은 드론 충돌 회피 기법에 대한 포괄적인 리뷰를 제공하는 논문으로, 기존 연구들을 정리하고 도전 과제와 향후 연구 방향을 제시하는 내용이다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함한 연구 논문으로 보기 어렵다."
A Study on Safety Issues of Human-Machine Transitions in Autonomous Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408640,"With the rapid development of artificial intelligence, sensor and computer, autonomous driving has become an important field and future trend of the automobile industry. However, it is still impossible to achieve the autonomous driving in all conditions in the short term, which may give rise to safety problems because of transitions between human and machine in the autonomous driving. This paper studied the definition and categorization of human-machine transitions, and proposed key issues concerning environment perception, personnel training and the transition mechanism by looking at the development of human-machine transitions in other industries. Also, this paper made a review of domestic and foreign literatures on these issues. The environment perception was found to be a key factor affecting human-machine transitions. There have been studies on factors that could influence the perception of drivers. However, the influencing mechanism remains unclear, preventing it from being applied in the design of human-machine transitions. Researchers have reached consensus on the significance of pre-use training on autonomous driving and compared the effects of various training programs. However, a comprehensive training system is not available yet, and there are no studies on the training criteria, preventing the accessibility of regulatory training in the short term. The design of human-machine transitions is still in the initial stage, consisting of a one-time reminder to the driver and waiting for taken over. Some researchers are working on the multi-step transition design and attempting to apply it in the real scenario, however its effects still need to be validated. Finally, the paper proposed suggestions on studies of human-machine transition based on domestic and foreign research.",No,"본 논문은 기존 문헌과 연구들을 종합적으로 검토하고 인간-기계 전환의 안전 문제에 대해 개념적 정의와 주요 이슈를 제시하는 리뷰 성격의 연구로 보입니다. 직접적인 실험, 데이터 분석, 또는 독창적인 연구 결과를 포함하지 않아 연구 논문으로 보기 어렵습니다."
IoT and Machine Learning-Based Systems for Predicting Cattle Health Status for Precision Livestock Farming,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083995,"The livestock farming industry has turned into a focal point for research nowadays. Particularly, the animal health conditions are of utmost importance because the human beings are dependent on animals in order to fulfil their requirements of dairy products and meat. Instead of reacting to diseases after their outbreak, the modern technologies present a possibility for monitoring the key health parameters of animals on a regular basis such as amount of food and fluids consumed, movement, body condition score etc. By collecting this information and applying advanced artificial intelligence and machine learning, the farmers can recognize, foresee and avert the outbreak of animal diseases, which can aid them in better decision making and perform timely interventions. A review of the literature on the use of Internet of Things and machine learning based systems to predict the health status of cattle by measuring their health parameters using different types of sensors and other wearable technologies for precision livestock farming is presented in this study.",No,"초록에서 제시된 내용은 기존 문헌을 검토하는 리뷰 연구임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험 데이터에 대한 언급이 없다. 따라서 본 논문은 연구 논문이 아닌 문헌 리뷰에 해당한다."
A Machine Learning Based Approach To Detect Fake News In Social Media,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10541688,"The News available on online resources such as social media applications like facebook, twitter etc. are propagating fast that too in a uncontrolled way and is a matter of high concern in its capability to endure a lot of social and national damage. This damage can prove to be destructive. A lot of researchers are working on detecting this unreliable and falsified information. In this paper, we will be making a short analysis on research related work done till now on fake news detection, using supervised ML algorithms which can catgorize news as true or false by utilising python modules such as sci-kit learn and NLP module for literal analysis. This paper is more focused towards NLP rather than the machine learning domain. This textual analysis process will include processes like feature extraction and vectorization which will be performed using python sci-kit module to carry out feature extraction and pre-processing of text from the dataset, as this library deliver helpful tools like implementation of vectorizer for feature extraction process. Then we will evaluate the performance of different ML algorithms based on several performance factors like accuracy, F1 score, precision and recall. Also, we will work on selecting the best features for the machine learning model to give high precision in detecting fake news. Today the majority of the people have a phone with them and are connected to people around the world through the means of social networking sites. These sites gave everyone the power to keep their opinions but today people are using this power to spread mis-information. This is causing mistrust among people and society. Today fake news is the major reason for the communal riots, government distrust, and a lot more.",No,"초록 내용은 기존 연구들을 분석하고 여러 머신러닝 알고리즘을 적용해 성능을 평가하는 수준에 머물러 있으며, 독창적인 연구 방법론이나 새로운 기여가 명확히 제시되어 있지 않습니다. 따라서 직접 기여하는 독창적인 연구 논문으로 보기 어렵습니다."
Surface Water Monitoring Systems—The Importance of Integrating Information Sources for Sustainable Watershed Management,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089410,"The complex interactions from anthropogenic activities, climate change, sedimentation and the input of wastewater has significantly affected the aquatic environment and entire ecosystem. Over the years, the researchers have investigated water monitoring approaches in terms of traditional monitoring or even integrated systems to handle such an environmental assessment and predictions based on warning systems. However, research into the selection and optimization of water monitoring systems by the combination of parallel approach in terms of sampling techniques, process analysis and results is limited. The research objectives of the present study are to evaluate the existing water monitoring systems based on the latest approach and then provide insights into factors affecting sensor implementation at sampling locations. Here we summarize the advancement and trends of various water monitoring systems as well as the suitability of sensor placement in the area by reviewing more than 300 papers published between 2011 and 2022. The research highlights the urgency of an integrative approach with regard to water monitoring systems including water quality model and water quantity model. A framework is proposed to incorporate all water monitoring approaches, sampling techniques, and predictive models to provide comprehensive information about environmental assessment. It was observed that the urgency of model-based approaches as verification and fusion of data assemble has the ability to improve the performances of the systems. Furthermore, integrated systems with the inclusion of a separate modeling approach through integrated, semi-mechanistic models, data science and artificial intelligence are recommended in the future. Overall, this study provides guidelines for achieving standardized water management by implementing integrated water monitoring systems.",No,"본 논문은 300편 이상의 기존 연구를 종합하여 수질 모니터링 시스템의 현황과 통합적 접근의 중요성을 논의하는 리뷰 논문으로 보입니다. 직접적인 실험이나 새로운 데이터 분석, 독창적인 연구 결과를 제시하기보다는 기존 연구를 평가하고 통합적 프레임워크를 제안하는 데 중점을 두고 있습니다."
A hybrid convolutional neural networks with extreme learning machine for WCE image classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419037,"Wireless Capsule Endoscopy (WCE) is considered as a promising technology for non-invasive gastrointestinal disease examination. This paper studies the classification problem of the digestive organs for wireless capsule endoscopy (WCE) images aiming at saving the review time of doctors. Our previous study has proved the Convolutional Neural Networks (CNN)-based WCE classification system is able to achieve 95% classification accuracy in average, but it is difficult to further improve the classification accuracy owing to the variations of individuals and the complex digestive tract circumstance. Research shows that there are two possible approaches to improve classification accuracy: to extract more discriminative image features and to employ a more powerful classifier. In this paper, we propose to design a WCE classification system by a hybrid CNN with Extreme Learning Machine (ELM). In our approach, we construct the CNN as a data-driven feature extractor and the cascaded ELM as a strong classifier instead of the conventional used full-connection classifier in deep CNN classification system. Moreover, to improve the convergence and classification capability of ELM under supervision manner, a new initialization is employed. Our developed WCE image classification system is named as HCNN-NELM. With about 1 million real WCE images (25 examinations), intensive experiments are conducted to evaluate its performance. Results illustrate its superior performance compared to traditional classification methods and conventional CNN-based method, where about 97.25% classification accuracy can be achieved in average.",Yes,"본 논문은 WCE 이미지 분류를 위한 하이브리드 CNN과 ELM 기반의 새로운 분류 시스템을 제안하고, 약 100만 개의 실제 데이터를 사용한 실험을 통해 기존 방법 대비 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Machine Learning-Driven Optimization of Inverter Drive Parameters for Enhanced Electric Vehicle Efficiency: An In-Depth Analysis and Application,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235206,"Inverter drives play a crucial role in various industrial applications, where optimizing their parameters is essential for enhancing efficiency and performance. This work presents an advanced technique to optimize the DC Link Voltage (u_dc_k) parameter in inverter drives using machine learning techniques. Traditional methods of parameter optimization have faced challenges in terms of complexity and adaptability. This work proposes a data-driven approach that leverages machine learning models, including linear regression, decision tree, and random forest, to predict the optimized u_dc_k values. The dataset, comprising multiple features related to inverter drives, undergoes pre-processing to handle missing data and scale features. The performance of the machine learning models is evaluated using the root mean squared error (RMSE) metric, and k-fold cross-validation is employed for model comparison. Our results demonstrate the effectiveness of machine learning techniques in predicting optimized u_dc_k values, with the best-performing model yielding improved performance compared to traditional methods.",Yes,"본 논문은 인버터 드라이브의 DC Link Voltage 최적화를 위해 머신러닝 기법을 적용한 독창적인 연구 내용을 포함하고 있으며, 데이터 전처리, 모델 평가 및 비교를 통해 성능 향상을 입증하고 있다. 이는 직접적인 연구 기여를 하는 연구 논문으로 판단된다."
The Challenges of Autonomous Unmanned Aerial System Test and Evaluation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641409,"With the development of artificial intelligence, computing and sensor technology, the autonomy of Unmanned Aerial System (UAS) has improved continually, but the autonomous capability levels vary. Hence, how to describe and evaluate autonomy of UAS scientifically and reasonably has become a hot topic in UAS test and evaluation area. In this paper, the developing status of research about autonomous UAS test and evaluation was reviewed. Then the concept meaning of UAS autonomy and the basic features of autonomy were analyzed comprehensively and systematically. Aiming at characteristics of UAS autonomy, the challenges of autonomous UAS test and evaluation were analyzed in depth from three aspects, autonomy levels, testing complexity and uncertainty and man-machine trust problem. Finally, according to current UAS testing methods and characteristics of autonomous UAS, the architecture of UAS autonomy test and evaluation, testing methods and contents were proposed, which would provide technical support for autonomous UAS test and evaluation.",No,"본 논문은 자율 무인 항공 시스템(UAS)의 자율성 평가와 테스트 방법에 대한 현황 검토 및 개념 분석, 그리고 도전 과제와 제안된 평가 아키텍처를 다루고 있으나, 독창적인 실험 결과나 새로운 연구 방법론의 직접적인 기여를 포함하고 있지 않습니다. 따라서 기존 연구를 종합하고 제안하는 리뷰 및 개념적 논의에 가까운 논문으로 판단됩니다."
Empowering First Responders through Automated Multimodal Content Moderation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457689,"Social media enables users to spread information and opinions, including in times of crisis events such as riots, protests or uprisings. Sensitive event-related content can lead to repercussions in the real world. Therefore it is crucial for first responders, such as law enforcement agencies, to have ready access, and the ability to monitor the propagation of such content. Obstacles to easy access include a lack of automatic moderation tools targeted for first responders. Efforts are further complicated by the multimodal nature of content which may have either textual and pictorial aspects. In this work, as a means of providing intelligence to first responders, we investigate automatic moderation of sensitive event-related content across the two modalities by exploiting recent advances in Deep Neural Networks (DNN). We use a combination of image classification with Convolutional Neural Networks (CNN) and text classification with Recurrent Neural Networks (RNN). Our multilevel content classifier is obtained by fusing the image classifier and the text classifier. We utilize feature engineering for preprocessing but bypass it during classification due to our use of DNNs while achieving coverage by leveraging community guidelines. Our approach maintains a low false positive rate and high precision by learning from a weakly labeled dataset and then, by learning from an expert annotated dataset. We evaluate our system both quantitatively and qualitatively to gain a deeper understanding of its functioning. Finally, we benchmark our technique with current approaches to combating sensitive content and find that our system outperforms by 16% in accuracy.",Yes,"논문은 딥 뉴럴 네트워크를 활용한 자동 멀티모달 콘텐츠 중재 시스템을 제안하고, 이미지 및 텍스트 분류기를 결합한 새로운 다중 수준 분류기를 개발하는 등 독창적인 연구 내용을 포함하고 있습니다. 또한, 약하게 라벨링된 데이터와 전문가 주석 데이터로 학습하여 성능을 평가하고 기존 방법과 비교 분석한 점에서 연구 논문에 해당합니다."
A kinect-based workplace postural analysis system using deep residual networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088272,"Human behavior understanding is a well-known area of interest for computer vision researchers. This discipline aims at evaluating several aspects of interactions among humans and system components to ensure long term human well-being. The robust human posture analysis is a crucial step towards achieving this target. In this paper, the deep representation learning paradigm is used to analyze the articulated human posture and assess the risk of having work-related musculoskeletal discomfort in manufacturing industries. Particularly, we train a deep residual convolutional neural network model to predict body joint angles from a single depth image. Estimated joint angles are essential for ergonomists to evaluate ergonomic assessment metrics. The proposed method applies the deep residual learning framework that has demonstrated impressive convergence speed and generalization capabilities in addressing different vision tasks such as object recognition, localization and detection. Moreover, we extend the state-of-the-art data generation pipeline to synthesize a dataset that features simulations of manual tasks performed by different workers. An inverse kinematics stage is proposed to generate the corresponding ground truth joint angles. Experimental results demonstrate the generalization performance of the proposed method.",Yes,"본 논문은 딥러닝 기반의 새로운 작업장 자세 분석 시스템을 제안하고, 심층 잔차 신경망을 활용하여 관절 각도를 예측하는 독창적인 연구 내용을 포함하고 있다. 또한, 데이터 생성 파이프라인 확장과 역운동학 기법 도입 등 실험적 방법론과 결과를 제시하여 직접적인 연구 기여를 하고 있음을 보여준다."
Machine Learning for Detecting Subtle Signs of EyeDisease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612587,"Eye disorders can have serious repercussions, such as blindness or visual impairment, if they are not identified or addressed, Treatment that works and preventing permanent harm depend on early diagnosis and prediction. Using cutting-edge clinical observations, this method looks for diseases such diabetic retinopathy, cataracts, myopia, glaucoma, age-related eye illnesses, and hypertension-related eye disorders. This study presents an innovative method for early detection of eye health disorders: The Eye Health Station. It uses machine learning algorithm, by leveraging EfficientNet's superior performance in image classification tasks, our system analyzes ocular images, such as retinal scans, to identify potential indicators of eye disorders at an early stage., The proposed system seeks to discover possible markers of eye problems by evaluating several factors taken from ocular pictures and patient data, including retinal scans and medical history. The system is able to learn from various datasets and gradually increase its accuracy since machine learning techniques are included into the system.",Yes,"논문 초록에서 제안된 ""Eye Health Station"" 시스템은 머신러닝 알고리즘을 활용하여 안구 질환의 초기 징후를 탐지하는 독창적인 방법을 제시하고 있으며, 다양한 데이터셋을 학습하여 정확도를 향상시키는 연구 내용을 포함하고 있다. 이는 직접적인 연구 기여와 새로운 방법론 개발에 해당한다."
Scalable Analytics Platform for Machine Learning in Smart Production Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869075,"Manufacturing industry is facing major challenges to meet customer requirements, which are constantly changing. Therefore, products have to be manufactured with efficient processes, minimal interruptions, and low resource consumptions. To achieve this goal, huge amounts of data generated by industrial equipment needs to be managed and analyzed by modern technologies. Since the big data era in manufacturing industry is still at an early stage, there is a need for a reference architecture that incorporates big data and machine learning technologies and aligns with the Industrie 4.0 standards and requirements. In this paper, requirements for designing a scalable analytics platform for industrial data are derived from Industrie 4.0 standards and literature. Based on these requirements, a reference big data architecture for industrial machine learning applications is proposed and compared to related works. Finally, the proposed architecture has been implemented in the Lab Big Data at the SmartFactoryOWL and its scalability and performance have been evaluated on parallel computation of an industrial PCA model. The results show that the proposed architecture is linearly scalable and adaptable to machine learning use cases and will help to improve the industrial automation processes in production systems.",Yes,"본 논문은 Industrie 4.0 표준과 문헌을 바탕으로 산업용 머신러닝 애플리케이션을 위한 확장 가능한 빅데이터 아키텍처를 제안하고, 이를 실제로 구현 및 평가한 연구 내용을 포함하고 있다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 판단된다."
Classification Of a bank data set on various data mining platforms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8391441,"The process of extracting meaningful rules from big and complex data is called data mining. Data mining has an increasing popularity in every field today. Data units are established in customer-oriented industries such as marketing, finance and telecommunication to work on the customer churn and acquisition, in particular. Among the data mining methods, classification algorithms are used in studies conducted for customer acquisition to predict the potential customers of the company in question in the related industry. In this study, bank marketing data set in UCI Machine Learning Data Set was used by creating models with the same classification algorithms in different data mining programs. Accuracy, precision and f- measure criteria were used to test performances of the classification models. When creating the classification models, the test and training data sets were randomly divided by the holdout method to evaluate the performance of the data set. The data set was divided into training and test data sets with the 60-40%, 75-25% and 80-20% separation ratios. Data mining programs used for these processes are the R, Knime, RapidMiner and WEKA. And, classification algorithms commonly used in these platforms are the k-nearest neighbor (k-nn), Naive Bayes, and C4.5 decision tree.",Yes,논문은 은행 마케팅 데이터셋을 활용하여 여러 데이터 마이닝 플랫폼에서 분류 알고리즘을 적용하고 성능을 비교하는 실험적 연구를 수행하고 있다. 이는 직접적인 데이터 분석과 모델 평가를 포함하는 독창적인 연구 내용으로 판단된다.
Multi-Label Classification of Lung Diseases Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664582,"Assistance for doctors in disease detection can be very useful in environments with scarce resources and personnel. Historically, many patients could have been cured with early detection of the disease. The application of deep learning techniques in the fields of medical imaging, on large datasets, has allowed computer algorithms to produce as effective results as medical professionals. To assist doctors, it is essential to have a versatile system that can timely detect multiple diseases in the lungs with high accuracy. Over time, although many classifiers and algorithms have been implemented, however, deep learning models (i.e., CNN, Deep-CNN, and R-CNN) are known to offer better results. After a thorough literature review of the state-of-the-art techniques, this work applies various models such as MobileNet, DenseNet, VGG-16, EfficientNet, Xception, and InceptionV3 to the selected large dataset. The goal is to enhance the accuracy of these algorithms by experimenting with parameter optimizations. We observe that MobileNet produces better results as compared to other models. We implement a deep convolutional GAN to produce synthetic X-ray images containing various pathologies already included in the chosen imbalanced dataset namely NIH Chest X-ray containing 14 classes. The synthetic dataset contains 1193 samples belonging to five classes. We test the suggested model using evaluation measures like recall, precision, and F1-score, along with binary accuracy. The suggested deep learning model produces recall as high as 57%, binary accuracy as 93.4%, F1-Score as 0.553, and AUC as 81. After the inclusion of generated synthetic samples, the value of the F1-score becomes 0.582 resulting in a 5% increase. Though, Generative Adversarial Network (GAN) shows lower performance, however, we encourage further research and experiments to find the versatility of GANs in the field of medical imaging.",Yes,"본 논문은 다양한 딥러닝 모델을 적용하고 파라미터 최적화를 통해 폐 질환 다중 분류 정확도를 향상시키는 독창적인 연구를 수행하였으며, GAN을 활용해 합성 데이터를 생성하여 성능 개선을 시도하는 등 직접적인 연구 기여가 포함되어 있다. 따라서 연구 논문에 해당한다."
Fake Profile Detection Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459570,"Nowadays, Social Media Platforms on behalf of entities or individuals, can harm (SMPs) are being utilized by an enormous number their reputations and reduce the number of likes of users to get connected with their friends and and followers they receive. Additionally, they family. There are sites like Facebook, Instagram, experience unnecessary confusion with other Twitter where people spend a significant amount of time to get updated about the world. The data uploaded on social media contains their personal information, thoughts on certain topics, news, etc. The social media platforms verify the authenticity of the registered user. However, some of the users hide their identities and these people are threats to the security of other users’ data. These bot accounts are used to scam, or purposefully cause harm to people. There is a need for detection techniques to find and eradicate these bots as quickly as possible. In this work, we have proposed a Machine learning based model that can identify fake or bot created accounts accurately. This paper is divided into multiple parts: Introduction, Literature Review, Methodology, Results and Discussion, Conclusion. To validate the authenticity of our work, we have experimented over the publicly available dataset TwiBot-20 and achieved accuracy of 87%.",Yes,"논문 초록에서 머신러닝 기반의 가짜 프로필 탐지 모델을 제안하고, 공개 데이터셋을 이용해 실험을 수행하여 87%의 정확도를 달성했다고 명시하고 있습니다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 나타냅니다."
Seat Belt And Helmet Detection Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725574,"The proposed system relates the current situation of multitudinous problems in traffic rules and regulations in different countries. Further, examines how any accident or causality can be averted using enhance methods of natural language processing. Mainly in highways, people just focus on speed and overtaking the other vehicles, rather than following traffic rules and taking basic safety measures. And due to these highways, police barricading is also not possible. Therefore, it is highly requisite to adopt an automatic seat belt and helmet detection system which captures the image of rule brakers, so that necessary legal action should be taken against them. The system works on real time detection and used for continuous surveillance for safety purposes. In the paper we solve the issue of manually checking the safety measures while driving a vehicle to avoid any accidents and deaths due to fast moving vehicles without wearing proper safety measures. The elucidated process in the paper is based on real-time surveillance system and obligate everyone to wear seat belts and helmets to increase the safety of individuals by the help of OpenCV (Open-Source Computer Vision Library) and deep learning algorithm which is embedded in Raspberry Pi. Addition to this, all the pictures and videos are saved in a database which is only accessed by higher government officials for future security purpose and expediate the tackling process of traffic rules violation due to the negligence and irresponsibility of law enforcement officials as well as common citizens.",Yes,"논문 초록에서 제안된 시스템은 딥러닝 알고리즘과 OpenCV를 활용한 실시간 안전벨트 및 헬멧 착용 감지 시스템을 개발하는 내용을 포함하고 있어, 독창적인 연구 및 구현이 이루어진 것으로 보입니다. 따라서 직접 기여하는 연구 논문에 해당한다고 판단됩니다."
Drone net architecture for UAS traffic management multi-modal sensor networking experiments,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8396716,"Drone Net is a conceptual architecture to integrate passive sensor nodes in a local sensor network along with traditional active sensing methods for small Unmanned Aerial System (sUAS) traffic management. The goal of the proposed research architecture is to evaluate the feasibility of the use of multiple passive sensor nodes integrating Electro-Optical/Infrared (EO/IR) and acoustic arrays networked around a UAS Traffic Management (UTM) operating region (Class G uncontrolled airspace for general aviation). The Drone Net approach will be further developed based on the feasibility analysis provided here, to compare to and/or be used in addition to RADAR (Radio Detection and Ranging) and Automatic Dependent Surveillance-Broadcast (ADS-B) tracking and identification in future experiments. We hypothesize that this hybrid passive plus active sensing approach can better manage non-compliant small UAS (without ADS-B transceivers) along with compliant UAS and general aviation in sensitive airspace, urban locations, and geofenced regions. Numerous commercial interests are developing UTM instrumentation for compliant and non-compliant drone detection and counter measures, but performance in terms of ability to detect, track, classify (bird, bug, drone, general aviation), identify, and localize aerial objects has not been standardized or well developed to compare multi-sensor solutions. The proposed Drone Net open system reference architecture is designed for passive nodes organized in a network, which can be integrated with RADAR and ADS-B. Here we present preliminary proof of concept results for two primary methods of truth comparison for generation of performance in terms of true and false positives and negatives for detection, classification, and identification. The first ground truth method designed and evaluated uses sUAS Micro Air Vehicle Link (MAVLink) ADS-B data along with EO/IR range detection experiments. The second ground truth method requires human review of triggered detection image capture and allows for truth performance assessment for non-compliant sUAS and other aerial objects (birds and bugs). The networked passive sensors have been designed to meet Class G and geo-fence UTM goals as well as assist with urban UTM operations. The approach can greatly complement NASA UTM collaboration and testing goals for 2020 and the “last fifty foot challenge” for package delivery UAS operations. The EO/IR system has been tested with basic motion detection for general aviation and sUAS in prior work, which is now being extended to include more sensing modalities and more advanced machine vision and machine learning development via the networking of the nodes and ground computing. The paper will detail the hardware, firmware and software architecture, and preliminary efficacy of the two ground truth methods used to compute standard performance metrics.",Yes,"논문 초록에서 제안된 Drone Net 아키텍처는 다중 수동 센서 노드와 능동 센서의 통합을 통한 UAS 교통 관리 시스템에 대한 개념적 설계와 예비 실험 결과를 포함하고 있어, 독창적인 연구 내용과 실험적 기여가 명확히 드러난다. 또한, 하드웨어, 펌웨어, 소프트웨어 아키텍처 및 성능 평가 방법을 상세히 다루고 있어 연구 논문으로 판단된다."
System abnormality detection in stock market complex trading systems using machine learning techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8285660,"Stock market trading systems are real time systems that process thousands of data per minute and are considered to be critical as well as complex. It incorporates the features of complex business processing and sophisticated in-memory processing techniques for speed and throughput. These systems are distributed in nature, and they use a large number of processing nodes incorporating fault tolerance mechanisms. Complex systems also have a large effective number of strongly interdependent variables. Hence detecting faults and failures in stock market systems is a complex and cumbersome task. The study explores machine learning techniques to detect anomalous behavior to provide warnings before a system results in a fault or failure state. The study extensively utilizes a supervised learning approach with machine learning algorithms such as C4.5, Naïve Bayes, and ensemble techniques; bagging and Random Forest. The system statistics captured from log files are preprocessed and transformed to eliminate system environment dependencies. For each of the three components the initial feature selection is carried out manually using domain knowledge and expertise. Initial feature selection based on domain expertise was required as the number of features per component is large and does not closely relate to the system state. Feature selection methods (Info Gain algorithm with Ranker search) have been successfully employed to filter out unrelated attributes and to reduce computational complexity. A comparative evaluation is performed under each component status prediction. This study also utilizes oversampling techniques to overcome limitations caused by the class imbalance phenomena. A range of evaluators are used to analyze the results and effectiveness of the models. The highest accuracy and Receiver Operating Characteristic (ROC) values are achieved when C4.5 decision tree is applied to the oversampled feature set and when the Random Forest algorithm is applied to the oversampled feature set. However, precision, recall and F-measure values vary. Root cause detection for anomalies and numeric values for system health predictions are future work in the research.",Yes,"본 논문은 주식 시장 거래 시스템의 이상 탐지를 위해 여러 머신러닝 기법을 적용하고, 데이터 전처리, 특징 선택, 오버샘플링 등 구체적인 연구 방법론을 제시하며 성능 평가를 수행하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
"Remote Sensing Image Retrieval in the Past Decade: Achievements, Challenges, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016217,"Remote sensing image retrieval (RSIR) aims to search and retrieve the images of interest from a large remote sensing image archive, which has remained to be a hot topic over the past decade. Benefited from the advent and progress of deep learning, RSIR has been promoted by developing novel approaches, constructing new datasets, and exploring potential applications. To the best of our knowledge, there lacks a comprehensive review of RSIR achievements, including systematic and hierarchical categorization of RSIR methods and benchmark datasets over the past decade. This article, therefore, provides a systematic survey of the recently published RSIR methods and benchmarks by reviewing more than 200 papers. To be specific, in terms of image source, label, and modality, we first group the RSIR methods into some hierarchical categories, each of which is reviewed in detail. Following the categorization of the RSIR methods, we list the benchmark datasets publicly available for performance evaluation and present our newly collected RSIR dataset. Moreover, some of the existing RSIR methods are selected and evaluated on the representative benchmark datasets. The results demonstrate that deep learning-based methods are currently the dominant RSIR approaches and outperform handcrafted feature-based methods by a significant margin. Finally, we discuss the main challenges of RSIR and point out some potential directions for the future RSIR research.",No,"이 논문은 지난 10년간의 원격 탐사 영상 검색(RSIR) 연구를 체계적으로 정리하고 평가한 리뷰 논문으로, 직접적인 독창적 연구 결과보다는 기존 연구들의 분류, 평가, 그리고 향후 연구 방향 제시에 중점을 두고 있습니다. 따라서 새로운 연구 기여보다는 종합적 개관에 해당합니다."
Driver identification using histogram and neural network from acceleration data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359893,"Sensor technology has continuously improved in term of size and cost. It encourages car companies to embed various types of sensors in their cars. The most common sensors that can be found are location sensor and acceleration sensor. The sensors generate a lot of useful data which could be applied for many future applications such as car tracking or emergency warning. One of the most interesting applications among others is analyzing the driver behavior. There are some previous works using this information to define types of drivers that are related to levels of accident risk. However, the driver behavior not only can separate the type of drivers but also can identify driving characteristic of an individual driver. The driver identification benefits a security application and it can eliminate the global car theft issue. Because the driver behavior cannot be repeated or copied like other anti-theft systems so this is an essential solution to stop increasing the number of stolen cars every year. In this paper, we propose a methodology to identify each driver and classify group of drivers using only accelerating behavior with machine learning technique. In order to identify an individual driver, the acceleration data have been transformed to a histogram in order to represent the accelerating behavior then we utilize a neural network model. We evaluate our methodology with data that have been collected from the acceleration sensors installed on 13 shuttle buses in our campus for more than 10 months. The results show the accuracy of the proposed methodology up to 88.3% for driver identification and up to 92% for driver classification.",Yes,"본 논문은 가속도 데이터를 이용한 운전자 식별 및 분류를 위한 새로운 방법론을 제안하고, 신경망 모델을 활용하여 실험 데이터를 통해 성능을 평가한 연구 내용을 포함하고 있다. 이는 독창적인 연구 기여를 포함한 연구 논문으로 판단된다."
Integration of multimodal technologies for a rowing platform,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4957238,"This paper presents the integration of multimodal technologies to measure and transmit different variables and stimuli involved in the human motion analysis, both for the training and for the transfer of users skills achieved by means of a mechatronic rowing platform. Although the mechanical design is the core of this project, this paper describes the integration and interaction of two multimodal systems (the human being and the rowing platform). These systems works together using different sensors and methodologies directly integrated to the human body to obtain detailed information related to the synchronization and correlation among the trajectories, proximal distance coupling, velocities, muscular activation and muscular force, transversal force in the oars and head position acquired through digital image processing and machine learning techniques. This information is evaluated and used in the rendering section through audio-tactile stimuli for the acceleration learning process.",Yes,논문 초록에서 멀티모달 기술을 통합하여 인간 운동 분석과 관련된 다양한 변수를 측정하고 이를 기반으로 한 메카트로닉 조정 플랫폼을 개발한 연구임을 알 수 있습니다. 이는 독창적인 시스템 통합과 데이터 처리 방법을 포함한 직접적인 연구 기여로 판단됩니다.
A Systematic Review and Future Perspective of Android Malware Detection Based Machine Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456033,"After becoming incorporated into normal daily life, smartphones gained access to private data. On most mobile devices running Android OS, malware gathers private user data without the user's awareness. Malware frameworks are becoming more and more complicated, making it challenging to identify them. Traditionally, researchers avoided detection processes that employed static and dynamic analysis by employing strategies such as encryption, packaging, and code obfuscation, but the current study reveals that machine learning-based identification of malicious code for Android is an efficient methodology. The effectiveness of the extracted features obtained as a result of malware detection indicates the efficiency of malware detection analysis. The positive aspects of previous reviews have been augmented in our paper by surveying in a variety of areas. Leading malware analysis tools used on Android systems were thoroughly examined. To fill detection gaps, it is necessary to improve effective measures and detection efficiency with regard to advanced malware. A summary of the difficulties addressed throughout the creation of malware classifiers is also included in the paper. The research work in this paper is concerned with the potential of detecting malware targeting Android devices using machine learning.",No,본 논문은 기존 연구들을 체계적으로 검토하고 안드로이드 악성코드 탐지에 관한 머신러닝 기법들의 현황과 향후 전망을 제시하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험을 포함한 연구 논문이 아니라 기존 연구의 종합 및 분석에 초점이 맞춰져 있습니다.
Feature-Level Attention-Guided Multitask CNN for Fault Diagnosis and Working Conditions Identification of Rolling Bearing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372133,"Accurate and real-time fault diagnosis (FD) and working conditions identification (WCI) are the key to ensuring the safe operation of mechanical systems. We observe that there is a close correlation between the fault condition and the working condition in the vibration signal. Most of the intelligent FD methods only learn some features from the vibration signals and then use them to identify fault categories. They ignore the impact of working conditions on the bearing system, and such a single-task learning method cannot learn the complementary information contained in multiple related tasks. Therefore, this article is devoted to mining richer and complementary globally shared features from vibration signals to complete the FD and WCI of rolling bearings at the same time. To this end, we propose a novel multitask attention convolutional neural network (MTA-CNN) that can automatically give feature-level attention to specific tasks. The MTA-CNN consists of a global feature shared network (GFS-network) for learning globally shared features and K task-specific networks with feature-level attention module (FLA-module). This architecture allows the FLA-module to automatically learn the features of specific tasks from globally shared features, thereby sharing information among different tasks. We evaluated our method on the wheelset bearing data set and motor bearing data set. The results show that our method has a better performance than the state-of-the-art deep learning methods and strongly prove that our multitask learning mechanism can improve the results of each task.",Yes,논문은 새로운 다중 작업 주의 기반 CNN 모델을 제안하여 베어링의 결함 진단과 작업 조건 식별을 동시에 수행하는 독창적인 연구 내용을 포함하고 있습니다. 또한 제안된 방법을 실제 데이터셋에 적용하여 기존 방법보다 우수한 성능을 입증하였으므로 연구 논문에 해당합니다.
Towards the Use of Language Models in Scientific Paper Recommender Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578590,"Within the educational and research community, Research Paper Recommender Systems debuted in the late 1990s and today, they constitute a specific research area. In this work, it is explored how the use of neural networks together with the incorporation of Natural Language Processing techniques, such as word embeddings and language models, affect the recom-mendation process of scientific papers. Three Deep Learning-based recommenders are explored: a neural collaborative filtering recommender, a recommender that uses word embeddings, and a recommender that incorporates language models. In addition, the results obtained are evaluated on two different datasets to see the effect of each of them on the recommendation process. While the first dataset only includes papers that have interested the user, the second one also includes papers that have not interested the user. The collaborative Deep Learning-based recommender constitutes the baseline against which to compare the rest of the developed recommenders. To evaluate the recommenders, each model is used to recommend 10 research papers for each user. The recommendations are evaluated and considered appropriate if they are related to the research field the user is interested in. The results confirm that the use of NLP techniques improves the performance of pure collaborative recommenders.",Yes,"본 논문은 신경망과 자연어 처리 기법을 활용한 연구 논문 추천 시스템을 개발하고, 세 가지 딥러닝 기반 추천 모델의 성능을 두 개의 데이터셋에서 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Recognizing User Preferences Based on Layered Activity Recognition and First-Order Logic,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735312,"Only few cognitive architectures have been proposed that cover the complete range from recognizers working on the direct sensor input, to logical inference mechanisms of classical artificial intelligence (AI). Logical systems operate on abstract predicates, which are often related to an action-like state transition, especially when compared to the classes recognized by pattern recognition approaches. On the other hand, pattern recognition is often limited to static patterns, and temporal and multi-modal aspects of a class are often not regarded, e.g. by testing only on pre-segmented data. Recent trends in AI aim at developing applications and methods that are motivated by data-driven real world scenarios, while the field of pattern recognition attempts to push forward the boundary of pattern complexity. We propose a new generic architecture to close the gap between AI and pattern recognition approaches. In order to detect abstract complex patterns, we process sequential data in layers. On each layer, a set of elementary classes is recognized and the outcome of the classification is passed to the successive layer such that the time granularity increases. Layers can combine modalities, additional symbolic information or make use of reasoning algorithms. We evaluated our approach in an on-line scenario of activity recognition using three layers. The obtained results show that the combination of concepts from pattern recognition and high-level symbolic information leads to a prosperous and powerful symbiosis.",Yes,"논문 초록에서 제안된 새로운 아키텍처와 이를 이용한 다층 활동 인식 방법 및 논리 추론 알고리즘의 결합을 통해 직접적인 연구 기여를 하고 있음을 알 수 있습니다. 또한, 온라인 시나리오에서 평가한 결과를 제시하여 독창적인 연구 내용을 포함하고 있음을 보여줍니다."
Big Data Visualisation in the Maritime Industry,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10794236,"VesselAI aims to develop, validate and demonstrate a unique framework to unlock the potential of extreme-scale data and advanced HPC, AI and Digital Twin technologies in the maritime industry. With the growth of data and the digitalization of the sector comes the need to process and visualise this information as the maritime industry generates and consumes huge amounts of different types of data every day. This paper presents literature review focusing on two aspects: (1) an examination of visualization tools available, and (2) an investigation into existing works and studies within the domain of Big Data visualization. This study addresses specific visualization requirements pertinent to the Maritime domain, including the necessity for intricate spatial-temporal visualizations encompassing diverse datasets such as weather patterns, vessel trajectories, and Automatic Identification System (AIS) data. VesselAI intends to build the VesselAI Visualisation and Reporting Engine to empower maritime users and stakeholders to make informed decisions and gather knowledge from data. To this end a platform based on Apache Superset was applied and tested in response to challenges faced by maritime stakeholders and the findings indicate that Apache Superset's robust capabilities, including a vast number of visualisation types supported, out-of-the-box data connectors, customization options, and security features, effectively met the requirements identified in the literature and by pilot users.",No,"본 논문은 문헌 리뷰와 기존 도구 및 연구 조사에 중점을 두고 있으며, 직접적인 실험 결과나 독창적인 연구 방법론 개발에 대한 기술이 부족합니다. 따라서 새로운 연구 기여보다는 기존 연구와 도구의 적용 및 평가에 초점이 맞춰져 있습니다."
Advancing Age Estimation from Facial Images Using Deep Learning Approaches and Ethical Considerations,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912143,"Estimation of age from facial images is an important application in various fields, like biometrics, forensics, and personalized healthcare. The proposed work investigated the use of a machine learning approach to predict human age from facial features only, obtained from images. Our method used Convolutional Neural Networks (CNNs) in feature extraction and then combined regression models for age estimation. The used dataset is images of faces from different demographic groups, which makes the model robust and generalizable. All critical challenges related to variations in lighting, facial expression, and age distribution are adequately covered. The preprocessing steps, which include normalization and data augmentation, were properly applied to improve the performance of the models and reduce overfitting. Measures to report on prediction accuracy are also reported by using metrics such as Mean Absolute Deviation (MAD) and Root Mean Square Deviation (RMSD). The approach CNN-based shows a higher accuracy and computation rate compared to the baseline methods. Additionally, it presents practical applications of the models in missing people cases or for age-sensitive services. Ethical concerns such as privacy and control of bias are also of high concern and thus point to the need for clear and equitable use of such technology. This research seriously tests deep learning and shows its effectiveness in improving the estimation of age from facial images, providing important insights in relation to biometric solutions and more.",Yes,"논문은 얼굴 이미지에서 나이를 추정하기 위해 CNN과 회귀 모델을 활용한 독창적인 딥러닝 기반 방법을 제안하고, 데이터 전처리 및 성능 평가 지표를 포함한 구체적인 연구 과정을 기술하고 있다. 또한, 기존 방법과 비교하여 성능 향상을 입증하는 실험 결과를 제시하므로 직접 기여하는 연구 논문에 해당한다."
Distributed Deep Reinforcement Learning for Autonomous Iot Healthcare Devices in the Cloud,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488976,"The ethical and philosophical problems concerning the cooperation of AI systems and human artists are also examined in this study. In addressing authorship, agency, and the very essence of creation, the changing position of artists as co-creators with intelligent algorithms is explored. It also looks at how AI can question and change conventional ideas of creative competence. Additionally, this study looks into how AI affects the promotion and distribution of art. AI-driven marketing tactics provide improved targeting of customers, personalized experiences, and optimal promotional efforts by utilizing insights based on data and predictive analytics. The study focuses on how these developments transform the relationships between galleries and artists and their patrons, ultimately fostering a more varied and inclusive art scene. The system's potential in many healthcare scenarios has been validated through simulations and practical experiments, which have received excellent feedback from healthcare providers. A critical review emphasizes the need to address practical deployment issues and data security concerns, while also highlighting the exciting convergence of IoT healthcare devices and DDRL. The paper ends with suggestions for more research, highlighting the significance of ethical issues, user interface improvements, and real-world validation. Through the smooth integration of DDRL-enhanced Internet of Things (IoT) medical equipment into clinical practice, our research eventually improves patient care and transforms the delivery of healthcare.",No,"초록 내용은 AI와 예술의 윤리적, 철학적 문제와 AI 기반 마케팅, 예술계 변화에 대한 논의가 주를 이루고 있으며, DDRL과 IoT 의료기기 관련 연구는 시뮬레이션과 실험 결과에 대한 간략한 언급에 그칩니다. 구체적인 독창적 연구 방법론이나 실험 결과에 대한 상세한 기술이 부족하여 직접 기여하는 연구 논문으로 보기 어렵습니다."
A flexible machine learning based framework for state of charge evaluation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9855050,"Batteries State-of-Charge (SoC) must be accurately monitored for safe battery operations, and to extend battery life. Machine Learning (ML) algorithms allow to perform the SoC estimation on a data-based approach, avoiding the need for a physical model for each different battery. In this work, a new ML-based framework for the SoC evaluation is proposed, exploiting constant current discharges for model training, rather than the commonly exploited standard drive cycle profiles. This allows avoiding the conversion processes from the drive cycles vehicle acceleration set-point into a current profile, which lead to vehicle-dependent data and the need for a conversion tool. Currents, voltages and temperatures related to different current discharge rates were measured for a Panasonic 18650 Lithium-Ion battery cell. These data were used to train and optimize a Support Vector Regression (SVR) model in the MATLAB environment. Subsequently, different data were combined together to emulate a real vehicle discharge process and were used for evaluating the model. A Root Mean Square Error (RMSE) of 0.564% was obtained, proving that the SVR model trained with constant current discharges data has been capable to estimate the SoC of the tested drive cycles operations.",Yes,"본 논문은 배터리의 충전 상태(SoC)를 평가하기 위한 새로운 머신러닝 기반 프레임워크를 제안하고, 실험 데이터를 수집하여 Support Vector Regression 모델을 훈련 및 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Evaluation of activity monitors to estimate energy expenditure in manual wheelchair users,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333626,"In an effort to make activity monitors usable by manual wheelchair users with spinal cord injury (SCI), our study examines the validity of SenseWearreg Armband (SenseWear) and RT3 in assessing energy expenditure (EE) during wheelchair related activities. This paper presents the data obtained from six subjects (n=6) with SCI performing three activities, including wheelchair propulsion, armergometer exercise and deskwork. The analysis presented here compares the EE estimated from the SenseWear and the RT3 with respect to the EE measured from a portable metabolic cart. It was found that the SenseWear overestimated EE for resting (+5.78%), wheelchair propulsion (+88.20%, +46.20%, and +138.21% for the three trials at different intensities, respectively), arm-ergometer exercise (+55.05%, +26.91%, and +39.17% for the three trials at different intensities, respectively) and deskwork (+13.11%). The results also indicate that RT3 underestimated EE for resting (-3.06%), wheelchair propulsion (-24.23%, -19.42%, and -9.98% for the three trials at different intensities, respectively), arm-ergometer exercise (-49.06%, -53.69% and -52.08 for the three trials at different intensities, respectively) and measured EE relatively accurate for deskwork. Good and moderate Intraclass correlations were found between EE measured by metabolic cart and EE estimated by SenseWear (0.787, p<0.0001) and RT3 (0.705, p<0.0001). Weka, machine learning software, was used to select attributes and model EE equations for the SenseWear and the RT3. Excellent and good Intraclass correlations were found between the EE measured by the metabolic cart and the estimated EE based on the models for SenseWear (0.944, p<0.0001) and RT3 (0.821, p<0.0001). Future work will test more subjects to refine the model and provide manual wheelchair users with a valid tool to gauge their daily physical activity and EE.",Yes,"본 논문은 척수손상 환자 대상 수동 휠체어 사용자의 에너지 소비량을 측정하는 활동 모니터의 유효성을 평가하기 위해 실험을 수행하고, 측정값과 모델링 결과를 비교 분석하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
EchoWrite 2.0: A Lightweight Zero-Shot Text-Entry System Based on Acoustics,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793596,"Limited by size, shape, and other factors, it is rather inconvenient to interact with new smart devices by traditional methods. Acoustic-based methods following a machine learning approach have been put forward to resolve this problem in previous works. But they possess limitations of heavy training overhead, low performance for unseen users, and intensive computation cost. Following our previous work in this area, we further overcome shortcomings of existing work and propose a lightweight and zero-shot text-entry system for unseen users based on acoustic sensing. The key novelty of this work is proposing a new model training strategy including dataset construction and augmentation methods to effectively enhance generalization ability of a simple learning model with as few training data as possible, based on our insight into the problem. We design and implement a real-time Android application system called EchoWrite 2.0 to validate our idea with extensive experiments. Results show that EchoWrite 2.0 can recognize digits, English letters, and words with an accuracy of 85.3%, 73.2%, and 96.9%, respectively, for unseen users without providing any data to the learning model. The comparison with related work in different aspects shows overall superiority of EchoWrite 2.0.",Yes,"논문은 기존 연구의 한계를 극복하기 위한 새로운 모델 훈련 전략과 데이터 증강 방법을 제안하며, 이를 바탕으로 실시간 안드로이드 애플리케이션을 설계하고 실험을 통해 성능을 검증하였다. 이러한 내용은 독창적인 연구 기여를 포함하는 연구 논문에 해당한다."
Enhancing Student Outcomes with LSTM-CNN and Data Analytics in Higher Education,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10925677,"With a focus on the use of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) approaches to predict students' academic performance, the study provides an extensive review of the literature on the application of artificial neural networks (ANNs) and machine learning (ML) techniques. The study highlights the possible advantages of implementing cutting-edge technology innovations like analytics and data mining in learning environments. The analysis reveals that the reviewed literature primarily focuses on higher education. The results show that LSTM-CNN and data mining methodologies are consistently integrated, suggesting a trend in using these combined approaches to evaluate academic progress. No observable patterns were found in spite of the variability in the input variables, which was dictated by the study's context and the availability of data. It is acknowledged, nevertheless, that there is a dearth of hard data regarding the effective application of these strategies for raising student achievement and outcomes. The study emphasizes how critical it is to close the theoretical and practical divides regarding the limited applicability of LSTM-only CNN models in practical educational contexts. It highlights the need for more research to address this discrepancy and suggests putting more effort into creating and putting into practice strategies that directly improve student performance and help schools achieve their goals.",No,초록에서 본 연구는 기존 문헌을 광범위하게 검토하고 LSTM-CNN 및 데이터 분석 기법의 적용 현황과 한계를 논의하는 문헌 리뷰 성격이 강합니다. 직접적인 실험 결과나 독창적인 연구 기여 내용이 명확히 제시되지 않아 연구 논문으로 보기 어렵습니다.
Analysis of the State of High-Voltage Current Transformers Based on Gradient Boosting on Decision Trees,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186646,"This paper addresses the problem of instrument current transformers technical state assessment based on machine learning methods. The introductory parts of the paper provide a detailed analysis of modern methods and approaches for technical state assessment of high-voltage power equipment of power plants and substations as well as a review of modern software tools and the latest trends in the given field of study. Justification of the relevance of the presented research aimed at instrument current transformers technical state assessment is provided along with the motivation for machine learning methods application for improvement of the accuracy and quality of high-voltage equipment state classification. Within the framework of the study, a comparative analysis of gradient boosting on decision trees and random forest algorithms was carried out for a given mathematical problem formulation. The main stages of processing the initial dataset are proposed as a step-by-step procedure, including feature extraction, feature transformation, feature interactions, etc. The outperforming efficiency of gradient boosting on decision trees algorithm was validated for real power equipment fleet. The resulting classification quality metrics of current transformers technical state assessment, Precision and Recall, are estimated to be 87.1% and 83.7%, correspondingly.",Yes,"논문은 고전압 전류 변압기 상태 평가를 위한 기계 학습 방법을 적용하고, 특히 그래디언트 부스팅과 랜덤 포레스트 알고리즘을 비교 분석하는 독창적인 연구를 수행하였다. 또한 실제 데이터에 대한 실험과 성능 평가 지표를 제시하여 직접적인 연구 기여가 포함되어 있음을 보여준다."
Examining digital forensics in the context of Shamoan attack behavior within fog computing and exploring threat intelligence for Incident Response,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482310,"Cybercrime is on the rise in modern society. Thus, the investigation of cybercrimes has used digital forensics. Numerous studies that analyze malware types, cyber-attacks, and other related topics have been conducted. Studies and analyses of APTs, particularly the Shamoon attack, as a basis for research. The purpose of this study was to evaluate the efficacy of supervised machine learning classifiers in the context of intrusion detection, with a focus on Support Vector Machine, Random Forest, Logistic Regression, and Gaussian Naive Bayes. The NSL-KDD dataset was used for the evaluation, and each classifier’s efficacy was assessed by contrasting its results based on accuracy, precision, recall, and F1-Score.With an accuracy of 99%, it can be inferred from the observed results that the Random Forest classifier performs better than the other classifiers for the specified dataset and parameters. To build on this work, future studies may examine how well these classifiers operate in scenarios involving several classes and think about adding only the most necessary characteristics for more focused intrusion detection.The FPSO mechanism puts the TSP’s operational process into practice. The FPSO mechanism carries out insertion and swap operations. After evaluating the fitness function, the nearest neighbour’s algorithm is used to determine the best shortest path. After evaluation, the best local and best global solutions are discovered. The appropriate positions and speeds are lastly updated. The generated optimal path can be used to assess the distribution of movement for Shamoon attacks. The suggested system’s efficacy has been evaluated by evaluating the fitness value and optimal cost. Distribution of the Shamoon data attack has been observed. Lastly, a threat intelligence plan is proposed to investigate and examine the behavior and spread of Shamoon attacks in the margins of fog computing.",Yes,본 논문은 Shamoon 공격 행위와 관련된 디지털 포렌식 및 침입 탐지에 대해 머신러닝 분류기 성능 평가와 최적 경로 탐색 알고리즘 적용 등 구체적이고 독창적인 연구 방법과 결과를 제시하고 있다. 이는 기존 연구를 바탕으로 새로운 실험과 분석을 수행한 연구 논문으로 판단된다.
Sentimental Analysis of airline tweets using Machine Learning Algorithm and Regular Expression,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10602480,"Sentiment analysis is a technique that uses natural language processing techniques to analyze the feelings and usage of textual content evaluation.Sentiment analysis makes use of information identified and taken from a variety of sources. It looks at a variety of factors to try and ascertain a customer’s attitude. It is extensively used throughout the world for sentiment analysis and opinion extraction, enabling business organizations to understand user preferences. This paper presents a thorough yet user-friendly method for evaluating sentiment, which primarily displays aggregate ratings based on both positive and negative words. Based on the ratings, the tweets are then classified as Positive, Negative, or Neutral. Over the past ten years, there has been a notable increase in the application of opinion mining and sentiment evaluation techniques. Many studies in this field seek to determine people’s attitudes, beliefs, and feelings about a particular topic or individual. The evaluation’s primary objective is to use a machine learning model to ascertain the sentiment of the reviews, and then to compare the outcome with a manual review of the data. This could help scientists represent and analyze assessments across domains and impartially support data evaluation. For review evaluation, a hybrid approach combining a supervised machine learning algorithm with natural language processing methods is suggested. Finding the first-class version is the aim of this mission in order to gauge the sentiment of airline tweets. Throughout the process of conducting research and considering various approaches and factors to consider, we noticed that approaches such as naive bayes and logistic regression have not been thoroughly investigated since along with data imbalancing was also not look into consideration.So We will assess a more practical method in this mission that makes use of regular expression, oversampling and ml models and look into data balancing.",Yes,"논문 초록에서 기계 학습 모델과 정규 표현식을 활용한 감성 분석 방법을 제안하고, 기존 연구에서 충분히 다루어지지 않은 나이브 베이즈, 로지스틱 회귀 및 데이터 불균형 문제를 다루는 실험적 연구임을 명확히 밝히고 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함한 연구 논문으로 판단됩니다."
"StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222343,"In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called “stacked generalization”) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.",Yes,"본 논문은 스태킹 앙상블 학습을 위한 시각화 기반의 지식 생성 모델과 시각적 분석 시스템(StackGenVis)을 제안하며, 이를 통해 성능 지표 조정, 데이터 및 알고리즘 선택, 모델 성능 평가 등 직접적인 연구 기여를 포함하고 있다. 또한 실제 사례와 전문가 평가를 통해 시스템의 유용성을 검증하여 독창적인 연구 내용을 담고 있음을 보여준다."
Feature Analysis for Fake Review Detection through Supervised Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259828,"Nowadays, review sites are more and more confronted with the spread of misinformation, i.e., opinion spam, which aims at promoting or damaging some target businesses, by misleading either human readers, or automated opinion mining and sentiment analysis systems. For this reason, in the last years, several data-driven approaches have been proposed to assess the credibility of user-generated content diffused through social media in the form of on-line reviews. Distinct approaches often consider different subsets of characteristics, i.e., features, connected to both reviews and reviewers, as well as to the network structure linking distinct entities on the review-site in exam. This article aims at providing an analysis of the main review- and reviewer-centric features that have been proposed up to now in the literature to detect fake reviews, in particular from those approaches that employ supervised machine learning techniques. These solutions provide in general better results with respect to purely unsupervised approaches, which are often based on graph-based methods that consider relational ties in review sites. Furthermore, this work proposes and evaluates some additional new features that can be suitable to classify genuine and fake reviews. For this purpose, a supervised classifier based on Random Forests have been implemented, by considering both well-known and new features, and a large-scale labeled dataset from which all these features have been extracted. The good results obtained show the effectiveness of new features to detect in particular singleton fake reviews, and in general the utility of this study.",Yes,논문 초록에서 새로운 특징(feature)을 제안하고 이를 기반으로 감독 학습(Random Forest) 분류기를 구현하여 가짜 리뷰 탐지 성능을 평가한 점이 명확히 기술되어 있습니다. 이는 기존 연구를 분석하는 것뿐만 아니라 직접적인 실험과 성능 검증을 포함한 독창적인 연구 기여임을 보여줍니다.
Affective Computing: A Topic-Based SER Approach on Collaborative Discussions in Academic Setting,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342963,"One of the biggest concerns in the modern day especially in the educational domain centers on the student's mental health. High rates of anxiety and depression have especially brought the attention of researchers in engineering education to apply affective computing to help with students' academic performance. It is known that a person's emotional states cause physiological and physical changes in the body. Emotions may impact facial expression, tone of speech, blood pressure, pulse, etc. Since visual and auditory signals are two variables that can be measured without the need to attach any physical device to the individuals, they are most studied in this field. Speech in particular has been known as a means that transfers much information about the mental and emotional states of the person. Speech Emotion Recognition (SER) is a growing field that has been applied in several domains including engineering education. Recent advancements in AI, Natural Language Understanding (NLU), and Large Language Models (LLM) have significantly streamlined this line of research. In this work which is a continuation of our prior work, we propose a speech analysis model that extracts both the emotions and topics from verbal discussions in a computer science classroom to understand if the expressed emotions were mostly about the course related topics or not. The goal of this research is to develop a tool that helps educators gain insights into the students' emotional states in teamwork and also understand the context of their conversations. We further analyze if the expressed emotions in the verbal class discussions are mostly about the course content or other subjects outside class setting. To expand the emotion analysis module we added a new layer to our developed pipeline by passing the speech data into the ChatGPT API to generate summarized scripts and extract additional classes of emotion. The preliminary results from this study are promising, indicating the potential value of this research direction and its prospects for further development. Application of this model in the educational domain can greatly benefit both educators and students and allows the instructors to make necessary interventions needed to maximize students' positive experiences in team settings while considering their emotional states.",Yes,"본 논문은 음성 감정 인식 모델을 개발하고, 이를 학습 환경에서 학생들의 감정과 대화 주제를 분석하는 데 적용하는 독창적인 연구 내용을 포함하고 있다. 또한, ChatGPT API를 활용한 감정 분석 확장 등 구체적인 방법론과 초기 결과를 제시하여 직접적인 연구 기여를 하고 있음을 알 수 있다."
Prediction of difficulties in Intubation using an Expert system,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007952,"Expert anesthesiologist inserts a tube into the respiratory passage of the patient who is undergoing surgery in Intensive Care Unit or Operation Theater. The patient is unable to breath on their own during the surgery. This process helps in providing artificial assistance in breathing and prevents suffocation. Any interruption in oxygen supply or difficulties in intubation may result in acute internal body damages or may lead to death of the patient. Authors have carried out a literature review and field study to identify list of risk parameters leading to difficulty in intubation. Only 1 to 5 percentage of patients who are undergoing intubation suffer from difficult intubation. There is an acute shortage of expert anesthesiologist in hospital and employing an expert anesthesiologist will be very expensive. In US anesthesiologist have to perform at least 150 successful intubations in first pass to consider him as an expert. The author's main aim for designing an expert system using machine learning algorithms was to predict the difficulties in securing airway and also create an allocation system which allocates an expert anesthesiologist for difficult cases based on the results produced by the system. This paper discusses the procedure to carry out Intubation to emulate the high cognitive process of an expert anesthesiologist. The outcomes of the prediction process divides intubation into easy, difficult and impossible. The authors have designed framework of data sets, risk parameters, rules and algorithms. The expert system gives the prediction results of difficulty in intubation which will be validated by expert anesthesiologist.",Yes,"본 논문은 문헌 조사와 현장 연구를 통해 위험 인자를 도출하고, 기계 학습 알고리즘을 활용한 전문가 시스템을 설계하여 난삽관 예측 모델을 개발하는 독창적인 연구 내용을 포함하고 있다. 또한, 예측 결과를 전문가가 검증하는 절차도 포함되어 있어 직접적인 연구 기여가 명확하다."
Evaluation of machine learning and deep learning-based intrusion detection systems in in-vehicle networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10324639,"With the growing complexity and connectivity of Intelligent Connected Vehicles (ICVs), In-vehicle communication plays an important role in today's driving environment. The Controller Area Network (CAN) is the most used protocol in an in-vehicle network to exchange data between Electronic Control Units (ECUs). Although in-vehicle communications must be regarded as the last line of security defence for ICV, recent reports have indicated vulnerability to threats due to its broadcasting design. In this context, this paper targets to comprehensively review the advantages and limitations of the existing machine learning and deep learning approaches designed for Intrusion Detection Systems (IDS). We evaluate their performance and computational resource requirements and propose potential enhancements. Finally, open challenges and future research directions for in-vehicle communication cyber security are highlighted as observations and recommendations.",No,"본 논문은 기존 머신러닝 및 딥러닝 기반 침입 탐지 시스템을 종합적으로 리뷰하고 평가하는 내용을 담고 있으며, 직접적인 독창적 연구 결과나 새로운 실험적 기여보다는 기존 연구의 장단점 분석과 향후 연구 방향 제시에 초점이 맞춰져 있습니다. 따라서 독창적인 연구 내용을 포함한 연구 논문으로 보기 어렵습니다."
A Systematic Review of Textile Anomaly Detection Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482127,"Determining defects in textiles has become an increasing concern to maintain a standard of optimal quality. During mass production, there is a high possibility that certain fabrics are damaged and thus, identifying anomalies has become a crucial task. This paper contains a detailed overview of state-of-the-art techniques in textile defect detection, offering insights into the advancements and challenges faced by this domain. The review encompasses a thorough examination of machine learning and deep learning models like Convolution Neural Networks, Generative Adversarial Networks, autoencoders, and ensemble models, analyzing how Computer Vision has yielded promising results. Attention has also been placed while evaluating innovative algorithms like RNet, UNet, AMTFnet, and RDUNet. The purpose of this research is to guide practitioners in selecting the appropriate methodologies while recognizing anomalies, thereby fostering the development of robust systems in textile industries.",No,"본 논문은 기존 연구들을 체계적으로 정리하고 분석하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 문헌 고찰에 해당합니다."
Real-time identification and tracking of emission from vessels based on Automatic Identification System Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538564,"Real-time and automatic tracking emissions of vessels can help the maritime traffic department and environmental monitoring department to study quantitatively and provide foundation for setting pollution reduction measures, and defining policies for management departments. Traditionaly method is based on fuel consumptions of vessels which need the cooperation of vessel operators; it is hard to realised real-time computation of emission data. Among various sensors used for vessel identification, Automatic Identification System (AIS) have emerged as the state-of-the-art in recent years, providing much richer information than synthetic aperture radar. It has been applied in emission inventory of vessels for several years. This paper provides a review of the AIS-based emission inventory literature which includes empirical, experimental and analytical aspects. At last, we compared the AIS-based method and Fuel-based method and found that AIS-based method can provide more elaborate temporal and spatial emission distribution characteristics.",No,"논문 초록은 AIS 기반 선박 배출량 산정 방법에 대한 문헌 리뷰를 제공하고 있으며, 기존 연구들을 비교 분석하는 내용에 초점이 맞춰져 있습니다. 따라서 독창적인 연구 결과나 새로운 실험적 기여보다는 기존 연구의 종합 및 비교에 해당하므로 연구 논문으로 보기 어렵습니다."
A Mobile Telematics Pattern Recognition Framework for Driving Behavior Extraction,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986619,"Mobile telematics is a relatively new innovation that involves collecting data on driving behavior using the internal sensors in a smartphone rather than from an in-vehicle data recorder. However, telematics data are usually not labeled, which makes extracting driving patterns from them very difficult. Therefore, unsupervised learning algorithms play an important role in this field. In addition, most current research is based on datasets developed in a laboratory or from site investigations and questionnaires, which are very different from real-world driving behaviors. To advance unsupervised learning techniques in this field, and to fill the gap in findings based on real-world data, we have developed an unsupervised pattern recognition framework for mobile telematics data. The framework comprises three main components: a self-organizing map, a nine-layers deep auto-encoder, and partitive clustering algorithms. The SOM algorithm reduces the complexity of the data, the deep auto-encoder extracts the features, and the clustering algorithm groups driving events with similar patterns into behaviors. Further, given clustering with mobile telematics data is an under-researched area, we undertook an empirical comparison of five well-known clustering algorithms to determine the strengths and weaknesses of each method and which is best suited to categorizing driving styles. The study was conducted with a real-world insurance dataset containing 500,000 journeys by 2500 drivers, and the results were evaluated against three measures- Davis Boulding, Calinski Harabasz, and execution time. Overall, we find that k-means clustering and a self-organizing map were able to extract more accurate patterns than others. A statistical analysis of the 29 clusters produced by SOM and k-means, revealed 29 unique driving styles, all of which can be found in the transportation literature. The results from the study, with support from the corresponding literature review, demonstrate the efficacy of the presented framework in unsupervised settings. Additionally, the results provide a basis for developing a future risk analysis and automatic decision support system for usage-based insurance companies.",Yes,"이 논문은 모바일 텔레매틱스 데이터를 활용한 비지도 학습 기반의 패턴 인식 프레임워크를 개발하고, 실제 보험 데이터셋을 사용해 여러 클러스터링 알고리즘을 비교 분석하는 독창적인 연구를 수행하였다. 따라서 새로운 방법론 제시와 실험적 검증을 포함한 직접적인 연구 기여가 명확하다."
Impact on Celebrity Endorsement Essentials on Brand Awareness and Purchase Intention in Men's Wear Industry in Sri Lanka,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857789,"In fashion marketing, companies use various strategies to attract consumers and boost brand visibility. Celebrity endorsements are a key method that significantly impacts consumer behavior and brand promotion. This study explores how celebrity credibility and attractiveness affect brand awareness and purchase intention through Structural Equation Modeling (SEM), focusing the Sri Lanka's branded men's wear industry. Serving such industry, this study has discovered, explained, and evaluated how celebrity endorsement affects the customers' perception on brand image and purchase intention on men's wear, through a quantitative research approach. The literature review first identified a conceptual model and then investigates the validity of such model to the men's wear industry where the celebrity endorsement is needed. The data were analyzed using confirmatory factor analysis, SEM and cluster analysis techniques, to capture relations and impacts among key variables of celebrity endorsement essentials by means of observed variables. As key findings, while presenting individual correlations among the key factors, a SEM model is also presented to describe the true relationships and their deviations when all celebrity endorsement essentials act together. Further, using the consumer data, the effective clusters that advertisers should target when they have a plan to endorse celebrities' features, is presented to provide useful insights to the advertisers. Additionally, study has practical implications for the creation of a data-driven tool that aids fashion businesses in developing AI-based celebrity endorsement applications.",Yes,본 논문은 구조방정식모델링(SEM)과 군집분석 등 정량적 연구 방법을 사용하여 셀러브리티 신뢰도와 매력이 브랜드 인지도 및 구매 의도에 미치는 영향을 실증적으로 분석하고 있다. 이는 독창적인 연구 모델을 제시하고 실증 데이터를 통해 검증한 연구 논문에 해당한다.
Mapping a Navigation System for Confined Space using CNN,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544781,"In today's world, a GPS based navigation system is inevitable. Though GPS provides a better navigation system, it compromises one's location privacy. Unintentional broadcast of one's location may open the gateway for malicious 3rd party applications that may not operate on a legal basis. To overcome privacy related issues, this paper proposes a method of navigation without the use of GPS. This can be achieved by combining Convolutional Neural Network(CNN) along with Dijkstra's Algorithm. In this work, initially the starting point of the user is identified by capturing the image of the nearest building, thereafter using the CNN model to process the image and identify the location. The dataset of more than 6,000 images was built considering various circumstances for training and testing of models. This dataset was fed to different deep learning models and CNN performed the best. The shortest path between the locations is calculated using a modified Dijkstra's algorithm. It uses priority queue to reduce the time complexity of traditional Dijkstra's algorithm. This paper will help to navigate through different college campuses/universities and other big premises without using GPS.",Yes,"논문은 GPS를 사용하지 않는 내비게이션 시스템을 CNN과 수정된 다익스트라 알고리즘을 결합하여 제안하고 있으며, 6,000장 이상의 이미지 데이터셋을 구축하고 모델을 학습시킨 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Machine Learning and IoT for Predicting the Productivity of MRI Equipment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10650835,"The rapid evolution and widespread accessibility of non-invasive medical imaging technologies, exemplified by Magnetic Resonance Imaging (MRI) and Computerized Tomography (CT), are fundamentally reshaping medical decisionmaking paradigms. These sophisticated imaging modalities, capable of extracting high-definition medical images, have emerged as integral components of modern healthcare, facilitating precise diagnostics and treatment planning. The escalating adoption of such technologies, however, has accentuated the need for a nuanced understanding and optimization of the performance and productivity of both medical equipment and the teams operating them, mainly due to the high costs and risks caused by their misuse. This work proposes using univariate analytical models to estimate the number of exams performed per day with machine learning algorithms. For such, different energy-related sensors monitoring 25 magnetic resonance equipment from three different brands were considered. The results of the research reveal a compelling validation of the proposed approach. A notably high Pearson correlation coefficient is observed between the predictions generated by the evaluated models and the real measurements obtained through the Radiology Information System (RIS). This robust correlation emphasizes the accuracy and reliability of the estimation models, validating their potential applicability in real-world healthcare scenarios. Furthermore, the study unveils an intriguing trend that distinguishes the performance of electric current sensors. Thirteen out of the 25 evaluated MRI machines demonstrate superior results when equipped with electric current sensors compared to other sensor types. This nuanced insight not only substantiates the critical role of energy-related sensors in predicting equipment performance but also underscores the importance of tailoring monitoring strategies to the unique characteristics of each machine.",Yes,"논문은 MRI 장비의 생산성을 예측하기 위해 머신러닝 알고리즘과 에너지 관련 센서를 활용한 독창적인 연구 방법을 제안하고, 실제 데이터를 통해 모델의 정확성을 검증하는 실험 결과를 포함하고 있다. 이는 직접 기여하는 연구 내용이 포함된 연구 논문임을 명확히 보여준다."
Potential Bottleneck and Measuring Performance of Serverless Computing: A Literature Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197837,"Trending form of cloud computing is Serverless computing, where developer just needs to focus on his code rather than worrying about server management. In serverless computing, application is nothing but collection of one or more functions, written for specific business functionality, which triggers on an event. There are various cloud service providers, i.e. Amazon, Microsoft, Google, IBM, etc. who provide serverless services, on pay as you use and auto scalable solution to execute the application code as a function. The developer just needs to upload the code for execution. The performance of the serverless computing may vary due to dynamic configuration of the solution, technologies and different technology used by the service provider.This paper reviews various past and recent work in the serverless computing to identify possible bottlenecks and the scope of measuring performance of serverless computing. It will also put some light to leverage machine learning in various possible ways to do performance engineering for future research.",No,본 논문은 서버리스 컴퓨팅 관련 기존 연구들을 리뷰하고 성능 병목 현상과 측정 방법을 정리하는 문헌 연구(literature study)로 보입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵습니다.
Classification of art paintings by genre,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967323,"This paper offers an approach to automatic art genre classification of paintings. Development of machine learning algorithms and increase of overall computing power improved speed and efficiency of feature extraction from digital images and with it opened a whole new set of possibilities in classification of visual data such as paintings and other visual art. Automatic classification is useful in large database processing (e.g. museums) and could be used as a commercial application on mobile platforms. Six genres are classified in the paper: realism, impressionism, cubism, fauvism, pointilism and naïve art. Some of the genres have now been tested for the first time. Used features are described as well as a measure of their usefulness. Rate of success for different classifiers is given. Accomplished results are similar to related work results.",Yes,"논문은 자동 미술 장르 분류를 위한 기계 학습 알고리즘 개발과 특징 추출 방법을 제안하고 있으며, 여섯 가지 장르에 대해 분류 성능을 평가하는 등 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구와 유사한 결과를 제시하면서도 일부 장르에 대해 최초로 테스트를 수행한 점에서 직접적인 연구 기여로 판단된다."
Utilizing the U-Net Network for Segmenting the Left Ventricle from Cine-MRI Scans,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10841228,"In this research, we studied the U-Net segmentation model for identifying and demonstrating addresses related to cardiac left ventricle contour in cine-MRI images. Efficacy of the model is evaluated across different stages such as preprocessing, training, validation and robustness testing along with comparisons to conventional methods and other deep learning architectures. But the preprocessing includes standardized pixel intensities, and by data augmentation using rotation, flip method or scaling techniques. The training and validation stages clearly show the robust consolidation of our model with little overfitting, guaranteeing decent performance on different datasets. The developed model is robust by passing the test against noise/ artifacts, and providing a very good segmentation in extreme situations so that there are no accuracy outliers represented. Comparative analyses show better sensitivity-specificity trade-offs and precision-recall characteristics versus other methods. In general, a U-Net model developed here works well for cardiac MRI analysis and can be used as an effective tool to help automatic LV segmentation accurate and efficient in clinical applications which will improve diagnosis of heart diseases from imaging points.",Yes,"논문은 U-Net 모델을 이용한 좌심실 분할에 대해 직접적인 연구를 수행하고 있으며, 데이터 전처리, 학습, 검증, 테스트 과정을 포함한 실험적 평가를 통해 모델의 성능을 입증하고 있다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문에 해당한다."
Cost-Sensitive Bootstrapped Weighted Random Forest for DoS attack Detection in Wireless Sensor Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707254,"Security of Wireless Sensor Networks is vital as this class of networks is increasingly being used for mission-critical applications, surveillance, military and disaster management monitoring. Machine learning algorithms are used nowadays in Intrusion Detection Systems that form the first line of defense against security attacks. This work proposes a cost-sensitive machine learning-based classifier trained on the WSN-DS dataset [13] comprising flooding, TDMA/Scheduling, Black-hole, and Grey-hole attack samples. Our proposed algorithm handles the imbalanced nature of the dataset efficiently without relying on resampling techniques. Other techniques for handling imbalance can induce extra computational processing that is unsuitable for battery-powered and resource-constrained sensor networks. Given this, we propose Cost-Sensitive Bootstrapped Weighted Random Forest (CSBW-Random Forest), which demonstrated superior performance over existing works. Our method gives the accuracy, precision, recall, and F1-score of 0.997, and per-class performance scores are also in the range of 0.95 to 0.99, which is significantly better than existing literature. The analysis also indicates that the proposed work is giving a better true positive rate (0.979), false-positive rate (0.003), false-negative rate (0.020) than related works. Experimentation with proportionately increasing data samples also validates the higher performance of our model.",Yes,"논문 초록에서 제안된 Cost-Sensitive Bootstrapped Weighted Random Forest 알고리즘은 기존 방법들보다 우수한 성능을 보이며, 데이터 불균형 문제를 해결하는 새로운 기법을 포함하고 있다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문임을 나타낸다."
A Novel Deeper One-Dimensional CNN With Residual Learning for Fault Diagnosis of Wheelset Bearings in High-Speed Trains,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8584445,"The health condition of a wheelset bearing, the key component of a railway bogie, has a considerable impact on the safety of a train. Traditional bearing fault diagnosis techniques generally extract signals manually and then diagnose the bearing health conditions through the classifier. However, high-speed trains (HSTs) are usually faced with variable loads, variable speeds, and strong environmental noise, which pose a huge challenge to the application of the traditional bearing fault diagnosis methods in wheelset bearing fault diagnosis. Therefore, this paper proposes a 1D residual block, and based on the block, a novel deeper 1D convolutional neural network (Der-1DCNN) is proposed. The framework includes the idea of residual learning and can effectively learn high-level and abstract features while effectively alleviating the problem of training difficulty and the performance degradation of a deeper network. Additionally, for the first time, we fully use the wide convolution kernel and dropout technology to improve the model's ability to learn low-frequency signal features related to the fault components and to enhance the network's generalization performance. By constructing a deep residual learning network, Der-1DCNN can adaptively learn the deep fault features of the original vibration signal. This method not only achieves very high diagnostic accuracy for the fault diagnosis task of wheelset bearings in HSTs under strong noise environment, but also its performance is quite superior when the train's working load changes without any domain adaptation algorithm processing. The proposed Der-1DCNN is evaluated on the dataset of the multi-operating conditions of the wheelset bearings of HSTs. Experiments show that this method shows a better diagnostic performance compared with the state-of-the-art deep learning methods of bearing fault diagnosis, which proves the method's effectiveness and superiority.",Yes,"본 논문은 기존 방법의 한계를 극복하기 위해 새로운 1D 잔차 블록과 심층 1D CNN 구조를 제안하고, 이를 통해 고속열차 휠셋 베어링 결함 진단 성능을 향상시키는 독창적인 연구 내용을 포함하고 있다. 또한 실험을 통해 제안한 모델의 우수성을 검증하여 직접적인 연구 기여를 하고 있음을 보여준다."
Leveraging ML Power for Crowdfunding Success Evaluation and Security Enforcement,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10581049,"Numerous online portals have been developed over time, to host large-scale and global crowdfunding campaigns[1] in order to fulfill the funding goals of the listed projects. Therefore it becomes essential to be able to assess the probability of success of these campaigns based on the provided parameters. In this dynamic landscape of Online crowdfunding, ensuring the security[2] and integrity[3]of such a model to safeguard the interests of the investors is imperative to develop a high degree of credibility of the platform. This research introduces a sophisticated Machine Learning (ML) model[4]which is tailored according to a specifically extracted dataset which works upon a comprehensive range of features like number of images, campaign timeline, textual description, security score, associated industry and other related attributes. A distinctive feature of this study includes engineering textual features[5]from the campaign description to analyze its impact on the accuracy of the predictive model. One of the key components of this research initiative lies in discerning the scam or fraudulent campaigns listed upon the platform. The model was evaluated under several metrics to ensure its thoroughness. Notably, the Logistic Regression (LR) model was the best performer, showing a favorable ROC curve and a high accuracy of 0.899, which indicated that our predictive model was successful in its objective. Furthermore, by emphasizing the identification and suspension of fraudulent campaigns, our study fills a significant research gap and safeguards investor interests while promoting platform confidence. Our study increases investor trust in online crowdfunding ecosystems and by offering a strong foundation for well-informed decision-making. Our research offers a novel and thorough method for using machine learning to improve the effectiveness, security, and legitimacy of crowdfunding platforms.",Yes,"논문 초록에서 머신러닝 모델을 개발하고 평가한 구체적인 연구 방법과 결과가 명확히 제시되어 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문임을 알 수 있습니다. 또한, 사기 캠페인 식별 및 보안 강화라는 새로운 문제 해결에 기여하고 있음을 보여줍니다."
Developing machine learning tools for long-lead heavy precipitation prediction with multi-sensor data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116011,"A large number of extreme floods were closely related to heavy precipitation which lasted for several days or weeks. Long-lead prediction of extreme precipitation, i.e., prediction of 6-15 days ahead of time, is important for understanding the prognostic forecasting potential of many natural disasters, such as floods. Yet, long-lead flood forecasting is a challenging task due to the cascaded uncertainty with prediction errors from measurements to modeling, which makes the current physics-based numerical simulation models extremely complex and inaccurate. In this paper, we formulate the modeling work as a machine learning problem and introduce a complementary data mining framework for heavy precipitation prediction. Heavy precipitation that may lead to extreme floods is a rare event. Long-lead prediction requires the corresponding feature space to be sampled from extremely high spatio-temporal dimensions. Such a complexity makes long-lead heavy precipitation prediction a high dimensional and imbalanced machine learning problem. In this work, we firstly define the extreme precipitation and non-extreme precipitation clusters and then design the Nearest-Sample Choosing method to handle the imbalanced data sets. We introduce streaming feature selection and subspace learning to extract the most relevant features from high dimensional data. We evaluate the machine learning tools using historical flood data collected in the State of Iowa, the United States and associated hydrometeorological variables from 1948 to 2010.",Yes,"논문은 장기 예측을 위한 중폭우 예측 문제를 머신러닝 문제로 정의하고, 불균형 데이터 처리 및 특징 선택 기법을 설계하는 등 독창적인 연구 방법론을 제시하고 있다. 또한, 실제 역사적 데이터를 활용해 제안한 기법을 평가하여 직접적인 연구 기여를 포함하고 있음을 보여준다."
DenGue CarB: Mosquito Identification and Classification using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9357133,"This research paper discusses a web-based application that assists Public Health Officers in the dengue identification process. The mosquito classification is done using image processing and machine learning techniques. The training models are developed using Convolutional Neural Networks Algorithm, Support Vector Machine Algorithm, and K-Nearest Neighbors Algorithm to validate the results to determine the most accurate and suitable algorithm. this paper discusses the previous related research work on its significance and drawbacks while highlighting design, methods, and implementation in the solution. We conclude that the CNN algorithm provides the highest accuracy among the machine learning techniques used.",Yes,본 논문은 모기 분류를 위한 이미지 처리 및 머신러닝 기법을 적용하여 직접적인 모델 개발과 성능 평가를 수행하고 있어 독창적인 연구 내용을 포함하고 있다. 또한 여러 알고리즘을 비교 분석하여 최적의 방법을 제시하는 연구 기여가 명확하다.
Leveraging Industry 4.0 in Education for Remote Implementation in a Team-Based Computer Engineering Capstone Project,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892929,"This Innovative Practice Category Full Paper presents the remote implementation of an embedded systems capstone project for computer engineering students. A capstone project is a feature of most undergraduate programs in computer engineering. Such a project is usually meant to expose students to the development of a large system from conceptualization to its final implementation, involving substantial design and development of hardware and software components. In our university, students were given the opportunity to work on a cutting-edge problem focused on healthcare - “Designing a wearable device that automatically detects human activities”. It is an area where a large amount of research is ongoing, and hundreds of scientific papers are published every year. In addition, students had to evaluate and adopt techniques from existing literature and adapt them to meet the problem requirements. Students equipped themselves with state-of-the-art hardware, Bio-signal processing, machine learning, power optimization and secure communications to design the wearable. Thus, the project reinforced their knowledge of fundamentals, while exposing them to a problem with no obvious solution. Through the capstone project, students are able to better appreciate the relevance of the various components in the computer engineering curriculum to large-scale computer engineering projects. Students are organized into teams of six to execute the project. The COVID-19 pandemic resulted in the university migrating the teaching online for all courses. This was particularly challenging to implement for the capstone project as one of the key requirements is for the members to work together and subsystems to interact with each other. A course refresh resulted in a credits update and this provided a unique opportunity for the teaching team to re-design the project with core Industry 4.0 technologies such as hardware acceleration, remote processing, networking, remote analytics, and secure protocols. This work presents a framework for its implementation, reviews the challenges encountered and the processes put in place to ensure data security and smooth running of the project in the event of future disruptions. Quantitative and qualitative results from the course feedback surveys are analyzed to gauge student response to the implementation and compared to the previous version.",No,"본 논문은 원격으로 진행된 캡스톤 프로젝트의 교육적 실행 사례와 그 과정에서의 도전과 해결책을 다루고 있으며, 직접적인 독창적 연구 결과나 새로운 과학적 발견을 제시하지 않는다. 따라서 연구 논문보다는 교육 혁신 사례 보고에 해당한다."
Unmasking Manipulations: Recent Advances in Image Tampering Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10923430,"This work aims at exploring the cutting-edge developments in image tampering detection, to unveil the latest techniques and methodologies employed to identify and expose digital image manipulations. The paper begins by reviewing the evolving landscape of image tampering and the growing need for advanced detection mechanisms in an era of sophisticated editing tools.The paper delves into pixel-based analysis, frequency-based analysis, statistical approaches, and the integration of deep learning in image tampering detection. Highlighting the strengths and limitations of each method, the review provides insights into how these approaches contribute to the ongoing battle against image forgeries.Additionally, the role of metadata examination, watermarking, and steganalysis is discussed in the context of enhancing the robustness of tampering detection. The paper addresses challenges posed by the rapid evolution of tampering techniques and the emergence of adversarial attacks, emphasizing the importance of adaptive and resilient detection strategies. Real-world applications of image tampering detection, ranging from criminal investigations to journalistic ethics, are explored to demonstrate the practical significance of the discussed advancements. The paper concludes by proposing avenues for future research, emphasizing the need for interdisciplinary collaboration and the development of standardized benchmarks to evaluate the effectiveness of emerging detection methods.",No,"초록 내용은 최신 이미지 변조 탐지 기술에 대한 종합적인 리뷰와 기존 연구들의 개요를 제공하는 데 중점을 두고 있으며, 독창적인 연구 결과나 새로운 실험적 기여가 명확히 제시되어 있지 않습니다. 따라서 이 논문은 연구 논문이라기보다는 리뷰 논문에 해당합니다."
Leveraging Tactile Sensors for Low Latency Embedded Smart Hands for Prosthetic and Robotic Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751605,"Tactile sensing is a crucial perception mode for robots and human amputees in need of controlling a prosthetic device. Today, robotic and prosthetic systems are still missing the important feature of accurate tactile sensing. This lack is mainly due to the fact that the existing tactile technologies have limited spatial and temporal resolution and are either expensive or not scalable. In this article, we present the design and implementation of a hardware–software embedded system called SmartHand. It is specifically designed to enable the acquisition and real-time processing of high-resolution tactile information from a hand-shaped multisensor array for prosthetic and robotic applications. During data collection, our system can deliver a high throughput of 100 frames per second, which is 13.7× higher than previous related work. This has allowed the collection of a new tactile dataset consisting of 340 000 frames while interacting with 16 objects from everyday life during five different sessions. Together with the empty hand, the dataset presents a total of 17 classes. We propose a compact yet accurate convolutional neural network that requires one order of magnitude less memory and 15.6× fewer computations compared with related work without degrading classification accuracy. The top-1 and top-3 cross-validation accuracies on the collected dataset are, respectively, 98.86% and 99.83%. We further analyze the intersession variability and obtain the best top-3 leave-one-out-validation accuracy of 77.84%. We deploy the trained model on a high-performance ARM Cortex-M7 microcontroller achieving an inference time of only 100 ms minimizing the response latency. The overall measured power consumption is 505 mW. Finally, we fabricate a new control sensor and perform additional experiments to provide analyses on sensor degradation and slip detection. This work is a step forward in giving robotic and prosthetic devices a sense of touch by demonstrating the practicality of a smart embedded system that uses a scalable tactile sensor with embedded tiny machine learning.",Yes,"논문은 새로운 하드웨어-소프트웨어 임베디드 시스템 설계 및 구현, 고해상도 촉각 데이터셋 수집, 경량화된 신경망 모델 제안과 실제 마이크로컨트롤러에의 배포 등 독창적인 연구 내용을 포함하고 있습니다. 이는 기존 기술 대비 성능 향상과 실용성 증대를 보여주는 직접적인 연구 기여로 판단됩니다."
Enhancing Thyroid Disease Diagnosis Through Machine Learning: A Classification-Based Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10864062,"Thyroid disease has recently adopted the related classification which has been worthy in the contemporary system of healthcare delivery because of the advances in diagnostics. This work proposes an automated diagnostic system for thyroid disease using machine learning approach to improve the diagnostic accuracy of the disease. The presented model also uses the features selection and classifiers like SVM and Random Forest for the classification of different thyroid disorders including hypothyroidism, hyperthyroidism, and euthyroidism. Missing values in the dataset are treated to provide actual worth in the results while imbalance is also dealt with. The process involves feeding its input a well-constructed data set after which it is checked for its performance through a validation process. The performance of the model is evaluated based on thyroid disease related parameters and the findings show a better performance compared to conventional diagnosis techniques. In this study, the authors also present their future work on how to possibly incorporate this machine learning framework into clinical practice although the study shows how this framework will help the healthcare workers to make better decisions in relation to thyroid diseases.",Yes,"본 논문은 머신러닝 기법을 활용한 갑상선 질환 진단 자동화 시스템을 제안하고, 데이터 전처리, 특징 선택, 분류기 적용 및 성능 평가를 포함한 구체적인 연구 과정을 다루고 있어 독창적인 연구 내용을 포함한 연구 논문으로 판단된다. 또한 기존 진단 기법과의 성능 비교를 통해 기여를 명확히 하고 있다."
Achievable Rate Approximation of Large Intelligent Surface Based on Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10540586,"In order to enhance the achievable rate of large intelligent surfaces (LIS) afflicted by hardware impairment (HWI), existing LIS systems have adopted a distributed deployment scheme that takes into consideration the impact of HWI. This study aims to investigate the optimization problem of matrix design for distributed LIS in the presence of HWI, as well as to reduce the required number of channel samples and computational complexity for calculating the HWI equivalent noise density and utility. A deep learning-based approach is proposed in this work, which not only learns environment-related information between LISs aiming for maximizing the achievable rate of LIS systems with HWI, but also uses small training samples so to ease the system complexity.",Yes,"본 논문은 하드웨어 결함이 있는 대형 지능형 표면(LIS)의 성능 향상을 위해 분산 배치 및 행렬 설계 최적화 문제를 다루고 있으며, 딥러닝 기반 접근법을 제안하여 직접적인 성능 개선과 계산 복잡도 감소를 목표로 한다. 이는 독창적인 연구 내용과 기여를 포함한 연구 논문으로 판단된다."
A case study on machine learning model for code review expert system in software engineering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104731,"Code review is a key tool for quality assurance in software development. It is intended to find coding mistakes overlooked during development phase and lower risk of bugs in final product. In large and complex projects accurate code review is a challenging task. As code review depends on individual reviewer predisposition there is certain margin of source code changes that is not checked as it should. In this paper we propose machine learning approach for pointing project artifacts that are significantly at risk of failure. Planning and adjusting quality assurance (QA) activities could strongly benefit from accurate estimation of software areas endangered by defects. Extended code review could be directed there. The proposed approach has been evaluated for feasibility on large medical software project. Significant work was done to extract features from heterogeneous production data, leading to good predictive model. Our preliminary research results were considered worthy of implementation in the company where the research has been conducted, thus opening the opportunities for the continuation of the studies.",Yes,"논문은 머신러닝을 활용한 코드 리뷰 전문가 시스템에 대한 새로운 접근법을 제안하고, 실제 대규모 의료 소프트웨어 프로젝트에 적용하여 예비 연구 결과를 도출하였다. 이는 독창적인 연구 내용과 실험적 평가를 포함한 연구 논문으로 판단된다."
Benchmark and Design Support for Demand-Oriented Cloud-Communication Architectures of Cyber-Physical Production Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10312564,"The usage of cloud applications in industrial automation domain is boosted by the desire to apply computational-intensive machine-learning (ML) models and access production data from all over the world. Accomplished by the demand for powerful hardware platforms for ML and centralized data storage, the connectivity performance also increases and allows high data transmission rates. Nevertheless, requirements for data transmission, like safety, security, and real-time must be considered in line with the production use case. Finding a suitable infrastructure with communication protocols for data transmission to transform legacy production plants into cyber-physical production systems (CPPS) is overwhelming due to the many available communication methods. Design support for CPPS would increase the acceptance of cloud-based solutions by engineers of automated production systems that are not specifically familiar with high-level information technology systems. This paper introduces an investigation and design recommendations for application-layer communication protocols usually used in industrial applications. The overall goal is to support the engineering for the different automation levels on field-, edge-, and cloud-level. In this paper, design measures are firstly derived from related work. Secondly, an analysis of the timing behavior and the CPU resources are carried out. Finally, the findings are collected in a summarizing rating table that briefly suggests adjusting the performance and the impact on the overall system design by selecting suitable communication protocols.",Yes,"본 논문은 산업 자동화 분야에서 클라우드 기반 통신 프로토콜의 성능 분석과 설계 지원 방안을 직접 연구하여, 타이밍 동작과 CPU 자원 분석 등 구체적인 실험과 평가를 수행하고 있다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문에 해당한다."
ECG multi-class classification using neural network as machine learning model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8379901,"The main objective of this paper is to prepare a Clinical Decision Support System (CDSS) for a multi-class classification of ElectroCardioGram (ECG) signals into certain cardiac diseases. This CDSS is based on Artificial Neural Network (ANN) as a machine learning classifier and uses time scale input features. Fourty eight (48) ECG signals were selected from MIT-BIH arrhythmia database, of one minute recording. Unfortunately, among several types of learning algorithms for the ANN classifier, finding the appropriate one demands a comparative study. So, in this study, we have evaluated the impact of two learning algorithms, which are the Levenberg-Marquardt (trainlm) and the Bayesian-Regularization (trainbr) on the proposed ANN performance. Consequently, we have achieved that trainbr reaches the most accurate result (93.8%), while trainlm generates the highest classification speed (0.582s). Subsequently, in order to assess the efficiency of this work, a second comparative study with related works, is done. Therefore, despite not being in the same working conditions, the obtained accuracy (93.8%) is considered acceptable.",Yes,"본 논문은 ECG 신호의 다중 클래스 분류를 위한 인공신경망 기반 임상 의사결정 지원 시스템을 제안하고, 두 가지 학습 알고리즘의 성능을 비교 평가하는 독창적인 연구를 수행하였다. 이는 기존 연구와 차별화된 실험적 기여와 성능 분석을 포함하고 있어 연구 논문에 해당한다."
Dysarthric Speech Detection Using Hybrid Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10308386,"Dysarthria is a speech disorder caused by weak or poorly coordinated speech-related muscles. Dysarthria can be caused by various factors such as stroke, multiple sclerosis, or cerebral palsy, as well as brain injury or certain medications. The ultimate goal of our paper is to make a suitable and accurate tool for detecting dysarthric speech that can be used in clinical settings for early diagnosis, treatment planning for dysarthric individuals. In this work, the machine learning models namely CNN (Convolutional Neural Network), and hybrid models, such as CNN, combined with LSTM (Long Short-Term Memory), and CNN combined with GRU (Gated Recurrent Unit) are trained on TORGO dataset, to classify the dysarthric speech from non-dysarthric speech. The model's performance is evaluated on the testing data and all of these models produce more than 95% accuracy rate. From this study, we understood that this methodology can be used in developing Automatic Speech Recognition(ASR) for people with dysarthria, which would become an essential technology for many applications.",Yes,"본 논문은 CNN, LSTM, GRU 등 다양한 머신러닝 모델을 활용하여 난조음(dysarthric speech) 탐지에 대한 실험적 연구를 수행하고, TORGO 데이터셋을 이용해 모델을 학습 및 평가한 결과를 제시하고 있다. 이는 독창적인 연구 방법과 실험 결과를 포함한 연구 논문에 해당한다."
Efficient autism spectrum disorder prediction with eye movement: A machine learning framework,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344638,"We propose an autism spectrum disorder (ASD) prediction system based on machine learning techniques. Our work features the novel development and application of machine learning methods over traditional ASD evaluation protocols. Specifically, we are interested in discovering the latent patterns that possibly indicate the symptom of ASD underneath the observations of eye movement. A group of subjects (either ASD or non-ASD) are shown with a set of aligned human face images, with eye gaze locations on each image recorded sequentially. An image-level feature is then extracted from the recorded eye gaze locations on each face image. Such feature extraction process is expected to capture discriminative eye movement patterns related to ASD. In this work, we propose a variety of feature extraction methods, seeking to evaluate their prediction performance comprehensively. We further propose an ASD prediction framework in which the prediction model is learned on the labeled features. At testing stage, a test subject is also asked to view the face images with eye gaze locations recorded. The learned model predicts the image-level labels and a threshold is set to determine whether the test subject potentially has ASD or not. Despite the inherent difficulty of ASD prediction, experimental results indicates statistical significance of the predicted results, showing promising perspective of this framework.",Yes,"본 논문은 자폐 스펙트럼 장애(ASD) 예측을 위해 새로운 기계 학습 기반의 특징 추출 방법과 예측 프레임워크를 제안하고 있으며, 실험을 통해 그 유의미성을 검증하고 있다. 이는 기존 연구와 차별화된 독창적인 연구 내용을 포함한 연구 논문으로 판단된다."
A Theoretical Comparison of Federated Learning with Differential Privacy and Blockchain for Security and Privacy in IoMT,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10857505,"The advancement of decentralized, real-time data collection through the Internet of Medical Things is transforming the healthcare industry. However, this innovation brings forth significant privacy, security, and scalability challenges. Federated Learning offers a reliable solution by enabling distributed machine learning while preserving data localization. This paper introduces two frameworks-Federated Learning combined with Differential Privacy and Blockchain-enhanced Federated Learning-to enhance robustness in IoMT systems. We compare these frameworks theoretically, evaluating their effectiveness in mitigating risks related to data confidentiality, adversarial resilience, scalability, and computational efficiency. FL-DP provides formal privacy guarantees through differential privacy techniques but is limited by the need to manage the privacy budget (E), especially in large-scale deployments. Alternatively, Blockchain-based FL maintains data integrity and decentralized trust using consensus mechanisms such as Proof of Work and Proof of Stake, but it encounters challenges related to scalability and computational efficiency. Our findings suggest that the choice between FL-DP and Blockchain-based FL depends on the specific security and privacy requirements of the IoMT application. FL-DP is better suited for privacy-critical applications where strict data confidentiality is paramount, while Blockchain-based FL is more appropriate when data integrity and trust are the primary concerns.",Yes,"본 논문은 Federated Learning과 Differential Privacy, Blockchain을 결합한 두 가지 프레임워크를 제안하고 이들의 보안 및 개인정보 보호 성능을 이론적으로 비교하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Machine Learning Techniques for Analysis of Mars Weather Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10194233,"The exploration of Mars has provided vast amounts of weather data that present unique challenges for analysis and prediction. To address these challenges, this research paper focuses on the application of machine learning techniques for the analysis of Mars weather data. The objective is to develop models that can effectively extract patterns, uncover hidden relationships, and enable accurate predictions in the Martian weather system. The research begins with a comprehensive review of the available Mars weather data. The dataset, consisting of historical records, serves as the foundation for training and evaluating machine learning models. The analysis of the Mars weather dataset reveals the planet's icy and harsh climate, with average maximum temperatures of around -21°C and average minimum temperatures of -80°C. The temperature variations show that the minimum temperature fluctuates within a narrower range of 20-30°C over the course of Mars sols from 0 to 2000, while the maximum temperature experiences larger variations of about 40-50°C. During this time, the atmospheric pressure on Mars fluctuates between 720 Pa and 950 Pa. In addition, using the elbow method revealed that 3 clusters were the ideal number for identifying distinct patterns in the weather data. The linear regression model also attained an accuracy of 85%, demonstrating its efficacy in forecasting weather patterns on Mars.",Yes,"본 논문은 화성 기상 데이터 분석을 위해 머신러닝 기법을 적용하고, 데이터 클러스터링과 선형 회귀 모델을 개발하여 예측 정확도를 평가하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Interpretable Anomaly Detection for Knowledge Discovery in Semiconductor Manufacturing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384026,"Machine Learning-based Anomaly Detection approaches are efficient tools to monitor complex processes. One of the advantages of such approaches is that they provide a unique anomaly indicator, a quantitative index that captures the degree of ’outlierness’ of the process at hand considering possibly hundreds or more variables at the same time, the typical scenario in semiconductor manufacturing. One of the drawbacks of such approaches is that Root Cause Analysis is not guided by the system itself. In this work, we show the effectiveness of a method, called DIFFI, to equip Isolation Forest, one of the most popular Anomaly Detection algorithms, with interpretability traits that can help corrective actions and knowledge understanding. Such approach is validated on real world semiconductor manufacturing data related to a Chemical Vapor Deposition process.",Yes,"논문은 Isolation Forest 기반의 이상 탐지 알고리즘에 해석 가능성을 부여하는 새로운 방법(DIFFI)을 제안하고, 이를 실제 반도체 제조 데이터에 적용하여 검증한 연구 내용을 포함하고 있다. 이는 독창적인 연구 기여로 볼 수 있어 연구 논문에 해당한다."
Pragmatic Analysis of IoMT Network Modelling Techniques from a Statistical Perspective,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751848,"Internet of Medical Things (IoMT) networks are high precision wireless interfaces that require design of accurate sensing, actuation, and processing devices. These devices include wireless electrocardiograph (ECG) sensors, electroencephalograph (EEG) sensors, wireless blood pressure monitoring devices, etc. Due to direct patient interface, these devices are required to have superior performance in terms of sensing accuracy, processing efficiency, and communication quality of service (QoS) parameters. A wide variety of models are proposed by IoMT researchers to perform this task, but each of these models vary in terms of network size, deployment complexity, cost of deployment, processing delay, etc. These models include machine learning routing techniques, blockchain based security methods, privacy preservation methods, high-precision sensor design methods, high-performance communication interfaces, etc. Due to such a wide variation in performance, IoMT network design requires continuous validation, which increases time-to-market, thereby increasing deployment cost. In order to reduce number of validations, a statistical survey of models for IoMT network design is discussed in this text. This discussion is focussed towards evaluating various characteristics, advantages, limitations, and future research scopes in existing models. readers would be able to identify best performing model(s) for a given IoMT application. It is followed by a statistical analysis of the reviewed IoMT network design models in terms of end-to-end delay, communication QoS, network security, scalability, computational complexity, cost of deployment, and application of use. This statistical performance evaluation will further assist readers to statistically compare the reviewed methods, and identify best performing model(s) & their combinations for context-specific network deployments. Due to this, readers would be able to reduce network design & validation delay, which will further assist in reducing design costs, thereby facilitating faster deployments. The paper text also recommends various fusion, and transfer learning-based techniques which can be used for improving performance of the reviewed models.",No,본 논문은 IoMT 네트워크 설계 모델들에 대한 통계적 분석과 기존 연구들의 장단점 및 향후 연구 방향을 논의하는 리뷰 및 설문 조사 성격의 논문으로 보입니다. 직접적인 실험 결과나 새로운 모델 제안 등 독창적인 연구 기여가 포함되어 있지 않아 연구 논문으로 보기 어렵습니다.
A systematic assessment and meta-analysis of machine learning methods for predicting and Classifying severe Long Term kidney disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525982,"Heart failure (HF), atherosclerotic cardiovascular disease, and end-stage kidney disease are all signs of long term kidney disease brought on by type II diabetes (CKD). However, because it has no symptoms, CKD is frequently overlooked or misdiagnosed. Without kidneys, a human beings can able live for about 18 days, necessitating dialysis and a kidney transplant. It is complex to have accurate tools for early diagnosis of kidney diseases. Additionally, patients frequently forego the common urine protein-based CKD detection test. It describes a medical condition that damages the kidneys and has an impact on the body as a whole. Inadequate diagnosis and care can lead to end-stage renal disease, which eventually kills the patient. In response to the shortcomings of conventional biomarkers and the requirement for early therapeutic intervention in cats with CKD to improve outcomes, new renal biomarkers for the detection of glomerular or tubular dysfunction have been discovered and validated. Changes in the concentrations of these biomarkers in the blood or urine may reveal early kidney damage or forecast the development of kidney disease before changes in conventional biomarkers can be seen. On Utilization of machine learning (ML) approaches, CKD prediction is simple. The method of the present study for predicting long term kidney disease and classifying it on basis of the clinical diagnosis information includes data preprocessing, a method for handling missing data, feature reduction and feature extraction. We examine some recent works on ML-based CKD detection and classification in this systematic review. It analyze of the step of data processing from data collection to data classification on utilizing the dataset preprocessing step, hybrid or ensemble feature extraction technique, meta-heuristics based feature selection technique and finally detection and classification of the kidney disease. To be more precise, we group this systematic review into two important types: supervised learning, and unsupervised learning based CKD detection and classification. Peer-reviewed papers were gathered using reputable search engines like Elsevier, IEEE Xplore, PubMed, Scopus, Web of Science, and others. Finally, we briefly discuss current problems and the requirements for linking academic research on CKD detection and classification to industry-based solutions that will improve standard clinical care.",No,"이 논문은 기존 연구들을 체계적으로 검토하고 메타분석을 수행하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구들의 종합과 평가에 중점을 두고 있습니다."
HEND-FL: Accurate Federated Learning Using Homomorphic Encryption and a New Distributed Protocol,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928012,"Federated learning (FL) is the latest development of distributed machine learning(ML), in which data is acquired and processed locally on the client, and then updated ML parameters are transmitted to a central server for aggregation. However, FL also brings some challenges, since it may reveal private information by analyzing upload parameters (such as weights trained in deep neural networks) from the client. To tackle the problem, this paper introduces a decentralized system, HEND-FL, which relies on a group of computing nodes to achieve aggregate calculations of parameters. In order to ensure the data confidentiality and the privacy of the data provider, this proposed system combines an interactive protocol and homomorphic encryption, and uses the Chinese Remainder Theorem to optimize the speed of decryption. Based on the discrete logarithm problem of elliptic curves, the scheme can be proved to be secure under the defined threat model. We evaluate the effectiveness of our plan and compare it with existing related work on the MNIST dataset. The results show that our scheme has almost the same accuracy in the plaintext and encrypted state. Compared with other encryption schemes, due to our smaller ciphertext size and the assistance of multiple computing nodes, the burden on the central server is significantly reduced.",Yes,"이 논문은 연합학습에서 프라이버시 보호를 위한 새로운 분산 프로토콜과 동형암호를 결합한 시스템(HEND-FL)을 제안하고, 보안성 증명과 성능 평가를 포함한 실험 결과를 제시하고 있다. 이는 기존 연구와 차별화된 독창적인 연구 내용을 포함한 연구 논문으로 판단된다."
Smart Patient Monitoring and Recommendation (SPMR) Using Cloud Analytics and Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496106,"The escalating global prevalence of chronic and lifestyle-related illnesses presents substantial societal and economic challenges. This work delves into an extensive review of healthcare monitoring systems tailored for chronic and lifestyle disorders. Subsequently, we propose a pioneering Smart Patient Monitoring and Recommendation (SPMR) framework, leveraging Deep Learning (DL) and cloud-based analytics. SPMR ensures continuous monitoring and predictive insights into a patient’s authentic health status using data from vital signs and contextual activities collected via Ambient Assisted Living devices. Within the predictive DL component of the LIP module, we employ Categorical Cross Entropy (CCE) Optimization to forecast real-world health conditions using unbalanced datasets derived from Chronic Blood Pressure Disorder case studies. Significantly, SPMR’s capability to deliver real-time preventive measures and treatments persists even without Internet or cloud connectivity. This circumvents the need to replicate Machine Learning (ML) models and associated procedures in local setups, thus streamlining operations. Comparative analysis against analogous models showcases the considerable effectiveness of our proposed model, notably enhancing accuracy by up to 8 to 18 percent. Moreover, both the overall F-score and the emergency class F-score exhibit marked improvements of 17% and 36%, respectively. These outcomes underscore SPMR’s pivotal role, especially during crises, emphasizing its significance in healthcare monitoring systems.",Yes,"논문은 기존 연구를 검토한 후, 딥러닝과 클라우드 분석을 활용한 새로운 스마트 환자 모니터링 및 추천(SPMR) 프레임워크를 제안하고 있으며, 실제 사례 연구를 통해 모델의 성능을 비교 분석하여 정확도와 F-score 향상을 입증하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함한 연구 논문임을 나타낸다."
Quality Assessment for Natural and Screen Content Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8977791,"Quality assessment (QA) of screen content images (SCIs) has gained more and more popularity. SCIs are very different from natural images (NIs) which have been dealing with by most researchers in the literature. QA methods specifically designed for NIs also can be used to evaluate the quality of SCIs. Yet, their performances are unsatisfactory. This may due to the statistical differences of SCIs and NIs. In this paper, SCIs and NIs QA methods in the literature are being compared and studied for both SCIs and NIs benchmarked databases. It is found out that methods that incorporate gradient features work well for both SCIs and NIs. This points out a possible way to utilize gradient features to come out with a QA method that works for both SCIs and NIs simultaneously. Hence, application related to SCIs and NIs such as deep learning and multitasking for person tracking system can be improved with the QA method.",Yes,"논문 초록에서 기존 QA 방법들을 비교 분석하고, SCIs와 NIs 모두에 적용 가능한 새로운 품질 평가 방법의 가능성을 제시하고 있어 독창적인 연구 기여가 포함된 것으로 판단됩니다. 또한, gradient 특징을 활용한 새로운 접근법을 제안함으로써 연구적 기여를 하고 있습니다."
A Generative Deep Learning Framework Across Time Series to Optimize the Energy Consumption of Air Conditioning Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676657,"Working towards active buildings that fully integrate efficient demand management with renewable energy sources and storage, energy efficiency is an important step, as building inefficiencies cause energy wastage and increase energy-related expenses. Currently, static thermal setpoints are typically used to maintain the inside temperature of a building at a comfortable level irrespective of its occupancy. This paper introduces a deep learning framework that trains across time series to forecast the temperatures of a future period directly where a particular room is unoccupied and optimises the setpoints of the room. To the best of our knowledge, this is the first study to use a state-of-the-art deep learning method trained across series to accurately predict temperatures for the subsequent optimal control of room setpoints. In contrast to traditional forecasting approaches that build isolated models to predict each series, our framework uses global recurrent neural network models that are trained with a set of relatively short temperature series, allowing the models to learn cross-series information. The predicted temperatures were then used to define the optimal thermal setpoints to be used inside the room during the unoccupied periods. We evaluate the prediction accuracy of our deep learning framework against a set of state-of-the-art forecasting models and can outperform those by a large margin. Furthermore, we analyse the usage of our deep learning framework to optimise the energy consumption of an air conditioning system in a real-world scenario using temperature data from a university lecture theatre. Based on simulations, we show that our proposed framework can lead to savings of approximately 20% and 15%, respectively, compared to the traditional temperature control model that does not use optimisation techniques and a programmable thermostat.",Yes,"본 논문은 딥러닝 프레임워크를 개발하여 시간 시계열 데이터를 활용한 온도 예측 및 에너지 최적화 방법을 제안하고, 기존 모델 대비 성능 향상을 실험적으로 입증하는 독창적인 연구 내용을 포함하고 있습니다. 따라서 직접 기여하는 연구 논문에 해당합니다."
CFD Investigation of Dispersion of Airborne Particulate Contaminants in a Raised Floor Data Center,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142865,"Modern data center facilities administrators are finding it increasingly difficult to lower the costs incurred in mechanical cooling of their IT equipment. This is especially true for high computing applications like Artificial Intelligence, Bitcoin Mining, Deep Learning, etc. Airside Economization/free air cooling reduces the mechanical cooling costs by using outside air to cool IT equipment under favorable ambient conditions. In this process, administrators risk their equipment to the exposure of fine particulate/gaseous contaminants that might enter the data center facility with the cooling airflow. Literature suggests that the nature of failures caused by particulate contamination is very intermittent which makes the failures tough to predict. While the recommended filters can remove PM10-2.5, it's the fine and ultra-fine particulates like DPM (Diesel Particulate Matter), corrosive salts of high ionic content like sulfates and nitrates with low DRH (Deliquescent Relative Humidity) values that are the cause of concern. The present investigation utilizes a 3 - D CFD modeling of particle-laden flow in a rectangular flow domain, imitating the flow through floor tiles as in a raised floor data center. Literature was reviewed to study various numerical models that have been used for simulating particle dispersion and particle deposition in ventilated rooms, air ducts and particle behavior across physical obstructions of various geometries. A Discrete Phase Modeling approach was chosen using ANSYS FLUENT to calculate trajectories of the dispersed contaminants. 6SigmaRoom was used to predict accurate boundary and flow conditions of the fluid flow leaving the floor tiles.",Yes,논문 초록에서 3D CFD 모델링과 수치 시뮬레이션을 활용하여 데이터 센터 내 미세 입자 분산을 직접 조사하는 독창적인 연구를 수행한 것으로 보입니다. 이는 기존 문헌 검토뿐만 아니라 새로운 계산 모델 적용과 결과 예측을 포함하고 있어 연구 논문에 해당합니다.
Effective Features Selection and Machine Learning Classifiers for Improved Wireless Intrusion Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530969,"Machine learning algorithms are effective means applied to wireless intrusion detection systems (WIDS) in an attempt to protect computing resources against unauthorized access. A key aspect for an improved WIDS based on machine learning classification is features selection. This paper considers multiclass classification that utilizes four effective features sets of 32, 10, 7 and 5 features, respectively. The classes represent 15 types of 802.11 MAC layer attacks, and the features represent 802.11 frame fields information. The experimental results utilized the Aegean Wi-Fi Intrusion Dataset (AWID) to evaluate the performance of seven well-known machine learning classifiers, namely, AdaBoost, Random Forest, Random Tree, J48, logit Boost, Multi-Layer Perceptron, and ZeroR with respect to the selected features set. The presented work outperforms previous related work in terms of number of classes, features and accuracy. The proposed system using the Random Forest algorithm and 32 features achieves a maximum accuracy of 99.64%. By using logit Boost with five features, we achieved a maximum accuracy of 99.53%.",Yes,"본 논문은 무선 침입 탐지 시스템에서 효과적인 특징 선택과 여러 머신러닝 분류기를 적용하여 성능을 평가하는 실험적 연구를 수행하고 있으며, 기존 연구 대비 향상된 정확도를 제시하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Decoding of finger movement using kinematic model classification and regression model switching,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836126,"Brain Computer Interface (BCI) is one of the clinical applications that may help to restore communication to people with motor disabilities. Electrocorticography (ECoG) is a semi invasive record to brain signals from electrode grids on the cortex surface. ECoG signal makes possible localization of the source of neural signals due to its high spatial resolution. This study is a step towards exploring the usability of ECoG signal as BCI input technique and a multidimensional BCI control. The objective of this deterministic approach is to predict individual finger movement from ECoG signal by combining both classification and regression problems in machine learning of signal responses (regression via classification), on the other hand addressing the signal responses variability within a single subject. The dataset used in this work is the one presented in the fourth dataset from BCI competition IV. The difficulty is that; there is no simple and direct relation between ECoG signals and finger movements. This research work starts in two directions. The first direction is related to the decoding of the finger position signal to obtain a finger movement state signal. The second direction is related to the ECoG recorded signal, in order to obtain the corresponding brain signal of each finger movement. The work consists of five main phases (decoding finger state, pre-processing, features acquisition, classification, and regression). This approach suggests kinematic finger model which is applied on the finger muscle signal to generate the finger kinematic state signal. For feature extraction we used shift invariant wavelet decomposition and multi-taper frequency spectrum, followed by Gram-Schmidt test for selection. Linear support vector machine (SVM) is used for classification. Regression models are established by using the finger position training signal and the acquired ECoG features. To predict the finger movement signal under test; switching between these regression models is made. Finally the predicted finger movement signal is correlated with the measured one for evaluation. Results show that the average correlation measure between real and predicted movement is 0.82. This result is higher than the one obtained by the competition winner (0.46).",Yes,"논문은 ECoG 신호를 이용해 개별 손가락 움직임을 예측하는 새로운 기법을 제안하고, 분류 및 회귀 모델을 결합한 독창적인 연구 방법론과 실험 결과를 포함하고 있다. 또한, 기존 연구 대비 향상된 성능(상관계수 0.82)을 보이며 직접적인 연구 기여를 하고 있음을 명확히 보여준다."
Non-Invasive Skin Disease Diagnostic System Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10840609,"Human skin is an important part of the body that needs attention. Skin diseases are quite common in a country like India with varied climate and high pollution levels. The usual diagnosis of skin diseases is based on visual examination and certain tests performed by a dermatologist. But this is a time-consuming process and can also end up in the wrong diagnosis. Skin disease diagnosis has undergone a revolution because of the significant development in the field of machine learning, notably convolutional neural networks (CNN). CNNs have the potential to completely change the way skin illnesses are diagnosed, tracked, and treated. This study investigates the application of various machine learning techniques in skin problem identification. By using CNN analysis of skin disease images, identification of patterns and making accurate predictions can be done with an unprecedented accuracy and consistency. In addition to diagnosis, machine learning models can treat different skins individually according to their type, thus simplifying the doctor's role. This study aims to use CNN to improve dermatology practices by introducing efficiency and accuracy. This study includes a comprehensive literature review examining existing CNN models and performance metrics. Goals include collecting health data, using CNN's predefined models, comparing their performance, recommending the best models for diagnosing skin diseases, and improving the user interface. It presents a benchmark-based neural network model that may contribute to advances in dermatology in identifying skin diseases through machine learning using data from HAM10000 and DERMNET. The effectiveness of the model is determined by comparing it with other existing models by evaluating quality indicators such as accuracy, precision, and F1 score.",Yes,"논문은 CNN을 활용한 피부 질환 진단 시스템을 제안하고, 기존 모델들과 성능을 비교 평가하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 데이터셋을 활용한 모델 개발과 성능 지표 분석을 통해 직접적인 연구 기여를 하고 있음을 보여줍니다."
"Predicting Events in MOBA Games: Prediction, Attribution, and Evaluation",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736637,"The multiplayer online battle arena (MOBA) games have become increasingly popular in recent years. Consequently, many efforts have been devoted to providing pregame or in-game predictions for them. These predictions can be used in many MOBA esports-related applications, such as artificial intelligence commentator systems, in-game data analysis, and game-assistant bots. However, these works are limited in the following two aspects: the lack of sufficient in-game features and the absence of interpretability in the prediction results. These two limitations greatly restrict the practical performance and industrial application of the current works. In this work, we collect a large-scale dataset containing rich in-game features for the popular MOBA game Honor of Kings. We then propose to predict four types of prediction tasks in an interpretable way by attributing the predictions to the input features using two gradient-based attribution methods: Integrated Gradients and SmoothGrad. To evaluate the explanatory power of different models and attribution methods, a fidelity-based evaluation metric is further proposed. Finally, we evaluate the accuracy and fidelity of several competitive methods to assess how well machines predict events in MOBA games.",Yes,"논문은 MOBA 게임에서의 이벤트 예측을 위해 대규모 데이터셋을 수집하고, 해석 가능한 예측 모델과 새로운 평가 지표를 제안하는 등 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구의 한계를 극복하고 새로운 방법론을 제시하는 직접적인 연구 기여로 판단된다."
Cross validating hyperspectral with Ultrasound-based skin thickness estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8077565,"Our work is focused on the development of non-invasive methods to estimate skin constitutive elements. Such methods can play an important clinical and scientific role in detecting the early onset of skin tumors. Given current statistics by the American Academy of Dermatology suggesting that more than 10 people die each hour worldwide due to skin related conditions, this has potentially high impact on the delivery of skin cancer diagnostics, and patient mortality and morbidity. It can also serve as a valuable tool for research in cosmetology and pharmaceuticals in general. We combine a physics-based model of human skin with machine learning and hyperspectral imaging to non-invasively estimate physiological skin parameters, including melanosomes, collagen, oxygen saturation, blood volume, and skin thickness. While some prior work has been done in this regard, no validation against ground truth has occurred whatsoever. In this specific study we develop a protocol to validate our methodology for estimating one of these skin parameters, skin thickness, using a dataset of 48 hyperspectral signatures obtained in vivo, and cross-validate our depth estimates with a gold standard obtained via Ultrasound. Relative to this gold standard, we find promising mean absolute errors of less than 0.1 mm for skin thickness estimation.",Yes,"본 논문은 피부 두께 추정을 위한 비침습적 방법을 개발하고, 이를 초분광 영상과 초음파 기반 측정값을 이용해 검증하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 실제 데이터셋을 활용한 검증 프로토콜을 제시하며 기존 연구와 차별화된 기여를 하고 있어 연구 논문에 해당합니다."
Tremor Class Scaling for Parkinson Disease Patients Using an Array X-Band Microwave Doppler-Based Upper Limb Movement Quantizer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509550,"Consensus criteria for tremor classification in Parkinson’s disease (PD) patients are clinically important for automatically evaluating the PD rating scale. Wearable sensing tools with direct contact measurements can obtain physiological signals to monitor tremor symptoms. Then, machine learning algorithms (MLAs) can train the frequency-based parameters and motion features to accurately measure PD-related tremors. Noncontact measurement with customized computer information devices can also digitalize the digitized handwritten patterns with the bespoke movements that a hand makes for identifying tremor classes. However, wearable sensors need a set of multiple electrodes to be placed on a patient’s body to acquire biosignals, and the setup does not allow continuous measurement and limits the patient’s motion range. The handwritten patterns of noncontact-based methods need frequency-domain and linearization transformations. In addition, feature extraction methods and MLAs are limited in complex computations and adaptive applications. Hence, in this work, a noncontact measurement with an array X-band microwave (10 GHz) Doppler-based linear quantizer is designed to continuously measure upper limb movements for tremor class scaling. To overcome the complex computations, time-domain parametric features, including zero crossing (ZC), Willison amplitude (WAMP), and waveform length (WL) indexes, are used to extract the physical changes in the oscillation frequencies, amplitudes, and directions of tremor signals for scaling upper limb tremor (ULT) levels. In the experiments involving 10 subjects, the proposed noncontact bioradar sensor could quantify asymmetrical and irregular oscillations with a positive correlation (mean R2 > 0.85) between the three indexes (ZC, WAMP, WL) and various oscillation frequencies. The linear relationship quantizer could predict the ULT levels from 0 Hz to 8 Hz for PD patients (typical tremor frequency: 4–6 Hz). It could also map the three indexes into colored visual representation for computerized visual analysis.",Yes,"본 논문은 X-밴드 마이크로파 도플러 기반의 비접촉식 센서를 설계하고, 이를 통해 파킨슨병 환자의 상지 떨림을 정량화하는 독창적인 방법을 제안하고 실험을 수행한 연구이다. 새로운 측정 기법과 특징 추출 방법을 적용하여 떨림 수준을 예측하는 실험적 결과를 포함하고 있어 직접 기여하는 연구 논문으로 판단된다."
Exploring Forecasting and Prediction Processes for Decision-Making to Promote the Photovoltaic Energy Integration into the Grid: A Mini Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286623,"As solar energy is increasingly integrated into the electricity grid, the power system faces several technical challenges posed by the fluctuating renewable source. This paper reviews the forecast and prediction processes, which are crucial components for Grid-Connected PV power plants. According to the literature, photovoltaic predictions can be made for various time scales, ranging from very short-term forecasts to short-term, medium-term, and long-term projections. The temporal resolution depends on the specific needs and applications of the prediction, such as optimizing energy management or planning for grid integration. The spatial resolution corresponds to the sampling area of the prediction. The accuracy of a photovoltaic (PV) forecast can be measured by comparing the predicted values with the actual observed values. Several performance parameters provide quantitative measures to aid in evaluating the efficacy of the forecasting model, such as RMSE, r2, MAE, and MAPE. Drawing on existing literature, the scientific community experiences various forecast techniques, which are classified mainly into persistence methods, physical techniques, statistical techniques, and hybrid models, with a prevailing trend toward hybrid deep-learning models in last years. However, certain gaps exist, particularly related to complexity and time consumption. With increasing PV penetrations, the impact of incorrect forecasts in the grid can be larger. In some electricity markets, solar producers can face penalties when deviations between forecast and produced energy exceed a tolerance band. Numerous companies actively participate in the field of PV forecasting. Among them are Schneider Electric, Enphase Energy, SteadySun, and GreenPowerMonitor.",No,"본 논문은 기존 문헌을 종합하여 태양광 에너지 예측 및 예보 기법을 리뷰하는 미니 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 기술을 정리하는 데 중점을 두고 있습니다."
Modeling Human Innate Immune Response Using Graph Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641838,"Since the rapid outbreak of Covid-19, profound research interest has emerged to understand the innate immune response to viruses to enable appropriate vaccination. This understanding can help to inhibit virus replication, prolong adaptive immune response, accelerated virus clearance, and tissue recovery, a key milestone to combat coronaviruses (CoVs), e.g., Covid-19. An innate immune system triggers inflammatory responses against CoVs upon recognition of viruses. An appropriate defense against various coronavirus strains requires a deep understanding of the innate immune response system. Current deep learning approaches focus more on Covid-19 detection and pay no attention to understand the immune response once a virus invades. In this work, we propose a graph neural network-based (GNN) model that exploits the interactions between pattern recognition receptors (PRRs)to understand the human immune response system. PRRs are germline-encoded proteins that identify molecules related to pathogens and initiate a defense mechanism against the related pathogens, thereby aiding the innate immune response system. An understanding of PRR interactions can help to recognize pathogen-associated molecular patterns (PAMPs) to predict the activation requirements of each PRR. The immune response information of each PRR is derived from combining its historical PAMPs activation coupled with the modeled effect on the same from PRRs in its neighborhood. On one hand, this work can help to understand how long Covid-19 can confer immunity for a strong immune response. On the other hand, this GNN-based understanding can also abode well for appropriate vaccine development efforts against CoVs. Our proposal has been evaluated using CoVs immune response dataset, with results showing an average IFNs activation prediction accuracy of 90%, compared to 85% using feed-forward neural networks.",Yes,"본 논문은 그래프 신경망(GNN)을 활용하여 인간의 선천 면역 반응을 모델링하는 독창적인 연구를 수행하였으며, PRR 상호작용을 분석하여 면역 반응 예측 정확도를 평가하는 실험 결과를 제시하고 있다. 이는 기존 연구와 차별화된 새로운 방법론과 실험적 기여를 포함한 연구 논문에 해당한다."
An efficient approach for spammer detection on Twitter and their behavior analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774154,"Twitter is the 4th most popular social networking site and almost 353 million are using Twitter worldwide. The main problems faced by Twitter users are spam, malicious automation, and platform manipulation. Twitter’s developer API is exposed to make it easy to interact. So, the anti-spam system on Twitter is known by spammers. Here, we are using some features that are related to account details of Twitter users with two different data sets having one lakh data for each. We propose a method to classify spam and non-spam users using five machine learning classifiers. We evaluated the performance using accuracy, precision, and recall. Then we are proposing a fuzzy-based approach for user behavior analysis, inspired by the Five-factor model of behavior theory. Only a few works are discussing the behavior of the spammer, and we found that there exists a relationship between the traits of a person and spamming.",Yes,"본 논문은 트위터 스패머 탐지를 위한 새로운 분류 방법과 퍼지 기반 사용자 행동 분석 기법을 제안하며, 두 가지 데이터셋을 활용해 성능 평가를 수행하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Transferring Age and Gender Attributes for Dimensional Emotion Prediction from Big Speech Data Using Hierarchical Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552276,"The continuous speech emotion prediction is a challenging task. This research has many important applications in real-life, especially on human-computer interaction and its applications are expected to become increasingly wider and deeper. Previous studies have show that three dimensional attributes of emotion such as arousal, valence and dominance are related each other. Moreover, age and gender of speaker affect the recognition of speech emotion. Based on these observation, we propose a new dimensional attributes prediction model using multi-task learning, aiming to improve the emotion recognition performance and generalization capabilities. The present methods utilize the big speech data to train the age and gender submodel, which will be transferred to the main modelłła hierarchical deep learning model, using age and gender as the high level attributes of the emotion. The publicly available databases IEMOCAP and aGender have been conducted to evaluate the performance and the accuracy of the proposed work. Experiment results of within-corpus evaluation show that the proposed approach has superior performance compared to state of the art result.",Yes,"본 논문은 다중 작업 학습과 계층적 딥러닝 모델을 활용하여 나이와 성별 속성을 감정 예측에 전이하는 새로운 모델을 제안하고, 공개 데이터셋을 통해 성능을 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Towards a Geometric Understanding of Spatiotemporal Graph Convolution Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10518107,"Spatiotemporal graph convolutional networks (STGCNs) have emerged as a desirable model for skeleton-based human action recognition. Despite achieving state-of-the-art performance, there is a limited understanding of the representations learned by these models, which hinders their application in critical and real-world settings. While layerwise analysis of CNN models has been studied in the literature, to the best of our knowledge, there exists no study on the layerwise explainability of the embeddings learned on spatiotemporal data using STGCNs. In this paper, we first propose to use a local Dataset Graph (DS-Graph) obtained from the feature representation of input data at each layer to develop an understanding of the layer-wise embedding geometry of the STGCN. To do so, we develop a window-based dynamic time warping (DTW) method to compute the distance between data sequences with varying temporal lengths. To validate our findings, we have developed a layer-specific Spatiotemporal Graph Gradient-weighted Class Activation Mapping (L-STG-GradCAM) technique tailored for spatiotemporal data. This approach enables us to visually analyze and interpret each layer within the STGCN network. We characterize the functions learned by each layer of the STGCN using the label smoothness of the representation and visualize them using our L-STG-GradCAM approach. Our proposed method is generic and can yield valuable insights for STGCN architectures in different applications. However, this paper focuses on the human activity recognition task as a representative application. Our experiments show that STGCN models learn representations that capture general human motion in their initial layers while discriminating different actions only in later layers. This justifies experimental observations showing that fine-tuning deeper layers works well for transfer between related tasks. We provide experimental evidence for different human activity datasets and advanced spatiotemporal graph networks to validate that the proposed method is general enough to analyze any STGCN model and can be useful for drawing insight into networks in various scenarios. We also show that noise at the input has a limited effect on label smoothness, which can help justify the robustness of STGCNs to noise.",Yes,"본 논문은 STGCN의 레이어별 임베딩 기하학적 특성을 이해하기 위한 새로운 방법론(DS-Graph, 윈도우 기반 DTW, L-STG-GradCAM)을 제안하고, 이를 다양한 데이터셋과 네트워크에 적용하여 실험적으로 검증하였다. 이는 기존 연구와 차별화된 독창적인 연구 기여를 포함하고 있으므로 연구 논문에 해당한다."
Restaurant Recommendation System for User Preference and Services Based on Rating and Amenities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862048,"Recommendation systems are being enforced to offer personalized set of services to the users. They are basically build to produce recommendations or suggestions (like restaurants, places...) that comply with user's concern and that can be applied to multiple fields. To enhance the quality and service of Recommendation systems and to resolve any issues related to it, various effective techniques linked to data management can be made use of. The current paper proposes a machine learning algorithms to resolve the issue of personalized Restaurant selection relying upon tripadvisor.com search data. The facilities provided by the hotel along with user's comments are being utilized. The NLP - Natural Language Processing is imbibed for examining and tagging all the previous user's comments (whether positive or negative) for every hotel, thereafter computing the overall % of the comments and storing the output. In the process of Restaurant recommendation, first the user chooses the hotel's features according to his interest and centered on this, the corresponding hotels are fetched and the user comments are examined to identify the hotel with the highest ranking. Eventually, the highest rated hotel is being recommended to the user by the restaurant recommended system. The proposed sentimental score measure NLP algorithm is used for finding the aspect and sentiments of the user comments. Natural language processing (NLP) is one of the machines learning technique to analyze, understand, and derive meaning from human language in a smart and useful way. The evaluation results reveal that the proposed NLP algorithm improves the performance when compared to existing algorithms. The focus of the research work is to offer list of recommended restaurants that is more precise and accessible. The conclusion and results reveal that the suggested approach yields high accuracy.",Yes,"본 논문은 사용자 평점과 편의시설 데이터를 활용한 맞춤형 식당 추천 시스템을 제안하며, 자연어 처리 기반의 감성 점수 산출 알고리즘을 개발하고 성능 평가를 수행하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Analysis and Simulation of Humanoid Gait,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7402132,"The development of mobile robots is related to multidisciplinary areas of mechanics, electronics, computing and biology. Current research have made possible the development of robots that assist in exploration, rescue and rehabilitation, using artificial intelligence techniques, optimization and dynamic analysis of multibody systems. This work aims the synthesis of a humanoid robot and presents the theoretic robot kinematic model allowing define its movement in order to maintain dynamic balance and evaluate the torques applied on their actuators using CAD-CAE simulation. From the 3D computational model and using numerical simulation of multibody systems, it was possible to obtain the dynamic torques in the structure of the robot and the position of its center of mass and legs along the defined path. With this study was evaluated a methodology for multibody simulation mechanisms applied in projects of humanoid robots.",Yes,"논문은 휴머노이드 로봇의 보행 동작을 위한 이론적 운동학 모델을 제시하고, CAD-CAE 시뮬레이션을 통해 동적 토크와 무게중심 위치를 평가하는 독창적인 연구 내용을 포함하고 있습니다. 이는 직접적인 연구 기여로 볼 수 있는 시뮬레이션 및 분석 결과를 담고 있습니다."
Critical Factors Affecting Fast-Track Construction Management for Infrastructure Projects: Validated Scale Development and K-Means Clustering Algorithms Insights,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550696,"The fast-track project concept has become crucial in today’s competitive business landscape, driven by escalating market demands. This research aims to pinpoint success-determining factors in such projects, a quest initiated through an extensive literature review and further explored via structured interviews and questionnaires. This study unfolds in multiple phases, beginning with interviews of five experts with over 25 years of experience validating 10 critical factors: planning, scheduling, customer commitment, communication, risk management, technology adoption, project skills, material management, activity breakdown, and leadership. Subsequent phases involve quantitative approaches, including a survey with 125 respondents, predominantly Filipino professionals with substantial construction experience, and a follow-up phase with 200 Filipino respondents. These phases leverage principal component and confirmatory factor analysis, alongside machine learning techniques like K-means clustering, to analyze each success factor’s significance and perception variations. The analysis found high component loadings above 0.6 for critical success factors, confirmed by Cronbach’s Alpha values within the acceptable range of 0.6 to 0.8, but faced challenges in model fit, indicated by mixed results in Confirmatory Factor Analysis with communalities below 0.4 and inconsistent appropriate indices. The paper also looks into the application of the K-means clustering algorithm. Finally, it lays the groundwork for future research, emphasizing the need for diverse, experienced perspectives in understanding fast-track project dynamics.",Yes,"본 논문은 문헌 검토, 전문가 인터뷰, 설문조사, 통계 분석 및 머신러닝 기법을 활용하여 패스트트랙 건설 관리의 성공 요인을 규명하는 독창적인 연구를 수행하고 있다. 이는 직접적인 데이터 수집과 분석을 통한 새로운 지식 창출에 해당하므로 연구 논문으로 판단된다."
On Applicability of Imagery-Based CNN to Computational Offloading Location Selection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999423,"The progress in computational offloading is heavily pushing the development of the modern Information and Communications Technology domain. The growth in resource-constrained Internet of Things devices demands the development of new computational offloading strategies to be sustainably integrated in beyond 5G networks. One of the solutions to said demand is enabling Mobile Edge Computing (MEC) powered by advanced methods of Machine Learning (ML). This paper proposes the application of ML-powered computational offloading strategy in a wireless cellular network by applying the traditional fundamental Travelling Salesman Problem (TSP) on computational offloading location selection. The main specificity of the proposed approach is the use of imagery data. Thus, the paper executes a literature review to identify existing strategies. It further proposes a novel method utilizing the location-like imagery data to identify the most suitable computational location by executing the search for an identified route between locations using the proposed Deep Learning (DL) model. The model was evaluated and achieved MAE – 1,575, MSA – 10,119,205, R2 – 0.98 on the testing dataset, which outperforms or is comparable with other well-known architectures. Moreover, the training time is proven to be 2–10 times faster. Interestingly, the MAE values are relatively low compared to the target values that should be predicted (despite rather high MSE results), which is confirmed by the almost perfect R2 value. It is concluded that the proposed neural network can predict the target values, and this solution can be applied to real-world tasks.",Yes,"본 논문은 기존 연구를 검토한 후, 위치 기반 이미지 데이터를 활용한 새로운 딥러닝 모델을 제안하고 이를 평가하여 성능 지표를 제시하고 있다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문으로 판단된다."
A Research on the Vulnerabilities of PLC using Search Engine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8939961,"The development and convergence of information and communication technologies have been leading to various studies on big data, artificial intelligence, internet of things, and smart cars to be introduced to our daily life. Especially, Smart factory which also base on ICT technology mentioned above are gradually applied to automation of intelligent manufacturing processes which is used to take place in a factory to drive down manufacturing costs and time to profit. Like governments that already encourage the introduction of smart factories, individuals and organizations are also considering introduction of smart factories. Smart factory requires modbus protocol to communicate and it must come with ICS, SCADA, PLC. These systems are greatly important in terms of security. However, using some of public website like Shodan makes it possible to search easily networks related to critical systems and devices. For safe and secure operation in smart factory, we need to raise awareness to these kind of security vulnerabilities in the ICS so that smart factories can cope with security accidents that might occur. In addition, for successfully introduction and activation of smart factories, security vulnerabilities must be reduced by using security measures and security devices should be applied for the ICS. As a baseline work for security improvement, we arrange the potential security vulnerabilities by conducting various real-world experiments and propose some security measures. Through this research, we expect to encourage security awareness of using smart factories which are undoubtedly important issue.",Yes,"논문 초록에서 실제 실험을 통해 PLC 및 ICS의 보안 취약점을 분석하고, 이를 기반으로 보안 대책을 제안하는 연구 내용을 포함하고 있어 독창적인 연구 기여가 있다고 판단됩니다. 따라서 연구 논문에 해당합니다."
AI-Powered Code Reviews: Leveraging Large Language Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10829223,"As the complexity and volume of software development continue to grow, the need for efficient and thorough code review processes becomes increasingly critical. This paper explores the integration of Large Language Models (LLMs), such as ChatGPT and Bard, into code review workflows to enhance software quality and security. By leveraging the natural language processing capabilities of LLMs, we aim to streamline the identification of code issues, detect potential security vulnerabilities, and provide developers with actionable feedback. Through a comprehensive analysis of current literature, case studies, and experimental data, this study evaluates the impact of AI-assisted code reviews on developer productivity and code quality. We also address the challenges and limitations of relying on LLMs, including context comprehension and potential biases. Our findings suggest that while LLMs offer significant advantages in automating and improving code reviews, they should complement rather than replace human expertise. This paper provides insights into best practices for integrating LLMs into development workflows, ultimately contributing to more robust and secure software systems.",Yes,"논문은 LLM을 활용한 코드 리뷰의 효율성과 품질 향상에 대한 실험 데이터와 사례 연구를 포함하여 AI 보조 코드 리뷰의 영향을 평가하는 독창적인 연구 내용을 담고 있다. 또한, 도전 과제와 한계점도 다루며 새로운 통합 방안을 제시하는 점에서 연구 논문으로 판단된다."
Deep Learning -Based Algorithm for MRI Lumbar Vertebrae Instance Segmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329828,"Magnetic resonance (MR) morphometric analysis of the spine plays crucial role in clinical practice for diagnosis various anomalies, including osteoporosis, age-related degenerative changes, etc. However, the current approach to MR morphometry relies on manual measurements by radiologists, which is a time-consuming process. Therefore, automating morphometric analysis is important to speed up the measurements and reduce the workload on radiologists. In order to achieve this, automated segmentation of the vertebrae can be implemented. This work proposes a deep learning (DL)-based algorithm for segmenting lumbar vertebrae in magnetic resonance (MR) images. The algorithm utilizes the Mask-RCNN deep convolutional neural network, which is a cutting-edge tool for multiple objects (instance) segmentation. The network was trained and evaluated on a dataset of MR images, consisting of 200 subjects (100 control and 100 patients with at least one deformed vertebrae). These images were carefully labelled. The trained Mask-RCNN model demonstrated exceptional segmentation performance on the test dataset. The median Dice similarity coefficient, a widely used metric for evaluating segmentation accuracy, was 0.95 for patients and 0.96 for controls. The results can be utilized to automate the assessment of lumbar vertebrae deformities, including the identification of deformity type (wedge-shaped or biconcave, etc.), as well as the determination of deformity grade.",Yes,"본 논문은 딥러닝 기반의 새로운 알고리즘을 제안하고, 이를 MR 영상 데이터에 적용하여 직접 실험 및 평가를 수행한 연구 내용을 포함하고 있다. 따라서 독창적인 연구 기여가 있는 연구 논문으로 판단된다."
A Survey on RealSense: in context of Research and Application,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225558,"RealSense technology is such a technology which is implemented for machines and device for measuring the capabilities of those apparatus. The capabilities are related to the perceptions of those machines and devices. This technology mainly learns a machine to understand the world with cogitation. RealSense technology introduces cameras to explore the environment with ins and out as it is parallel working with high definition cameras. This technology is used in many categories. Object detection, facial expression detection, shape measurement, image processing, and so on. Among all of the types, RealSense technology is used in huge amount for object detection. In this category real sense mainly focus on the object which is vitally important for understanding the full scene, and understanding the other aspect of the scene with which the object can be classify for better understanding. This paper mainly provides an overview of RealSense technique to the modern technology with its 3D visual functionalities adding the developed application which is published based on Microsoft Kinet and also with SR300 technology. Also this work can give a clear overview to the researchers about the fact that which domain of research is easier with this RealSense technology and in which domain they get more accuracy.",No,"본 논문은 RealSense 기술에 대한 개요와 응용 분야를 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 실험적 기여를 포함하고 있지 않습니다. 따라서 연구 논문보다는 기술 및 연구 동향을 소개하는 리뷰 논문에 해당합니다."
Ensemble Similarity Measures for Clustering Terms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5171010,"Clustering semantically related terms is crucial for many applications such as document categorization, and word sense disambiguation. However, automatically identifying semantically similar terms is challenging. We present a novel approach for automatically determining the degree of relatedness between terms to facilitate their subsequent clustering. Using the analogy of ensemble classifiers in machine learning, we combine multiple techniques like contextual similarity and semantic relatedness to boost the accuracy of our computations. A new method, based on Yarowskypsilas word sense disambiguation approach, to generate high-quality topic signatures for contextual similarity computations, is presented. A technique to measure semantic relatedness between multi-word terms, based on the work of Hirst and St. Onge is also proposed. Experimental evaluation reveals that our method outperforms similar related works. We also investigate the effects of assigning different importance levels to the different similarity measures based on the corpus characteristics.",Yes,"논문 초록에서 새로운 방법론(예: Yarowskypsilas 기반 주제 시그니처 생성, 다중 단어 용어 간 의미 관련성 측정 기법)을 제안하고, 이를 실험적으로 평가하여 기존 연구보다 우수한 성과를 보였다고 명시하고 있습니다. 이는 독창적인 연구 내용을 포함한 연구 논문임을 나타냅니다."
"Evaluation of PSE, STFT and probability coefficients for classifying two directions from EEG using radial basis function",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435664,"EEG (Electroencephalography) is a recording of electrical activities of brain measured from scalp. Brain is a control center for almost all functions of body. As EEG originates from brain, it contains various components related to cognitive activities of brain. Hence, it also contains information regarding the motor functions associated with movement of the body. EEG is commonly recorded for purposes of diagnosis and research associated with diseases like epilepsy, seizures, sleep disorders etc. But apart from these applications it can also be used to map various motor movements being thought of. This may lead to development of landmark devices in the field of rehabilitation of physically challenged individuals. Here we intend to extract the features and classify the directions using EEG. At initial stage it is desired to classify two movements i.e. left and right, but the method can be extended for the classification of other directions as well. In present scenario the most suitable methods for classification problems can be developed using machine learning algorithms. In this work the features like probability co efficient, PSE (power spectral entropy) and STFT (Short Time Fourier Transform) are extracted and evaluated for their efficiency in classification. Radial Basis Function is used for classifying these features. The study shows probability co efficient and STFT have yielded about 60% accuracy in classifying raw EEG signals proving them advantageous over power spectral entropy.",Yes,"논문은 EEG 신호에서 특징을 추출하고 이를 분류하는 새로운 방법을 제안하고 평가하는 연구를 수행하고 있어 독창적인 연구 내용이 포함되어 있다. 또한, 다양한 특징 추출 기법과 분류 알고리즘의 성능을 비교 분석하는 실험적 기여가 명확히 드러난다."
A Comprehensive Survey on Computational Aesthetic Evaluation of Visual Art Images: Metrics and Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439479,"Computational image aesthetic evaluation is a computable human aesthetic perception and judgment realized by machines, which has a significant impact on a variety of applications such as image advanced search and promotional exhibition of painting arts. Various approaches have been proposed in copious literature trying to solve this challenging problem. However, there have been few attempts in reviewing works from different types of visual arts, due to their significant differences in visual features and aesthetic principles. In this survey, we present a comprehensive listing of the reviewed works on aesthetic assessment of photographs and paintings, mainly highlighting the contributions and innovations of the existing approaches. We firstly introduce aesthetic assessment benchmark datasets in different categories. Then, conventional aesthetic evaluation approaches based on handcrafted features are reviewed. Besides, we systematically evaluate recent deep learning techniques that are useful for developing robust models for aesthetic prediction tasks in scoring, distribution, attribute, and description. Moreover, the possibility of aesthetic-aware color enhancement, recomposition of photo images, and automatic generation of aesthetic-guided art paintings through computational approaches are summarized. Finally, challenges and potential future directions for this field are discussed. We hope that our survey could serve as a comprehensive reference source for future research on computational aesthetics in visual media.",No,"본 논문은 기존 연구들을 종합적으로 정리하고 평가하는 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험을 제시하지 않습니다. 따라서 연구 논문보다는 연구 동향과 현황을 소개하는 리뷰 논문에 해당합니다."
Research on factors analysis model of dualistic soil salinization sensitivity in typical northwestern arid area,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4423510,"Soil secondary salinization is one of the typical ecological side effects caused by land and water resources development in northwestern arid China. Factors that affect the occurrences and developments of salinization come from both natural conditions and human activities. Research on the mechanisms of salinization, build dynamic prediction model of salt accumulation and analyze sensitivities to different factors would supply effective references to the prediction and prevention of soil salinization. It is well known that related factors are always intertexture together, affecting each other, which result in multivariable, nonlinear and overall influences that work on the process of soil salinization. Artificial intelligence technologies may play important role in this domain. In this paper, genetic artificial neural network based model is built to simulate and evaluate soil salt accumulation and sensitivity of soil salinization. Example is taken from the Shule River watershed, typical arid area in northwestern China. Basic data of June 2000 are prepared depending on GIS and Remote Sensing. Precipitations, evaporations, groundwater levels, groundwater chemical analysis data and soil accumulation data are achieved and interpolated in the research area. Slope of the land are derived from DEM, MODIS images are used in the process of dealing with land use information. At the same time, landform and soil type are considered in model building. Soil salt accumulation is analyzed with its 8 influenced factors with verified models. Results showing that groundwater TDS is the most sensitive factor followed by groundwater level, evaporation and the depth of upper bed of clay. In most cases clay layers play key roles in soil salt accumulation, precipitation and slop have similar sensitivities. Results would have better research and application value in arid areas of northwestern China.",Yes,"본 논문은 인공신경망 기반 모델을 구축하여 토양 염분 축적과 염분 민감도를 분석하는 독창적인 연구를 수행하였으며, 실제 데이터를 활용해 모델을 검증하고 결과를 도출하였다. 이는 직접적인 연구 기여와 새로운 분석 방법론을 포함하고 있음을 보여준다."
A Deep Learning-based Approach for an Automated Vulnerability Detection in Source Code,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385129,"With the advancements and significant increase in today’s software projects, software vulnerabilities have become a crucial and serious concern to the software industry. These software defects pose familiar cybersecurity threats and identifying potential vulnerable code is difficult because of various elements related to volume, diversity, and complexity. Traditionally, vulnerabilities in source code have been detected either statically by relying on expert knowledge, which is time consuming, or through dynamic analysis tools. However, lacking robust investigation of source code resulted in decreased efficacy of the existing solutions. In this respect, as an alternative to traditional source code detection methods, deep learning seems to have become an attractive solution. Thus, in this research work, we propose a deep learning-based neural networks approach to construct a function-level vulnerability analysis for an automated identification of vulnerabilities in source code. The proposed solution can handle source code vulnerability detection on function level without expert knowledge by utilizing deep learning convolutional neural networks (CNNs) and artificial neural networks (ANNs). To generate meaningful numerical feature representation of input source code functions, we have generated abstract syntax tree (AST). Moreover, we enhanced the deep learning neural network models through the selection of optimal parameter values and employing a validation methodology. To evaluate the performance of the proposed models we used a public dataset with verified vulnerability labels using various performance metrics such as Precision, Accuracy, F1-score, Recall and ROC-AUC. After conducting multiple experiments, the best result (Accuracy = 84.22%) was achieved using our ANN model.",Yes,"이 논문은 딥러닝 기반의 신경망 모델을 제안하여 소스 코드 내 취약점을 자동으로 탐지하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 모델 성능 평가를 위한 실험과 결과를 제시하여 직접적인 연구 기여를 하고 있음을 보여줍니다."
Research on logistics autonomous mobile robot system,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558574,"In recent years, Robots have become an important part of the rapid development of modern logistics system; the extent of its application has become an important measure factor to determine competition and the future development of inter-enterprise. The logistics autonomous mobile robot has a path planning, navigation, information fusion, autonomous decision-making and other human-like artificial intelligence, and it can improve the efficiency and level of intelligence of modern logistics. Due to the mobile robot working in the outdoor environment with uncertainty and complexity, its system inevitably has the uncertainty of the kinematic model, and is affected by the unknown external disturbance. In this case, it must ensure that the robot system can stabilize movement in outdoor environment and accurately reach the designated position. This is mainly related to the robot trajectory tracking problem. And the mobile robot to achieve high precision trajectory tracking control, the control method is the key. The main content of this paper is divided into two parts, one part is the introduction of the system structure of the logistics autonomous mobile robot; the other is a detailed analysis of the role of sliding mode control and neural network control in the robot system.",Yes,"논문 초록에서 물류 자율 이동 로봇 시스템의 구조 소개와 슬라이딩 모드 제어 및 신경망 제어의 역할에 대한 상세 분석을 다루고 있어, 독창적인 연구 내용과 기술적 기여가 포함된 연구 논문임을 알 수 있습니다. 따라서 직접 기여하는 연구 논문에 해당합니다."
LETS: A Label-Efficient Training Scheme for Aspect-Based Sentiment Analysis by Using a Pre-Trained Language Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9503416,"Recently proposed pre-trained language models can be easily fine-tuned to a wide range of downstream tasks. However, a large-scale labelled task-specific dataset is required for fine-tuning creating a bottleneck in the development process of machine learning applications. To foster a fast development by reducing manual labelling efforts, we propose a Label-Efficient Training Scheme (LETS). The proposed LETS consists of three elements: (i) task-specific pre-training to exploit unlabelled task-specific corpus data, (ii) label augmentation to maximise the utility of labelled data, and (iii) active learning to label data strategically. In this paper, we apply LETS to a novel aspect-based sentiment analysis (ABSA) use-case for analysing the reviews of the health-related program supporting people to improve their sleep quality. We validate the proposed LETS on a custom health-related program-reviews dataset and another ABSA benchmark dataset. Experimental results show that the LETS can reduce manual labelling efforts 2-3 times compared to labelling with random sampling on both datasets. The LETS also outperforms other state-of-the-art active learning methods. Furthermore, the experimental results show that LETS can contribute to better generalisability with both datasets compared to other methods thanks to the task-specific pre-training and the proposed label augmentation. We expect this work could contribute to the natural language processing (NLP) domain by addressing the issue of the high cost of manually labelling data. Also, our work could contribute to the healthcare domain by introducing a new potential application of NLP techniques.",Yes,"본 논문은 사전학습된 언어 모델을 활용한 새로운 학습 기법(LETS)을 제안하고, 이를 건강 관련 프로그램 리뷰 분석에 적용하여 실험적으로 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법이 기존 기법 대비 라벨링 비용 절감과 성능 향상을 입증하여 직접적인 연구 기여를 하고 있음을 보여준다."
Wearable PPG sensor based alertness scoring system,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037345,"Quantifying mental alertness in today's world is important as it enables the person to adopt lifestyle changes for better work efficiency. Miniaturized sensors in wearable devices have facilitated detection/monitoring of mental alertness. Photoplethysmography (PPG) sensors through Heart Rate Variability (HRV) offer one such opportunity by providing information about one's daily alertness levels without requiring any manual interference from the user. In this paper, a smartwatch based alertness estimation system is proposed. Data collected from PPG sensor of smartwatch is processed and fed to machine learning based model to get a continuous alertness score. Utility functions are designed based on statistical analysis to give a quality score on different stages of alertness such as awake, long sleep and short duration power nap. An intelligent data collection approach is proposed in collaboration with the motion sensor in the smartwatch to reduce battery drainage. Overall, our proposed wearable based system provides a detailed analysis of alertness over a period in a systematic and optimized manner. We were able to achieve an accuracy of 80.1% for sleep/awake classification along with alertness score. This opens up the possibility for quantifying alertness levels using a single PPG sensor for better management of health related activities including sleep.",Yes,"본 논문은 스마트워치의 PPG 센서 데이터를 활용하여 기계학습 모델을 통해 연속적인 각성 점수를 산출하는 시스템을 제안하고 있으며, 배터리 소모를 줄이기 위한 지능형 데이터 수집 방법도 함께 제시하고 있다. 이는 기존 연구와 차별화된 독창적인 연구 내용과 실험 결과(80.1% 정확도)를 포함하고 있어 연구 논문에 해당한다."
Health and Environment Monitoring System for Viral Respiratory Diseases,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306600,"The proposed system in this work is a contactless health and environment monitoring system designed after conducting a literature review on the most recent research on viral respiratory diseases and their impact on businesses, risk factors of a location, and surveillance of people for various safety measures and causes leading to its spread. The proposed system takes the form of an Android Mobile Application, designed to function as a safety entry checkpoint for various organizations, including businesses and educational institutions. It aims to detect viral respiratory diseases by leveraging the capabilities of a smartphone camera without requiring any additional external hardware specifications. Furthermore, the system incorporates users’ vaccination information, enabling a comprehensive assessment of health and safety. The key algorithms presented in this work focus on measuring critical body vital signs, including heart rate, temperature, and oxygen saturation (Oxygen saturation (SpO2)) levels. By utilizing advanced techniques such as mask detection and social distance monitoring, the system ensures the recognition of disease indicators and triggers appropriate actions to minimize the risk of transmission. To assess the safety of an area or environment, the proposed system employs the Euclidean distance metric and MobileNetV2, a state-of-the-art deep learning model. This combination enables real-time evaluation and analysis of the surroundings, providing crucial information for making informed decisions regarding the level of safety in a given location.",Yes,"논문 초록에서 제안된 시스템은 기존 연구를 바탕으로 새로운 안드로이드 애플리케이션을 설계하고, 심박수, 체온, 산소포화도 측정 알고리즘과 딥러닝 모델을 활용한 실시간 환경 평가 기법을 포함하고 있어 독창적인 연구 내용과 직접적인 기여가 포함되어 있음을 알 수 있습니다. 따라서 연구 논문에 해당합니다."
A Comparative Approach to Threshold Optimization for Classifying Imbalanced Data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061736,"For the practical application of a classifier, it is necessary to select an optimal output probability threshold to obtain the best classification results. There are many criteria one may employ to select a threshold. However, selecting a threshold will often involve trading off performance in terms of one metric for performance in terms of another metric. In our literature review of studies involving selecting thresholds to optimize classification of imbalanced data, we find there is an opportunity to expand on previous work for an in-depth study of threshold selection. Our contribution is to present a systematic method for selecting the best threshold value for a given classification task and its desired performance constraints. Just as a machine learning algorithm is optimized on some training data set, we demonstrate how a user-defined set of performance metrics can be utilized to optimize the classification threshold. In this study we use four popular metrics to optimize thresholds: precision, Matthews’ Correlation Coefficient, f-measure and geometric mean of true positive rate, and true negative rate. Moreover, we compare classification results for thresholds optimized for these metrics with the commonly used default threshold of 0.5, and the prior probability of the positive class (also known as the minority to majority class ratio). Our results show that other thresholds handily outperform the default threshold of 0.5. Moreover, we show that the positive class prior probability is a good benchmark for finding classification thresholds that perform well in terms of multiple metrics.",Yes,"논문 초록에서 제시된 내용은 임계값 최적화 방법에 대한 체계적인 연구와 성능 지표를 활용한 새로운 최적화 기법을 제안하는 등 독창적인 연구 기여를 포함하고 있습니다. 또한, 기존 방법과의 비교 실험을 통해 새로운 인사이트를 제공하고 있어 연구 논문에 해당합니다."
"A Comprehensive Review on COVID-19 Detection Based on Cough Sounds, Symptoms, CXR, and CT Images",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517993,"The worldwide spread of the coronavirus illness has led to the requirement of creating machine-based technologies to identify the diseases. The worldwide pandemic caused by new coronaviruses has resulted in a significant loss of life and necessitates the development of several affordable diagnostic methods to detect the presence of COVID-19 infection. Thankfully, the current era of advanced technology, including transfer learning (TL) approaches, has improved several areas of human health and enabled the identification of chronic and communicable diseases. There is a need for thorough investigation in order to combat the transmission of this alarming virus via the use of evidence-based intelligence models and implementation of preventive measures. The present systematic review focuses on the examination of TL and fuzzy ensemble techniques that have been described in the literature pertaining to strategies for detecting COVID-19. Multiple studies have used cough sounds, CT scans, X-ray images, and symptoms information to identify cases of COVID-19. The application of DL/ML, TL, fuzzy ensemble, and fuzzy inference approaches for COVID-19 identification is discussed in this paper.",No,"초록에서 해당 논문은 기존 연구들을 종합적으로 검토하는 체계적 문헌고찰(review)임을 명확히 밝히고 있으며, 직접적인 독창적 연구 결과나 실험 수행에 대한 언급이 없다. 따라서 새로운 연구 기여보다는 기존 연구들의 분석과 요약에 중점을 둔 논문으로 판단된다."
Exploring Machine Learning Strategies for Intrusion Detection in Wireless Sensor Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543320,"This survey provides a detailed examination of the integration of machine learning (ML) techniques in intrusion detection systems (IDS) tailored for wireless sensor networks (WSNs). With the growing importance of WSNs across various applications such as environmental monitoring and the development of smart cities, ensuring their security from potential intrusions is of utmost significance. The paper initiates by elucidating the foundational concepts underpinning WSNs and the associated security challenges. This is succeeded by discussing the diverse machine learning paradigms and their pertinence in the WSN domain. The core of this survey is dedicated to an in-depth analysis of the various ML methodologies utilized in intrusion detection for WSNs—covering supervised, unsupervised, and semi-supervised techniques. A meticulous comparative study highlights each method’s strengths, limitations, and specific use cases. Furthermore, the paper brings to light the challenges faced when integrating ML within WSNs, with a particular emphasis on issues related to privacy. On a prospective note, the research navigates into the future trends in the realm of WSN security. Here, the promise of enhanced ML algorithms, the potential of hybrid models, and the impact of upcoming technological advancements on the synergy between WSNs and ML are explored. The insights from this review not only validate the game-changing role of ML in bolstering WSN security but also pave the way for further inquiries and innovations in the domain.",No,초록에서 해당 논문은 무선 센서 네트워크의 침입 탐지에 머신러닝 기법을 적용한 기존 연구들을 종합적으로 검토하는 서베이 논문임을 명확히 밝히고 있습니다. 따라서 직접적인 독창적 연구 결과나 실험 기여보다는 기존 연구들의 분석과 비교에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
GREAT- IQA: Integrating Global Perception and Local Task-Specific Information for CT Image Quality Assessment,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10403138,"Image quality assessment (IQA) is a crucial step for computed tomography (CT) system design and scanning protocol optimization. The lack of high-quality/high-dose and low-quality/low-dose CT image pairs makes only the no-reference IQA (NR-IQA) method available for clinical practice. Recently, deep learning (DL) techniques have been proven effective for CT IQA. However, applying DL-based methods to CT IQA faces two major challenges: 1) labels are noisy due to inter-observer variability and 2) IQA results fail to reflect the task-specific performance of CT images perfectly. In this work, we presented a novel NR-IQA model to integrate Global peRcEption and locAl Task-specific information for CT images, termed GREAT-IQA. First, we used a convolutional neural network to learn global perception-related features from training data, which leverages two objective IQA metrics, i.e. peak signal-to-noise ratio and structural similarity index, to reduce the inter-observer variability. Second, to more precisely quantify the task-specific performance of CT images, we considered two local diagnosis-related features of the chest and abdominal CT images: low-contrast detectability and edge sharpness, where contrast-to-noise ratio and maximum gradient and variability of gradients are used to measure them separately. Finally, the overall image quality is predicted by combining the global image assessment and local region of interest assessment components. By conducting experiments on a total of 1,600 degraded chest and abdominal CT images, reconstructed retrospectively with different doses and kernels, and evaluated with three experienced experts, this paper shows the proposed method outperforms the competing IQA methods, especially full-reference IQA methods.",Yes,"본 논문은 CT 영상 품질 평가를 위한 새로운 비참조 IQA 모델(GREAT-IQA)을 제안하고, 이를 위해 딥러닝 기반의 글로벌 및 로컬 특성 통합 방법을 개발하여 실험적으로 기존 방법 대비 우수한 성능을 입증하였다. 이는 독창적인 연구 내용과 실험적 기여를 포함하는 연구 논문에 해당한다."
Fake Currency Detection using Image Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392045,"Advancements in color printing have led to a rise in the number of counterfeit currency notes being created and duplicated on a massive scale. In the past, it was very difficult to do this, but now anyone with a laser printer can easily do it. Due to the rise of fake notes, the number of counterfeit currency notes has increased significantly in India. This is a major issue that the country is facing due to its various problems such as black money and corruption. A system that can detect the fake currency notes in a fraction of the time is being developed. The proposed system will use image processing techniques to verify the authenticity of Indian currency notes. This method will involve extracting various features from the notes. Some of these include the Bleed Line, Water marking, Fluorescence, Security thread, Micro Lettering, and Identification mark. The proposed system is using software known as MATLAB. The Support vector machine is a supervised learning model that is used for extracting the notes' features. It is also used for analyzing data related to regression and classification. We have also tested the other methods related to this proposed work, such as the graphical black Box algorithm. The proposed system has various advantages, such as its high-performance speed and simplicity. It will be able to determine if the currency notes are real or fake. Its accuracy is also measured using the Confusion Matrix.",Yes,논문 초록에서 제안된 시스템은 이미지 처리 기법과 머신러닝 모델(SVM)을 활용하여 위조 화폐를 감지하는 독창적인 방법을 개발하고 평가하는 연구임을 명확히 나타내고 있습니다. 이는 기존 기술을 적용하고 성능을 측정하는 직접적인 연구 기여로 판단됩니다.
Reliability analysis on case-study traffic sign convolutional neural network on APSoC,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8347234,"Deep learning has been widely used to solve computer vision applications such as autonomous cars and Advanced Driver Assistance Systems (ADAS). One of these computer vision applications is traffic sign recognition that plays an essential role in autonomous cars and ADAS. All-Programmable System-on-Chip (APSoC) is an excellent target platform for implementing ADAS applications due to its flexibility. However, as its Programmable Logic (PL) is implemented with SRAM cells and traffic sign recognition is a safety critical application, the reliability under radiation must be analyzed and hardening or mitigation techniques may be required. Thus, random and piled-up fault injections in the APSoC configuration bits were carried out as well as neutron-radiation experiments in order to evaluate the reliability of a Convolutional Neural Network (CNN). Comparison with related work shows that timing-multiplexing neural network architectures present an insignificantly higher Architecture Vulnerability Factor (AVF) than parallel pipelined architectures.",Yes,본 논문은 APSoC 기반의 교통 표지판 인식 CNN의 신뢰성 분석을 위해 결함 주입 및 중성자 방사선 실험을 수행하는 등 구체적인 실험과 분석을 포함하고 있어 독창적인 연구 내용을 담고 있다. 따라서 직접 기여하는 연구 논문에 해당한다.
Study on Online Review Based Consumer sentimental Analysis using Machine Learning Approaches,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9848932,"Analysing a vast quantity of social media data, which expands itself in volume, subjectivity, and heterogeneity on a manual basis becomes more difficult as technology progresses. In real-world applications, machine learning techniques are being used to address this issue. The goal of this article is to describe research that was conducted to assess the utility, breadth, and application of machine learning algorithms for Consumer Sentiment Analysis (CSA) in online reviews. We present a thorough evaluation of the literature in order to evaluate, examine, study and understand methodologies with directions, in order to uncover research gaps, hence showing the pairing's potential reach in the future. The major purpose is to read and analyse machine learning techniques used in the hotel and tourist industry to analyse customer sentiment in online evaluations. This research is crucial for service providers since it enables them to design customer management strategies for service selection. Additionally, there is a significant influence on scholars' future study orientations.",No,초록 내용은 기존 문헌을 평가하고 연구 동향과 갭을 분석하는 문헌 리뷰 성격에 가깝습니다. 독창적인 실험 결과나 새로운 알고리즘 제안 등 직접적인 연구 기여가 명확히 드러나지 않습니다.
Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10685214,"With the rapid development of Large Language Models (LLMs), a large number of machine learning models have been developed to assist programming tasks including the generation of program code from natural language input. However, how to evaluate such LLMs for this task is still an open problem despite of the great amount of research efforts that have been made and reported to evaluate and compare them. This paper provides a critical review of the existing work on the testing and evaluation of these tools with a focus on two key aspects: the benchmarks and the metrics used in the evaluations. Based on the review, further research directions are discussed.",No,"본 논문은 기존 연구들을 비판적으로 검토하고 평가 지표와 벤치마크를 정리하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구의 종합과 분석에 초점을 맞추고 있습니다."
Extracting Keywords From Text Using NLP On Azure Virtual Machine,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396295,"NLP methods (Natural Language Processing) are used in this project to approach fetching the all keywords by written content and are deployed on an Azure Virtual Machine (VM). Both texts summarization and information retrieval, keyword extraction is essential. The project seeks to create an accurate and efficient method for identifying and retrieving keywords from several sources text sources by leveraging the plus point of NLP libraries and tools. By utilizing Azure VM's scalability and computational power, the project provides reliable processing of significant amounts of text data. The project's conclusion provides a useful tool for improving document categorization, subject analysis, and content summarization, with the added benefit of utilizing cloud resources for top performance. As a way to determine distinctions between every single viewpoint of the individual by algorithms, we have considered numerous factors models and their implementation in this work. We initially learned how to convert PDF files to text before learning how to get keywords out of documents using a variety of algorithms, including RAKE YAKE TFIDF and modifying them in a way that ensures we receive the best-extracted keywords that relate to the document and aid in comprehending it. In the end, we evaluated various methods using mined keywords. Additionally, it gave us the ability to judge which algorithm was the most effective overall. Given the actual facts, we could make an educated judgment as to which method produces the best keywords.",Yes,"본 논문은 여러 알고리즘(RAKE, YAKE, TFIDF 등)을 적용하고 수정하여 키워드 추출 방법을 개발 및 평가하는 과정을 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한, Azure VM을 활용한 구현과 성능 평가를 통해 직접적인 연구 기여를 하고 있음을 알 수 있다."
A Review Paper on Drunk Driving Detection System using IOT & ML Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913592,"Nowadays, there is a significant increase in drunken-driving accidents from the past few years. Drunk driving has emerged as a significant problem in recent times. With the help of different technological implementations, many preventive measures are taken to date, such as alcohol detection sensors, detection using speech, detection using driving patterns, detection using IOT & Machine Learning techniques etc. But each system has its limitations, such as usability, complexity, scalability, burdensome implementation. This paper describes the various approaches to detect drunken driving with the advantages and disadvantages of each. Out of these approaches, the article focuses on the detailed literature review to discuss existing machine learning algorithms and IOT methodologies to improve the accuracy of the system which can be used in detecting drunken driving. At last, the comparative review is done for all these approaches and we found that Random forest classifier with two stage model is most efficient, having high accuracy.",No,"본 논문은 기존 연구들을 종합하여 다양한 음주운전 탐지 시스템의 장단점을 비교하고 문헌 리뷰를 수행하는 리뷰 논문입니다. 독창적인 연구 결과나 새로운 실험, 모델 개발 등의 직접적인 연구 기여가 포함되어 있지 않으므로 연구 논문에 해당하지 않습니다."
An ANFIS Based Derivations of Inference Rules for Users’ Adoptions of Autonomous Vehicles,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9297811,"Autonomous Vehicles (AVs) have great potential and can improve transportation efficiency and safety through minimal manual intervention and optimized traffic control systems. Advances in artificial intelligence and real-time data processing technology have promoted the development of practical AVs. AV manufacturers are trying to understand the potential factors that may affect consumers' acceptance of autonomous vehicles. However, there is very little research on autonomous vehicles and consumers. In order to understand these factors, this research will use UTAUT 2, as a research framework to predict consumer intentions and behaviors. This research will first review the literature, invite experts to define and evaluate appropriate criteria and dimensions, and use the ANFIS is used to derive the decision rules, and the weights of the corresponding rules are compared. The resulting analysis can be used as a basis for predicting consumer acceptance of AVs in the future.",Yes,본 논문은 UTAUT 2 이론을 기반으로 전문가 평가와 ANFIS 기법을 활용하여 자율주행차에 대한 소비자 수용 의도를 예측하는 독창적인 연구 방법과 결과를 제시하고 있다. 이는 기존 연구를 검토하는 데 그치지 않고 새로운 분석 모델을 개발하고 적용한 연구 논문으로 판단된다.
Instance Segmentation of Neuronal Nuclei Leveraging Domain Adaptation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622821,"The detection and localization of individual cell nuclei in dense neural scenes collected by microscopy traditionally depends on human-expert-intensive manual markup for training and evaluating automatic algorithms. These approaches are expensive, time-intensive, and require domain expertise. To develop automatic approaches, the annotated content needs to match the collection conditions (e.g. stain, cell-type) and small changes to these conditions often requires additional matching annotated content. Our approach leverages supervised domain adaptation approach with application to the instance segmentation of nuclei in the brain. The efficacy of this approach is demonstrated experimentally by characterizing the performance of adapting models learned on content not well matched to the target domain. Quantitative results demonstrate performance improvements relative to previous related work. High Performance Computing (HPC) applications of this technology include Human-in-the-Loop (HIL) retraining leveraging active learning or similar machine learning approaches.",Yes,"논문은 신경 세포 핵의 인스턴스 분할을 위한 도메인 적응 기법을 제안하고, 실험을 통해 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 또한 기존 연구 대비 개선된 정량적 결과를 제시하여 직접적인 연구 기여가 있음을 보여준다."
Deciphering Key Genes in Colon Cancer Through Deep Learning Techniques,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10670845,"Identification and prognostication of colon cancer hold pivotal significance for biomedical research studies. Cancer is a genetically related disease in which defective genes are prone to making changes in expression. So, detecting colon cancer at an early stage can contribute to increasing patient survival rates. In recent years, computer-aided diagnosis systems utilizing Deep Learning techniques have emerged for the accurate diagnosis of colon cancer in the healthcare sector. In this methodology, microarray analysis is utilized to reveal the characteristics of normal and diseased genes. In the research work, high-dimensional microarray colon cancer data are utilized to detect the cancer disease. The primary function of the research work is to develop a model using Deep Learning techniques to identify key genes associated with colon cancer and predict the disease. An unsupervised Autoassociator Dimensionality Reduction Technique (ADRT), is applied to identify significant biomarker genes while filtering out redundant and noisy genes present in the gene data. To predict colon cancer, Deep Neural Network classifier is utilized on the selected genes and the classifier’s performance is calculated using evaluation metrics. Finally, comparative analyses are performed before and after applying the dimensionality reduction technique. The experimental outcomes demonstrate that the research model after applying ADRT, significantly improves the detection analysis in terms of minimizing the dimensionality of the gene data and increasing the accuracy value. Moreover, the research model can be smoothly integrated into medical healthcare systems, facilitating the precise diagnosis of Colon cancer.",Yes,"본 논문은 딥러닝 기법을 활용하여 대장암 관련 주요 유전자를 식별하고 질병 예측 모델을 개발하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 차원 축소 기법과 분류기 성능 평가를 통해 실험적 결과를 제시하여 직접적인 연구 기여를 하고 있음을 보여줍니다."
Empirical Analysis of Sentence Embedding Techniques for Answer Retrieval in Marathi Question Answering,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987436,"It is vital that any type of text can be portrayed in a machine-readable language so that it can be processed and interpreted. Text embeddings, which allow a machine to interpret any type of text by mathematically expressing it, naturally minimise this work. To extract the concept of interdependence across words or sentences, the degree of connection, synonym recognition, idea segmentation, selectional preferences, and analogies, text embedding are very useful. Using closest neighbor lookup of vectors, a vector representation of a text in a semantic vector space may be utilized for answer retrieval according to the vectors’ similarity and proximity. Traditional QA systems require a huge number of high-quality language-specific resources to perform multiple activities. The precision of several embedding techniques presently supporting Marathi, an Indo-Aryan language, is explored in this research. The performance of several sentence embedding algorithms is evaluated to select the most relevant and related answer text within the corpus given a natural language query in Marathi. Pragmatic tests are performed across supervised machine learning models to perform categorization and sentence embeddings in order to evaluate and compare the veracity of various embedding techniques. The proposed research findings concentrate on identifying the most effective embedding approaches from the restricted resources available for the Marathi language, laying the groundwork for future study in this language.",Yes,"본 논문은 마라티어 질문응답 시스템에서 문장 임베딩 기법들의 성능을 평가하고 비교하는 실증적 연구를 수행하여, 해당 언어에 적합한 임베딩 방법을 제안하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Smart Career Guidance System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676408,"We have developed a career guidance system that helps those students who are about to begin their higher education. Most of the time, students are not aware of what career path to follow or which academic major is in accordance with their interests. The system analyzes students' skills, abilities, and interests and recommends the five fields which are most suitable for them. This project helps students identify a specific domain that fits their skills and interests. Smart Career Guidance System is a web-based application built on the Django framework. We have deployed various Machine Learning techniques and algorithms to mimic a one-on-one meeting with an experienced career counselor. The data was collected in the form of a questionnaire that was based on Holland Occupational Themes and the Theory of Multiple Intelligences. A total of 392 graduates completed this online survey. SMOTE oversampling is used to evaluate the machine learning classifiers since the data is highly imbalanced. We tested XGBoost and Random Forest classifiers for recommending the best-suited career options which furnish AUC-ROC performance scores of 0.9952 and 0.9963 respectively. A fine-tuned version of the Random Forest Classifier has successfully attained an AUC-ROC performance score of 0.9976 which indicates the minimal false-positive rate. Ms. Areej Kamal, Ms. Hadia Rafiq and Ms. Batul Naushad have collaboratively conducted all activities of the project including data collection and cleaning, literature review, testing of ML models and development of the final solution. Mr. Shahab Tahzeeb directed and supervised all phases of the project with his immense knowledge and expertise.",Yes,"논문은 머신러닝 기법을 활용하여 학생들의 진로를 추천하는 시스템을 개발하고, 데이터 수집, 모델 평가 및 성능 개선 과정을 포함한 독창적인 연구 내용을 담고 있습니다. 따라서 직접 기여하는 연구 논문에 해당합니다."
More Diverse Means Better: Multimodal Deep Learning Meets Remote-Sensing Imagery Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174822,"Classification and identification of the materials lying over or beneath the earth's surface have long been a fundamental but challenging research topic in geoscience and remote sensing (RS), and have garnered a growing concern owing to the recent advancements of deep learning techniques. Although deep networks have been successfully applied in single-modality-dominated classification tasks, yet their performance inevitably meets the bottleneck in complex scenes that need to be finely classified, due to the limitation of information diversity. In this work, we provide a baseline solution to the aforementioned difficulty by developing a general multimodal deep learning (MDL) framework. In particular, we also investigate a special case of multi-modality learning (MML)-cross-modality learning (CML) that exists widely in RS image classification applications. By focusing on “what,” “where,” and “how” to fuse, we show different fusion strategies as well as how to train deep networks and build the network architecture. Specifically, five fusion architectures are introduced and developed, further being unified in our MDL framework. More significantly, our framework is not only limited to pixel-wise classification tasks but also applicable to spatial information modeling with convolutional neural networks (CNNs). To validate the effectiveness and superiority of the MDL framework, extensive experiments related to the settings of MML and CML are conducted on two different multimodal RS data sets. Furthermore, the codes and data sets will be available at https://github.com/danfenghong/IEEE_TGRS_MDL-RS, contributing to the RS community.",Yes,"논문은 멀티모달 딥러닝 프레임워크를 개발하고, 다양한 융합 아키텍처를 제안하며, 이를 원격탐사 이미지 분류에 적용하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 또한, 실험을 통해 제안한 방법의 효과를 입증하고 있어 연구 논문에 해당한다."
Short term power load forecasting using Deep Neural Networks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876196,"Accurate load forecasting greatly influences the planning processes undertaken in operation centres of energy providers that relate to the actual electricity generation, distribution, system maintenance as well as electricity pricing. This paper exploits the applicability of and compares the performance of the Feed-forward Deep Neural Network (FF-DNN) and Recurrent Deep Neural Network (R-DNN) models on the basis of accuracy and computational performance in the context of time-wise short term forecast of electricity load. The herein proposed method is evaluated over real datasets gathered in a period of 4 years and provides forecasts on the basis of days and weeks ahead. The contribution behind this work lies with the utilisation of a time-frequency (TF) feature selection procedure from the actual “raw” dataset that aids the regression procedure initiated by the aforementioned DNNs. We show that the introduced scheme may adequately learn hidden patterns and accurately determine the short-term load consumption forecast by utilising a range of heterogeneous sources of input that relate not necessarily with the measurement of load itself but also with other parameters such as the effects of weather, time, holidays, lagged electricity load and its distribution over the period. Overall, our generated outcomes reveal that the synergistic use of TF feature analysis with DNNs enables to obtain higher accuracy by capturing dominant factors that affect electricity consumption patterns and can surely contribute significantly in next generation power systems and the recently introduced SmartGrid.",Yes,"본 논문은 딥 뉴럴 네트워크를 활용한 단기 전력 부하 예측이라는 구체적이고 독창적인 연구 내용을 포함하고 있으며, 실제 4년간의 데이터셋을 이용해 새로운 특징 선택 절차와 모델 성능을 평가하는 실험적 기여를 하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Cyber-Crime Detection: Experimental Techniques Comparison Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033332,"Cyber-crime is one of the main problems the world face, and machine learning plays a key part in contemporary operating systems for giving better transformation in the security environment and cybercrime detection. While detecting cybercrimes is difficult, it is possible to gain advantages from machine learning to generate models to assist in predicting and detecting cybercrimes. The researchers have proven that the majority of the models can work effectively in identifying cybercrime, they can span from 70% to 90% in accuracy measuring. The objective of this research paper is to conduct experimental techniques comparison analysis for cyber-crime detection by reviewing all possible machine learning algorithms for automatic detection. The key focus of the study is on the use of eight classifiers models which are Logistic Regression (LR), Decision Tree (DT), K-nearest Neighbors (KNN), Support Vector Machine (SVM), Naive Bayes (NB), Random Forest (RF), eXtreme Gradient Boosting (XGBoost) and Multiple layer perception (MLP). From the experiment conducted, the high prediction came from MLP which is 96% accuracy of the cyber-crime methods based on existing cyber-crime data.",Yes,"논문 초록에서 8가지 머신러닝 분류기 모델을 사용하여 사이버 범죄 탐지 실험을 수행하고, 그 결과를 비교 분석한 내용을 포함하고 있어 직접적인 실험 연구와 독창적인 기여가 있는 연구 논문으로 판단됩니다."
Designing an Empathy Training for Depression Prevention Using Virtual Reality and a Preliminary Study,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10108699,"As virtual reality (VR) is labeled by many as “an ultimate empathy machine,” immersive VR applications have the potential to assist in empathy training for mental healthcare such as depression [21]. In responding to the increasing numbers of diagnosed depression throughout COVID-19, a first-person VR adventure game called “Schwer” was designed and prototyped by the authors' research team to provide a social support environment for depression treatment. To continue the study and assess the training effectiveness for an appropriate level of empathy, this current article includes a brief survey on data analytics models and features to accumulate evidence for the next phase of the study, an interactive game-level design for the “Reconstruction” stage, and a preliminary study with data collection. The preliminary study was conducted with a post-game interview to evaluate the design of the levels and their effectiveness in empathy training. Results showed that the game was rated as immersive by all participants. Feedback on the avatar design indicated that two out of three of the non-player characters (NPCs) have made the intended effect. Participants showed mostly positive opinion towards their experienced empathy and provided feedback on innovative teleport mechanism and game interaction. The findings from the literature review and the results of the preliminary study will be used to further improve the existing system and add the data analytics model training. The long-term research goal is to contribute to the healthcare field by developing a dynamic AI-based biofeedback immersive VR system in assisting depression prevention.",Yes,"본 논문은 가상현실 기반 공감 훈련 게임을 설계하고, 그 효과를 평가하기 위한 예비 연구를 수행하여 직접적인 연구 결과와 데이터를 제시하고 있다. 또한, 문헌 검토와 실험 결과를 바탕으로 시스템 개선과 후속 연구 방향을 제안하는 등 독창적인 연구 기여가 포함되어 있다."
Time Sensitive Networking for Future Enabling Technologies: Overview and Measurement Issues for Metrological Characterization,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10615861,"The consolidation of Industry 4.0 and the perspective of the evolution toward the new Industry 5.0 paradigm are drastically accelerating the integration and the performance improvement of critical enabling technologies, as well as the uptake of new ones inside all industrial processes and future emerging applications. For example, increasingly sophisticated sensors and ultra-high-definition cameras, machine learning algorithms, and Artificial Intelligence are now starting to work together effectively. They can deliver real-world advantages for many applications, from manufacturing processes to advanced applications in modern vehicles, aircraft, and surveillance. Communication technologies play a fundamental role in these aims, and they have to ensure proper bandwidths, security, latencies, synchronisation, packet loss, reliability, and interoperability. Time Sensitive Networking (TSN) represents the technology that fulfils the mandatory requirements for these parameters. To satisfy the related (hard) constraints and to speed up the diffusion of TSN, it becomes of paramount importance to accurately measure and characterise the relevant features of all the devices involved in the channel access and data transfer operations, starting from the network nodes and bridges, up to the communication links. In this framework, the paper aims to briefly analyse the TSN domain and the general measurement issues (methods, procedures, test sites, test setup) to address the metrological performance assessment of the network devices involved in such applications.",No,"본 논문은 Time Sensitive Networking(TSN) 기술과 관련된 개요 및 측정 문제를 분석하는 리뷰 성격의 논문으로 보이며, 독창적인 연구 결과나 실험적 기여보다는 기존 기술과 측정 방법에 대한 정리와 평가에 중점을 두고 있습니다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단하기 어렵습니다."
Optimization of Deep Neural Network Model for Embedded Applications: A Case Study of Retinal Vessels Segmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10162079,"Medical science is an area where deep neural networks (DNNs) have been successfully applied for disease diagnosis based on image analysis. The accurate diagnosis of different eye diseases requires retinal vessels segmentation in retinal fundus image. The focus of this work is the improvement of innovative, lightweight deep learning (DL) models that can be used on embedded systems (ESs) for eye diseases diagnosis in the early stage at the point of care in clinics and general hospitals. This work aims to optimize the encoder-decoder-based architecture to show the possibility of porting a DL model onto ESs but without compromising on segmentation performance. The performance was evaluated based on open-source retinal image databases: DRIVE, and CHASE_DB1. The proposed model achieved encouraging evaluation metrics in addition to attributing a significantly lower number of parameters. Moreover, the proposed model outperformed the previous related works while its computational complexity is much lower.",Yes,"본 논문은 임베디드 시스템에서 사용할 수 있는 경량 딥러닝 모델을 최적화하고, 망막 혈관 분할 성능을 평가하는 독창적인 연구 내용을 포함하고 있다. 또한, 기존 연구 대비 성능과 계산 복잡도를 비교 분석하여 직접적인 연구 기여를 하고 있음을 보여준다."
A Novel Methodology for Improving Applications of Modern Predictive Modeling Techniques to Linked Data Sets Subject to Mismatch Error,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486610,"In recent years, the rise of social media platforms such as Twitter/X has provided social scientists with a wealth of user-content data. Combining social media and survey data has the potential to produce a comprehensive source of information for social research. These data are often collected from multiple sources and combined by probabilistic record linkage. For the analysis of these linked data files, advanced machine learning techniques, such as random forests, boosting, and related ensemble methods, have become essential tools for survey methodologists and data scientists. There is, however, a potential pitfall in the widespread application of these techniques to linked data sets that needs more attention. Linkage errors such as mismatch and missed-match errors can distort the true relationships between variables and adversely alter the performance metrics routinely output by predictive modeling techniques, such as variable importance, confusion matrices, RMSE, etc. Thus, the actual predictive performance of these machine-learning techniques may not be realized. In this paper, we describe a methodology designed to adjust modern predictive modeling techniques for the presence of mismatch errors in linked data sets. The proposed approach, based on mixture modeling, is general enough to accommodate various predictive modeling techniques in a unified fashion. We evaluate the performance of our proposed methodology with simulations implemented in R. We conclude with recommendations for future work.",Yes,"본 논문은 링크 오류가 포함된 데이터셋에 대해 현대 예측 모델링 기법을 조정하는 새로운 방법론을 제안하고 있으며, 이를 시뮬레이션을 통해 평가하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Multi-Sensory Stress Detection System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463214,"In the contemporary workplace, chronic stress is increasingly prevalent, bearing serious consequences such as health issues, workplace-related suicides, and fatalities. Addressing this urgent challenge, we present the “Multi-Sensory Stress Detection System,” a groundbreaking solution designed to proactively identify, analyze, and mitigate stress, thereby promoting a healthy work-life balance and preventing potential harm. Central to our strategy is the utilization of the WESAD (Wearable Stress and Affect Detection) dataset, capturing critical biological markers through wrist- and chest-worn (RespiBAN) devices. A robust preprocessing pipeline ensures data accuracy by transforming non-stationary data into a stationary format. We harness data-driven insights for stress detection using advanced Deep Learning Models, rigorously validated against existing research. The proposed CNN-based model achieves an accuracy of 98.73 %, underscoring the critical importance of precise stress detection in mitigating psychological impediments in an individual's life. This innovative technology carries significant implications for trauma recovery and workplace stress management, and it seamlessly integrates with the Internet of Things (IoT) to usher in a revolution in stress management in the contemporary environment. This innovation has broad applications in the healthcare industry, particularly for patients dealing with PTSD and autism.",Yes,"논문은 WESAD 데이터셋을 활용한 스트레스 감지 시스템을 제안하고, 비정상 데이터를 정상 데이터로 변환하는 전처리 과정과 CNN 기반 딥러닝 모델을 개발하여 98.73%의 정확도를 달성하는 등 독창적인 연구 방법과 결과를 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Failure Modes or Not Failure Modes? Integrating Machine Learning in Aerospace Safety Assessment Processes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749313,"Although not yet a reality, recent developments have sparked interest in adopting machine learning (ML) across various aviation applications. Potential applications include control, health management, collision avoidance, and single pilot operations. However, integrating ML in aviation presents significant safety challenges due to its complex behavior compared to traditional software. This paper examines the impact of ML's uncertainty on aviation safety assessment processes, focusing on defining ML failure modes for learning-enabled components. Through a literature review and semantic analysis based on current aviation safety standards like ARP 4754A and its successor ARP 4754B, we identify ML-specific factors that contribute to failures of ML-enabled systems. Using a case study on multimodal visual navigation, this paper validates proposed failure modes and their contributions. Additionally, it demonstrates the application of traditional assessment methods to ML-enabled systems.",Yes,"논문은 ML의 불확실성이 항공 안전 평가에 미치는 영향을 분석하고, ML 고유의 실패 모드를 정의하며, 사례 연구를 통해 이를 검증하는 등 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Feedforward Neural Network Models for FPGA Routing Channel Width Estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855861,"Since interconnects play the increasingly important role in delay and area of the Fieldprogrammable gate array (FPGA) implementations, routing architecture design has become the focus of much work related to FPGA architecture development. This paper leverages feedforward neural networks to derive accurate models of the routing channel width in homogeneous FPGA architecture with two advanced intelligence learning techniques: Gradient-based learning algorithm (GLA) and Extreme learning machine (ELM). The resultant models can be used in the early stages of FPGA architecture development to facilitate fast design space exploration which is difficult to achieve in the traditional experiment-based method. The proposed models are evaluated by comparing the estimated channel widths to the real values generated from a CAD tool VTR over IWLS2005 benchmark circuits. Results show that the GLA model achieves the estimation accuracy 3.98% and the ELM model has the accuracy 3.91%, which show significant improvement over existing estimation approaches.",Yes,"본 논문은 FPGA 라우팅 채널 폭 추정을 위해 피드포워드 신경망 모델을 제안하고, 두 가지 학습 기법을 적용하여 기존 방법보다 정확도를 향상시킨 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 모델을 실제 CAD 도구와 벤치마크 회로를 통해 평가한 실험 결과를 제시하고 있어 연구 논문에 해당한다."
"BCI System, Current Gaps and its Future in Home Automation",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10346896,"Brain-computer interface (BCI) is an association between brain and device that helps signals from the human brain to help in controlling activity, such as automatic control of home devices, gaming, robot, or a prosthetic limb. BCI is mainly useful for individuals who cannot control the devices using body parts. The paper aims to review the EEG-based BCI systems with various brain control signals, problem in current system and further area where work can be done to improve BCI. This paper is presented as Components of Brain Computer Technology overview, and gaps with current systems and discusses the journals on the same. The research paper also discusses the implementation of BCI using Artificial Intelligence/ML models supported by python language. This is an effort to evaluate the effects of environment on the brain waves and future connected application areas in BCI.",No,초록에서 본 논문은 EEG 기반 BCI 시스템에 대한 리뷰와 현재 시스템의 문제점 및 향후 연구 방향을 제시하는 개요 논문으로 보입니다. 독창적인 실험 결과나 새로운 연구 기여보다는 기존 연구를 정리하고 논의하는 내용에 중점을 두고 있어 연구 논문으로 보기 어렵습니다.
Learning Style Identification Using Semisupervised Self-Taught Labeling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415253,"Education is a dynamic field that must be adaptable to sudden changes and disruptions caused by events like pandemics, war, and natural disasters related to climate change. When these events occur, traditional classrooms with traditional or blended delivery can shift to fully online learning, which requires an efficient learning environment that meets students’ needs. While learning management systems support teachers’ productivity and creativity, they typically provide the same content to all learners in a course, ignoring their unique learning styles. To address this issue, we propose a semisupervised machine learning approach that detects students’ learning styles using a data mining technique. We use the commonly used Felder-Silverman learning style model and demonstrate that our semisupervised method can produce reliable classification models with few labeled data. We evaluate our approach on two different courses and achieve an accuracy of 88.83% and 77.35%, respectively. Our work shows that educational data mining and semisupervised machine learning techniques can identify different learning styles and create a personalized learning environment.",Yes,"논문은 반지도 학습(semisupervised learning) 기법을 활용하여 학습자 스타일을 식별하는 독창적인 연구 방법을 제안하고, 실제 데이터에 적용하여 성능을 평가한 연구 내용을 포함하고 있다. 이는 직접적인 연구 기여를 담고 있는 연구 논문으로 판단된다."
Joint Front–Edge–Cloud IoVT Analytics: Resource-Effective Design and Scheduling,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817449,"A tremendous amount of visual data are bing collected by the Internet of Video Things (IoVT) systems in which ubiquitous cameras deployed in cities enable new applications in the domains of smart transportation and public security. However, the limited resources in terms of communication, computing, and caching (3C) in the conventional cellular network make it challenging to adopt centralized artificial intelligence (AI) to conduct real-time video-based data analytics. In this work, based on the 5G network architecture with edge servers, a three-phase resource-effective solution is proposed to perform surveillance operations in a large-scale wireless IoVT. The proposed strategy integrates front-end cameras with simple on-chip neural networks performing real-time object-of-interest segmentation, edge servers, and cloud servers with AI functionality carrying out image-based target recognition and video-based target analytics tasks. More importantly, we design the optimal 3C strategy to achieve the best video analytics performance constrained by computing offload ratio, network resource allocation and video-related parameters. Extensive simulations with deep neural networks implemented both at the front-end cameras and in the cloud server have validated the effectiveness of the proposed solution.",Yes,"본 논문은 5G 네트워크 아키텍처를 기반으로 한 IoVT 시스템에서 실시간 영상 분석을 위한 자원 효율적 설계 및 스케줄링 전략을 제안하고, 최적의 3C 전략을 설계하여 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법의 효과를 심층 신경망 시뮬레이션을 통해 검증한 점에서 연구 논문으로 판단된다."
Existing Tower Infrastructure Classification and Analysis for 5G Implementation in Jakarta Area Using Machine Learning Models,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335369,"The Ministry of Communication and Informatics plans to construct infrastructure for fifth-generation network deployment in Indonesia. However, their readiness to support 5G systems should be evaluated. There is non-standalone architecture for 5G network related to the use of existing towers. The NSA architecture enables the use of towers that meet the 5G specification. Machine learning can be used to classify these towers quickly. Based on working frequency on the existing tower data, this research conduct tower classification using machine learning models to get the tower's quantity that are ready for the tower's non-standalone architecture and visualize the data on the open street map to evaluate the network needed in the area. The considered area for the tower data is Jakarta Area. The data shows 839 towers need to be upgraded to the fifth-generation frequency. The data also shows 1152 towers that are ready for the fifth-generation network. This result can benefit the operator in estimating the investment of the tower to be ready for the new technology and impact the CAPEX view to construct the new tower.",Yes,"본 논문은 5G 네트워크 구축을 위한 기존 타워 인프라의 분류 및 분석을 위해 머신러닝 모델을 적용한 연구를 수행하고 있으며, 이를 통해 타워의 5G 준비 상태를 평가하는 독창적인 연구 결과를 제시하고 있다. 따라서 직접적인 연구 기여가 포함된 연구 논문으로 판단된다."
DivGroup: A Diversified Approach to Divide Collection of Patterns into Uniform Groups,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546203,"Similarity based grouping of patterns has been explored profusely under the well celebrated clustering paradigm in pattern recognition and machine learning. In clustering, objects in the same cluster are similar to each other and objects belonging to different clusters are dissimilar in a corresponding sense. However, it is not rare to come across situations where instead of a similarity based grouping, forming groups of diverse objects is needed. Resource allocation across different parts of an organization, performing cross-validation splits of dataset with class imbalance, heterogeneous or mixed ability partitioning of students, etc. are the applications of grouping which require each group to contain diverse set of patterns. Moreover, these applications also demand different groups to be similar to each other in some sense. In this work, we propose a generic framework for partitioning a collection of patterns into a set of groups such that the above two criteria are fulfilled. To the best of our knowledge, this is the first work to propose such a framework irrespective of any particular application. Towards this end, it turns out that finding an optimal solution to the problem that we developed is NP Hard. So we Propose an approximate solution for the same. We conduct experiments on both synthetic and real world datasets to evaluate the performance of the proposed algorithm. We show the merit of the algorithm by comparing the results with some related state-of-the-art baseline methods.",Yes,"논문은 기존 연구와 차별화된 새로운 문제 정의와 이를 해결하기 위한 일반적인 프레임워크를 제안하며, NP-hard 문제에 대한 근사 해법을 개발하고 실험을 통해 성능을 평가하고 있다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 나타낸다."
eXplainable Reinforcement Learning Using Introspection in a Competitive Scenario,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10814839,"Reinforcement learning (RL) is inspired by behavioral psychology and helps solve problems where there is no previous data; that is, the agent learns through trial and error by interacting with the environment. Explainable RL aims to solve problems related to trust and transparency that people without technical knowledge might have about these systems. This work proposes a novel explainable reinforcement learning approach based on introspection in Deep Q-network and Proximal Policy Optimization algorithms. The integration of the introspection method empowers RL agents to assess the probability of success in a game, solely based on the Q-values obtained. In this regard, the agent will be able to measure how high the chance of winning for each available action during the game using the value function approximation’s output. Finally, the introspection-based agents could win several rounds during training, being more competitive than their opponents in different game moments. The computed probabilities of success, showed that although the agent was able to complete a reasonable number of games and generated strategies to win, the agent could not maintain a constant rhythm and learning process.",Yes,"본 논문은 Deep Q-network와 Proximal Policy Optimization 알고리즘에 기반한 새로운 설명 가능한 강화학습 방법을 제안하고 있으며, 이를 통해 RL 에이전트가 게임 내 성공 확률을 평가하는 능력을 갖추도록 하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안된 방법의 성능을 실험적으로 검증하여 경쟁력 있는 에이전트를 구현한 점에서 직접적인 연구 기여가 있다고 판단된다."
Exploiting 2D Coordinates as Bayesian Priors for Deep Learning Defect Classification of SEM Images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9453405,"Deep Learning approaches have revolutionized in the past decade the field of Computer Vision and, as a consequence, they are having a major impact in Industry 4.0 applications like automatic defect classification. Nevertheless, additional data, beside the image/video itself, is typically never exploited in a defect classification module: this aspect, given the abundance of data in data-intensive manufacturing environments (like semiconductor manufacturing) represents a missed opportunity. In this work we present a use case related to Scanning Electron Microscope (SEM) images where we exploit a Bayesian approach to improve defect classification. We validate our approach on a real-world case study and by employing modern Deep Learning architectures for classification.",Yes,"논문은 SEM 이미지의 결함 분류를 위해 2D 좌표를 베이지안 사전으로 활용하는 새로운 접근법을 제안하고, 실제 사례 연구와 현대 딥러닝 아키텍처를 통해 검증한 내용을 포함하고 있다. 이는 독창적인 연구 기여를 포함한 연구 논문으로 판단된다."
Experimenting With an Efficient Driver Behavior Dynamical Model Applicable to Simulated Lane Changing Tasks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10658638,"We test an approach to modelling the car driver behaviour during simulated lane changing tasks, aiming to obtain a sufficiently precise model in the simplest possible form, namely, with a small number of parameters. Various applications of such models are available in the literature. Based on a recent review of the research to date, the cybernetic single-loop transfer function models employing McRuer’s theory are applied. The purpose of the presented method is to evaluate the optimal structure of the transfer function via cross-validation as a technique known from machine learning. The experiments utilize a driving simulator with in-house developed software; this configuration facilitates acquiring the data at the desired sampling frequency and in a manner that ensures the repeatability of the test process scenarios. Using the crossvalidation results, we evaluate the second-order model with a derivative state and a reaction delay component as an optimal structure for approximating the measured data, which originated from a set of measurements on 92 active drivers. Even though more complex driving tasks could require high-order models, driver’s control action during our specific experiment is described through only four parameters. The parameters are jointly determined by the current driver’s mental state and the testing conditions defined in our scenario. Since the parameters are related to his/her dynamical behaviour, they allow easier mutual comparison of the drivers than complex models with many parameters. The results are verified via establishing a relationship to the multi-loop model presented in the recent literature. The larger dataset enables evaluating the confidence intervals of the drivers’ parameters which is inconvenient with 4 to 10 drivers commonly presented in the relevant sources.",Yes,"본 논문은 운전자 행동 모델링을 위한 새로운 방법론을 제안하고, 92명의 운전자 데이터를 활용해 모델 구조를 최적화하는 실험적 연구를 수행하였다. 이는 기존 연구를 바탕으로 직접적인 데이터 수집과 모델 평가를 포함한 독창적인 연구 내용이 포함된 연구 논문임을 보여준다."
Image Dehazing Assessment: A Real-World Dataset and a Haze Density-Aware Criteria,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366874,"Full-reference image dehazing quality assessment (FR-IDQA) evaluates the visual quality of a dehazed image by measuring its differences with a clear reference. The existing FR-IDQA methods are not convincing due to the lack of well-aligned datasets of hazy and clear image pairs and the limited hand-crafted features make it difficult to simulate the complicated perception by the human visual system (HVS). In this work, we build a real-world image dataset, namely RW-Haze, which comprises natural hazy images and their well-aligned clear references. Each clear image is paired with several hazy images with diverse haze levels from slight to heavy. Meanwhile, the existing FR-IDQA works evaluate the dehazed image quality in a global manner, without considering local haze distributions in the original hazy image. Actually, the perceived haze in a natural hazy image is not uniformly distributed, and the haze density varies with scene depth. Based on this priori observation, we design a haze density-aware convolutional neural network (CNN), namely DehIQA, for FR-IDQA. It adopts transfer learning to alleviate the issue of lacking sufficient labeled data. Specifically, we divide image dehazing assessment into two tasks. The source task is to classify unpaired clear and hazy images, which enforces the deep network to learn haze-related features. The target task is image quality assessment, which is achieved by transferring the trained model for the source task to the target task. Considering the fact that the perceived distortion in a dehazed image is also not uniform, we present a haze density-aware mechanism into DehIQA, which assigns different weights for different local regions in a dehazed image in terms of the dark channel of the original hazy image. Extensive experimental results show that DehIQA outperforms the state-of-the-art (SOTA) works on the benchmark dataset and achieves better consistency with human perceptions.",Yes,"본 논문은 실제 세계의 이미지 데이터셋을 구축하고, 기존 방법의 한계를 극복하기 위해 새로운 CNN 기반 평가 모델을 설계 및 학습하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 전이 학습과 지역별 안개 밀도 가중치 메커니즘을 도입하여 성능을 향상시킨 점에서 직접적인 연구 기여가 명확하다."
Dual consistency loss for contour-aware segmentation in medical images,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10340931,"Medical image segmentation is a paramount task for several clinical applications, namely for the diagnosis of pathologies, for treatment planning, and for aiding image-guided surgeries. With the development of deep learning, Convolutional Neural Networks (CNN) have become the state-of-the-art for medical image segmentation. However, issues are still raised concerning the precise object boundary delineation, since traditional CNNs can produce non-smooth segmentations with boundary discontinuities. In this work, a U-shaped CNN architecture is proposed to generate both pixel-wise segmentation and probabilistic contour maps of the object to segment, in order to generate reliable segmentations at the object’s boundaries. Moreover, since the segmentation and contour maps must be inherently related to each other, a dual consistency loss that relates the two outputs of the network is proposed. Thus, the network is enforced to consistently learn the segmentation and contour delineation tasks during the training. The proposed method was applied and validated on a public dataset of cardiac 3D ultrasound images of the left ventricle. The results obtained showed the good performance of the method and its applicability for the cardiac dataset, showing its potential to be used in clinical practice for medical image segmentation.Clinical Relevance— The proposed network with dual consistency loss scheme can improve the performance of state-of-the-art CNNs for medical image segmentation, proving its value to be applied for computer-aided diagnosis.",Yes,"본 논문은 의료 영상 분할을 위한 새로운 U자형 CNN 아키텍처와 dual consistency loss라는 독창적인 손실 함수를 제안하여 경계 인식 성능을 향상시키는 연구 내용을 포함하고 있다. 또한, 제안된 방법을 실제 심장 3D 초음파 데이터셋에 적용하고 성능을 검증한 점에서 직접적인 연구 기여가 명확하다."
Digital Twin Technology in Healthcare: A Literature Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10586661,"In our study, we review the literature from popular databases on how digital twins and machine learning can change healthcare and improve patient outcomes. The main research direction is the history of this area of investigation: the approaches to methodology; the recognition of trends in their reproduction; and the realization of empirics in practice. The paper also addresses the challenges as well as prospects in the field with a keen focus on digital twins’ impact on healthcare innovation and service delivery. The purpose of the research is to evaluate the potential of digital twin technology to change the healthcare system by means of novel medical practices and improving patient care. It is expected that digital twin technology will usher in a new era in healthcare. Presenter IEEE member ID 90261329.",No,"본 논문은 기존 문헌을 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않고 있다. 따라서 새로운 연구 기여보다는 기존 연구 동향과 전망을 정리하는 데 중점을 두고 있다."
Work in Progress: Congestion Control in mmWave Fluctuating Scenarios in 5G-A/6G,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10811159,"The development of the next generation of cellular networks, 5G-Advanced and 6G (5G-A/6G), offers higher speeds, sub-millisecond latency, and providing wider coverage, which are key to meeting the requirements of new and demanding applications. As such, there are several challenges in managing and leveraging this opportunity to ensure that channel conditions do not impact the application due to mobility and obstructions. 5G introduces the use of millimeter wave (mmWave), and the commercial deployment of this technology brings to light the issues related to propagation and mobility in high-frequency bands, and their impact on applications has begun to be studied by exploring the response of the congestion control algorithms (CCAs) in representative scenarios. However, the highly variable channel conditions in mmWave require that they be defined as fluctuating bandwidth scenarios. It is necessary to determine how the transport layer can take advantage of the maximum available capacity and not affect the applications, and whether the network promises are fulfilled. This work aims to design and evaluate a CCA capable of efficiently adapting to the fluctuating bandwidth of extreme condition scenarios of 5G-A/6G mobile networks and to compare it with the state-of-the-art algorithms. As a first result, a set of simulation-based evaluation using 5GLENA was discussed. A combination of CCAs and different radio link control (RLC) buffer sizes allows us to highlight the importance of defining appropriate metrics and including convergence times as part of the evaluation performance of the algorithms. Algorithms such as HighSpeed and New Reno are among the slowest to converge, while BBR is the fastest. The analysis and characterization of the algorithms will allow us to define the requirements for designing an Machine Learning (ML) based algorithm with optimal performance and better convergence times.",Yes,"본 논문은 5G-A/6G 네트워크의 mmWave 변동 시나리오에서 동작하는 새로운 혼잡 제어 알고리즘(CCA)을 설계하고 평가하는 연구를 수행하고 있으며, 기존 알고리즘과의 비교 분석을 통해 성능 개선 방향을 제시하고 있다. 이는 직접적인 연구 기여와 실험적 평가를 포함한 독창적인 연구 내용으로 판단된다."
Channel Selection Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463321,"The channel plays an important role in any wireless communication system. If there exists only one channel between the transmitter and the receiver, if the link fails, then the communication cannot be established. The reliability of communication can be improved by introducing multiple communication channels. Not only the number of channels but also the type of channels being used has an impact on the system. Although there have been few works done in this direction, works related to long distances have not been given importance. Further, manual channel switching is the recommended choice, but manual switching is greatly impacted by the people involved in the mechanism and may not be accurate all the time. Keeping these in view, this paper proposes a channel selection mechanism based on Wi-Fi and LoRa (Long Range) technologies. The advantage is that this mechanism takes into account both radio technologies to choose the best channel for the given conditions. Further, machine learning-based techniques are introduced to learn the best channel to use based on historical data, which helps in achieving automatic channel selection. This will be particularly useful in dynamic environments, where the channel conditions can change frequently. To validate the proposed concept, various experiments are carried out and from the experimental results, it is observed that the KNN algorithm achieves good performance.",Yes,"논문 초록에서 제안된 채널 선택 메커니즘은 Wi-Fi와 LoRa 기술을 결합하고, 머신러닝 기법을 활용하여 자동 채널 선택을 구현하는 독창적인 연구 내용을 포함하고 있다. 또한, 실험을 통해 제안된 방법의 성능을 검증한 점에서 연구 논문으로 판단된다."
A Comprehensive Cybersecurity Resilience Framework Augmenting Smart Grid Stability,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10744380,"Smart grids have emerged as intelligent systems comprising efficient sensor technology for effective power transmission and distribution. These critical modern infrastructures for energy distribution have been facing escalating cybersecurity threats. This research presents a novel cybersecurity framework for the mitigation and prevention of such attacks. The framework proposed involves a machine learning based defense mechanism that assists in enhancing the grid’s reliability even further against potential attacks. The research uses a computer simulated setting to evaluate cybersecurity attack scenarios and their effects, on grid operations and overall system security. This article provides a review of existing literature on the cybersecurity challenges that modern smart grids encounter along with the strategies used to counter these threats. The developed framework serves as a cybersecurity solution that employs mathematical methods to bolster grid stability and management identify anomalies and prevent potential cyber-attacks. The framework also leverages Machine Learning models to refine its operations and enhance its ability to identify risks effectively. The results indicate that this innovative approach surpasses methods in reducing cybersecurity risks of this nature. The effectiveness of the proposed cybersecurity model, in mitigating cyber threats within grids is further validated by its detection accuracy rate.",Yes,"논문은 스마트 그리드의 사이버보안 위협을 완화하기 위한 새로운 프레임워크를 제안하고, 머신러닝 기반 방어 메커니즘을 개발하여 시뮬레이션을 통해 그 효과를 평가하는 독창적인 연구 내용을 포함하고 있다. 기존 문헌 검토뿐만 아니라 수학적 방법과 머신러닝 모델을 활용한 구체적인 연구 기여가 명확히 드러나 있다."
Challenges of machine learning-based RUL prognosis: A review on NASA's C-MAPSS data set,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613682,"The estimation of a system's or a component's remaining useful life (RUL) is considered the most complex task in predictive maintenance, at the same time the most beneficial one. In this brief review paper, we survey the state-of-the-art in machine learning-based RUL prognosis based on research on NASA's C-MAPSS data set. We identify the frequently used models, comparatively evaluate model performance and survey the used feature extraction methods. As a main contribution, we formulate challenges in the field, independently of the C-MAPSS data set. Among the challenges are interpretability, model uncertainty and domain adaptation, i.e. transfer learning. The identified challenges may serve to identify potential research directions, in order to further push the field of machine learning applied to RUL prognosis.",No,"본 논문은 NASA의 C-MAPSS 데이터셋을 기반으로 한 기계학습 기반 RUL 예측 연구들을 리뷰하고, 주요 도전과제를 정리하는 리뷰 논문이다. 따라서 직접적인 독창적 연구 결과나 실험을 제시하기보다는 기존 연구들을 종합하고 분석하는 데 중점을 두고 있다."
Identifying IT Purchases Anomalies in the Brazilian Government Procurement System Using Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838233,"The Department of Research and Strategic Information (DIE), from the Brazilian Office of the Comptroller General (CGU), is responsible for investigating potential problems related to federal expenditures. To pursue this goal, DIE regularly has to analyze large volumes of data to search for anomalies that can reveal suspicious activities. With the growing demand from the citizens for transparency and corruption prevention, DIE is constantly looking for new methods to automate these processes. In this work, we investigate IT purchases anomalies in the Federal Government Procurement System by using a deep learning algorithm to generate a predictive model. This model will be used to prioritize actions carried out by the office in its pursuit of problems related to this kind of purchases. The data mining process followed the CRISP-DM methodology and the modeling phase tested the parallel resources of the H2O tool. We evaluated the performance of twelve deep learning with auto-encoder models, each one generated under a different set of parameters, in order to find the best input data reconstruction model. The best model achieved a mean squared error (MSE) of 0.0012775 and was used to predict the anomalies over the test file samples.",Yes,본 논문은 브라질 정부 조달 시스템의 IT 구매 이상 탐지를 위해 딥러닝 알고리즘을 사용하여 예측 모델을 개발하는 독창적인 연구를 수행하고 있다. 데이터 마이닝과 모델링 과정을 체계적으로 진행하며 성능 평가를 통해 최적 모델을 도출한 점에서 직접적인 연구 기여가 포함되어 있다.
"MLOps Components, Tools, Process, and Metrics: A Systematic Literature Review",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855428,"With the growing popularity of machine learning, implementations of the environment for developing and maintaining these models, called MLOps, are becoming more common. The number of publications in this area is relatively small, although growing rapidly. Our goal was to review the current state of the literature in the MLOps area and answer the following research questions: What classes of tools are used in MLOps environments? Which tool implementations are the most popular? What processes are implemented within MLOps? What metrics are used to measure the effectiveness of MLOps implementation? Based on this review, we identified classes of tools included in the MLOps architecture, along with their most popular implementations. While some tools originate from DevOps practices, others, such as Model Orchestrators, Feature Stores, and Model Repositories, are unique to MLOps. We propose a reference MLOps architecture based on these findings and outline the stages of the model production process. We also sought metrics that would allow us to assess and compare the effectiveness of MLOps practices, but unfortunately, we were unable to find a satisfactory answer in this area.",No,"이 논문은 MLOps 관련 기존 문헌을 체계적으로 검토하는 문헌 리뷰 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않습니다. 따라서 새로운 연구 기여보다는 기존 연구를 종합하고 분석하는 데 초점이 맞춰져 있습니다."
A Contactless Method for Measuring Unbalanced Loading of Utility Transformer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940787,"Three phase unbalancing remains a serious power quality issue in overhead power distribution lines. Variation of the load is a major contributor in unbalancing. This paper reports a contactless method with a minimum number of magnetic field sensors for measuring unbalance loading of the distribution network. The network is developed in Simulink based on parameters of a typical Pakistan distribution network. This paper also discusses about the orientation of magnetic field sensor so that the results are not affected by any field. The optimal location of sensors will help the system to differentiate between balanced and unbalanced loading. Sensor placement system was developed through MATLAB programming. Another contribution in this paper is the use of a pattern recognition technique instead of the current reconstruction algorithm commonly found in the literature review. With the help of pattern recognition based machine learning, the system is able to predict the unbalance loading of distribution lines.",Yes,"본 논문은 기존 연구와 달리 최소한의 자기장 센서를 이용한 비접촉식 부하 불균형 측정 방법을 제안하고, 센서 배치 및 패턴 인식 기반 기계 학습 기법을 활용하여 부하 불균형을 예측하는 독창적인 연구 내용을 포함하고 있다. 시뮬링크와 MATLAB을 활용한 시스템 개발 및 새로운 알고리즘 적용이 직접적인 연구 기여로 판단된다."
Recent Advances of Affect Detection from Arabic Text,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809155,"Emotion Detection (ED) from text has been an active research field recently. It has attracted the attention of researchers as it can measure the emotional contexts while humans interact with computers. Humans could express their emotion in various ways; using typed text, facial expressions, speech, gestures, and physiological measures. ED is considerably different from sentiment analysis SA, where SA goal is to detect polarity from text such as positive, negative or neutral. On the other hand, ED aims to recognize emotions from input text. Emotions can be modeled as discrete categories, e.g. Ekmans six basic emotions (angry, fear, joy, disgust, surprise and sadness). On the other hand there is the dimensional model that express emotions as valence, arousal and dominance values. Social media provides a rich source of emotional text, e.g. Twitter and Facebook. In this paper, we provide a review of recent work on ED from Arabic text. We discuss approaches (lexicons, machine learning, deep neural networks and ensemble approaches), tools for text processing, and We also discuss description of the most popular datasets in this domain.",No,"본 논문은 아랍어 텍스트에서 감정 인식에 관한 최근 연구들을 리뷰하는 논문으로, 직접적인 독창적 연구 결과나 실험을 제시하지 않고 기존 연구들을 종합하여 설명하는 리뷰 논문에 해당합니다. 따라서 새로운 연구 기여를 포함한 연구 논문으로 보기 어렵습니다."
Hierarchical Knee Image Synthesis Framework for Generative Adversarial Network: Data From the Osteoarthritis Initiative,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775719,"Medical images synthesis is useful to address persistent issues such as the lack of training data diversity and inflexibility of traditional data augmentation faced by medical image analysis researchers when developing their deep learning models. Generative adversarial network (GAN) can generate realistic image to overcome the abovementioned problems. We proposed a GAN model with hierarchical framework (HieGAN) to generate high-quality synthetic knee images as a prerequisite to enable effective training data augmentation for deep learning applications. During the training, the proposed framework embraced attention mechanism before the 256 ×256 scale in generator and discriminator to capture salient information of knee images. Then, a novel pixelwise-spectral normalization configuration was implemented to stabilize the training performance of HieGAN. We evaluated the proposed HieGAN on large scale knee image dataset by using Am Score and Mode Score. The results showed that HieGAN outperformed all relevant state-of-art. Hence, HieGAN can potentially serve as an important milestone to promote future development of more robust deep learning models for knee image segmentation. Future works should extend the image synthesis evaluation to clinical-related Visual Turing Test and synthetic data augmentation for deep learning segmentation task.",Yes,"본 논문은 새로운 GAN 모델(HieGAN)을 제안하고, 이를 통해 고품질 무릎 이미지 합성을 수행하는 독창적인 연구 내용을 포함하고 있다. 또한, 모델의 성능 평가를 통해 기존 기법 대비 우수함을 입증하여 직접적인 연구 기여가 있음을 보여준다."
"HRES Systems State of the Art: Topologies, Sizing Approaches, and Evaluation Criteria",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10152341,"Hybrid Renewable Energy Sources (HRES) are widely employed to create power because of the constant rise in electricity usage and environmental concerns. Their benefits include cleanliness, accessibility, affordability, and availability. To combat the variability and randomness of a single RES, such as solar and wind power, hybrid RESs have been developed with a battery for the storage of extra throughout the day and the support of load demand during erratic and nighttime hours and/or the use of a more traditional source such as a diesel generator (DG) in the effect of climate change or during the event of a blackout. This paper reviews recent literature and the most effective methods and approaches for sizing and optimizing hybrid PV/WT/DG/batteries systems for a rural zone. The most popular topologies for the implementation of this system are compared in this paper. The characteristics of the classic, Artificial Intelligence (AI), and hybrid methodologies were evaluated along with software tools.",No,"본 논문은 하이브리드 재생에너지 시스템(HRES)의 최신 문헌과 방법론을 리뷰하는 내용으로, 직접적인 실험이나 새로운 연구 결과를 제시하기보다는 기존 연구들을 종합하고 비교하는 리뷰 논문에 해당한다. 따라서 독창적인 연구 기여가 포함된 연구 논문으로 보기 어렵다."
Navigating the Depths: A Comprehensive Survey of Deep Learning for Passive Underwater Acoustic Target Recognition,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10716649,"The field of deep learning is a rapidly developing research area with numerous applications across multiple domains. Sonar (SOund Navigation And Ranging) processing has traditionally been a field of statistical analysis. However, in the past ten to fifteen years, the rapid growth of deep learning has challenged classical approaches with modern deep learning-based methods. This survey provides a systematic overview of the Underwater Acoustic Target Recognition (UATR) domain within the area of deep learning. The objective is to highlight popular design choices and evaluate the commonalities and differences of the investigated techniques in relation to the selected architectures and pre-processing methods. Furthermore, this survey examines the state of UATR literature through the identification of prominent conferences and journals which points new researchers in directions where to allocate UATR related publications. Additionally, popular datasets and available benchmarks are identified and analysed for complexity coverage. This work targets researchers new to the field as well as experienced researchers that want to get a broader overview. Nonetheless, experienced sonar engineers with a strong background within classical analysis also benefit from this survey.",No,"본 논문은 딥러닝 기반 수중 음향 표적 인식 분야에 대한 종합적인 서베이(조사) 논문으로, 기존 연구들을 체계적으로 정리하고 평가하는 데 중점을 두고 있습니다. 따라서 직접적인 독창적인 연구 결과나 새로운 실험적 기여를 포함하지 않아 연구 논문에 해당하지 않습니다."
Revisiting the Efficacy of Image-Based Recognition of Arabic Handwriting: The Case of Whole Scripts,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10525737,"Recognizing handwritten scripts by machine learning has a tremendous number of applications in the automation of industry. Moreover, the datasets for such kind of applications in the Arabic language are heterogeneous and sometimes unusable, despite the potential benefit to millions of users who write in Arabic. Our study shows that segmenting the composing letters of handwritten words in Arabic and representing them as images do not enhance or benefit an accurate recognition of handwritten Arabic scripts, thus hindering many industrial applications from being migrated or localized to Arabic-speaking countries like Egypt. In this work, we question the limited use of general-purpose image-based machine learning approaches for categorizing offline Arabic handwritten scripts from heterogeneous datasets of letters and words. The paper reviews some of the existing approaches and proposes novel methods to improve word segmentation. Nevertheless, our findings indicate that these methods yield consistently low accuracy. We argue about the need for future research to not only improve the accuracy of recognizing offline handwritten Arabic scripts but also re-evaluate current treat-ments.",Yes,"논문은 기존 접근법을 검토하고 새로운 단어 분할 방법을 제안하는 등 독창적인 연구 내용을 포함하고 있으며, 아랍어 필기체 인식의 정확도 향상과 관련된 실험적 결과를 보고하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Multi-Class Classification of Upper Limb Movements With Filter Bank Task-Related Component Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10135081,"The classification of limb movements can provide with control commands in non-invasive brain-computer interface. Previous studies on the classification of limb movements have focused on the classification of left/right limbs; however, the classification of different types of upper limb movements has often been ignored despite that it provides more active-evoked control commands in the brain-computer interface. Nevertheless, few machine learning method can be used as the state-of-the-art method in the multi-class classification of limb movements. This work focuses on the multi-class classification of upper limb movements and proposes the multi-class filter bank task-related component analysis (mFBTRCA) method, which consists of three steps: spatial filtering, similarity measuring and filter bank selection. The spatial filter, namely the task-related component analysis, is first used to remove noise from EEG signals. The canonical correlation measures the similarity of the spatial-filtered signals and is used for feature extraction. The correlation features are extracted from multiple low-frequency filter banks. The minimum-redundancy maximum-relevance selects the essential features from all the correlation features, and finally, the support vector machine is used to classify the selected features. The proposed method compared against previously used models is evaluated using two datasets. mFBTRCA achieved a classification accuracy of 0.4193 ± 0.0780 (7 classes) and 0.4032 ± 0.0714 (5 classes), respectively, which improves on the best accuracies achieved using the compared methods (0.3590 ± 0.0645 and 0.3159 ± 0.0736, respectively). The proposed method is expected to provide more control commands in the applications of non-invasive brain-computer interfaces.",Yes,"본 논문은 다중 클래스 상지 움직임 분류를 위한 새로운 기계 학습 방법(mFBTRCA)을 제안하고, 이를 기존 방법들과 비교 평가하여 성능 향상을 입증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Modeling Visual Impairments with Artificial Neural Networks: a Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350909,"We present an approach to bridge the gap between the computational models of human vision and the clinical practice on visual impairments (VI). In a nutshell, we propose to connect advances in neuroscience and machine learning to study the impact of VI on key functional competencies and improve treatment strategies. We review related literature, with the goal of promoting the full exploitation of Artificial Neural Network (ANN) models in meeting the needs of visually impaired individuals and the operators working in the field of visual rehabilitation. We first summarize the existing types of visual issues, the key functional vision-related tasks, and the current methodologies used for the assessment of both. Second, we explore the ANNs best suitable to model visual issues and to predict their impact on functional vision-related tasks, at a behavioral (including performance and attention measures) and neural level. We provide guidelines to inform the future research about developing and deploying ANNs for clinical applications targeting individuals affected by VI.",No,본 논문은 인공신경망을 활용한 시각장애 모델링에 관한 기존 연구들을 종합적으로 검토하는 리뷰 논문입니다. 따라서 직접적인 독창적 연구 결과나 실험적 기여보다는 기존 문헌의 정리와 향후 연구 방향 제시에 중점을 두고 있습니다.
An Intelligent Calibration Testing of Electricity Meter using XGBoost for Manufacturing 4.0,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10127767,"The manufacturing company with a core business of manufacturing electricity meters has a testing system for the metrology industry, namely calibration testing. The part of calibration testing on an electricity meter is the verification test, which uses the method of comparing it to the standard meter test bench to calculate the accuracy error of the measurement. However, the problem is that carrying out this test requires a long cycle time, and it is difficult to increase production capacity without adding a standard meter calibration test bench which has an expensive investment. The smart factory concept that supports industry 4.0 can open up opportunities for research on information technology using artificial intelligence with machine learning models to solve this problem with the idea of creating intelligent testing. This research requires data collection on a calibration test bench machine, then processed to find model predictions so that they can be implemented into an intelligent test using the XGBoost Regression with Hyperparameter Tuning and Optimization methods as the Goal of this Research. In the results of this research, the evaluation of the XGBoost using the Hyperparameter Tuning and Optimization method, which is implemented in this case, could improve the accuracy and RMSE data testing modelling comparing other scenario models as defined before in the literature review. So, this can be an excellent solution to be applied in metrology manufacturing, especially verification tests in Manufacturing Calibration Testing on Electricity Meter, which is faster and low-investment testing with the implementation of an Intelligent Manufacturing Calibration Test.",Yes,"본 논문은 전기 계량기 교정 테스트의 정확도 향상과 테스트 시간 단축을 위해 XGBoost 기반의 기계 학습 모델을 개발하고 최적화하는 독창적인 연구를 수행하였다. 이는 기존 방법의 한계를 극복하기 위한 새로운 인공지능 적용 연구로, 직접적인 연구 기여가 포함된 논문임을 보여준다."
Modeling of cognition using EEG: A review and a new approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6129270,"Understanding the secrets underlying the brain functioning would be the noble achievement of this era. Learning how brain learns would be the milestone to guide the researchers of artificial intelligence, neurology and psychology. With the advent of “Integrate and Fire” model of neuron proposed about a hundred years ago, the brain research has picked up its pace in the study of different aspects of brain functionality. Many cognitive architectures have been proposed with an aim of simulating and understanding human cognition. On the other hand, many technologies have emerged that can measure the parameters of the brain activity. Among them, Electroencephalogram (EEG) stands as a reliable tool in the study of brain functioning. Simplified wireless EEGs are readily available now which can send data recorded by its electrodes to a computer for further processing. We have chosen this tool to detect different aspects of cognition and to predict the brain functioning behind it. A lot of studies from the past two decades have already revealed varying EEG patterns related to cognition. In this paper, we have proposed to extract different features from visual, tactile, auditory and psychomotor stimuli to work on different cognitive aspects such as memory, emotion, arousal, fatigue and distraction and to investigate its affect on the EEG. A methodology to model cognitive functions by relating the varying event related potential, brain waves, spectral density and latency in EEG outcomes are then related with the stimuli features to predict the cognitive state of mind.",Yes,"논문 초록에서 EEG 데이터를 이용해 인지 기능을 모델링하는 새로운 방법론을 제안하고 있으며, 다양한 인지 상태를 예측하기 위한 구체적인 연구 내용을 포함하고 있다. 이는 기존 연구를 리뷰하는 것뿐만 아니라 독창적인 연구 기여를 포함한 연구 논문임을 나타낸다."
Advancing IoT Security: A Review of Intrusion Detection Systems Challenges and Emerging Solutions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10883899,"The rapid proliferation of the Internet of Things (IoT) has revolutionized industries through interconnected devices and smart decision-making. However, this expansion has also introduced significant security challenges, as IoT networks are characterized by heterogeneity, resource constraints, and evolving threats. Intrusion Detection Systems (IDS) have emerged as essential mechanisms to complement preventive measures, yet existing solutions fall short of addressing the full spectrum of IoT-specific vulnerabilities. This paper critically reviews state-of-the-art IDS techniques, including advanced methods such as machine learning, federated learning, blockchain, and hybrid detection. Additionally, emerging approaches like Generative Adversarial Networks (GANs), reinforcement learning, and bio-inspired algorithms are explored for their potential to enhance IDS adaptability and scalability. The role of complementary security techniques, such as penetration testing, is highlighted in validating and strengthening IDS implementations. Applications in critical areas such as smart cities are discussed, emphasizing the need for robust and efficient security mechanisms. Key challenges such as interoperability, real-time detection, and resource efficiency are analyzed, and future research directions are proposed to develop comprehensive IDS frameworks tailored to the dynamic and diverse IoT ecosystem.",No,"본 논문은 IoT 보안 분야의 침입 탐지 시스템(IDS)에 대한 기존 연구들을 종합적으로 검토하는 리뷰 논문으로, 직접적인 실험이나 새로운 방법론 제안 등 독창적인 연구 결과를 포함하고 있지 않습니다. 따라서 연구 논문보다는 현황 분석과 향후 연구 방향 제시에 중점을 둔 리뷰 논문에 해당합니다."
SmartFire Car: An Image Processing and Artificial Intelligence-Based Fire Detection and Extinguishing System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456422,"In this work, we present a novel image processing and artificial intelligence-based autonomous vehicle system for swift fire detection and suppression. The suggested approach offers a unique and economical option, especially for developing nations, to properly tackle fire-related threats in crowded residential spaces and outdoors. The algorithm used by the system, the Haar cascade, is effective in spotting fire in real-time video feeds. After detecting and determining the ideal stopping distance from the fire, the SmartFire car begins to effectively and efficiently extinguish it. To put the proposed theoretical system to the test in actual practical settings, we developed our prototype. The capacity of the SmartFire prototype to measure distances from the fire source and continue operating amid several fire sources has been used to evaluate its effectiveness. To make the system reliable and efficient, its limitations, such as the smallest fire size needed for detection, have also been examined. This work has the potential to significantly contribute to the modernization of the traditional fire-fighting system, particularly in developing countries, leading to more effective fire-handling capabilities.",Yes,"논문은 화재 감지 및 진압을 위한 독창적인 이미지 처리 및 인공지능 기반 자율주행 차량 시스템을 제안하고, 이를 실제 프로토타입으로 구현하여 성능을 평가한 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 결과가 포함된 연구 논문으로 판단된다."
Comparative Analysis of Deep Learning Methods in the Realm of Sentiment Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075107,"Recent advances in deep learning have suggested number of methods which can be employed in several domains. Text classification is one of the most common natural language processing tasks and have given relevant results at the level of text classification to perform sentiment analysis. This paper compares the efficacy of different algorithms used to perform sentiment analysis. The comparison offers a global vision to contribute to a relevant system that can evaluate the different types of sentiment analysis by a Corpus (restaurant reviews). In our study we have used word embedding techniques to compare the efficacy of the simple RNN., LSTM., and BERT neural networks in the context of sentiment analysis. This research indicates that the use of BERT and LSTM yields the better outcomes., although BERT requires a longer training period.",Yes,"본 논문은 다양한 딥러닝 알고리즘(RNN, LSTM, BERT)을 사용하여 감성 분석 성능을 비교하는 실험적 연구를 수행하고 있으며, 이를 통해 BERT와 LSTM의 우수성을 제시하는 독창적인 연구 결과를 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Implementation Vulnerability Analysis: A case study on ChaCha of SPHINCS,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426079,"Deployment of Post Quantum Cryptography(PQC) primitives ensures forward secrecy for today's communication against tomorrow's quantum adversary. However, these primitives have to be realized in conventional hardware, which may be vulnerable to side-channel attacks. Therefore, vulnerability analysis of these primitives is essential before deployment. In this paper, a NIST favoured digital signature primitive- SPHINCS is taken for analysis. ChaCha and BLAKE are ciphers that form the building blocks of SPHINCS. These ciphers are based on Addition, Rotation, and XOR(ARX) operations. The literature review has shown ARX ciphers to be vulnerable against implementation attacks. In this work, an effective countermeasure for the aforementioned building blocks is explored. This is achieved through the following: Parallel Prefix Adders are taken for addition operation in these ciphers instead of the native adder in Electronic Design Automation(EDA) tools. Distinct profiles are created which include the cipher using a particular adder with its best-suited implementation style. An optimized version of Threshold Implementation(TI) is adopted on the profiles as a countermeasure for the attacks on the unprotected implementations. Finally, we evaluate the protected profiles' resistance using Test Vector Leakage Assessment(TVLA) and Deep learning techniques. Such an analysis that follows a generic framework will be straightforward to automate. We believe this will serve to be useful for standardization of Threshold Schemes.",Yes,"본 논문은 SPHINCS의 구성 요소인 ChaCha와 BLAKE 암호의 구현 취약점을 분석하고, 이를 방어하기 위한 최적화된 Threshold Implementation 기법을 제안하며, 실험적 평가를 수행하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Automated Hematological Analysis:Advances in Segmentation Approaches and Future Directions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10894459,"Hematological analysis is crucial for diagnosing blood disorders such as leukemia and anemia, a process that has traditionally depended on manual microscopy, and is susceptible to variability and human error. The quest for more reliable and efficient diagnostic methods has led to the development of automated systems. This review examines the methods used for automated blood cell segmentation, with a particular emphasis on deep learning approaches such as semantic and instance segmentation. Advanced models like Mask R-CNN and DeepLabv3+ were evaluated for their effectiveness in accurately segmenting and identifying blood components in complex histological images. The findings revealed that Mask R-CNN achieved a segmentation accuracy of 92%, while DeepLabv3+ came in close behind at 94.5%. Both models significantly surpassed traditional methods, which generally provide accuracy rates of 75–80 %. Despite these advancements, the review highlights ongoing challenges related to the robustness and scalability of these techniques in clinical settings. The paper concludes by emphasizing the transformative potential of deep learning in hematological diagnostics and calls for further research to address current limitations and improve patient outcomes.",No,"본 논문은 기존 연구들을 종합하여 자동 혈액학 분석의 분할 기법들을 검토하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 요약하고 평가하는 개관 논문에 해당합니다."
Lightweight and Efficient Convolutional Neural Network for Road Scene Semantic Segmentation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10053967,"Recently, Intelligent Transportation Systems (ITS) have become one of the most important fields of research topics, while it provides advanced road scene monitoring. Actually, computer vision is one of the most widely used fields in ITS, while it offers various tasks, such as object detection, image classification, and segmentation. Besides, Convolutional Neural Networks (CNNs) have shown their effectiveness in deep learning tasks, particularly in computer vision. In fact, road scene semantic segmentation task is a critical and fateful issue that can be addressed using CNN which requires effective precision as well as fewer parameters. The majority of related work on road scene segmentation proposes models that focus on one aspect, the precision, or the parameters requirement, which makes it hard to use in real-time applications when the precision is not a priority. To solve this issue, we propose a new network based on an encoder-decoder architecture, which compromises the precision with fewer parameters. Our proposed model is trained from scratch using only 0.64M parameters. The experiments are evaluated on the popular CamVid dataset, and the results show that our proposed CNN achieves better performance with fewer parameter resources.",Yes,"논문은 도로 장면 의미 분할을 위한 경량화된 CNN 모델을 새롭게 제안하고, 해당 모델을 직접 설계하고 학습시켜 실험을 통해 성능을 평가한 연구 내용을 포함하고 있다. 이는 독창적인 연구 기여를 포함하는 연구 논문에 해당한다."
Ionospheric Scintillation Forecasting Using Machine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10640525,"This study explores the use of historical data from Global Navigation Satellite System (GNSS) scintillation monitoring receivers to predict the severity of amplitude scintillation, a phenomenon where electron density irregularities in the ionosphere cause fluctuations in GNSS signal power. These fluctuations can be measured using the S4 index, but real-time data is not always available. The research focuses on developing a machine learning (ML) model that can forecast the intensity of amplitude scintillation, categorizing it into low, medium, or high severity levels based on various time and space-related factors. Among six different ML models tested, the XGBoost model emerged as the most effective, demonstrating a remarkable 77% prediction accuracy when trained with a balanced dataset. This work underscores the effectiveness of machine learning in enhancing the reliability and performance of GNSS signals and navigation systems by accurately predicting amplitude scintillation severity.",Yes,"논문은 GNSS 신호의 세기 변동을 예측하기 위해 다양한 머신러닝 모델을 개발하고 평가하는 독창적인 연구를 수행하고 있으며, 특히 XGBoost 모델을 활용해 77%의 예측 정확도를 달성한 점에서 직접적인 연구 기여가 포함되어 있다. 따라서 본 논문은 연구 논문에 해당한다."
Towards a Distributed Framework for Multi-Agent Reinforcement Learning Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286212,"Some of the most important publications in deep reinforcement learning over the last few years have been fueled by access to massive amounts of computation through large scale distributed systems. The success of these approaches in achieving human-expert level performance on several complex video-game environments has motivated further exploration into the limits of these approaches as computation increases. In this paper, we present a distributed RL training framework designed for super computing infrastructures such as the MIT SuperCloud. We review a collection of challenging learning environments-such as Google Research Football, StarCraft II, and Multi-Agent Mujoco- which are at the frontier of reinforcement learning research. We provide results on these environments that illustrate the current state of the field on these problems. Finally, we also quantify and discuss the computational requirements needed for performing RL research by enumerating all experiments performed on these environments.",Yes,"논문은 분산 강화학습 훈련 프레임워크를 제안하고, 여러 도전적인 학습 환경에서의 실험 결과를 제공하며, 계산 요구사항을 정량화하는 등 직접적인 연구 기여를 포함하고 있다. 이는 단순한 리뷰나 개념적 논의가 아닌 독창적인 연구 내용임을 나타낸다."
Neural Joint Entropy Estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903402,"Estimating the entropy of a discrete random variable is a fundamental problem in information theory and related fields. This problem has many applications in various domains, including machine learning, statistics, and data compression. Over the years, a variety of estimation schemes have been suggested. However, despite significant progress, most methods still struggle when the sample is small, compared to the variable’s alphabet size. In this work, we introduce a practical solution to this problem, which extends the work of McAllester and Statos. The proposed scheme uses the generalization abilities of cross-entropy estimation in deep neural networks (DNNs) to introduce improved entropy estimation accuracy. Furthermore, we introduce a family of estimators for related information-theoretic measures, such as conditional entropy and mutual information (MI). We show that these estimators are strongly consistent and demonstrate their performance in a variety of use cases. First, we consider large alphabet entropy estimation. Then, we extend the scope to MI estimation. Next, we apply the proposed scheme to conditional MI estimation, as we focus on independence testing tasks. Finally, we study a transfer entropy (TE) estimation problem. The proposed estimators demonstrate improved performance compared to existing methods in all of these setups.",Yes,논문 초록에서 제안된 방법이 기존 연구를 확장하고 딥 뉴럴 네트워크를 활용한 새로운 엔트로피 및 상호정보량 추정기를 개발하여 성능을 향상시킨다고 명시하고 있습니다. 이는 독창적인 연구 내용과 직접적인 기여를 포함하는 연구 논문임을 나타냅니다.
SitPose: Real-Time Detection of Sitting Posture and Sedentary Behavior Using Ensemble Learning With Depth Sensor,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10901866,"Poor sitting posture can lead to various work-related musculoskeletal disorders (WMSDs). Office employees spend approximately 81.8% of their working time seated, and sedentary behavior can result in chronic diseases such as cervical spondylosis and cardiovascular diseases. To address these health concerns, we present SitPose, a sitting posture and sedentary detection system utilizing the latest Kinect depth camera. The system tracks 3-D coordinates of bone joint points in real-time and calculates the angle values of related joints. We established a dataset containing six different sitting postures and one standing posture, totaling 33409 data points, by recruiting 36 participants. We applied several state-of-the-art machine learning algorithms to the dataset and compared their performance in recognizing the sitting poses. Our results show that the ensemble learning model based on the soft voting mechanism achieves the highest F1 score of 98.1%. Finally, we deployed the SitPose system based on this ensemble model to encourage better sitting posture and to reduce sedentary habits.",Yes,"본 논문은 Kinect 깊이 센서를 이용해 3D 관절 좌표를 추적하고, 머신러닝 알고리즘을 적용하여 앉은 자세를 인식하는 독창적인 시스템(SitPose)을 개발한 연구 내용을 포함하고 있다. 또한, 데이터셋 구축과 성능 평가를 통해 새로운 방법론을 제안하고 실험 결과를 제시하였으므로 연구 논문에 해당한다."
Sentiment Analysis with Deep Learning Methods for Performance Assessment and Comparison,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544219,"The activity of obtaining and evaluating perspectives of individuals, feelings, mindsets of others, views, and so on, toward various things including subjects, goods, and ideas is known as sentiment analysis (SA), often called sentiment mining (SM). People are producing massive quantities of thoughts and feedback regarding goods, offerings, and daily operations as a result of the quick expansion of using online applications like blogs, social media platforms, and web pages. Companies, government agencies, and institutions can collect and evaluate general population attitudes along with opinions using sentiment analysis to acquire business insight and improve decision-making. The paper represents a complete research on sentiment analysis based on DL (deep learning) approaches to give researchers an idea of the evaluation of feelings and associated disciplines. This research represents the previous studies of emotional analysis and illustrates the methodology of our work. The methodology explains data extraction, data preprocessing, text preprocessing, feature extraction, feature selection, and so on. The dataset applied in the study is an IMDb movie reviews dataset containing equal amounts of samples for training and testing. Then, we discussed sentiment analysis techniques which are Simple Neural Networks (SNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN). Using the methods, the outcome states that the simple neural network model generates an accuracy of 74.99% and a Convolutional Neural Network of 85.79%. Besides, the Recurrent Neural Network shows 86.46% which is the highest one. Furthermore, based on the results of the confusion matrix, we investigated the optimum model to attain the highest precision, recall, and F1 score.",Yes,"논문은 IMDb 영화 리뷰 데이터셋을 사용하여 여러 딥러닝 모델(SNN, CNN, RNN)을 적용하고 성능을 평가하는 구체적인 연구 방법론과 실험 결과를 제시하고 있습니다. 이는 기존 연구를 정리하는 것뿐만 아니라 직접적인 실험과 성능 비교를 포함한 독창적인 연구 내용으로 판단됩니다."
Mutual Learning-Based Framework for Enhancing Robustness of Code Models via Adversarial Training,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10764905,"Deep code models (DCMs) have achieved impressive accomplishments and have been widely applied to various code-related tasks. However, existing studies show that some DCMs have poor robustness, and even small noise in the input data can lead to erroneous outputs. This phenomenon can seriously hinder the application of these DCMs in real-world scenarios. To address this limitation, we propose MARVEL, a mutual learning-based framework for enhancing the robustness of DCMs via adversarial training. Specifically, MARVEL initializes two identical DCMs, one of which receives Gaussian-distorted data and performs adversarial training, and the other receives the clean data. Then these two DCMs work together to not only fit the true labels but also fit each other’s internal parameters. Our intuition is that the DCM can enhance robustness by training noisy data, while the DCM achieves accurate prediction performance by learn the clean data. Their mutual learning enables the DCM to balance both robustness and predictive performance.We selected three popular DCMs, five open-source datasets, and three state-of-the-art attack methods to evaluate the performance of MARVEL on 45 (3×5×3) downstream tasks composed of their combinations. Additionally, we set two of the state-of-the-art robustness enhancement techniques as baselines. The experimental results show that MARVEL significantly enhances the robustness of DCMs across all 45 tasks. In 43 out of 45 tasks, MARVEL outperforms the two baselines with an average improvement of 15.33% and 31.88%, respectively. At the same time, MARVEL can maintain the inherent accuracy with an error margin within +-2.43% compared to the original DCMs.CCS CONCEPTS• Software and its engineering → Search-based software engineering; • Computing methodologies → Artificial intelligence.",Yes,"본 논문은 기존 딥 코드 모델의 취약점을 해결하기 위해 상호 학습 기반의 새로운 적대적 학습 프레임워크(MARVEL)를 제안하고, 이를 다양한 데이터셋과 공격 방법에 적용하여 성능을 평가한 실험 결과를 포함하고 있다. 이는 독창적인 연구 방법론과 실험적 기여를 포함한 연구 논문에 해당한다."
Brain-Computer Interface Controlled Functional Electrical Stimulation: Evaluation With Healthy Subjects and Spinal Cord Injury Patients,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764729,"This work presents the design, implementation, and feasibility evaluation of a Motor Imagery (MI) based Brain-Computer Interface (BCI) developed to control a Functional Electrical Stimulation (FES) device. The aim of this system is to assist the upper limb motor recovery of patients with spinal cord injury (SCI). With this BCI-controlled FES system, the user performs open and close MI with either the left or right hand, which if detected is used to provide visual feedback and electroestimulation to muscles in the forearm to perform the corresponding grasping movement. The system was evaluated with seven healthy subjects (HS group) and two SCI patients (SC group) in several experimental sessions across different days. Each experimental session consisted of a training routine devoted to collect calibration EEG data to train the BCI machine learning model, and of a validation routine devoted to validate system in online operation. The online system validation showed an accuracy of the recognition of the MI task that ranged between 78% and 81% for HS participants and between 63% and 93% for SCI participants. Additionally, the time taken by the BCI system to trigger the FES device ranged between 7.05 and 7.29 s for HS participants and between 8.43 s and 13.91 s for SCI participants. Finally, significant negative correlations were observed ( r=−0.418 , p=0.024 and r=−0.437 , p=0.018 for left and right hand MI conditions, respectively) between the online BCI performance with a quantitative EEG parameter based on event-related desynchronization/synchronization analysis. The results of this work indicate the feasibility of the proposed BCI coupled to a FES device to be used for SCI patients with a moderate level of disability and provides evidence of the functionality of the proposed BCI system in a motor rehabilitation context.",Yes,"논문은 BCI 기반 FES 시스템의 설계, 구현 및 평가를 포함하여 직접적인 실험과 데이터 분석을 수행한 독창적인 연구 내용을 담고 있습니다. 또한, 건강한 피험자와 척수 손상 환자를 대상으로 한 실험 결과를 제시하며 시스템의 유효성을 검증하고 있어 연구 논문에 해당합니다."
Train a Robot to Climb Staircase Using Vision-Base System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930098,"Currently, robots are used for different types of work, such as the manufacturing industry, healthcare, and the hotel industry. According to the current epidemic situation, the usage of robots was increased because of the need to reduce human interaction. As a result, they have to walk around the workplace, because of that, they may have to climb staircases. The world has many types of robots. Here the selected robot is a humanoid. This proposal is concerned with how to detect the staircase, count steps, get dimensions of it, and move the robot on it by keeping body balance. First, want to know what the objects are, then walk. The identified images from the image sensors will get as input. The technology stack that is used for image analysis is a method related to computer vision in deep learning. Other than that, while climbing the stairs robot needs to identify whether the staircase is over or not. Here we introduce a new concept: get the number of steps required to climb before climbing the stairs. It is related to how humans identify things by seeing and making decisions. The need to take the dimensions of the stairs is that when considering the stairs, they have their height, width, and range. Therefore, it is imperative to calculate the dimensions of each staircase separately. Creating a balance system similar to human balance is a great advantage in robotics. To implement such a system, there is an upright pose controller to allow the robot to walk stably by preventing tilting of the robot during walking on uneven floor. In general, for us to do that we need to calculate the global inclination of the floor is a key factor. It can be measured with a 2-axis accelerator meter, and it is installed in the inertial sensor.",Yes,"논문 초록에서 로봇이 계단을 인식하고, 계단의 치수를 측정하며, 균형을 유지하면서 계단을 오르는 새로운 시스템을 제안하고 구현하는 구체적인 연구 내용이 포함되어 있습니다. 이는 독창적인 연구 기여를 포함한 연구 논문으로 판단됩니다."
Energy Consumption Prediction using Data Stream Learning for Commercial Buildings,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145123,"Smart cities should be built on harmonious relationships with the environment, with rational measures to use environmental resources in many ways. Resources essential to human life such as water and energy must have special attention. With the advance of technologies use, human being becomes one of the main promoters of information and solutions for smart cities. Stream machine learning is an approach where data becomes available sequentially, called data stream, which adapts the prediction model with fixed window-sized data stream. The objective of this work is to analyze the effectiveness of buildings energy consumption using stream learning. To achieve this goal, MOA11Massive Online Analysis tool was used to evaluate the performance of the selected stream learning algorithms on an an open database. Batch algorithms were also used as testbed, to analyze the performance of stream algorithms related to them. Our work shows that stream learning can show good results in this scenario.",Yes,본 논문은 상업용 건물의 에너지 소비 예측을 위해 스트림 머신러닝 알고리즘을 적용하고 그 성능을 평가하는 독창적인 연구를 수행하고 있다. 데이터 스트림 학습 기법을 사용하여 실제 데이터베이스를 대상으로 실험을 진행한 점에서 직접적인 연구 기여가 포함된 논문으로 판단된다.
A Broad Survey on Detection of Depression in Societal Platforms using Machine Learning Model for the Public Health Care System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193515,"Anxiety and depression are on the rise, particularly since the COVID-19 epidemic, yet detection rates have not kept pace. There has been a lot of talk about people showing signs of mental health problems on social media sites like Facebook, Twitter etc. The social media anxiety and sadness detected using machine learning algorithms is considered and reviewed in this research. Soon after depression was recognized as a major public health problem around the world, efforts were made to improve its detection. The speed with which technology is developing is changing the way people talk to one another. Standardized scales that rely on patients’ subjective reactions or clinical diagnoses from attending clinicians are typically used to detect depression, despite their limitations. First, the replies patients give on conventional standardized measures may be influenced by factors such as the patient’s current mental state, the nature of the clinician-patient relationship, the patient’s current mood, and the patient’s previous experiences and memory bias. Social media platforms like Twitter, Facebook, Telegram, and Instagram have exploded in popularity as places for people to open up about their innermost thoughts, psyche, and feelings with the proliferation of the Internet. Text is analyzed using psychological analysis to pull out relevant aspects, characteristics, and information from the perspectives of users. Psychological analysts rely on social media for the early identification of depressive symptoms and patterns of behavior. A person’s social network may tell us a lot about the thoughts and actions that precede the start of depression, such as the person’s isolation, the importance they place on themselves, and the hours they spend awake. This research presents a brief review that attempts to synthesize the literature on the use of Machine Learning (ML) techniques on social media text data for the purpose of detecting depressive symptoms and to point the way toward f...",No,"초록에서 본 논문은 기존 연구들을 종합하여 머신러닝을 이용한 우울증 탐지에 관한 문헌 리뷰를 수행한 것으로 보이며, 직접적인 독창적 연구 결과나 실험 데이터를 제시하지 않는다. 따라서 새로운 연구 기여보다는 기존 연구의 개관 및 정리에 초점이 맞춰져 있다."
Improve Toulmin's Argumentation Model Using Naïve Bayes Theory in Medical Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107947,"The field of argument in artificial intelligence significantly increased the results in the practical working group. The problem of inconsistency in treatments is one of the most important challenges in the field of medicine in the world. This paper is used to improve the argument of Toulmin's model when dealing with conflicting problems in medicine using naive Bayes' theory as qualifier in toulmin's model. inference rules were used to relate the patient's symptoms to the use of therapies, which eventually led to the diagnosis of therapies, where many features of each drug component were used to compete between supporting the drug and attacking it and then making the decision using the naïve Bayes technique about whether or not to give the drug based on the features of treatment and the features of the patient. The dataset collected from the Iraqi educational hospitals and it annotated by a team of experts working in the medical field. For testing proposed system used 200 samples for two diseases (100 sample for hypertension disease and 100 samples for angina pectoris disease). The accuracy achieved in angina pectoris and hypertension diseases were 95% and 94%, respectively, using the confusion matrix method to evaluate the results of the proposed model.",Yes,"논문은 Toulmin의 논증 모델을 나이브 베이즈 이론과 결합하여 의료 분야의 치료 불일치 문제를 해결하는 새로운 방법을 제안하고 있으며, 실제 의료 데이터셋을 사용해 모델을 평가한 실험 결과를 포함하고 있다. 이는 독창적인 연구 내용과 실험적 기여가 포함된 연구 논문임을 보여준다."
Optimizing Ocular Disease Detection: A Deep Learning Approach with VGG19 and InceptionV3,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10808478,"Early detection and treatment are especially important as global eye diseases including diabetic retinopathy, glaucoma, age-related macular degeneration, and cataracts create major public health issues. Ophthalmology has changed as recent advancements in Deep Learning (DL) and medical imaging allow accurate and fast ailment diagnosis. This work investigates fundus images-based automated categorization of ocular diseases using VGG19 architecture and InceptionV3. Combining two DL approaches InceptionV3 transfer learning model, well-known for their performance in image recognition applications, and VGG19, a deep neural network noted for depth and accuracy in visual classification—the approach followed InceptionV3 showed an 89.44% accuracy after being trained and evaluated using a 5,000 patient record Ocular Disease intelligence recognition (ODIR) dataset including color fundus images and diagnostic keywords; VGG19 shown a higher accuracy of 94.95%, so implying its improved performance in disease categorization. VGG19 proves to outperform InceptionV3 in accuracy, and loss by being able to generalize over numerous patient demographics and imaging modalities. Most importantly for early intervention and improved patient outcomes in clinical settings, our comparison analysis underlines VGG19's accuracy and resilience in automated illness diagnosis.",Yes,"논문은 VGG19와 InceptionV3 딥러닝 모델을 활용하여 안과 질환 자동 분류를 수행하고, 두 모델의 성능을 비교 분석하는 독창적인 연구 내용을 포함하고 있다. 또한, 실제 환자 데이터셋을 사용하여 모델을 학습 및 평가한 결과를 제시하고 있어 연구 논문에 해당한다."
Cardiac MR Images Segmentation For Identification Of Cardiac Diseases Using Fuzzy Based Approach,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214080,"The proposed research work presents image segmentation of cardiac MRI Images of ventricle segmentation. Most important task in image analysis is Segmentation of Images. Data Mining and Machine Learning approaches are now a day very much used for Left Ventricle Segmentation on Cardiac System which efficiently uses different kind of algorithms. There are various measuring tools to evaluate the chest pain, cardiac function, neurologic deficits, by Cardiac System. Today the cardiac related diseases are increasing too much in our society. So, earlier identification of disease is crucial for urgent treatment of cardiac diseases. Doctors are providing their suggestion on the basis of manual inspection with MRI and CT scans. Here, the task is based on technical work with morphological, threshold based segmentation and fuzzy based edge detection approach is applied for better classification of diseases. The task is used to classify cardiac arrhythmia cases, abnormal cardiac cases, left ventricular cases problems. So specifically medical based image segmentation consists higher impotency which betterment the organs localizations for betterment of the quality of the diagnosis and crucial works and become crucial stages for evaluation of functionality of heart failure with pre existing approaches. Here, the the performance produces by this method is more than 90% of accuracy for detection and classification of cardiac diseases. Some of the statistical parameter are basically MSE, PSNR, MAE etc are significantly performs better for the fuzzy based approach with better quality performance of MR images.",Yes,"논문 초록에서 제안된 연구는 심장 MRI 이미지 분할을 위한 퍼지 기반 접근법을 사용하여 직접적인 알고리즘 개발과 성능 평가를 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한, 정확도 및 통계적 지표를 통해 방법의 유효성을 검증하는 실험적 결과도 제시하고 있다."
A Privacy-Preserving-Based Secure Framework Using Blockchain-Enabled Deep-Learning in Cooperative Intelligent Transport System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9505613,"Cooperative Intelligent Transport System (C-ITS) is a promising technology that aims to improve the traditional transport management systems. In C-ITS infrastructure Autonomous Vehicles (AVs) communicate wirelessly with other AVs, Road Side Units (RSUs) and Traffic Command Centres (TCCs) using an open channel Internet. However, the use of the Internet brings inherent vulnerabilities related to privacy (e.g., adversary performing inference and data poisoning attacks), and security (e.g., AVs can be compromised using advanced hacking techniques) issues and prevents the faster realization of C-ITS applications. To address these challenges, this paper presents a privacy-preserving-based secure framework to provide both privacy and security in C-ITS infrastructure. The proposed framework provides two level of security and privacy using blockchain and deep learning modules. First, a blockchain module is designed to securely transmit the C-ITS data between AVs–RSUs-TCCs, and a smart contract-based enhanced Proof of Work (ePoW) technique is designed to verify data integrity and mitigate data poisoning attacks. Second, a deep-learning module is designed that includes Long-Short Term Memory-AutoEncoder (LSTM-AE) technique for encoding C-ITS data into a new format to prevent inference attacks. The encoded data is used by the proposed Attention-based Recurrent Neural Network (A-RNN), for intrusive events recognition in C-ITS infrastructure. The proposed A-RNN is trained using Truncated Backpropagation Through Time (BPTT) algorithm. The framework is further validated and tested using two publicly available ToN-IoT and CICIDS-2017 datasets. The proposed framework is compared with peer privacy-preserving intrusion detection techniques, and the result shows the effectiveness of the proposed framework over several state-of-the-art techniques in both blockchain and non-blockchain systems.",Yes,"본 논문은 C-ITS 인프라에서 프라이버시와 보안을 제공하기 위한 블록체인과 딥러닝 기반의 새로운 보안 프레임워크를 제안하고, 이를 검증하기 위해 공개 데이터셋을 사용한 실험 결과를 제시하고 있다. 이는 독창적인 연구 내용과 실험적 기여를 포함하는 연구 논문에 해당한다."
Deep Ensemble-Based Classifier for Transfer Learning in Rotating Machinery Fault Diagnosis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730901,"Nowadays, intelligent models can correctly detect faults by analysing signals from rotating machinery. However, most of the studies are run in controlled environments and the performance in industrial real-world environments is not yet fully validated. Hence, a suitable tool to implement fault diagnosers is transfer learning, this topic is under development and challenges persist. This paper proposes a framework for creating accurate 1D-CNN based fault classifiers that can be transferred between different rotating machines and working conditions. Multiple Bayesian processes select architecture parameters and hyperparameters, which minimize a loss function related to their transferability to other machines and to the same machine under different operating conditions (such as load and engine speed). The resulting model is fitted to heterogeneous fault diagnosis data resulting in a 1D-CNN ensemble that improves the performance of the unitary model. Subsequently, the transfer learning capability of the ensemble is analyzed on two source data sets using function and parameter based transfer. The results are compared with classical fault diagnosis classifiers. Finally, additional transfer operations on five target domain datasets validate our framework on limited labeled samples and allow interpretation of the ensemble results. The ultimate goal is to find an ensemble that can generalize fault diagnosis on rotating machinery for easy implementation and update in industrial settings.",Yes,"본 논문은 1D-CNN 기반의 새로운 앙상블 분류기 프레임워크를 제안하고, 베이지안 프로세스를 활용한 하이퍼파라미터 최적화 및 전이 학습 성능 평가를 포함하여 직접적인 연구 기여를 하고 있다. 또한 다양한 데이터셋에 대한 실험과 비교 분석을 통해 독창적인 연구 내용을 담고 있으므로 연구 논문에 해당한다."
Web-derived Emotional Word Detection in social media using Latent Semantic information,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004881,"Public sentiment permeated through social media is usually regarded as an important measure for public opinion monitoring, policy making, and so forth. However, the deluge of user-generated content in web, especially in social platform, causes great challenge to public sentiment analysis tasks. Therefore, Web-derived Emotional Word Detection (WEWD) is proposed as a fundamental tool aims to alleviate this problem. Most previous works on WEWD focus on rules, syntax, and sentence structures, a few utilize semantic information which has the potential to further increase the accuracy and efficiency of WEWD. In this paper, we propose a Global-Local Latent Semantic (GLLS) framework for WEWD to make a full use of latent semantic information with the help of multiple sense word embedding technology. We devise two computational WEWD models, called Ensemble GLLS (EGLLS) and Deep GLLS (DGLLS). EGLLS exploits an ensemble learning way to fuse the global and local latent semantics while DGLLS takes advantage of deep neural network. We also design an old-new corpus enrich technique to help increase the effectiveness of the overall training and detecting process. To the best of our knowledge, this is the first work which applies multiple sense word embedding and deep neural network in WEWD related tasks. Experiments on real datasets demonstrate the effectiveness of the proposed idea and methods.",Yes,"본 논문은 Web-derived Emotional Word Detection(WEWD) 문제에 대해 새로운 Global-Local Latent Semantic(GLLS) 프레임워크와 두 가지 모델(EGLLS, DGLLS)을 제안하며, 다중 의미 단어 임베딩과 딥 뉴럴 네트워크를 활용한 독창적인 연구 내용을 포함하고 있다. 또한 실제 데이터셋을 이용한 실험을 통해 제안 방법의 효과를 검증하고 있어 연구 논문에 해당한다."
Seeking Multiple Solutions: An Updated Survey on Niching Methods and Their Applications,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782373,"Multimodal optimization (MMO) aiming to locate multiple optimal (or near-optimal) solutions in a single simulation run has practical relevance to problem solving across many fields. Population-based meta-heuristics have been shown particularly effective in solving MMO problems, if equipped with specifically-designed diversity-preserving mechanisms, commonly known as niching methods. This paper provides an updated survey on niching methods. This paper first revisits the fundamental concepts about niching and its most representative schemes, then reviews the most recent development of niching methods, including novel and hybrid methods, performance measures, and benchmarks for their assessment. Furthermore, this paper surveys previous attempts at leveraging the capabilities of niching to facilitate various optimization tasks (e.g., multiobjective and dynamic optimization) and machine learning tasks (e.g., clustering, feature selection, and learning ensembles). A list of successful applications of niching methods to real-world problems is presented to demonstrate the capabilities of niching methods in providing solutions that are difficult for other optimization methods to offer. The significant practical value of niching methods is clearly exemplified through these applications. Finally, this paper poses challenges and research questions on niching that are yet to be appropriately addressed. Providing answers to these questions is crucial before we can bring more fruitful benefits of niching to real-world problem solving.",No,"주어진 논문은 최신 연구 동향과 기존 연구들을 종합하여 정리한 서베이 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문이라기보다는 기존 연구를 요약하고 분석하는 리뷰 논문에 해당합니다."
Channel Charting: Locating Users Within the Radio Environment Using Channel State Information,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444621,"We propose channel charting (CC), a novel framework in which a multi-antenna network element learns a chart of the radio geometry in its surrounding area. The channel chart captures the local spatial geometry of the area so that points that are close in space will also be close in the channel chart and vice versa. CC works in a fully unsupervised manner, i.e., learning is only based on channel state information (CSI) that is passively collected at a single point in space, but from multiple transmit locations in the area over time. The method then extracts channel features that characterize large-scale fading properties of the wireless channel. Finally, the channel charts are generated with tools from dimensionality reduction, manifold learning, and deep neural networks. The network element performing CC may be, for example, a multi-antenna base-station in a cellular system and the charted area in the served cell. Logical relationships related to the position and movement of a transmitter, e.g., a user equipment (UE), in the cell, can then be directly deduced from comparing measured radio channel characteristics to the channel chart. The unsupervised nature of CC enables a range of new applications in UE localization, network planning, user scheduling, multipoint connectivity, hand-over, cell search, user grouping, and other cognitive tasks that rely on CSI and UE movement relative to the base station, without the need of information from global navigation satellite systems.",Yes,"논문은 채널 상태 정보(CSI)를 활용하여 무선 환경 내 사용자 위치를 추정하는 새로운 프레임워크인 채널 차팅(Channel Charting)을 제안하며, 이는 독창적인 연구 방법과 알고리즘(차원 축소, 매니폴드 학습, 딥 뉴럴 네트워크)을 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
An approach using machine learning and public data to detect traffic jams,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9498622,"With the growth of large cities, it becomes necessary to overcome several challenges related to infrastructure. In this context, urban mobility, including transportation systems, is a key topic. It is known that more efficient traffic management makes cities more efficient as a whole, improving the quality of life of their residents and tourists. A common problem that arises concerns traffic jams, encompassing their detection and management. With proper traffic jam detection, it is possible to implement traffic management methods to prevent long-term occurrences or even notify drivers to avoid specific locations. Some traffic jam detection solutions use sensors spread across cities, while others are based on sensors in drivers' smartphones. These approaches have limitations related mainly to the information reliability or to the cost and complexity of implementation. In this work, we propose an approach using Machine Learning techniques applied to public data of transportation systems to detect urban traffic jams. The proposed approach is evaluated using the publicly available GPS data from the buses of the Rio de Janeiro/Brazil transportation system and traffic jam events posted on Twitter by the Rio de Janeiro City Hall.",Yes,"논문은 기계 학습 기법과 공개 데이터를 활용하여 교통 체증을 탐지하는 새로운 접근법을 제안하고 있으며, 이를 리우데자네이루 버스 GPS 데이터와 시청 트위터 데이터를 이용해 평가하고 있다. 이는 독창적인 연구 내용과 실험적 검증을 포함한 연구 논문에 해당한다."
A Framework for IoT Based Appliance Recognition in Smart Homes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551993,"Internet of Things (IoT) technologies will play an important role in enabling the smart grid achieving its goals in monitoring, protecting, and controlling by incorporating sensors, actuators, and metering devices while supporting various network functions and system automation. In this regard, home energy management systems (HEMS) enable customers efficiently use energy by managing their consumption, providing feedback information and improving control of major appliances. This work proposes a novel framework for IoT based appliance recognition in smart homes. It consists of two parts: training framework and inference framework. The proposed framework allows incorporating different loads in the monitoring system and enables selecting and testing specific parameters related to dataset configuration, feature extraction, and classifier model setting. The work contributes by developing an easy-to-use tool that allows customization of the training/prediction parameters according to the user criterion. Once the data and all its parameters are loaded, a novel feature extraction algorithm is used to obtain a total of ten statistical features. For the classification task, three machine learning models are included: a feed-forward neural network, a long short-term memory and a support vector machine. In addition, the user can apply a set of techniques to handle imbalanced classes, and also measure the influence of the selected features in the classifiers’ prediction by performing a feature importance analysis.",Yes,"논문은 IoT 기반 가전제품 인식 프레임워크를 제안하며, 새로운 특징 추출 알고리즘과 여러 머신러닝 모델을 적용하는 독창적인 연구 내용을 포함하고 있다. 또한, 사용자 맞춤형 툴 개발과 불균형 클래스 처리 기법 등 구체적인 연구 기여가 명확히 드러난다."
Bridging the Gap Between Voltage Over-Scaling and Joint Hardware Accelerator-Algorithm Closed-Loop,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9353544,"Voltage over-scaling (VOS) optimizes energy while causing timing errors due to an unsustainable clock frequency. Many algorithms, such as in multimedia and machine learning applications, are capable of tolerating such errors. VOS has never been investigated in hardware accelerators running closed-loop algorithms. As the errors impact most decisions and actions in the subsequent steps, closed-loops dynamically change the execution flow. Timing errors should be evaluated by an accurate gate-level simulation, but a large gap still remains: how these timing errors propagate from the underlying hardware all the way up to the entire algorithm run, where they just may degrade the performance and quality of service of the application at stake? This paper tackles this issue showing a framework for VOS investigation, embracing any kind of application. Our framework simulates the VOS-induced timing errors at gate-level, dynamically linking the hardware result with the algorithm and vice versa during the evolution of the runtime of the application. The state-of-the-art VOS literature for video encoding application fails to assess the ultimate impacts of VOS-induced timing errors, as current works open the encoding loops. Unlike those, our work investigates the ultimate impact of a hardware accelerator dynamically carrying through to the video encoder all VOS-induced timing errors and preserving the full compliance to the standard. We employ a parallel sum of absolute differences (SAD) hardware accelerator as a case study. We assess the performance of the overall encoder under varying timing guardbands. Next, it is demonstrated that, under VOS, the ultimate impact in compression efficiency is related to the video’s motion intensity. Additionally, the advantages of timing guardband controlled reduction are clearly quantified in our results by virtue of the framework. Reducing at maximum 9.5% the clock frequency, energy savings (up to 16.5% in energy/operation) are achieved in SAD for video compression.",Yes,"본 논문은 전압 과소 스케일링(VOS)에 따른 타이밍 오류가 하드웨어 가속기와 알고리즘의 폐쇄 루프에 미치는 영향을 분석하는 새로운 프레임워크를 제안하고, 이를 실제 비디오 인코딩 가속기 사례에 적용하여 성능과 에너지 효율을 평가하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
A measurement method for intrusion detection in cyber IoT data stealing attacks,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10175888,"The low complexity levels of IoT devices increase vulnerability and expose low-cost devices to possible cyber at-tacks, especially voted to data breaches. The adoption of machine learning algorithms to overcome such an issue onboard could result in an extensive use of their hardware capabilities, possibly limiting their primary goal in the network they are involved. To this aim, the paper proposes a gateway-like detection system, based on the usage of lightweight and open-source measurement software and a very straightforward rule-based detector, to implement on very low-cost devices to prevent intrusion for data stealing purposes. The advantage of the proposed solution is its applicability to different data streams, from interactive to asyn-chronous traffic typologies, and the simplicity of the detection mechanism: no specific hardware-related compatibility issues, very low memory footprint, and CPU usage burden, allowing the procedure to safely work in background mode and no need to decrypt data content, warranting privacy to the user. The adopted traffic measurement software is CICFlowMeter and the rule-based detector is implemented in Python language. Obtained performance highlights a 98.1% accuracy and 96.4% sensitivity in test conditions, keeping the running time significantly lower than most common machine learning techniques. To quantify the impact on the execution time, several experiments were carried out on a very popular processing system (i.e. Raspberry PI), and in some cases, one order of magnitude has been gained concerning machine learning techniques.",Yes,"논문은 IoT 데이터 도난 공격에 대한 침입 탐지 방법을 제안하며, 경량화된 측정 소프트웨어와 규칙 기반 탐지기를 사용한 구체적인 시스템 구현과 성능 평가를 포함하고 있다. 이는 독창적인 연구 내용과 실험 결과를 포함한 연구 논문으로 판단된다."
Dual-Mode Multispectral Imaging System for Food and Agricultural Product Quality Estimation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444559,"Multispectral imaging (MSI) coupled with artificial intelligence (AI), machine-learning (ML), and signal-processing techniques work as a feasible alternative for laboratory testing, especially in food quality control. Most of the recent related research has been focused on reflectance MSI but a system with both reflectance and transmittance capabilities would be ideal for a wide array of specimen types including solid and liquid samples. In this article, a device that includes a dedicated reflectance mode and a dedicated transmittance mode is proposed. Dual-mode operation where fast switching between two modes is facilitated. An innovative merged mode is introduced in which both reflectance and transmittance information of a specimen are combined to form a higher-dimensional dataset with more features. Spatial and temporal variations of measurements are analyzed to ensure the quality of measurements. The concept is validated using a standard color palette, and specific case studies are done for standard food samples such as turmeric powder and coconut oil proving the validity of proposed contributions. The classification accuracy of standard color palette testing was over 90% and the accuracy of coconut oil adulteration was over 95%. while the merged mode was able to provide the best accuracy of 99% for the turmeric adulteration. A linear functional mapping was done for coconut oil adulteration with an R2 value of 0.9558.",Yes,"논문은 반사 및 투과 모드를 결합한 이중 모드 다중분광 이미징 시스템을 제안하고, 이를 통해 식품 샘플의 품질 평가를 수행하는 독창적인 장치와 방법론을 개발하였다. 또한, 실험적 검증과 정확도 평가를 포함하여 직접적인 연구 기여를 명확히 제시하고 있다."
Machine Learning Approaches and Analysis for Bangla Music Genre Classification,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10464795,"The challenge of classifying the genres of Bangla music has been given a machine learning based technique in this study. This paper has explored the use of feature extraction techniques for selecting highly co-related features and evaluated the performance of gradient boosting classifiers, CatBoost, and XGBoost, on a comprehensive Bangla music dataset. Our paper's proposed method achieves remarkable accuracy scores of 74% using CatBoost and XGBoost. This study emphasizes how machine learning approaches have enormous promise to address the difficult job of classifying genres in Bangla music. By using advanced feature extraction techniques and powerful gradient boosting classifiers,the proposed method were able to achieve promising results in this challenging task. The implications of the work can be extended to the development of intelligent music rec-ommendation systems and automated music tagging, facilitating enhanced user experiences in exploring and discovering Bangla music genres. As the field of machine learning continues to evolve, this research contributes valuable insights and avenues for future advancements in music genre classification and related domains.",Yes,"본 논문은 방글라 음악 장르 분류를 위한 기계 학습 기법을 제안하고, 특징 추출과 여러 분류기 성능 평가를 통해 74%의 정확도를 달성하는 독창적인 연구 내용을 포함하고 있다. 이는 기존 연구와 차별화된 실험적 기여를 포함한 연구 논문으로 판단된다."
Color-aware Exposure Correction for Endoscopic Imaging using a Lightweight Vision Transformer,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10600785,"Endoscopy is a widely used imaging technique for diagnosing diseases in hollow organs. However, endoscopic images often suffer from limited visibility and can be affected by many imaging artifacts, such as those related to under or overexposure, which can hamper the performance of AI-based diagnostic tools. Addressing this issue is challenging; thus, most previous work has focused on enhancing underexposed images. In this contribution, we propose an extension to the objective function and deep neural network layers of the IAT Vision Transformer model (Illumination Adaptive Transformer), designed initially for enhancing lowlight or ill-exposed images from natural scenes. Our approach specifically targets exposure correction in endoscopic imaging to preserve color and fine-scale details. First, an extra color normalization layer inside the IAT model has been integrated into the model, and secondly, the objective function has been extended with a Laplacian Pyramid Loss to evaluate different image patches, whereas a histogram-aware loss (HistoLoss) has been used to preserve the color quality of the enhanced endoscopic image, both of these combined allow for the output images not only to improve quantitatively but also qualitative wise. We evaluate our method on the Endo4IE dataset and demonstrate significant improvements over a previous method (Endo-LMSPEC) tailored specifically to endoscopic imaging. Compared with the state-of-the-art (Endo-LMSPEC), our approach achieves an SSIM increase of 2.3% and PSNR increase of 0.523 dB for overexposed images, along with an increase of 2.7% and 1.458 dB improvement in PSNR for underexposure, all while employing only ≈ 90k parameters and running at an inference time of ≈ 74 FPS, outperforming existing state-of-the-art methods on the same dataset and objective.",Yes,"본 논문은 기존 IAT Vision Transformer 모델을 확장하여 내시경 영상의 노출 보정을 위한 새로운 네트워크 구조와 목적 함수를 제안하는 독창적인 연구 내용을 포함하고 있습니다. 또한, 제안한 방법을 실제 데이터셋(Endo4IE)에서 평가하여 기존 최첨단 기법 대비 성능 향상을 입증하였으므로 연구 논문에 해당합니다."
Scaling Uncertainty Quantification From Patches to Scenes Through Discontinuity-Aware Stitching,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10486921,"Reconstructing spatially continuous 2-D fields out of their individually derived building blocks typically introduces artifacts that decrease the overall perceptual quality of the field. Machine learning (ML) applications encounter such a challenge when patching a U-net-like architecture output. Numerous techniques have been developed to mitigate this problem. Yet, few are informed scalable solutions. The present work manages the stitching of Unet-inferred images using Bayesian deep learning (BDL) probabilistic output. The ability to preserve a Bayesian prediction’s variance while effectively reducing the artifacts within a patched scene is presented through an example of predicting a field related to atmospheric radiance. In areas of high variance, adjacent patches of inferred atmospheric radiances may significantly vary in magnitude, leading to large undesirable spatial gradients in the combined (patched) product. Multiple weighted aggregation strategies and weighting schema are surveyed to investigate how to efficiently decrease artificial gradients in large images constructed by stitching several small predictions while maintaining naturally occurring gradients expected to appear in the mosaiced image. Structural similarity (SSIM) Index and visual information fidelity (VIF) are used to evaluate the perceptual quality of the resultant images and confirm the successful employment of Bayesian U-nets with well-calibrated uncertainty, yielding geospatial images with fewer artifacts than naive methods. Log-linear pooling (LLP) proved to be the optimal aggregation strategy tested for fusing patch uncertainties by retaining per-pixel Gaussian distributions and scaling uncertainties in a principled manner to maintain calibration across the spatial map.",Yes,"논문은 Bayesian deep learning을 활용한 Unet 출력 이미지의 패치 결합 문제를 해결하는 새로운 방법을 제안하고, 여러 가중치 집계 전략을 비교 분석하는 등 독창적인 연구 내용을 포함하고 있습니다. 또한, 제안된 방법의 성능을 평가하기 위한 실험과 결과 분석이 포함되어 있어 연구 논문에 해당합니다."
Autoencoder-Based Anomaly Detection in Network Traffic,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10720411,"Due to the continuously increasing number of resources and data availability in the cloud, the threats related to the security of computer networks and IT systems are critical. Threat detection systems based on deep neural networks and anomaly detection are trained on data related to normal activity so that the network can recognize unusual patterns and behaviours in the event of an attack or an attempt to infiltrate a given IT infrastructure. This paper presents the results of developing a neural network based on an autoencoder for anomaly detection in network packet data. The network was trained on data from the HIKARI-2021 dataset. The autoencoder aims to learn representations of normal network traffic and associate this type of traffic with a minimal reconstruction error. The obtained results were compared with those achieved by authors of other works. High accuracy and sensitivity were achieved at the cost of rather low precision, resulting in many false-positive results. A simple algorithm based on a single threshold value proved efficient but limited in terms of effectiveness. This problem can be resolved by changing the method of calculating the individual components of the vector, using only a subset of features, and deriving multiple vectors, one for each class separately, which has been described and analyzed in more detail.",Yes,"본 논문은 오토인코더 기반의 신경망을 개발하여 네트워크 트래픽 이상 탐지에 적용한 연구 결과를 제시하고 있으며, HIKARI-2021 데이터셋을 사용해 모델을 학습하고 성능을 평가하는 등 독창적인 실험과 분석을 포함하고 있다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
Data-Driven Estimation of Flowing Bottom-Hole Pressure in Petroleum Wells Using Long Short-Term Memory,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10903375,"Monitoring bottom-hole pressure (BHP) during flowing periods in petroleum wells is critically important for reservoir management, formation evaluation, lift optimization, and flow assurance. The advent of Permanent Downhole Gauges (PDGs) has changed paradigms regarding the availability and accessibility of this information. However, the costs associated with installing or replacing these gauges can sometimes be prohibitive. In this work, we propose a data-driven framework to fill the information gap when PDG data is unavailable. We used surface measurement data and wellhead gauge readings as inputs and train machine learning methods to accomplish this task. We implemented and evaluated several configurations of Long Short-Term Memory (LSTM) and compared their results with non-deep learning methods as Multi-Layer Perceptron (MLP) Neural Networks and Ridge Regression, taking as case study a real dataset from an offshore oilfield in the Brazilian Pre-salt. We show that our framework can provide reliable estimated values of flowing BHP, with error metrics MAPE and SMAPE consistently under 3%. We highlight the following main contributions: (1) the utilization of time-related data; (2) the development of a framework applicable across a wide range of reservoir and flow conditions; and (3) the potential for practical applications, including real-time monitoring.",Yes,"본 논문은 실제 석유정의 흐르는 저부압을 추정하기 위해 LSTM 기반의 데이터 기반 프레임워크를 개발하고, 이를 기존 방법들과 비교 평가하는 독창적인 연구 내용을 포함하고 있다. 또한 실제 현장 데이터를 활용하여 모델을 구현하고 검증한 점에서 연구 논문으로 판단된다."
Deep learning for virtual metrology: Modeling with optical emission spectroscopy data,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8065905,"Virtual Metrology is one of the most prominent Advanced Process Control applications in Semiconductor Manufacturing. The goal of Virtual Metrology is to provide estimations of quantities that are important for production and to assess process quality, but are costly or impossible to be measured. Virtual Metrology solutions are based on Machine Learning approaches. The bottleneck of developing Virtual Metrology solutions is generally the feature extraction phase that can be time-consuming, and can deeply affect the estimation performance. In particular, in presence of data with additional dimensions, such as time, feature extraction is typically performed by means of heuristic approaches that may pick features with poor predictive capabilities. In this work, we propose the usage of modern Deep Learning approaches to bypass manual feature extraction and to provide high-performance automatic Virtual Metrology modules. The proposed methodology is tested on a real industrial dataset related to Etching. The dataset at hand contains Optical Emission Spectroscopy data and it is paradigmatic of the feature extraction problem under examination.",Yes,"논문은 기존의 수동적 특징 추출 방식을 대체하기 위해 딥러닝을 적용한 새로운 방법론을 제안하고, 실제 산업 데이터셋에 대해 이를 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Integrating Computational Thinking Into the Curricula to Bridge the Skill Gap in Engineering Education,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892914,"This work-in-progress research-to-practice paper presents an intervention on integrating computational thinking modules into a software engineering course. The national consensus on the significance of computational thinking has prompted the expansion of related educational initiatives over the past decade. Since the definition of computational thinking by Wing in 2006, this concept has gained significant attention within the educational community. Particularly this surge of interest has led to extensive research into its conceptual foundations and subsequent integration into educational curricula since 2013. National initiatives have since emerged to incorporate computational thinking into the educational system. Furthermore, as artificial intelligence and computing systems become increasingly integrated into daily life, there is a growing demand from industries for a workforce and graduates adept at critical thinking and problem-solving. Aligned with this national movement, our study presents a two-year institutional initiative, aimed at integrating computational thinking into the software engineering program. The software engineering discipline extensively involves design thinking and problem-solving skills. However, we noticed that these higher-level skills are not imparted early in the program to teach students this method of thinking and approaching problems. To bridge this skill gap, we developed a set of computational thinking modules and integrated them into a gateway course in the software engineering program. Over two years, we implemented this intervention in an introductory-level course and evaluated its impact on students' computational thinking skills by analyzing their responses to a standard Computational Thinking Assessment survey. The results showed significant improvement in most components. These early findings underscore the effectiveness of integrating these computational thinking modules into the gateway courses, regardless of the specific course topic. A notable feature of these modules is their adaptability to diverse engineering courses, suggesting broader applicability across disciplines. Moving forward, our research aims to expand the integration of the computational thinking modules into various courses in other institutes across the nation and analyze their impact on student performance.",Yes,"본 논문은 소프트웨어 공학 교육 과정에 계산적 사고 모듈을 개발하고 통합하여 학생들의 기술 향상 효과를 평가하는 직접적인 연구를 수행하고 있다. 또한, 2년간의 개입과 평가 결과를 제시하며 독창적인 연구 기여를 포함하고 있음을 보여준다."
Survey: Handling on Difficulties in Internet of Things (IoT) Applications and Its Challenges,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057505,"Internet of things in broader sense and importance on protocols, technologies and application along related issues, is a collection of application and information mining system to naturally find and assembled data from web archives and administrations which can be in organized, unstructured or semi-organized form. Accuracy and relevance of information extracting from the internet is the most significant issue of concern for the realization of Data. Fast improvement of PC and data innovation a huge measure of tera-byte to peta-bytes information will ceaselessly be produced in huge scale, either being stored in massive storage devices or streaming into and out of the framework as information streams. Information mining, as the conjunction of various entwined disciplines, including insights, machine learning, design acknowledgment, database frameworks, data recovery, World-Wide Web, perception, and numerous application spaces, has gained awesome ground in the previous decade. The overview about IoT technologies, protocols and applications and related issues with comparison of other survey papers. Our main aim to provide a framework to researcher and application developer that how different protocols works, over view of some key issues of IoT and the connection amongst IoT and other embryonic advances including huge information examination and distributed computing.",No,"초록 내용은 IoT 관련 기술, 프로토콜, 응용 및 문제점에 대한 개괄적 조사와 기존 연구들의 비교를 다루고 있어, 독창적인 연구 결과나 실험적 기여보다는 문헌 조사 및 정리 성격이 강합니다. 따라서 직접적인 연구 논문보다는 서베이 논문에 해당합니다."
Forecasting the load flow of Engine Driven Pump based on Light Gradient Boosting Machine Model,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574553,"The flow rate of aircraft hydraulic system is one of the important parameters to measure the performance of aircraft. The stability of flow rate of hydraulic pipeline is related to the safety of hydraulic system and even aircraft flight. Aiming at the problem that it is difficult to measure the load flow of aircraft hydraulic system, a prediction method based on ensemble learning for the load flow of Engine Driven Pump (EDP) is proposed. According to the different working conditions of EDP, the Light Gradient Boosting Machine (LightGBM) model is established. Through Particle Swarm Optimization algorithm optimizes the super parameters of LightGBM algorithm, and realizes the regression prediction mode. Finally, compared with the multiple linear regression, random forest, experimental results indicate that the proposed model obviously can improve the prediction accuracy.",Yes,"본 논문은 엔진 구동 펌프의 부하 유량을 예측하기 위해 LightGBM 모델과 입자 군집 최적화 알고리즘을 적용한 독창적인 예측 방법을 제안하고 있으며, 실험을 통해 기존 방법과 비교한 결과를 제시하고 있다. 이는 직접적인 연구 기여와 새로운 모델 개발을 포함한 연구 논문으로 판단된다."
An Attention Based Approach for Sentiment Analysis of Food Review Dataset,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225637,"Sentiment Analysis is a technique related to text analysis and natural language processing used to detect various types of insights or information from a portion of text. Over the past few years, researchers have done many works regarding this. In Bangladesh, many online services like-e-com become very popular day by day. One of them is online food delivery services. We can order various foods of our choice from online and sometimes people gives reviews based on that food. Those reviews are usually discarded as unstructured data which of them have no work in further. In this piece of research focus primarily on those unstructured data to analyze them in a correct manner to find insight into customers' behavior and their reactions on those online platforms. To do this experiment first we collect data from websites. Later deep learning-based techniques applied here. For baseline structure, we have used both CNN and LSTM models. Then for improving the model accuracy an attention mechanism applied followed by CNN which gives us 98.45% accuracy. We've also evaluated our model performances with some evaluation metrics also. From them, CNN based attention model gives a higher f1-score of 0.93.",Yes,"본 논문은 온라인 음식 리뷰 데이터에 대한 감성 분석을 위해 CNN, LSTM 및 어텐션 메커니즘을 적용한 독창적인 딥러닝 모델을 제안하고 실험을 수행한 연구이다. 따라서 직접 기여하는 연구 내용이 포함된 연구 논문으로 판단된다."
An Image based Air Quality Analysis using Transfer Learning Algorithms,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985588,"Environment is a tough word to describe because, while it has a common definition that refers to the environment, it is also a concept that is related to whatever thing it is surrounded by. According to WHO figures, as a consequence of air pollution 7 million people are being affected and subjected to death. Among all the Environmental pollution, Air pollution is the major Environmental issue which was responsible for an estimated 160000 deaths in the worlds five biggest crises. WHO data shows seven million people were dying due to air pollution every year. Sulphur dioxide, nitrogen oxide, particulate matter (PM10, PM2.5, PM1), carbon monoxide, and other pollutants contribute to air pollution. Particulate matter is evaluated in this paper to evaluate air quality among all of these pollutants. The mechanisms of deep learning and transfer learning have been emphasized. In order for different scenarios and systems to access the efficient methods according to the need, a comparative study was developed between the Transfer learning and Deep learning methods to detect the classes as Unpolluted, moderately polluted, and highly polluted in terms of time, cost, and efficiency. The goal of this work is to summarize the technique of various mechanisms and use the information to determine the appropriate solution for the relevant need.",No,"초록에서 제시된 내용은 기존의 딥러닝과 전이학습 기법을 비교 분석하는 연구로 보이며, 새로운 독창적인 연구 방법이나 실험 결과에 대한 직접적인 기여가 명확하지 않습니다. 따라서 본 논문은 기존 기법들의 요약 및 비교에 초점을 맞춘 리뷰 또는 개념적 연구로 판단됩니다."
Effective Heart Disease Prediction Using Hybridmachine Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9388635,"Machine learning based heart disease prediction is presented in this research work. Heart related diseases increases everyday throughout the world due to stress, high blood pressure, family backgrounds, etc., Early prediction helps to prevent serious issues due to heart diseases. Classification approaches are widely used to analyse medical data to prevent major issues. Real time large medical data are evaluated through data mining applications to control, contradict and manage large data and make use of it to predict the risk factors. This research work is also aims to diagnose heart disease through support vector machine (SVM) and genetic algorithm (GA) as a data analytic approach. Experimental results demonstrate the superior performance of proposed approach in predicting heart disease.",Yes,논문 초록에서 제안된 연구는 심장병 예측을 위해 SVM과 유전 알고리즘을 결합한 하이브리드 머신러닝 기법을 사용하여 실험적 결과를 제시하고 있다. 이는 기존 연구와 차별화된 독창적인 방법론을 적용한 직접적인 연구 기여로 판단된다.
Deep Residual Convolutional and Recurrent Neural Networks for Temperature Estimation in Permanent Magnet Synchronous Motors,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8785109,"Most traction drive applications using permanent magnet synchronous motors (PMSMs) lack accurate temperature monitoring capabilities so that safe operation is ensured through expensive, oversized materials at the cost of its effective utilization. Classic thermal modeling is conducted with e.g. lumped-parameter thermal networks (LPTNs), which help to estimate internal component temperatures rather precisely but also require expertise in choosing model parameters and lack physical interpretability as soon as their degrees of freedom are curtailed in order to meet the real-time requirement. In this work, deep recurrent and convolutional neural networks with residual connections are empirically evaluated for their feasibility on the sequence learning task of predicting latent high-dynamic temperatures inside PMSMs, which, to the authors' best knowledge, has not been elaborated in previous literature. In a highly utilized PMSM for electric vehicle applications, the temperature profile in the stator teeth, winding, and yoke as well as the rotor's permanent magnets are modeled while their ground truth is available as test bench data. A model hyperparameter search is conducted sequentially via Bayesian optimization across different random number generator seeds in order to evaluate model training consistency and to find promising topologies as well as optimization strategies systematically. It has been found that the mean squared error and maximum absolute deviation performances of both, deep recurrent and convolutional neural networks with residual connections, meet those of LPTNs, without requiring domain expertise for their design. Code is available at [1] to assist related work.",Yes,"본 논문은 PMSM 내부 온도 예측을 위해 잔차 연결이 포함된 심층 순환 및 합성곱 신경망을 적용하고, 베이지안 최적화를 통해 모델 구조와 학습 전략을 체계적으로 탐색하는 독창적인 연구를 수행하였다. 기존 문헌에서 다루지 않은 문제에 대해 실험적 평가와 성능 비교를 통해 새로운 기여를 하고 있으므로 연구 논문에 해당한다."
Applying Neural Networks to detect the failures of turbines in thermal power facilities,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373231,"Due to the growing demand on electricity, how to improve the efficiency of equipment has become one of the critical issues in a thermal power plant. Related works reported that efficiency and availability depend heavily on high reliability and maintainability. Recently, the concept of e-maintenance has been introduced to reduce the cost of maintenance. In e-maintenance systems, the intelligent fault detection system plays a crucial role for identifying failures. Machine learning techniques are at the core of such intelligent systems and can greatly influence their performance. Applying these techniques to fault detection makes it possible to shorten shutdown maintenance and thus increase the capacity utilization rates of equipment. Therefore, this work applies Back-propagation Neural Networks (BPN) to analyze the failures of turbines in thermal power facilities. Finally, a real case from a thermal power plant is provided to evaluate the effectiveness.",Yes,"논문 초록에서 Back-propagation Neural Networks를 적용하여 터빈 고장을 분석하는 구체적인 연구 방법과 실제 사례 평가를 제시하고 있어, 독창적인 연구 내용과 직접적인 기여가 포함된 연구 논문으로 판단됩니다."
Whole brain volume and cortical thickness based automatic classification of Wilson’s disease,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8914413,"Wilson's disease (WD) is a progressive autosomal-recessive genetic disorder of copper metabolism that can induce cognitive, physical and psychiatric symptoms. Despite the wide use of machine learning methods in neuroimaging analysis, WD-related research has been very rare. In this work, we proposed and validated an efficient pipeline for an automated WD classification based on whole brain segmentation volumes and cortical thicknesses obtained from T1-weighted magnetic resonance images (MRIs). Three well-known supervised machine learning algorithms, including support vector machine (SVM), linear discriminant analysis (LDA), and logistic regression (LR), were evaluated and compared in the setting of WD classification. A total of 51 images, including 27 acquired from WD patients and 24 from age-matched healthy controls, were used in the validation analysis. Univariate feature selection was conducted to eliminate non-relevant features and retain features contributing to the classification performance. Two nested leave-one-out cross validations were adopted, with the inner folder used for optimal parameter estimation and the outer folder for classification performance evaluation. Experimental results showed that when employing volume features, SVM significantly outperformed both LDA and LR, yielding an overall accuracy of 96.1%, a sensitivity of 92.6% and a specificity of 100%. LR could also reach such best classification performance when using a combination of volumes and thicknesses as input features. This study provides a new non-invasive tool (MRI-based) for an automated detection of WD.",Yes,"본 논문은 윌슨병(Wilson’s disease) 분류를 위한 새로운 자동화된 머신러닝 기반 분석 파이프라인을 제안하고, 실제 환자 및 대조군 데이터를 이용해 성능을 검증하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접 기여하는 연구 논문에 해당한다."
Learning instance greedily cloning naive Bayes for ranking,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1565680,"Naive Bayes (simply NB) (Langley et al., 1992) has been widely used in machine learning and data mining as a simple and effective classification algorithm. Since its conditional independence assumption is rarely true, researchers have made a substantial amount of effort to improve naive Bayes. The related research work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs. Different from eager approach, the key idea for extending naive Bayes from the lazy approach is to learn a naive Bayes for each testing example. In recent years, some lazy extensions of naive Bayes have been proposed. For example, SNNB, LWNB, and LBR. All are aiming at improving the classification accuracy of naive Bayes. In many real-world machine learning and data mining applications, however, an accurate ranking is more desirable than an accurate classification. Responding to this fact, we present a lazy learning algorithm called instance greedily cloning naive Bayes (simply IGCNB) in this paper. Our motivation is to improve naive Bayes' ranking performance measured by AUC (Bradley, 1997; Provost and Fawcett, 1997). We experimentally tested our algorithm, using the whole 36 UCI datasets recommended by Weka, and compared it to C4.4 (Provost and Domingos, 2003), NB (Langley et al., 1992), SNNB (Xie, 2002) and LWNB (Frank, 2003). The experimental results show that our algorithm outperforms all the other algorithms used to compare significantly in yielding accurate ranking.",Yes,"논문 초록에서 제안된 IGCNB 알고리즘은 기존 나이브 베이즈의 순위 성능을 개선하기 위한 새로운 게으른 학습 방법으로, 직접적인 알고리즘 설계와 실험적 검증을 포함하고 있다. 이는 독창적인 연구 내용과 기여를 포함한 연구 논문임을 나타낸다."
Combining Metamorphic Testing and Machine Learning to Enhance OpenStreetMap,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10490297,"Metamorphic testing (MT) is a useful tool to test systems where an oracle is not available. MT relies on the definition of metamorphic relations (MR), that is, certain properties that relate a set of inputs and the set of outputs produced by the system under test (SUT) as response to these inputs. Usually, a violation of an MR implies that the SUT is faulty. However, some work on MT accepts, for certain SUTs, that the violation of an MR almost always is a symptom of an error but assumes the potential existence of false positives. This is the case, for instance, of our recent work where we applied MT to improve OpenStreetMap (OSM). Our MRs were able to uncover a large amount of errors in all the analyzed maps but we suffered the presence of a nonnegligible number of false positives. Therefore, an expert had to manually check the suspicious elements identified by our MRs. If we analyze large maps, then this manual task is unfeasible. In this article we solve the main limitation of our previous approach: we accurately and automatically discard false positives. Our new framework combines MT, along the same lines of our previous work, and machine learning. Specifically, we provide three models, one for each MR, based on the random forest model. The models were extensively trained with real data obtained from the application of MRs to maps of cities located in different continents. In order to evaluate the usefulness of our models, we tested them using different cities, in countries that were not considered in the training set. The results were very good: accuracy of the models is never lower than 0.90, it is usually much higher and in many situations reaches 1.0. The computation of the F1-scores yielded similar results.",Yes,"본 논문은 메타모픽 테스트와 머신러닝을 결합하여 OpenStreetMap의 오류를 자동으로 판별하는 새로운 프레임워크를 제안하고, 실제 데이터를 이용해 모델을 학습 및 평가한 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
Özgeçmişlerde varlık isimlerinin tanınması Named entity recognition in resumes,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296807,"Named entity recognition (NER) is used to extract information from various documents and texts such as names and dates. It is important to extract education and work experience information from resumes in order to filter them. Considering the fact that all information in a resume has to be entered to the company’s system manually, automatizing this process will save time of the companies. In this study, a deep learning-based semi-automatic named entity recognition system has been implemented with a focus on resumes in the field of IT. Firstly, resumes of employees from five different IT related fields have been annotated. Six transformer based pre-trained models have been adapted to named entity recognition problem using the annotated data. These models have been selected among popular models in the natural language processing field. The obtained system can recognize eight different entity types which are city, date, degree, diploma major, job title, language, country and skill. Models used in the experiments are compared using micro, macro and weighted F1 scores and the performance of the methods was evaluated. Taking these scores into account for test set the best micro and weighted F1 score is obtained by RoBERTa and the best macro F1 score is obtained by Electra model.",Yes,"본 논문은 이력서에서 명명된 개체 인식을 위해 딥러닝 기반의 반자동 시스템을 구현하고, 다섯 개 IT 분야 이력서를 주석 처리하여 여섯 개의 사전학습된 트랜스포머 모델을 적용 및 비교하는 독창적인 연구 내용을 포함하고 있다. 따라서 직접적인 연구 기여가 있는 연구 논문으로 판단된다."
"A Comprehensive Survey on Meta-Heuristic Algorithms for Feature Selection in High-Dimensional Data: Challenges, Applications, and Future Directions",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10783538,"Feature selection is crucial for improving machine learning models by reducing dimensionality and lowering computational costs. This survey paper provides an in-depth review on recent advancements in feature selection methods, with a focus on meta-heuristic algorithms. Known for their robustness and effectiveness, these algorithms have become key in tackling the combinatorial challenges in feature selection. The paper categorizes and evaluates various meta-heuristic approaches, including evolutionary algorithms, swarm intelligence, and hybrid techniques, highlighting their strengths, limitations, and applications across different fields. It also examines the integration of meta-heuristics with other optimization methods and machine learning frameworks, identifying current trends and challenges. The paper concludes by discussing future research directions, emphasizing the potential of meta-heuristic-based feature selection in handling high-dimensional data and complex real-world problems.",No,"본 논문은 메타 휴리스틱 알고리즘을 이용한 특징 선택 기법에 대한 종합적인 서베이 논문으로, 기존 연구들을 정리하고 평가하는 데 중점을 두고 있다. 따라서 직접적인 독창적인 연구 결과나 새로운 실험적 기여를 포함하지 않아 연구 논문으로 보기 어렵다."
AcouSense: A Deep Learning Based Real-Time Acoustic Triggering System,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673469,"In this era of technology, people use Headphones continuously during their work hours. To enhance the user experience, most of the headphones are equipped with a noise cancellation feature that completely wipes out any sound in the vicinity. Due to this, many users could not listen to external sounds around their surroundings. Recent data shows that such situations have led to major accidents and loss of life in the recent past. In this paper, we have proposed a novel methodology to tackle the problem in real time. Based on the research, we have collected 20 words that people often say during any emergency situation, known as the Attention words. The proposed methodology cautions the Headphone user whenever an attention word is uttered by people in the surroundings. This approach is also found out to be useful for deaf people who often find themselves in alerting circumstances due to their inability to hear. We have divided the target people into two categories: People using headphones while doing their chores and people with hearing disabilities. A unique solution in accordance with the methodology is proposed for each category of people. Based on the methodology, a prototype deep learning-based Convolutional Neural Network (CNN) is developed. The 10-layer CNN architecture is trained on the dataset of the attention word formed using the Google Speech Command Dataset. The deep learning model is further evaluated on the public dataset to review the accuracy of the model in a noisy environment. The CNN model gave a good accuracy of 91 %. The trained deep learning model is tested using a micro-controller which is further embedded into a headphone. The system activates its vibration module after listening to the attention word.",Yes,"본 논문은 긴급 상황에서 사용자의 주의를 환기시키기 위한 새로운 실시간 음향 트리거링 시스템을 제안하고, 10층 CNN 모델을 설계·학습시켜 정확도를 평가하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 실제 마이크로컨트롤러에 모델을 임베딩하여 프로토타입을 구현한 점에서 연구 논문으로 판단된다."
Towards Personalized Real-time Cardiodynamic Status Monitoring: Multi-scale Modeling of the Heart Sound,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9602890,"Cardiovascular diseases have been known as one of the diseases with an increasing number over the years and great intensity that led to enormous concern being attracted to a non-invasive heart sound detection technology. Knowledge in heart auscultation steadily becomes a pivotal work in this modern technology era. Moreover, in the presence of multiple scales of mathematical analysis allowing the stimulation on how the cardiac function at the whole-organ scope, this causes interest to developed monitoring of the cardio-dynamics activity on both patients and non-patients to measure their health status. In this study, we developed heart sound sensing with a cardio-dynamics recording system as a multi-scale model of the heart role, which allowed us to explore whether it can contribute to a new comprehension on health care of patients in a large population. We are mostly focusing on several aspects such as the concept of heart sounds and the link between cardiovascular dynamic and heart sounds; some technologies related to the processing and analysis of heart sound signals, containing segmentation, denoising, feature extraction also classification; and the deep learning algorithm employment in the heart sound managing and processing. At the beginning, we found that the estimation of the model parameters from heart sound with cardiac function at the whole organ scale and prognosis scores were very consistent. Since the employed multi-scale mathematical system are predictive. So here in this paper, we further illustrate the potential of the system upon the individualized planning of cardiac health and treatment in a case of cardiovascular disease.",Yes,"본 논문은 심장 소리 감지 및 심장 역학 기록 시스템을 다중 스케일 모델로 개발하고, 심장 기능과 심장 소리 간의 연관성을 분석하며, 딥러닝 알고리즘을 적용하는 등 독창적인 연구 내용을 포함하고 있다. 또한, 모델 파라미터 추정과 예후 점수의 일관성을 확인하여 개인 맞춤형 심장 건강 관리에 기여할 가능성을 제시하고 있어 연구 논문에 해당한다."
TASA: Temporal Attention With Spatial Autoencoder Network for Odor-Induced Emotion Classification Using EEG,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528328,"The olfactory system enables humans to smell different odors, which are closely related to emotions. The high temporal resolution and non-invasiveness of Electroencephalogram (EEG) make it suitable to objectively study human preferences for odors. Effectively learning the temporal dynamics and spatial information from EEG is crucial for detecting odor-induced emotional valence. In this paper, we propose a deep learning architecture called Temporal Attention with Spatial Autoencoder Network (TASA) for predicting odor-induced emotions using EEG. TASA consists of a filter-bank layer, a spatial encoder, a time segmentation layer, a Long Short-Term Memory (LSTM) module, a multi-head self-attention (MSA) layer, and a fully connected layer. We improve upon the previous work by utilizing a two-phase learning framework, using the autoencoder module to learn the spatial information among electrodes by reconstructing the given input with a latent representation in the spatial dimension, which aims to minimize information loss compared to spatial filtering with CNN. The second improvement is inspired by the continuous nature of the olfactory process; we propose to use LSTM-MSA in TASA to capture its temporal dynamics by learning the intercorrelation among the time segments of the EEG. TASA is evaluated on an existing olfactory EEG dataset and compared with several existing deep learning architectures to demonstrate its effectiveness in predicting olfactory-triggered emotional responses. Interpretability analyses with DeepLIFT also suggest that TASA learns spatial-spectral features that are relevant to olfactory-induced emotion recognition.",Yes,"본 논문은 EEG 데이터를 이용해 냄새에 의해 유발된 감정을 분류하는 새로운 딥러닝 아키텍처(TASA)를 제안하고 있으며, 기존 연구 대비 공간 정보 학습과 시간적 동적 특성 포착 방법을 개선하는 독창적인 연구 내용을 포함하고 있다. 또한 제안된 모델을 기존 방법들과 비교 평가하여 그 효과를 입증하고 있어 연구 논문에 해당한다."
The Lateral Control of Autonomous Vehicles: A Review,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8984590,"Human need safety, comfort, and speed in driving-requirements that can be fulfiled by autonomous vehicles that enable drivers to avoid obstacles and maintain a safe distance from other motorists. These function are executed through lateral vehicle control, which has been the subject of considerable research. The current research was aimed at providing a comprehensive review and description of previous investigations that implemented both conventional and innovative lateral control methods, such as proportional- integral-derivative control, fuzzy logic, artificial intelligence, neural networks, genetic algorithms, and combined approaches. The evaluated studies were also classified into two categories, namely simulation and experimental research that used real-world tools. The paper concludes with a recomendation to use an alternative method called direct inverse control. Which is a modification of neural network- based control. This method is advantageous because it uses output/input feedback, thereby effectively functioning in unpredicable terrain. This feature is highly suitable because autonomous vehicles are non-linear system.",No,"본 논문은 기존 연구들을 종합적으로 검토하고 분류하는 리뷰 논문으로, 직접적인 독창적 연구 결과나 새로운 실험 데이터를 제시하지 않습니다. 따라서 연구 논문보다는 기존 연구를 요약하고 평가하는 문헌 리뷰에 해당합니다."
Visual Quality Inspection of Pomegranate Crop Using a Novel Dataset and Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854435,"Grading and quality assessment of fruits is an important post-harvest management tool to define the appropriate use of the harvested fruit and reasonable pricing. In this work, the performance of 14 well-known deep learning models is evaluated in a pomegranate quality assessment. The models are trained with a novel image dataset including three quality classes of pomegranate fruit related to their postharvest intended use. Reported results indicated quality classification performances of up to 94.12% for MobileNet model. Experimental results reveal that transfer learning could provide promising results in fruit sorting problems with limited data.",Yes,본 논문은 석류 품질 평가를 위해 14개의 딥러닝 모델을 새로 수집한 이미지 데이터셋으로 학습시키고 성능을 평가하는 실험적 연구를 수행하였다. 이는 독창적인 데이터셋 구축과 모델 성능 검증을 포함한 직접적인 연구 기여로 판단된다.
Advances in High-Resolution Non-Destructive Defect Detection and Localization Enhanced by Intelligent Signal Processing,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10691271,"In development and production of microelectronics products the assessment of the condition of either the full component or only specific parts is of high relevance. To allow for screening and monitoring and to leave the part unaltered the inspection techniques are required to operate non-destructively. While this allows for full-cover final inspection it also enables repetitive monitoring beneficial for the exploration of material interactions, potential subsequent defect formations and consequently for failure isolation. With increasing complexity inherent in advancing microelectronic technologies, high reliability, robustness and thus, successful failure analysis is essential. The correspondingly shrinking feature sizes and the involvement of heterogeneous materials highly challenge existing techniques. Furthermore, the interpretation of the acquired data becomes increasingly difficult requiring specifically skilled operating personal. In previous studies machine learning (ML) approaches have been developed and evaluated for their ability to analyze signals acquired by scanning acoustic microscopy (SAM) with the goal of automated defect detection, characterization, and failure isolation. The present paper investigates different ML architectures to analyze the time signals after transformation into the spectral- and wavelet domains. Results showed that 2D CNNs analyzing the acquired acoustic signals in the wavelet domain representation performed best, however at the expense of additional computational effort. Furthermore, ML-based analysis was explored for its potential to locate and isolate electrically active defects in the depth-dimension based on thermal emissions using lock-in thermography (LIT). Obtained LIT-related results are promising, however require further research to fully enfold its potential. It was further found that transfer properties of the inspection tools interfere with the defect specific signal features and thus so far tie the trained models to the specific equipment used. Future work should therefore focus on removing the specific tool related transfer characteristics of the equipment from the measurement data to allow for intra-tool compatibility and thus a more generalized application. (Abstract)",Yes,"본 논문은 머신러닝 아키텍처를 활용한 신호 분석 기법을 개발하고 평가하는 등 직접적인 연구 방법과 실험 결과를 포함하고 있어 독창적인 연구 내용을 담고 있다. 또한, 기존 기술의 한계를 극복하기 위한 새로운 접근법과 향후 연구 방향을 제시하고 있어 연구 논문에 해당한다."
A Performance Evaluation on Distance Measures in KNN for Mobile Malware Detection,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8820510,"Most of the related works on mobile malware detection for Android Operating System (OS) that are based on machine learning often use classifiers' default settings, and focus on opting either the optimal features or classifier. Even if this approach is understandable and it has proven to provide valuable results classifiers different hyper-parameters should be configured properly in order to achieve classifier's best performance. Thus, this paper investigates the performance of one of the most simple machine learning classifier, such as K Nearest Neighbor (KNN), considering its different hyper-parameters with emphasis on different distance measures. The authors have performed an extensive comparison using various well known distance measures over the Drebin data set. Results show that the proper choice of the distance measure can provide a significant enhancement to the classification accuracy. Specifically, the Euclidean distance that is mostly used for KNN is not the optimal option, instead other distance measures i.e., Hamming, CityBlock, can boost classifier's performance in the context of mobile malware detection. For instance, CityBlock can improve KNN false positive rate up to 33% in comparison to the Euclidean distance.",Yes,"본 논문은 KNN 분류기의 다양한 거리 측정 방법을 비교 평가하여 모바일 악성코드 탐지 성능 향상에 기여하는 독창적인 연구 내용을 포함하고 있다. 즉, 기존 연구와 달리 하이퍼파라미터 조정과 거리 측정 방법의 효과를 실험적으로 분석한 직접적인 연구 결과를 제시하고 있다."
MDLF: A Multi-View-Based Deep Learning Framework for Individual Trip Destination Prediction in Public Transportation Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605211,"Understanding and predicting each individual’s real-time travel destination given the origin information in urban public transportation systems is crucial for personalized traveler recommendation, targeted demand management, dynamic traffic operations and so on. Existing methods are often based on modeling the regular travel patterns through analyzing the long-term personal travel information. They are suitable for destination prediction of individual regular trips with regular travel patterns, but may not work well for occasional trips with strong randomness and uncertainty, especially for the individuals with a few historical travel data. In this paper, we focus on more challenging issue about destination prediction of occasional trips. We design a general Multi-View Deep Learning Framework (MDLF) based on the data-driven insight that a location where a user will destine to is not only related to the user’s own travel preference to the location, but also influenced by crowd’s travel preference and the region’s characteristics of the location under certain spatiotemporal contexts. The destination of an individual’s occasional trip can be predicted by combining all these complementary influencing factors. The novelty of MDLF is mainly reflected in two aspects. The first is the effective feature extraction from multiple and complementary views. The second is that a CNN (Recurrent Neural Network) based deep learning component for predicting each occasional trip’s destination by calculating a moving trend score for each possible destination. We evaluate the MDLF based on two real-world smart card datasets collected by AFC (Automatic Fare Collection) Systems. The experimental results demonstrate the superiority of MDLF against other competitors.",Yes,"본 논문은 기존 연구와 차별화된 다중 관점 기반의 딥러닝 프레임워크(MDLF)를 설계하고, 이를 통해 개인의 우발적 여행 목적지를 예측하는 독창적인 방법론을 제안하고 있습니다. 또한 실제 스마트 카드 데이터를 활용한 실험을 통해 모델의 우수성을 검증하였으므로, 직접 기여하는 연구 논문에 해당합니다."
Speech-based Evaluation of Emotions-Depression Correlation,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927758,"Early detection of depression symptoms is fundamental to limit the onset of further associated behavioural disorders, such as psychomotor or social withdrawal. The combination of Artificial Intelligence and speech analysis revealed the existence of objectively measurable physical manifestations for early detection of depressive symptoms, constituting a valid support to evaluate these signals. To push forward the research state-of-art, this aim of this paper is to understand quantitative correlations between emotional states and depression by proposing a study across different datasets containing speech of both depressed/non-depressed people and emotional-related samples. The relationship between affective measures and depression can, in fact, a support to evaluate the presence of depression state. This work constitutes a preliminary step of a study whose final aim is to pursue AI-powered personalized medicine by building sophisticated Clinical Decision Support Systems for depression, as well as other psychological disorders.",Yes,"본 논문은 음성 기반 감정과 우울증 간의 정량적 상관관계를 분석하는 연구를 수행하며, 다양한 데이터셋을 활용해 우울증 조기 탐지에 기여하는 독창적인 연구 내용을 포함하고 있다. 또한 AI를 활용한 임상 의사결정 지원 시스템 개발을 목표로 하여 직접적인 연구 기여가 명확하다."
Predicting user comfort level using machine learning for Smart Grid environments,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5759178,"Smart Grid with Time-of-Use (TOU) pricing brings new ways of cutting costs for energy consumers and conserving energy. It is done by utilities suggesting the user ways to use devices to lower their energy bills keeping in mind its own benefits in smoothening the peak demand curve. However, as suggested in previous related research, user's comfort need must be addressed in order to make the system work efficiently. In this work, we validate the hypothesis that user preferences and habits can be learned and user comfort level for new patterns of device usage can be predicted. We investigate how machine learning algorithms specifically supervised machine learning algorithms can be used to achieve this. We also compare the prediction accuracies of three commonly used supervised learning algorithms, as well as the effect that the number of training samples has on the prediction accuracy. Further more, we analyse how sensitive prediction accuracies yielded by each algorithm are to the number of training samples.",Yes,"논문 초록에서 사용자의 편안함 수준을 예측하기 위해 기계 학습 알고리즘을 적용하고, 여러 알고리즘의 예측 정확도를 비교하는 등 직접적인 연구 방법과 실험을 수행한 내용이 포함되어 있습니다. 이는 독창적인 연구 기여를 포함한 연구 논문임을 나타냅니다."
Semantic Attention-guided Day-to-Night Image Translation Network,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10422093,"Perception tasks are critical for an autonomous driving system. In recent years, advancements in deep learning have enabled highly accurate perception. However, conquering perception tasks in poor visibility environments remains challenging. One of the main reasons is that most of the existing datasets are concentrated on visually clear environments. This makes it difficult to train deep learning-based perception models in limited visibility environments. Building a new dataset requires significant time and human resources, and annotating data from adverse visibility environments is even more challenging. To address these problems, many image translation methods have been proposed to translate annotated daytime images into nighttime ones to build a nighttime dataset without annotation. In this paper, we present an unsupervised day-to-night image translation network for generating synthetic data. Our proposed method extracts semantic information from input images. The extracted information is then applied to the image-to-image translation network as spatial attention maps. We conduct experiments to evaluate the proposed method. The experimental results show that our method outperforms the related works both qualitative and quantitative.",Yes,"본 논문은 기존 연구와 차별화된 새로운 비지도 학습 기반의 주간-야간 이미지 변환 네트워크를 제안하고, 이를 통해 합성 데이터를 생성하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안한 방법의 성능을 정성적 및 정량적으로 평가한 실험 결과를 제시하여 연구 기여가 명확하다."
DeepAid: A Mobile Cognitive Aid System with Compacted Deep Learning,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560137,"This paper presents DeepAid, a mobile system assisting cognitive decline patients. Existing designs of such systems suffer low accuracies for the recognition and long latencies for the interaction two major issues. DeepAid leverages deep learning to solve both issues at the same time. Deep learning naturally leads to high-accuracy object recognition. If it can further execute on the local device directly, the latency can be satisfied as well since frequent data transmission to the server in prior approaches can be avoided. The major contribution of this paper is to instrument the possibility that deep learning can be compact so as to achieve a good trade-off between accuracy and resource consumption, with the design of task decomposition and network scale minimization. To validate the effectiveness of DeepAid, we implement a series of experiments and compare with other related work. In the final, the DeepAid can achieve about 97% accuracy in object recognition with about 90ms time delay.",Yes,"본 논문은 DeepAid라는 모바일 인지 보조 시스템을 제안하고, 딥러닝 모델의 경량화 및 정확도와 자원 소비 간의 균형을 맞추는 새로운 설계 방식을 제시하고 있습니다. 또한, 실험을 통해 제안한 방법의 효과를 검증하여 독창적인 연구 기여를 포함하고 있음을 보여줍니다."
Network Immunity: What can we learn from nature for network protection?,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4610128,"In this paper we analyze Network Immunity as a bio-inspired approach for detecting anomalies in communication networks. We briefly review the basic methods of Artificial Immune Systems (AIS), identify their strengths and weaknesses, and evaluate their possible applications to intrusion detection in computer networks. After an overview of related work from the area of intrusion detection we collect key challenges anticipated for the realization of Network immunity based on AIS.",No,"초록에서 제시된 내용은 기존의 인공 면역 시스템(AIS) 방법들을 검토하고, 그 강점과 약점을 분석하며, 네트워크 침입 탐지에의 적용 가능성을 평가하는 개요성 연구에 가깝습니다. 독창적인 연구 결과나 실험적 기여가 명확히 드러나지 않아 직접 기여하는 연구 논문으로 보기 어렵습니다."
Bindi: Affective Internet of Things to Combat Gender-Based Violence,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780201,"The main research motivation of this article is the fight against gender-based violence and achieving gender equality from a technological perspective. The solution proposed in this work goes beyond currently existing panic buttons, needing to be manually operated by the victims under difficult circumstances. Instead, Bindi, our end-to-end autonomous multimodal system, relies on artificial intelligence methods to automatically identify violent situations, based on detecting fear-related emotions, and trigger a protection protocol, if necessary. To this end, Bindi integrates modern state-of-the-art technologies, such as the Internet of Bodies, affective computing, and cyber–physical systems, leveraging: 1) affective Internet of Things (IoT) with auditory and physiological commercial off-the-shelf smart sensors embedded in wearable devices; 2) hierarchical multisensorial information fusion; and 3) the edge-fog-cloud IoT architecture. This solution is evaluated using our own data set named WEMAC, a very recently collected and freely available collection of data comprising the auditory and physiological responses of 47 women to several emotions elicited by using a virtual reality environment. On this basis, this work provides an analysis of multimodal late fusion strategies to combine the physiological and speech data processing pipelines to identify the best intelligence engine strategy for Bindi. In particular, the best data fusion strategy reports an overall fear classification accuracy of 63.61% for a subject-independent approach. Both a power consumption study and an audio data processing pipeline to detect violent acoustic events complement this analysis. This research is intended as an initial multimodal baseline that facilitates further work with real-life elicited fear in women.",Yes,"논문은 인공지능과 IoT 기술을 활용하여 성별 기반 폭력 상황을 자동으로 인식하고 대응하는 독창적인 시스템(Bindi)을 제안하고 있으며, 자체 데이터셋(WEMAC)을 수집하여 성능 평가를 수행하는 등 직접적인 연구 기여를 포함하고 있다. 따라서 연구 논문에 해당한다."
Continual Learning for Multivariate Time Series Tasks with Variable Input Dimensions,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9679108,"We consider a sequence of related multivariate time series learning tasks, such as predicting failures for different instances of a machine from time series of multi-sensor data, or activity recognition tasks over different individuals from multiple wearable sensors. We focus on two under-explored practical challenges arising in such settings: (i) Each task may have a different subset of sensors, i.e., providing different partial observations of the underlying ‘system’. This restriction can be due to different manufacturers in the former case, and people wearing more or less measurement devices in the latter (ii) We are not allowed to store or re-access data from a task once it has been observed at the task level. This may be due to privacy considerations in the case of people, or legal restrictions placed by machine owners. Nevertheless, we would like to (a) improve performance on subsequent tasks using experience from completed tasks as well as (b) continue to perform better on past tasks, e.g., update the model and improve predictions on even the first machine after learning from subsequently observed ones. We note that existing continual learning methods do not take into account variability in input dimensions arising due to different subsets of sensors being available across tasks, and struggle to adapt to such variable input dimensions (VID) tasks. In this work, we address this shortcoming of existing methods. To this end, we learn task-specific generative models and classifiers, and use these to augment data for target tasks. Since the input dimensions across tasks vary, we propose a novel conditioning module based on graph neural networks to aid a standard recurrent neural network. We evaluate the efficacy of the proposed approach on three publicly available datasets corresponding to two activity recognition tasks (classification) and one prognostics task (regression). We demonstrate that it is possible to significantly enhance the performance on future and previous tasks while learning continuously from VID tasks without storing data.",Yes,"본 논문은 다변량 시계열 데이터의 연속 학습 문제에서 입력 차원이 가변적인 상황을 다루며, 이를 해결하기 위한 새로운 그래프 신경망 기반 조건화 모듈과 생성 모델을 제안하는 독창적인 연구 내용을 포함하고 있다. 또한, 제안 방법의 성능을 공개 데이터셋에서 평가하여 실험적 기여도 명확히 제시하고 있다."
