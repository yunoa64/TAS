Title,URL/DOI,Abstract
Supporting systematic literature reviews using deep-learning-based language models,https://doi.org/10.1145/3528588.3528658,"Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently."
A Reproducibility and Generalizability Study of Large Language Models for Query Generation,https://doi.org/10.1145/3673791.3698432,"Systematic literature reviews (SLRs) are a cornerstone of academic research, yet they are often labour-intensive and time-consuming due to the detailed literature curation process. The advent of generative AI and large language models (LLMs) promises to revolutionize this process by assisting researchers in several tedious tasks, one of them being the generation of effective Boolean queries that will select the publications to consider including in a review. This paper presents an extensive study of Boolean query generation using LLMs for systematic reviews, reproducing and extending the work of Wang et al. and Alaniz et al. Our study investigates the replicability and reliability of results achieved using ChatGPT and compares its performance with open-source alternatives like Mistral and Zephyr to provide a more comprehensive analysis of LLMs for query generation. Therefore, we implemented a pipeline, which automatically creates a Boolean query for a given review topic by using a previously defined LLM, retrieves all documents for this query from the PubMed database and then evaluates the results. With this pipeline we first assess whether the results obtained using ChatGPT for query generation are reproducible and consistent. We then generalize our results by analyzing and evaluating open-source models and evaluating their efficacy in generating Boolean queries. Finally, we conduct a failure analysis to identify and discuss the limitations and shortcomings of using LLMs for Boolean query generation. This examination helps to understand the gaps and potential areas for improvement in the application of LLMs to information retrieval tasks. Our findings highlight the strengths, limitations, and potential of LLMs in the domain of information retrieval and literature review automation. Our code is available online."
Comparing Generative AI Literature Reviews Versus Human-Led Systematic Literature Reviews: A Case Study on Big Data Research,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10938577,"Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are transforming research methodologies, including Systematic Literature Reviews (SLRs). While traditional, human-led SLRs are labor-intensive, AI-driven approaches promise efficiency and scalability. However, the reliability and accuracy of AI-generated literature reviews remain uncertain. This study investigates the performance of GPT-4-powered Consensus in conducting an SLR on Big Data research, comparing its results with a manually conducted SLR. To evaluate Consensus, we analyzed its ability to detect relevant studies, extract key insights, and synthesize findings. Our human-led SLR identified 32 primary studies (PSs) and 207 related works, whereas Consensus detected 22 PSs, with 16 overlapping with the manual selection and 5 false positives. The AI-selected studies had an average citation count of 202 per study, significantly higher than the 64.4 citations per study in the manual SLR, indicating a possible bias toward highly cited papers. However, none of the 32 PSs selected manually were included in the AI-generated results, highlighting recall and selection accuracy limitations. Key findings reveal that Consensus accelerates literature retrieval but suffers from hallucinations, reference inaccuracies, and limited critical analysis. Specifically, it failed to capture nuanced research challenges and missed important application domains. Precision, recall, and F1 scores of the AI-selected studies were 76.2%, 38.1%, and 50.6%, respectively, demonstrating that while AI retrieves relevant papers with high precision, it lacks comprehensiveness. To mitigate these limitations, we propose a hybrid AI-human SLR framework, where AI enhances search efficiency while human reviewers ensure rigor and validity. While AI can support literature reviews, human oversight remains essential for ensuring accuracy and depth. Future research should assess AI-assisted SLRs across multiple disciplines to validate generalizability and explore domain-specific LLMs for improved performance."
Work-in-Progress: Course-based Undergraduate Research Experience (CURE) with Generative AI in a Computer Science Course,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893139,"This work-in-progress innovative practice paper describes a novel integration of Generative AI with Course-based Undergraduate Research Experiences (CUREs). CUREs integrate research activities into the curriculum, allowing all students in a course to participate in inquiry-based research projects. Generative Artificial Intelligence (AI) applications are advanced AI designed to generate human-like responses by processing natural language inputs. These applications leverage machine learning models to produce outputs that can assist users in a variety of tasks from writing to coding. The integration of Generative AI with CURE had been adopted in a text-based machine learning course during the Fall 2023 semester. A comparative analysis had been conducted on student survey responses from Fall 2022 and Fall 2023 to evaluate the effectiveness of Generative AI in a CURE integrated course. Descriptive statistics and statistical tests were conducted to assess differences in student perceptions between the two semesters. Although the differences were not statistically significant, the results indicate a promising trend towards improved student perceptions of both the overall course effectiveness and the benefits of Generative AI in enhancing various aspects of the research process, especially the literature review."
Effectiveness of Generative Artificial Intelligence for Scientific Content Analysis,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10313167,"Generative artificial intelligence (GenAI) in general, and large language models (LLMs) in particular, are highly fashionable. As they have the ability to generate coherent output based on prompts in natural language, they are promoted as tools to free knowledge workers from tedious tasks such as content writing, customer support and routine computer code generation. Unsurprisingly, their application is also attractive to professionals in the research domain, where mundane and laborious tasks, such as literature screening, are commonplace. We evaluate Vertex AI ‘text-bison’, a foundational LLM model, in a real-world academic scenario by replicating parts of a popular systematic review in the information management domain. By comparing the results of a zero-shot LLM-based approach with those of the original study, we gather evidence on the suitability of state-of-the-art general-purpose LLMs for the analysis of scientific content. We show that the LLM-based approach delivers good scientific content analysis performance for a general classification problem (ACC =0.9), acceptable performance for a domain-specific classification problem (ACC =0.8) and borderline performance for a text comprehension problem (ACC ≈0.69). We conclude that some content analysis tasks with moderate accuracy requirements may be supported by current LLMs. As the technology will evolve rapidly in the foreseeable future, studies on large corpora, where some inaccuracies are tolerable, or workflows that prepare large data sets for human processing, may increasingly benefit from the capabilities of GenAI."
Machine learning based system for the automation of systematic literature reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10385372,"The paper gives an overview of a machine learning-based system developed to support systematic literature reviews (SLR). The objective of the system is to provide scientists and anyone else who gives scientific advice supporting policy development with a tool for literature search and appraisal that reduces the human effort. The structure of the system is presented along with the description of the communication between modules and data storage methods. The Kafka technology is used for inter-module communication and the system consists of several independent modules which can be easily expanded with new modules without the need to introduce significant changes. We propose to semi–automate the SLR processes by applying an active learning approach which is based on machine learning classification models and on manual screening by experts of a subset of articles. Using classification algorithms requires a numerical representations of articles. This work investigates the utility of bag of concepts approach for text representations in order to create classification models used as components of automated systematic literature review systems. The presented study uses the bag of concepts approach in which a set of concepts identified by an annotator is extended by the concepts which lie, within a given distance, on paths from the originally identified concepts to the root of the ontology tree. Experiments are performed on datasets from systematic literature reviews in the medical domain. We summarize the performance of the proposed system by evaluating the WSS@95% metrics of active learning processes for several SLR case studies."
Zero-BertXGB: An Empirical Technique for Abstract Classification in Systematic Reviews,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10845770,"Abstract classification in systematic reviews (SRs) is a crucial step in evidence synthesis but is often time-consuming and labour-intensive. This study evaluates the effectiveness of various Machine Learning (ML) models and embedding techniques in automating this process. Five diverse datasets are utilized: Aceves-Martins (2021), comprising 1,258 excluded and 230 included abstracts on the utilization of animal models in depressive behaviour studies; Bannach-Brown (2016), with 896 excluded and 73 included abstracts focusing on the methodological rigour of environmental health systematic reviews; Meijboom (2021), containing 599 excluded and 32 included abstracts on the retransitioning of Etanercept in rheumatic disease patients; Menon (2022), with 896 excluded and 73 included abstracts on environmental health reviews; and a custom Clinical Review Paper Abstract (CRPA) dataset, featuring 500 excluded and 50 included abstracts. A significant research gap in abstract classification has been identified in previous literature, particularly in comparing Large Language Models (LLMs) with traditional ML and Natural Language Processing (NLP) techniques regarding scalability, adaptability, computational efficiency, and real-time application. Addressing this gap, this study employs GloVe for word embedding via matrix factorization, FastText for character n-gram representation, and Doc2Vec for capturing paragraph-level semantics. A novel Zero-BertXGB technique is introduced, integrating a transformer-based language model, zero-shot learning, and an ML classifier to enhance abstract screening and classification into “Include” or “Exclude” categories. This approach leverages contextual understanding and precision for efficient abstract processing. The Zero-BertXGB technique is compared against other prominent LLMs, including BERT, PaLM, LLaMA, GPT-3.5, and GPT-4, to validate its effectiveness. The Zero-BertXGB model achieved accuracy values of 99.3% for Aceves-Martins2021, 92.6% for Bannach-Brown2016, 85.7% for Meijboom2021, 94.1% for Menon2022, and 98.8% for CRPA. The findings indicate that the Zero-BertXGB model, alongside other LLMs, can deliver reliable results with minimal human intervention, enhancing abstract screening efficiency and potentially revolutionizing systematic review workflows."
Optimizing Article Screening and Information Extraction: A Hybrid Approach with GeminiAI and Vector Database,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10740208,"The exponential growth of scientific literature poses a significant challenge to researchers, resulting in redundancy in R&D due to inefficient review mechanisms. Manual literature reviews are time-consuming and resource-intensive, particularly when screening abstracts and titles, highlighting the need for innovative solutions to optimize the review process. This study introduces a three-step methodology using the GeminiAI model to streamline literature reviews: (1) Initial Screening, (2) Abstract Detail Extraction, and (3) Final Integration, with a Vector Database enabling efficient semantic searches in PDF files. In the first phase, GeminiAI achieved an accuracy of 88.66% in evaluating titles and abstracts based on specific inclusion and exclusion criteria, demonstrating its capability to filter relevant literature efficiently. The second phase enhanced this process by extracting key details, such as research-related modalities, thereby significantly reducing the pool of relevant papers. In the final step, the integration of the Vector Database with GeminiAI excelled, achieving an 80% similarity score in extracting detailed information, which greatly facilitated review writing and minimized manual effort. The user-friendly website designed for this purpose enables seamless paper uploads, with the Vector Database automatically extracting relevant details to streamline the workflow and accelerate innovation. This approach underscores the power of AI in optimizing literature reviews, reducing manual screening time and resources, and mitigating the risk of overlooking critical information. The entire system, including source code and supplementary materials, is available on our publicly accessible GitHub repository. https://github.com/mammona/ai-powered-litreview.git"
A Trend of AI Conference Convergence in Similarity: An Empirical Study Through Trans-Temporal Heterogeneous Graph,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049682,"Publishing the research works on academic publications is an important part of the scientific process. Since the development of computer science research is very fast, researchers tend to publish the research works in a fast way, such as conferences whose review processes are faster than the journals. In the past decades, one conference usually focuses on a specific research field and the topic or method overlap between conferences is low. We have noticed that, in recent years, some topics or methods which were once studied in a small number of specific research fields have become popular in many other fields. Naturally, we come up with two research questions: (1) Do the conferences indeed become similar? and (2) How do conferences become similar? In this paper, we first use a trans-temporal heterogeneous graph network to model academic conferences in recent 20 years. Due to the large number of conferences, we categorize these conferences into 6 research fields for brevity. Then, we first quantitatively and qualitatively assess “Do the research fields become similar?” and then focus on exploring “How do research fields become similar?”. From the result, we find the reason for the research fields in computer science become similar is that AI becomes pervasive and researchers tend to apply the machine learning methods to different application fields. Since the methods become universal between different research fields, researchers should pay more attention to advanced information in other fields to motivate more interdisciplinary works. To assist the researchers to explore related interdisciplinary advanced information, it is crucial to measure the cross-field impact of papers using the citation information and recommend the paper which has a high cross-field impact on the related researchers. As for the newly published papers which do not have any citations, we also propose a cross-field impact prediction model to recommend the cutting-edge research works to related researchers accurately. Experiments conducted on real-world datasets verify the effectiveness of the proposed method."
Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10707620,"Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and to support the selection of relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine with approximately 74% recall and 15% precision. The use of such algorithms with conservative thresholds to minimize the risk of missing papers can already significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE."
Towards the Use of Language Models in Scientific Paper Recommender Systems,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10578590,"Within the educational and research community, Research Paper Recommender Systems debuted in the late 1990s and today, they constitute a specific research area. In this work, it is explored how the use of neural networks together with the incorporation of Natural Language Processing techniques, such as word embeddings and language models, affect the recom-mendation process of scientific papers. Three Deep Learning-based recommenders are explored: a neural collaborative filtering recommender, a recommender that uses word embeddings, and a recommender that incorporates language models. In addition, the results obtained are evaluated on two different datasets to see the effect of each of them on the recommendation process. While the first dataset only includes papers that have interested the user, the second one also includes papers that have not interested the user. The collaborative Deep Learning-based recommender constitutes the baseline against which to compare the rest of the developed recommenders. To evaluate the recommenders, each model is used to recommend 10 research papers for each user. The recommendations are evaluated and considered appropriate if they are related to the research field the user is interested in. The results confirm that the use of NLP techniques improves the performance of pure collaborative recommenders."